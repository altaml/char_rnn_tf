<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <page>
    <title>Bongard problem</title>
    <ns>0</ns>
    <id>1191936</id>
    <revision>
      <id>787686288</id>
      <parentid>754787067</parentid>
      <timestamp>2017-06-26T22:17:55Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4457">[[File:Bongard_problem_convex_polygons.svg|thumb|300px|An example Bongard problem, the common factor of the left set being convex shapes (the right set are instead all concave).]]
A '''Bongard problem''' is a kind of puzzle invented by the Russian [[computer scientist]] [[Mikhail Moiseevich Bongard]] (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his 1967 book on [[pattern recognition]]. Bongard, in the introduction of the book (which deals with a number of topics including [[perceptron]]s) credits the ideas in it to a group including [[M. N. Vaintsvaig]], [[V. V. Maksimov]], and [[M. S. Smirnov]].

==Overview==
The idea of a Bongard problem is to present two sets of relatively simple diagrams, say ''A'' and ''B''. All the diagrams from set ''A'' have a common factor or attribute, which is lacking in all the diagrams of set ''B''. The problem is to find, or to formulate, convincingly, the common factor. The problems were popularised by their occurrence in the 1979 book ''[[Gödel, Escher, Bach]]'' by [[Douglas Hofstadter]], himself a composer of Bongard problems. Bongard problems are also at the heart of the game [[Zendo (game)|Zendo]].

Many computational architectures have been devised to solve Bongard problems, the most extensive of which being [[Phaeaco]], by [[Harry Foundalis]],&lt;ref&gt;{{cite web|url=http://www.foundalis.com/res/diss_research.html|accessdate=24 October 2013|author=Harry Foundalis|title=The Bongard Problems}}&lt;/ref&gt; who left the field in 2008 due to ethical concerns regarding machines that can pass as human.&lt;ref&gt;{{cite web|url=http://www.foundalis.com/soc/why_no_more_Bongard.html|accessdate=24 October 2013|title=Why I stopped working on the Bongard Problems|author=Harry Foundalis}}&lt;/ref&gt;

==Scientific works on Bongard problems==

* Bongard, M. M. (1970). Pattern Recognition. Rochelle Park, N.J.: Hayden Book Co., Spartan Books.  (Original publication: Проблема Узнавания, Nauka Press, Moscow, 1967)
* Maksimov, V. V. (1975). Система, обучающаяся классификации геометрических изображений (A system capable of learning to classify geometric images; as translated from the Russian by Marina Eskina), in Моделирование Обучения и Поведения (Modeling of Learning and Behavior, in Russian), M.S. Smirnov, V.V. Maksimov (eds.), Nauka, Moskva.
* Hofstadter, D. R. (1979). Gödel, Escher, Bach: an Eternal Golden Braid. New York: Basic Books.
* Montalvo, F. S. (1985). Diagram Understanding: the Intersection of Computer Vision and Graphics. M.I.T. Artificial Intelligence Laboratory, A. I. Memo 873, November 1985.
* Saito, K., and Nakano, R. (1993) A Concept Learning Algorithm with Adaptive Search. Proceedings of Machine Intelligence 14 Workshop. Oxford University Press. See pp.&amp;nbsp;347–363.
* Hofstadter, D. R. and the Fluid Analogies Research Group (1995). [[Fluid Concepts and Creative Analogies|Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought]]. New York: Basic Books.
* Hofstadter, D. R. (1995). On Seeing A’s and Seeing As. Stanford Humanities Review 4/2 pp.&amp;nbsp;109–121.
* Hofstadter, D. R. (1997). Le Ton beau de Marot. New York: Basic Books.
* Linhares, A. (2000). [http://app.ebape.fgv.br/comum/arq/Linhares2.pdf A glimpse at the metaphysics of Bongard problems]. [[Artificial Intelligence (journal)|Artificial Intelligence]], Volume 121, Issue 1-2, pp.&amp;nbsp;251–270.
* Foundalis, H. (2006). Phaeaco: A Cognitive Architecture Inspired by Bongard’s Problems. Doctoral dissertation, Indiana University, Center for Research on Concepts and Cognition (CRCC), Bloomington, Indiana.
* Anastasiade, J., and Szalwinski, C. (2010). Building Computer-Based Tutors to Help Learners Solve Ill-Structured Problems. In [http://aace.org/conf/edmedia Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications 2010]. Toronto, Ontario, Canada: Association for the Advancement of Computing in Education. pp.&amp;nbsp;3726–3732.

==References==
&lt;references/&gt;
==External links==
{{Wikibooks|Puzzles|Bongard problems}}
*[http://www.foundalis.com/res/bps/bpidx.htm Index of Bongard problems]





[[Category:Computer-related introductions in 1967]]</text>
      <sha1>ormvysx9bwfhhch3z3kliwgg4i8a2mt</sha1>
    </revision>
  </page>
  <page>
    <title>Generative model</title>
    <ns>0</ns>
    <id>1222578</id>
    <revision>
      <id>808779702</id>
      <parentid>808779664</parentid>
      <timestamp>2017-11-05T03:13:01Z</timestamp>
      <contributor>
        <username>Nbro</username>
        <id>23779285</id>
      </contributor>
      <comment>/* Generative models */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6205">{{mi|
{{context|date=February 2012}}
{{no footnotes|date=August 2017}}
{{refimprove|date=August 2017}}
}}
In [[probability]] and [[statistics]], a '''generative model''' is a model for generating all values for a phenomenon, both those that can be observed in the world and &quot;target&quot; variables that can only be computed from those observed. By contrast, [[Discriminative model|discriminative models]] provide a model only for the target variable(s), generating them by analyzing the observed variables. In simple terms, discriminative models infer outputs based on inputs, while generative models generate both inputs and outputs, typically given some [[Latent variable|hidden parameters]].

Generative models are used in [[machine learning]] for either modeling data directly (i.e., modeling observations drawn from a [[probability density function]]), or as an intermediate step to forming a [[Conditional probability|conditional probability density function]]. Generative models are typically probabilistic, specifying a [[Joint distribution|joint probability distribution]] over observation and target (label) values.  A conditional distribution can be formed from a generative model through [[Bayes' rule]].

[[Claude Shannon|Shannon]] (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with &quot;representing and speedily is an good&quot;; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.

Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. They don't necessarily perform better than generative models at [[Classification (machine learning)|classification]] and [[Regression analysis|regression]] tasks. The two classes are seen as complementary or as different views of the same procedure.&lt;ref&gt;{{citation|editor-first=J. M. |editor-last=Bernardo|title=Bayesian statistics 8: proceedings of the eighth Valencia International Meeting, June 2-6, 2006|url={{google books |plainurl=y |id=Vh7vAAAAMAAJ|page=3}}|date=24 September 2007|publisher=Oxford University Press|isbn=978-0-19-921465-5|first=C. M. |last=Bishop |first2=J. |last2=Lasserre |contribution=Generative or Discriminative? getting the best of both worlds |pp=3–23}}&lt;/ref&gt;

== Types ==

=== Generative models ===

Types of generative models are:

* [[Gaussian mixture model]] (and other types of [[mixture model]])
* [[Hidden Markov model]]
* [[Stochastic context-free grammar|Probabilistic context-free grammar]]
* [[Naive Bayes]]
* [[Averaged one-dependence estimators]]
* [[Latent Dirichlet allocation]]
* [[Restricted Boltzmann machine]]
* [[Generative adversarial networks]]

If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to [[maximum likelihood estimation|maximize the data likelihood]] is a common method. However, since most statistical models are only approximations to the ''true'' distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a [[discriminative model]] (see above), although application-specific details will ultimately dictate which approach is most suitable in any particular case.

=== Discriminative models ===

* [[Logistic regression]],
* [[Support Vector Machines]],
* [[Maximum-entropy Markov model|Maximum Entropy Markov Model]],
* [[Conditional random field|Conditional Random Fields]],
* [[Neural network|Neural Networks]]

== Machine learning ==

A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal?
A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal.

Suppose the input data is &lt;math&gt;x \in \{0, 1\}&lt;/math&gt; and the set of labels for &lt;math&gt;x&lt;/math&gt; is &lt;math&gt;y \in \{0, 1\}&lt;/math&gt;.
A generative model learns the [[joint probability distribution]] &lt;math&gt;p(x,y)&lt;/math&gt; while a discriminative model learns the [[conditional probability distribution]] &lt;math&gt;p(y|x)&lt;/math&gt; “probability of y given x”.

Let's try to understand this with an example. Consider the following 4 data points:
&lt;math&gt;(x,y) = \{(0,0), (0,0), (1,0), (1,1)\}&lt;/math&gt;

For above data, &lt;math&gt;p(x,y)&lt;/math&gt; will be following:
{| class=&quot;wikitable&quot;
|-
!  !! &lt;math&gt;y=0
&lt;/math&gt;!! &lt;math&gt;y=1
&lt;/math&gt;
|-
| &lt;math&gt;x=0
&lt;/math&gt; || &lt;math&gt;1/2
&lt;/math&gt; ||&lt;math&gt;0
&lt;/math&gt;
|-
| &lt;math&gt;x=1
&lt;/math&gt; || &lt;math&gt;1/4
&lt;/math&gt; || &lt;math&gt;1/4
&lt;/math&gt;
|}

while &lt;math&gt;p(y|x)&lt;/math&gt; will be following:
{| class=&quot;wikitable&quot;
|-
!  !! &lt;math&gt;y=0
&lt;/math&gt; !! &lt;math&gt;y=1
&lt;/math&gt;
|-
| &lt;math&gt;x=0
&lt;/math&gt;
| &lt;math&gt;1

&lt;/math&gt; || &lt;math&gt;0
&lt;/math&gt;
|-
| &lt;math&gt;x=1
&lt;/math&gt; || &lt;math&gt;1/2
&lt;/math&gt; || &lt;math&gt;1/2
&lt;/math&gt;
|}

So, discriminative algorithms try to learn &lt;math&gt;p(y|x)&lt;/math&gt; directly from the data and then try to classify data. On the other hand, generative algorithms try to learn &lt;math&gt;p(x,y)&lt;/math&gt; which can be transformed into &lt;math&gt;p(y|x)&lt;/math&gt; later to classify the data. One of the advantages of generative algorithms is that you can use &lt;math&gt;p(x,y)&lt;/math&gt; to generate new data similar to existing data. On the other hand, discriminative algorithms generally give better performance in classification tasks.

== See also ==
{{Portal|Statistics}}
* [[Discriminative model]]
* [[Graphical model]]

==References==
{{reflist}}

==Sources==
* Shannon, C.E. (1948) &quot;[https://www.tnt.uni-hannover.de/edu/vorlesungen/InfoTheor/download/shannon1948.pdf A Mathematical Theory of Communication]&quot;, ''[[Bell System Technical Journal]]'', vol. 27, pp.&amp;nbsp;379–423, 623–656, July, October, 1948

{{Statistics|state=expanded}}



</text>
      <sha1>9goj6eppnshu6rvv7y2mqkbbd40kkx7</sha1>
    </revision>
  </page>
  <page>
    <title>Inductive bias</title>
    <ns>0</ns>
    <id>173926</id>
    <revision>
      <id>799307847</id>
      <parentid>743679085</parentid>
      <timestamp>2017-09-06T22:31:51Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>fixing incorrectly-formatted templates</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4752">The '''inductive bias''' (also known as '''learning bias''') of a [[learning algorithm]] is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.&lt;ref name=Mitchell1980&gt;
{{Citation
 | last = Mitchell
 | first = T. M.
 | title = The need for biases in learning generalizations
 | place = New Brunswick, New Jersey, USA
 | publisher = Rutgers University
 | series = CBM-TR 5-110
 | year = 1980
 | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.5466
 }}
&lt;/ref&gt;

In [[machine learning]], one aims to construct algorithms that are able to ''learn'' to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training.  Without any additional assumptions, this problem cannot be solved exactly since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase ''inductive bias''.&lt;ref name=Mitchell1980 /&gt;&lt;ref name=DesJardinsandGordon1995&gt;
{{Citation
 | last = DesJardins
 | first = M.
 | last2 = Gordon
 | first2 = D. F.
 | author2-link =
 | title = Evaluation and selection of biases in machine learning
 | series = Machine Learning Journal
 | volume = 5:1--17
 | year = 1995
 | url = http://citeseer.ist.psu.edu/article/desjardins95evaluation.html
}}
&lt;/ref&gt;

A classical example of an inductive bias is [[Occam's razor]], assuming that the simplest consistent hypothesis about the target function is actually the best. Here ''consistent'' means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm.

Approaches to a more formal definition of inductive bias are based on  [[mathematical logic]]. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. Unfortunately, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of [[neural networks]]), or not at all.

==Types==

The following is a list of common inductive biases in machine learning algorithms.

* '''Maximum [[conditional independence]]''': if the hypothesis can be cast in a [[Bayesian inference|Bayesian]] framework, try to maximize conditional independence. This is the bias used in the [[Naive Bayes classifier]].
* '''Minimum [[Cross-validation (statistics)|cross-validation]] error''': when trying to choose among hypotheses, select the hypothesis with the lowest cross-validation error. Although cross-validation may seem to be free of bias, the [[No free lunch in search and optimization|&quot;no free lunch&quot;]] theorems show that cross-validation must be biased.
* '''Maximum margin''': when drawing a boundary between two classes, attempt to maximize the width of the boundary. This is the bias used in [[support vector machines]]. The assumption is that distinct classes tend to be separated by wide boundaries.
* '''[[Minimum description length]]''': when forming a hypothesis, attempt to minimize the length of the description of the hypothesis. The assumption is that simpler hypotheses are more likely to be true. See [[Occam's razor]].
* '''Minimum features''': unless there is good evidence that a [[feature space|feature]] is useful, it should be deleted. This is the assumption behind [[feature selection]] algorithms.
* '''Nearest neighbors''': assume that most of the cases in a small neighborhood in [[feature space]] belong to the same class. Given a case for which the class is unknown, guess that it belongs to the same class as the majority in its immediate neighborhood. This is the bias used in the [[k-nearest neighbors algorithm]]. The assumption is that cases that are near each other tend to belong to the same class.

==Shift of bias==

Although most learning algorithms have a static bias, some algorithms are designed to shift their bias as they acquire more data.&lt;ref name=Utgoff1984&gt;
{{Citation
 | last = Utgoff
 | first = P. E.
 | title = Shift of bias for inductive concept learning
 | place = New Brunswick, New Jersey, USA
 | publisher = Doctoral dissertation, Department of Computer Science, Rutgers University
 | year = 1984
}}
&lt;/ref&gt; This does not avoid bias, since the bias shifting process itself must have a bias.

==See also==
{{portal|Software}}
* [[Bias]]
* [[Cognitive bias]]
* [[No free lunch in search and optimization]]

==References==
{{reflist}}

{{Biases}}


</text>
      <sha1>16bitfuk0ymuq6nq23qakew8omi6k6d</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Bayesian networks</title>
    <ns>14</ns>
    <id>1718975</id>
    <revision>
      <id>547373464</id>
      <parentid>500322264</parentid>
      <timestamp>2013-03-28T02:46:26Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8293927]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="122">{{Cat main|Bayesian network}}


</text>
      <sha1>k041puhavh5wkhm73874qyh6f0t7qbl</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Classification algorithms</title>
    <ns>14</ns>
    <id>1991254</id>
    <revision>
      <id>753358258</id>
      <parentid>717613671</parentid>
      <timestamp>2016-12-06T18:28:57Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed ; added  using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="320">{{Commons category|Classification algorithms}}
This category is about statistical classification algorithms. For more information, see '''[[Statistical classification]]'''.


[[Category:Statistical classification|Algorithms]]

</text>
      <sha1>0kuyvyx6xiuhstiy0pnqx5o7tcvy036</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Evolutionary algorithms</title>
    <ns>14</ns>
    <id>754055</id>
    <revision>
      <id>753173674</id>
      <parentid>753173185</parentid>
      <timestamp>2016-12-05T16:51:19Z</timestamp>
      <contributor>
        <username>Alejandrocaro35</username>
        <id>5658207</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="678">An '''evolutionary algorithm''' ('''EA''') is a [[heuristic]] [[Program optimization|optimization]] [[algorithm]] using techniques inspired by mechanisms from [[Evolution|organic evolution]] such as [[mutation]], [[genetic recombination|recombination]], and [[natural selection]] to find an optimal configuration for a specific system within specific constraints.

{{Cat main|Evolutionary algorithm}}
{{Commons cat|Evolutionary algorithms}}







</text>
      <sha1>9kmxjhoxnea5shavb406zd1101lvh18</sha1>
    </revision>
  </page>
  <page>
    <title>Semi-supervised learning</title>
    <ns>0</ns>
    <id>2829632</id>
    <revision>
      <id>811932250</id>
      <parentid>811431152</parentid>
      <timestamp>2017-11-24T21:42:15Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>[[User:JCW-CleanerBot#Logic|task]], replaced: journal=Proc Natl Acad Sci U S A. → journal=Proc Natl Acad Sci U S A using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19782">{{Machine learning bar}}
[[File:Example of unlabeled data in semisupervised learning.png|thumb|194px|An example of the influence of unlabeled data in semi-supervised learning.  The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example.  The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles).  This could be viewed as performing [[Cluster analysis|clustering]] and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.]]

'''Semi-supervised learning''' is a class of [[supervised learning]] tasks and techniques that also make use of unlabeled [[data]] for training – typically a small amount of [[labeled data]] with a large amount of unlabeled data.  Semi-supervised learning falls between [[unsupervised learning]] (without any labeled training data) and [[supervised learning]] (with completely labeled training data).  Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value.  Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.

As in the supervised learning framework, we are given a set of &lt;math&gt;l&lt;/math&gt; [[Independent identically distributed|independently identically distributed]] examples &lt;math&gt;x_1,\dots,x_l \in X&lt;/math&gt; with corresponding labels &lt;math&gt;y_1,\dots,y_l \in Y&lt;/math&gt;.  Additionally, we are given &lt;math&gt;u&lt;/math&gt; unlabeled examples &lt;math&gt;x_{l+1},\dots,x_{l+u} \in X&lt;/math&gt;.  Semi-supervised learning attempts to make use of this combined information to surpass the [[Statistical classification|classification]] performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.

Semi-supervised learning may refer to either [[Transduction (machine learning)|transductive learning]] or [[Inductive reasoning|inductive learning]].  The goal of transductive learning is to infer the correct labels for the given unlabeled data &lt;math&gt;x_{l+1},\dots,x_{l+u}&lt;/math&gt; only.  The goal of inductive learning is to infer the correct mapping from &lt;math&gt;X&lt;/math&gt; to &lt;math&gt;Y&lt;/math&gt;.

Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class.  The teacher also provides a set of unsolved problems.  In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular.  In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam.

It is unnecessary (and, according to [[Vapnik's principle]], imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.

==Assumptions used==

In order to make any use of unlabeled data, we must assume some structure to the underlying distribution of data.  Semi-supervised learning algorithms make use of at least one of the following assumptions.
&lt;ref name=&quot;Chapelle&quot;&gt;{{Cite book  | last1 = Chapelle | first1 = Olivier | last2 = Schölkopf | first2 = Bernhard | last3 = Zien | first3 = Alexander | title = Semi-supervised learning | year = 2006 | publisher = MIT Press | location = Cambridge, Mass. | isbn = 978-0-262-03358-9 | pages =  }}&lt;/ref&gt;

===Continuity assumption===

''Points which are close to each other are more likely to share a label.'' This is also generally assumed in supervised learning and yields a preference for geometrically simple [[decision boundary|decision boundaries]].  In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so that there are fewer points close to each other but in different classes.

===Cluster assumption===

''The data tend to form discrete clusters, and points in the same cluster are more likely to share a label'' (although data sharing a label may be spread across multiple clusters).  This is a special case of the smoothness assumption and gives rise to [[feature learning]] with clustering algorithms.

===Manifold assumption===
''The data lie approximately on a [[manifold]] of much lower dimension than the input space.''  In this case we can attempt to learn the manifold using both the labeled and unlabeled data to avoid the [[curse of dimensionality]].  Then learning can proceed using distances and densities defined on the manifold.

The manifold assumption is practical when high-dimensional data are being generated by some process that may be hard to model directly, but which only has a few degrees of freedom.  For instance, human voice is controlled by a few vocal folds,&lt;ref name = &quot;StevensKN&quot;&gt;Stevens, K.N.(2000), Acoustic Phonetics, MIT Press, {{ISBN|0-262-69250-3}}, 978-0-262-69250-2&lt;/ref&gt; and images of various facial expressions are controlled by a few muscles.  We would like in these cases to use distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images respectively.

==History==

The heuristic approach of ''self-training'' (also known as ''self-learning'' or ''self-labeling'') is historically the oldest approach to semi-supervised learning,&lt;ref name=&quot;Chapelle&quot;/&gt; with examples of applications starting in the 1960s (see for instance Scudder (1965)&lt;ref&gt;Scudder, H.J. Probability of Error of Some Adaptive Pattern-Recognition Machines. IEEE Transaction on Information Theory, 11:363–371 (1965). Cited in Chapelle et al. 2006, page 3.&lt;/ref&gt;).

The transductive learning framework was formally introduced by [[Vladimir Vapnik]] in the 1970s.&lt;ref&gt;Vapnik, V. and Chervonenkis, A. Theory of Pattern Recognition [in Russian].  Nauka, Moscow (1974). Cited in Chapelle et al. 2006, page 3.&lt;/ref&gt; Interest in inductive learning using generative models also began in the 1970s.  A [[Probably approximately correct learning|''probably approximately correct'' learning]] bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.&lt;ref name = &quot;Ratsaby&quot;&gt;Ratsaby, J. and Venkatesh, S. Learning from a mixture of labeled and unlabeled examples with parametric side information. In ''Proceedings of the Eighth Annual Conference on Computational Learning Theory'', pages 412-417 (1995). Cited in Chapelle et al. 2006, page 4.&lt;/ref&gt;

Semi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images.  For a review of recent work see a survey article by Zhu (2008).&lt;ref name=&quot;survey&quot;&gt;Zhu, Xiaojin. [http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf   Semi-supervised learning literature survey]. Computer Sciences, University of Wisconsin-Madison (2008).&lt;/ref&gt;

==Methods==

===Generative models===

Generative approaches to statistical learning first seek to estimate &lt;math&gt;p(x|y)&lt;/math&gt; {{Disputed|date=November 2017}} , the distribution of data points belonging to each class.  The probability &lt;math&gt;p(y|x)&lt;/math&gt; that a given point &lt;math&gt;x&lt;/math&gt; has label &lt;math&gt;y&lt;/math&gt; is then proportional to &lt;math&gt;p(x|y)p(y)&lt;/math&gt; by [[Bayes' theorem|Bayes' rule]].  Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about &lt;math&gt;p(x)&lt;/math&gt;) or as an extension of unsupervised learning (clustering plus some labels).

Generative models assume that the distributions take some particular form &lt;math&gt;p(x|y,\theta)&lt;/math&gt; parameterized by the vector &lt;math&gt;\theta&lt;/math&gt;.  If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.
&lt;ref&gt;Cozman, F. and Cohen, I. Risks of semi-supervised learning: how unlabeled data can degrade performance of generative classifiers.  In: Chapelle et al. (2006).&lt;/ref&gt;
However, if the assumptions are correct, then the unlabeled data necessarily improves performance.&lt;ref name = &quot;Ratsaby&quot;/&gt;

The unlabeled data are distributed according to a mixture of individual-class distributions.  In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions.  Gaussian mixture distributions are identifiable and commonly used for generative models.

The parameterized [[joint distribution]] can be written as &lt;math&gt;p(x,y|\theta)=p(y|\theta)p(x|y,\theta)&lt;/math&gt; by using the [[Chain rule (probability)|Chain rule]].  Each parameter vector &lt;math&gt;\theta&lt;/math&gt; is associated with a decision function &lt;math&gt;f_\theta(x) = \underset{y}{\operatorname{argmax}}\ p(y|x,\theta)&lt;/math&gt;.
The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by &lt;math&gt;\lambda&lt;/math&gt;:

:&lt;math&gt;\underset{\Theta}{\operatorname{argmax}}\left( \log p(\{x_i,y_i\}_{i=1}^l | \theta) + \lambda \log p(\{x_i\}_{i=l+1}^{l+u}|\theta)\right) &lt;/math&gt;
&lt;ref name=&quot;SSL_EoML&quot;&gt;Zhu, Xiaojin. [http://pages.cs.wisc.edu/~jerryzhu/pub/SSL_EoML.pdf   Semi-Supervised Learning] University of Wisconsin-Madison.&lt;/ref&gt;

===Low-density separation===

Another major class of methods attempts to place boundaries in regions where there are few data points (labeled or unlabeled).  One of the most commonly used algorithms is the [[Support vector machine#Transductive support vector machines|transductive support vector machine]], or TSVM (which, despite its name, may be used for inductive learning as well).  Whereas [[support vector machines]] for supervised learning seek a decision boundary with maximal [[Margin (machine learning)|margin]] over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data.  In addition to the standard [[hinge loss]] &lt;math&gt;(1-yf(x))_+&lt;/math&gt; for labeled data, a loss function &lt;math&gt;(1-|f(x)|)_+&lt;/math&gt; is introduced over the unlabeled data by letting &lt;math&gt;y=\operatorname{sign}{f(x)}&lt;/math&gt;.  TSVM then selects &lt;math&gt;f^*(x) = h^*(x) + b&lt;/math&gt; from a [[reproducing kernel Hilbert space]] &lt;math&gt;\mathcal{H}&lt;/math&gt; by minimizing the [[Regularization (mathematics)|regularized]] [[Empirical risk minimization|empirical risk]]:

:&lt;math&gt;f^* = \underset{f}{\operatorname{argmin}}\left(
\displaystyle \sum_{i=1}^l(1-y_if(x_i))_+ + \lambda_1 \|h\|_\mathcal{H}^2 + \lambda_2 \sum_{i=l+1}^{l+u} (1-|f(x_i)|)_+
\right) &lt;/math&gt;

An exact solution is intractable due to the non-[[convex function|convex]] term &lt;math&gt;(1-|f(x)|)_+&lt;/math&gt;, so research has focused on finding useful approximations.&lt;ref name=&quot;SSL_EoML&quot;/&gt;

Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).

===Graph-based methods===

Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example.  The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its &lt;math&gt;k&lt;/math&gt; nearest neighbors or to examples within some distance &lt;math&gt;\epsilon&lt;/math&gt;.  The weight &lt;math&gt;W_{ij}&lt;/math&gt; of an edge between &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;x_j&lt;/math&gt; is then set to &lt;math&gt;e^{\frac{-\|x_i-x_j\|^2}{\epsilon}}&lt;/math&gt;.

Within the framework of [[manifold regularization]],
&lt;ref&gt;{{cite journal|author1=M. Belkin |author2=P. Niyogi |title=Semi-supervised Learning on Riemannian Manifolds|journal=Machine Learning|volume=56|issue=Special Issue on Clustering|pages=209–239|year=2004|url=http://booksc.org/dl/11288633/421f61|doi=10.1023/b:mach.0000033120.25363.1e}}&lt;/ref&gt;
&lt;ref&gt;M. Belkin, P. Niyogi, V. Sindhwani. On Manifold Regularization. AISTATS 2005.&lt;/ref&gt;
the graph serves as a proxy for the manifold.  A term is added to the standard [[Tikhonov regularization]] problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space.  The minimization problem becomes

:&lt;math&gt;\underset{f\in\mathcal{H}}{\operatorname{argmin}}\left(
\frac{1}{l}\displaystyle\sum_{i=1}^l V(f(x_i),y_i) +
\lambda_A \|f\|^2_\mathcal{H} +
\lambda_I \int_\mathcal{M}\|\nabla_\mathcal{M} f(x)\|^2dp(x)
\right) &lt;/math&gt; &lt;ref name=&quot;SSL_EoML&quot;/&gt;

where &lt;math&gt;\mathcal{H}&lt;/math&gt; is a reproducing kernel Hilbert space and &lt;math&gt;\mathcal{M}&lt;/math&gt; is the manifold on which the data lie.  The regularization parameters &lt;math&gt;\lambda_A&lt;/math&gt; and &lt;math&gt;\lambda_I&lt;/math&gt; control smoothness in the ambient and intrinsic spaces respectively.  The graph is used to approximate the intrinsic regularization term.  Defining the [[Laplacian matrix|graph Laplacian]] &lt;math&gt;L = D - W&lt;/math&gt; where &lt;math&gt;D_{ii} = \sum_{j=1}^{l+u} W_{ij}&lt;/math&gt; and &lt;math&gt;\mathbf{f}&lt;/math&gt; the vector &lt;math&gt;[f(x_1)\dots f(x_{l+u})]&lt;/math&gt;, we have

:&lt;math&gt;\mathbf{f}^T L \mathbf{f} = \displaystyle\sum_{i,j=1}^{l+u}W_{ij}(f_i-f_j)^2 \approx \int_\mathcal{M}\|\nabla_\mathcal{M} f(x)\|^2dp(x)&lt;/math&gt;.

The Laplacian can also be used to extend the supervised learning algorithms： regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.

===Heuristic approaches===

Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework.  For instance, the labeled and unlabeled examples &lt;math&gt;x_1,\dots,x_{l+u}&lt;/math&gt; may inform a choice of representation, [[distance metric]], or [[Kernel(statistics)#In non-parametric statistics|kernel]] for the data in an unsupervised first step.  Then supervised learning proceeds from only the labeled examples.

''Self-training'' is a wrapper method for semi-supervised learning.&lt;ref&gt;{{Cite journal|title = Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study|url = https://link.springer.com/article/10.1007/s10115-013-0706-y|journal = Knowledge and Information Systems|date = 2013-11-26|issn = 0219-1377|pages = 245–284|volume = 42|issue = 2|doi = 10.1007/s10115-013-0706-y|language = en|first = Isaac|last = Triguero|first2 = Salvador|last2 = García|first3 = Francisco|last3 = Herrera}}&lt;/ref&gt;  First a supervised learning algorithm is trained based on the labeled data only.  This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm.  Generally only the labels the classifier is most confident of are added at each step.&lt;ref&gt;{{Cite journal|title = Self-Trained LMT for Semisupervised Learning|url = https://dx.doi.org/10.1155/2016/3057481|journal = Computational Intelligence and Neuroscience|date = 2015-12-29|pages = 1–13|volume = 2016|doi = 10.1155/2016/3057481|language = en|first = Nikos|last = Fazakis|first2 = Stamatis|last2 = Karlos|first3 = Sotiris|last3 = Kotsiantis|first4 = Kyriakos|last4 = Sgarbas}}&lt;/ref&gt;

[[Co-training]] is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.&lt;ref&gt;{{Cite book|title = Analysis of Co-training Algorithm with Very Small Training Sets|url = https://link.springer.com/chapter/10.1007/978-3-642-34166-3_79|publisher = Springer Berlin Heidelberg|date = 2012-11-07|isbn = 9783642341656|pages = 719–726|series = Lecture Notes in Computer Science|language = en|first = Luca|last = Didaci|first2 = Giorgio|last2 = Fumera|first3 = Fabio|last3 = Roli|editor-first = Georgy|editor-last = Gimel’farb|editor-first2 = Edwin|editor-last2 = Hancock|editor-first3 = Atsushi|editor-last3 = Imiya|editor-first4 = Arjan|editor-last4 = Kuijper|editor-first5 = Mineichi|editor-last5 = Kudo|editor-first6 = Shinichiro|editor-last6 = Omachi|editor-first7 = Terry|editor-last7 = Windeatt|editor-first8 = Keiji|editor-last8 = Yamada}}&lt;/ref&gt;

==In human cognition==

Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data (for a summary see &lt;ref name=&quot;ZhuGoldberg&quot;&gt;
{{Cite book  | last1 = Zhu | first1 = Xiaojin | last2 = Goldberg | first2 = Andrew B. | title = Introduction to semi-supervised learning. | year = 2009 | publisher = Morgan &amp; Claypool | isbn = 9781598295481 | pages =  }}&lt;/ref&gt;).  More natural learning problems may also be viewed as instances of semi-supervised learning.  Much of human [[concept learning]] involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).

Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.&lt;ref&gt;{{cite journal |author1=Younger B. A. |author2=Fearing D. D. | year = 1999 | title = Parsing Items into Separate Categories: Developmental Change in Infant Categorization | url = | journal = Child Development | volume = 70 | issue = | pages = 291–303 | doi=10.1111/1467-8624.00022}}&lt;/ref&gt;  More recent work has shown that infants and children take into account not only the unlabeled examples available, but the [[sampling (statistics)|sampling]] process from which labeled examples arise.&lt;ref&gt;{{cite journal|author1=Xu, F.  |author2=Tenenbaum, J. B. |lastauthoramp=yes |year=2007|title=Sensitivity to sampling in Bayesian word learning. Developmental Science|volume=10|pages=288–297|doi=10.1111/j.1467-7687.2007.00590.x|journal=Developmental Science}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author=Gweon, H., Tenenbaum J.B., and Schulz L.E|year=2010|title=Infants consider both the sample and the sampling process in inductive generalization|journal=Proc Natl Acad Sci U S A|volume=107|issue=20|pages=9066–71|url=http://www.pnas.org/content/107/20/9066|doi=10.1073/pnas.1003095107}}&lt;/ref&gt;

==See also==
* [[PU learning]]

==References==
{{Reflist}}

==External links==
* [http://manifold.cs.uchicago.edu/manifold_regularization/software.html] A freely available [[MATLAB]] implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.
* [http://sci2s.ugr.es/keel/algorithms.php#sub10] KEEL module for semi-supervised learning.
* [http://pages.cs.wisc.edu/~jerryzhu/ssl/software.html] Semi-Supervised Learning Software
* [http://scikit-learn.org/stable/modules/label_propagation.html] Semi-Supervised algorithms in scikit-learn  .

</text>
      <sha1>afcnybxawcg05wts3a6n0z4hyzgm03f</sha1>
    </revision>
  </page>
  <page>
    <title>Learning automata</title>
    <ns>0</ns>
    <id>3274742</id>
    <revision>
      <id>816111910</id>
      <parentid>782812363</parentid>
      <timestamp>2017-12-19T10:26:18Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.6.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5819">{{about|automata that learn|computational approaches to learn regular automata|Induction of regular languages}}
Learning automata is one type of Machine Learning algorithm studied since 1970s. Compared to other learning scheme, a branch of the theory of [[adaptive control]] is devoted to '''learning automata''' surveyed by Narendra and Thathachar (1974) which were originally described explicitly as [[finite state automata]]. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and Markov Decision Process (MDP) is used.

== History ==
Research in learning automata can be traced back to the work of Tsetlin in the early 1960s in the Soviet Union. Together with some colleagues, he published a collection of papers on how to use matrices to describe automata functions. Additionally, Tsetlin worked on ''reasonable'' and ''collective automata behaviour'', and on ''automata games''. Learning automata were also investigated by researches in the United States in the 1960s. However, the term ''learning automaton'' was not used until Narendra and Thathachar introduced it in a survey paper in 1974.

== Definition ==
A learning automaton is an adaptive decision-making unit situated in a random environment that learns the optimal action through repeated interactions with its environment. The actions are chosen according to a specific probability distribution which is updated based on the environment response the automaton obtains by performing a particular action.

With respect to the field of [[reinforcement learning]], learning automata are characterized as [[Markov decision process#Policy iteration|policy iterators]]. In contrast to other reinforcement learners, policy iterators directly manipulate the policy π. Another example for policy iterators are [[evolutionary algorithm]]s.

Formally, Narendra and Thathachar define a '''stochastic automaton''' to consist of:
* a set ''x'' of possible inputs,
* a set Φ = { Φ&lt;sub&gt;1&lt;/sub&gt;, ..., Φ&lt;sub&gt;''s''&lt;/sub&gt; } of possible internal states,
* a set α = { α&lt;sub&gt;1&lt;/sub&gt;, ..., α&lt;sub&gt;''r''&lt;/sub&gt; } of possible outputs, or actions, with ''r''≤''s'',
* an initial state probability vector ''p(0)'' = ≪ ''p''&lt;sub&gt;1&lt;/sub&gt;(0), ..., ''p''&lt;sub&gt;''s''&lt;/sub&gt;(0) ≫,
* a [[computable function]] ''A'' which after each time step ''t'' generates ''p''(''t''+1) from ''p''(''t''), the current input, and the current state, and
* a function ''G'': Φ → α which generates the output at each time step.
In their paper, they investigate only stochastic automata with ''r''=''s'' and ''G'' being [[bijective]], allowing them to confuse actions and states.
The states of such an automaton correspond to the states of a &quot;discrete-state discrete-parameter [[Markov process]]&quot;.&lt;ref&gt;(Narendra, Thathachar, 1974) p.325 left&lt;/ref&gt;
&lt;!---The following description had to be partly guessed on the base of the usual finite-state automaton definitions, since Narendra and Thathachar are not very explicit about that issue:---&gt;
At each time step ''t''=0,1,2,3,..., the automaton reads an input from its environment, updates ''p''(''t'') to ''p''(''t''+1) by ''A'', randomly chooses a successor state according to the probabilities ''p''(''t''+1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton.
&lt;!---End of guess.---&gt;
Frequently, the input set ''x'' = { 0,1 } is used, with 0 and 1 corresponding to a ''nonpenalty'' and a ''penalty'' response of the environment, respectively; in this case, the automaton should learn to minimize the number of ''penalty'' responses, and the feedback loop of automaton and environment is called a &quot;P-model&quot;. More generally, a &quot;Q-model&quot; allows an arbitrary finite input set ''x'', and an &quot;S-model&quot; uses the [[interval (mathematics)|interval]] [0,1] of [[real numbers]] as ''x''.&lt;ref&gt;(Narendra, Thathachar, 1974) p.325 right&lt;/ref&gt;

== Finite action-set learning automata ==
Finite action-set learning automata (FALA) are a class of learning automata for which the number of possible actions is finite or, in more mathematical terms, for which the size of the action-set is finite.

== References ==
* Philip Aranzulla and John Mellor &lt;!---
---Trying to flesh out the previous poor reference hint, I found Mellor's home page and added all publications of Mellor+Aranzulla found there. They seem to fit not very well into this article, however.---
---&gt;([https://web.archive.org/web/20131203032954/http://www.bradford.ac.uk/scim/staff-profiles/profile/?u=jemellor Home page]):
** Mellor J and Aranzulla P (2000): &quot;Using an S-Model Response Environment with Learnng Automata Based Routing Schemes for IP Networks &quot;, Proc. Eighth IFIP Workshop on Performance Modelling and Evaluation of ATM and IP Networks, pp 56/1-56/12, Ilkley, UK.
** Aranzulla P and Mellor J (1997): &quot;Comparing two routing algorithms requiring reduced signalling when applied to ATM networks&quot;, Proc. Fourteenth UK Teletraffic Symposium on Performance Engineering in Information Systems, pp 20/1-20/4, UMIST, Manchester, UK.
* {{cite journal|authors=Narendra K., Thathachar M.A.L.|title=Learning automata – a survey|journal=IEEE Transactions on Systems, Man, and Cybernetics|date=July 1974| volume= SMC-4| issue=4| pages=323–334| url=http://www.dklevine.com/archive/refs4481.pdf| doi=10.1109/tsmc.1974.5408453}}
* Mikhail L’vovich TSetlin., ''Automaton Theory and the Modelling of Biological Systems'', New York and London, Academic Press, 1973. ([http://www.worldcatlibraries.org/wcpa/ow/4f39ba3751b2260c.html Find in a library])

{{reflist}}

==See also==
*[[Reinforcement learning]]
*[[Game theory]]


</text>
      <sha1>4hth3294uhzo8vfvovqlmnbkavr2a9a</sha1>
    </revision>
  </page>
  <page>
    <title>Conditional random field</title>
    <ns>0</ns>
    <id>4118276</id>
    <revision>
      <id>804338799</id>
      <parentid>804338745</parentid>
      <timestamp>2017-10-08T09:48:27Z</timestamp>
      <contributor>
        <ip>78.158.147.89</ip>
      </contributor>
      <comment>/* Higher-order CRFs and semi-Markov CRFs */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20275">{{multiple issues|
{{context|date=January 2013}}
{{technical|date=June 2012}}
}}

{{machine learning bar}}
'''Conditional random fields (CRFs)''' are a class of [[statistical model| statistical modeling method]] often applied in [[pattern recognition]] and [[machine learning]] and used for [[structured prediction]]. CRFs fall into the sequence modeling family. Whereas a discrete [[statistical classification|classifier]] predicts a label for a single sample without considering &quot;neighboring&quot; samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in [[natural language processing]]) predicts sequences of labels for sequences of input samples.

CRFs are a type of [[discriminative model|discriminative]] [[Markov random field|undirected]] [[Statistical model|probabilistic]] [[graphical model]]. It is used to encode known relationships between observations and construct consistent interpretations. It is often used for [[sequence labeling|labeling]] or [[parsing]] of sequential data, such as natural language processing or [[bioinformatics|biological sequences]]&lt;ref name=&quot;Laf:McC:Per01&quot;&gt;{{cite conference | authors = Lafferty, J., McCallum, A., Pereira, F. |
title=Conditional random fields: Probabilistic models for segmenting and labeling sequence data|
booktitle =Proc. 18th International Conf. on Machine Learning |
publisher= Morgan Kaufmann|
date = 2001| pages= 282–289|url= http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;context=cis_papers }}
&lt;/ref&gt;
and in [[computer vision]].&lt;ref&gt;{{cite news
 | title = Multiscale conditional random fields for image labeling
 | last1 = He | first1 = X. | authorlink = Xuming He | last2 = Zemel | first2 = R.S. | last3 = Carreira-Perpinñán | first3 = M.A.
 | date = 2004
 | publisher = IEEE Computer Society
 | citeseerx = 10.1.1.3.7826
 }}&lt;/ref&gt;
Specifically, CRFs find applications in POS Tagging, [[shallow parsing]],&lt;ref&gt;{{cite conference| title=shallow parsing with conditional random fields|author1=Sha, F. |author2=Pereira, F. | date=2003 |  url= http://portal.acm.org/ft_gateway.cfm?id=1073473&amp;type=pdf&amp;CFID=4684435&amp;CFTOKEN=39459323}}&lt;/ref&gt;
[[named entity recognition]],&lt;ref&gt;{{cite conference|
url=http://www.aclweb.org/anthology/W04-1221.pdf |
title=Biomedical named entity recognition using conditional random fields and rich feature sets |
author= Settles, B.|date=2004|
booktitle=Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications |
pages= 104–107}}&lt;/ref&gt;
[[Gene prediction|gene finding]] and peptide critical functional region finding,&lt;ref&gt;{{cite conference | title = Analysis and Prediction of the Critical Regions of Antimicrobial Peptides Based on Conditional Random Fields. |
publisher = PLoS ONE |author1=Chang KY |author2=Lin T-p |author3=Shih L-Y |author4=Wang C-K |
date = 2015 |
url= http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0119490}}&lt;/ref&gt;
among other tasks, being an alternative to the related [[hidden Markov model]]s (HMMs). In computer vision, CRFs are often used for object recognition&lt;ref name=&quot;Rui:Gal:Gon15&quot;&gt;{{cite conference | title = UPGMpp: a Software Library for Contextual Object Recognition. |
author1=J.R. Ruiz-Sarmiento |author2=C. Galindo |author3=J. Gonzalez-Jimenez |
date = 2015 |
url= https://www.researchgate.net/publication/281620302_UPGMpp_a_Software_Library_for_Contextual_Object_Recognition |
booktitle= 3rd. Workshop on Recognition and Action for Scene Understanding (REACTS)}}&lt;/ref&gt; and image segmentation.

==Description==
[[John D. Lafferty|Lafferty]], [[Andrew McCallum|McCallum]] and Pereira&lt;ref name=&quot;Laf:McC:Per01&quot;/&gt; define a CRF on observations &lt;math&gt;\boldsymbol{X}&lt;/math&gt; and [[random variable]]s &lt;math&gt;\boldsymbol{Y}&lt;/math&gt; as follows:

&lt;blockquote&gt;Let &lt;math&gt;G = (V , E)&lt;/math&gt; be a graph such that
&lt;math&gt;\boldsymbol{Y} = (\boldsymbol{Y}_v)_{v\in V}&lt;/math&gt;,
 so that &lt;math&gt;\boldsymbol{Y}&lt;/math&gt; is indexed by the vertices of &lt;math&gt;G&lt;/math&gt;.
Then &lt;math&gt;(\boldsymbol{X}, \boldsymbol{Y})&lt;/math&gt; is a conditional random field when the random variables &lt;math&gt;\boldsymbol{Y}_v&lt;/math&gt;, conditioned on &lt;math&gt;\boldsymbol{X}&lt;/math&gt;, obey the [[Markov property]] with
respect to the graph: &lt;math&gt;p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \neq v) = p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \sim v)&lt;/math&gt;, where &lt;math&gt;\mathit{w} \sim v&lt;/math&gt; means
that &lt;math&gt;w&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; are [[Neighbourhood (graph theory)|neighbors]] in &lt;math&gt;G&lt;/math&gt;.
&lt;/blockquote&gt;

What this means is that a CRF is an [[Graphical model|undirected graphical model]] whose nodes can be divided into exactly two disjoint sets &lt;math&gt;\boldsymbol{X}&lt;/math&gt; and &lt;math&gt;\boldsymbol{Y}&lt;/math&gt;, the observed and output variables, respectively; the conditional distribution &lt;math&gt;p(\boldsymbol{Y}|\boldsymbol{X})&lt;/math&gt; is then modeled.

===Inference===
For general graphs, the problem of exact inference in CRFs is intractable. The inference problem for a CRF is basically the same as for an [[Markov random field#Inference|MRF]] and the same arguments hold.&lt;ref name=SuttonIntroduction&gt;{{cite arXiv |last1=Sutton |first1=Charles |last2=McCallum |first2=Andrew |class=stat.ML |year=2010 |eprint=1011.4088v1 |title=An Introduction to Conditional Random Fields}}&lt;/ref&gt;
However, there exist special cases for which exact inference is feasible:

* If the graph is a chain or a tree, message passing algorithms yield exact solutions. The algorithms used in these cases are analogous to the [[forward-backward algorithm|forward-backward]] and [[Viterbi algorithm]] for the case of HMMs.
* If the CRF only contains pair-wise potentials and the energy is submodular, combinatorial min cut/max flow algorithms yield exact solutions.

If exact inference is impossible, several algorithms can be used to obtain approximate solutions. These include:
* [[Belief_propagation#Approximate_algorithm_for_general_graphs|Loopy belief propagation]]
* Alpha expansion
* Mean field inference
* [[Linear programming relaxation]]s

===Parameter Learning===
Learning the parameters &lt;math&gt;\theta&lt;/math&gt; is usually done by [[maximum likelihood]] learning for &lt;math&gt;p(Y_i|X_i; \theta)&lt;/math&gt;.
If all nodes have exponential family distributions and all nodes are observed during training, this [[Optimization (mathematics)|optimization]] is convex.&lt;ref name=&quot;SuttonIntroduction&quot; /&gt; It can be solved for example using [[gradient descent]] algorithms, or [[Quasi-Newton method]]s such as the [[L-BFGS]] algorithm.
On the other hand, if some variables are unobserved, the inference problem has to be solved for these variables. Exact inference is intractable in general graphs, so approximations have to be used.

===Examples===
In sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables &lt;math&gt;X&lt;/math&gt; represents a sequence of observations and  &lt;math&gt;Y&lt;/math&gt; represents a hidden (or unknown) state variable that needs to be inferred given the observations.
The &lt;math&gt;Y_{i}&lt;/math&gt; are structured to form a chain, with an edge between each &lt;math&gt;Y_{i-1}&lt;/math&gt; and &lt;math&gt;Y_{i}&lt;/math&gt;. As well as having a simple interpretation of the &lt;math&gt;Y_{i}&lt;/math&gt; as &quot;labels&quot; for each element in the input sequence, this layout admits efficient algorithms for:
*  model ''training'', learning the conditional distributions between the &lt;math&gt;Y_{i}&lt;/math&gt; and feature functions from some corpus of training data.
* ''decoding'', determining the probability of a given label sequence &lt;math&gt;Y&lt;/math&gt; given &lt;math&gt;X&lt;/math&gt;.
* ''inference'', determining the ''most likely'' label sequence &lt;math&gt;Y&lt;/math&gt; given &lt;math&gt;X&lt;/math&gt;.

The conditional dependency of each &lt;math&gt;Y_{i}&lt;/math&gt; on &lt;math&gt;X&lt;/math&gt; is defined through a fixed set of ''feature functions'' of the form &lt;math&gt;f(i, Y_{i-1}, Y_{i}, X)&lt;/math&gt;, which can informally be thought of as measurements on the input sequence that partially determine the [[Likelihood function|likelihood]] of each possible value for &lt;math&gt;Y_{i}&lt;/math&gt;. The model assigns each feature a numerical weight and combines them to determine the probability of a certain value for &lt;math&gt;Y_{i}&lt;/math&gt;.

Linear-chain CRFs have many of the same applications as conceptually simpler hidden Markov models (HMMs), but relax certain assumptions about the input and output sequence distributions. An HMM can loosely be understood as a CRF with very specific feature functions that use constant probabilities to model state transitions and emissions. Conversely, a CRF can loosely be understood as a generalization of an HMM that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states, depending on the input sequence.

Notably in contrast to HMMs, CRFs can contain any number of feature functions, the feature functions can inspect the entire input sequence &lt;math&gt;X&lt;/math&gt; at any point during inference, and the range of the feature functions need not have a probabilistic interpretation.

==Variants==

===Higher-order CRFs and semi-Markov CRFs===

CRFs can be extended into higher order models by making each &lt;math&gt;Y_{i}&lt;/math&gt; dependent on a fixed number &lt;math&gt;o&lt;/math&gt; of previous variables &lt;math&gt;Y_{i-o}, ..., Y_{i-1}&lt;/math&gt;. In conventional formulations of higher order CRFs, training and inference are only practical for small values of &lt;math&gt;o&lt;/math&gt; (such as ''o'' ≤ 5),&lt;ref&gt;{{cite conference
| url = http://aclweb.org/anthology/D17-1044
| title = Learning the Structure of Variable-Order CRFs: a Finite-State Perspective
| last1 = Lavergne
| first1 = Thomas
| author-link1 =
| last2 = Yvon
| first2 = François
| author-link2 =
| date = September 7, 2017
| publisher = Association for Computational Linguistics
| book-title = Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
| pages = 433
| location = Copenhagen, Denmark
}}&lt;/ref&gt; since their computational cost increases exponentially with &lt;math&gt;o&lt;/math&gt;.

However, another recent advance has managed to ameliorate these issues by leveraging concepts and tools from the field of Bayesian nonparametrics. Specifically, the CRF-infinity approach&lt;ref&gt;{{cite journal
| title = The Infinite-Order Conditional Random Field Model for Sequential Data Modeling
| last1 = Chatzis
| first1 = Sotirios
| author-link1 =
| last2 = Demiris
| first2 = Yiannis
| author-link2 =
| year = 2013
| journal = IEEE Transactions on Pattern Analysis and Machine Intelligence
| pages = 1523-1534
| volume = 35(6)
}}&lt;/ref&gt;
constitutes a CRF-type model that is capable of learning infinitely-long temporal dynamics in a scalable fashion. This is effected by introducing a novel potential function for CRFs that is based on the Sequence Memoizer (SM), a nonparametric Bayesian model for learning infinitely-long dynamics in sequential observations &lt;ref&gt;{{cite conference
| title = Improvements to the Sequence Memoizer
| last1 = Gasthaus
| first1 = Jan
| author-link1 =
| last2 = Teh
| first2 = Yee Whye
| author-link2 =
| year = 2010
| book-title = Proc. NIPS
}}&lt;/ref&gt;. To render such a model computationally tractable, CRF-infinity employs a mean-field approximation &lt;ref&gt;{{cite journal
| title = EM Procedures Using Mean Field-Like Approximations for Markov Model-Based Image Segmentation
| last1 = Celeux
| first1 = G.
| author-link1 =
| last2 = Forbes
| first2 = F.
| author-link2 =
| last3 = Peyrard
| first3 = N.
| author-link3 =
| year = 2003
| journal = Pattern Recognition
| pages = 131-144
| volume = 36(1)
}}&lt;/ref&gt;
of the postulated novel potential functions (which are driven by an SM). This allows for devising efficient approximate training and inference algorithms for the model, without undermining its capability to capture and model temporal dependencies of arbitrary length.

There exists another generalization of CRFs, the '''semi-Markov conditional random field (semi-CRF)''', which models variable-length ''segmentations'' of the label sequence &lt;math&gt;Y&lt;/math&gt;.&lt;ref&gt;{{Cite book
| publisher = MIT Press
| pages = 1185–1192
| editors = Lawrence K. Saul, Yair Weiss, Léon Bottou (eds.)
| last = Sarawagi
| first = Sunita
| last2 = Cohen
| first2 = William W.
| chapter = Semi-Markov conditional random fields for information extraction
| chapter-url = http://papers.nips.cc/paper/2648-semi-markov-conditional-random-fields-for-information-extraction.pdf
| title = Advances in Neural Information Processing Systems 17
| url = http://papers.nips.cc/book/advances-in-neural-information-processing-systems-17-2004
| location = Cambridge, MA
| year = 2005
}}&lt;/ref&gt; This provides much of the power of higher-order CRFs to model long-range dependencies of the &lt;math&gt;Y_{i}&lt;/math&gt;, at a reasonable computational cost.

Finally, large-margin models for [[structured prediction]], such as the [[Structured SVM|structured Support Vector Machine]] can be seen as an alternative training procedure to CRFs.

===Latent-dynamic conditional random field===
'''Latent-dynamic conditional random fields''' ('''LDCRF''') or '''discriminative probabilistic latent variable models''' ('''DPLVM''') are a type of CRFs for sequence tagging tasks. They are [[latent variable model]]s that are trained discriminatively.

In an LDCRF, like in any sequence tagging task, given a sequence of observations '''x''' = &lt;math&gt;x_1,\dots,x_n&lt;/math&gt;, the main problem the model must solve is how to assign a sequence of labels '''y''' = &lt;math&gt;y_1,\dots,y_n&lt;/math&gt; from one finite set of labels {{mvar|Y}}. Instead of directly modeling {{mvar|P}}('''y'''|'''x''') as an ordinary linear-chain CRF would do, a set of latent variables '''h''' is &quot;inserted&quot; between '''x''' and '''y''' using the [[chain rule of probability]]:&lt;ref name=&quot;lvperceptron&quot;&gt;{{cite conference |author1=Xu Sun |author2=Takuya Matsuzaki |author3=Daisuke Okanohara |author4=Jun'ichi Tsujii |title=Latent Variable Perceptron Algorithm for Structured Classification |conference=IJCAI |year=2009 |pages=1236–1242}}&lt;/ref&gt;

:&lt;math&gt;P(\mathbf{y} | \mathbf{x}) = \sum_\mathbf{h} P(\mathbf{y}|\mathbf{h}, \mathbf{x}) P(\mathbf{h} | \mathbf{x})&lt;/math&gt;

This allows capturing latent structure between the observations and labels.&lt;ref name=&quot;morency&quot;&gt;{{Cite book | last1 = Morency | first1 = L. P. | last2 = Quattoni | first2 = A. | last3 = Darrell | first3 = T. | doi = 10.1109/CVPR.2007.383299 | chapter = Latent-Dynamic Discriminative Models for Continuous Gesture Recognition | title = 2007 IEEE Conference on Computer Vision and Pattern Recognition | pages = 1| year = 2007 | isbn = 1-4244-1179-3 | pmid =  | pmc = | url = http://dspace.mit.edu/bitstream/handle/1721.1/35276/MIT-CSAIL-TR-2007-002.pdf}}&lt;/ref&gt; While LDCRFs can be trained using quasi-Newton methods, a specialized version of the [[perceptron]] algorithm called the '''latent-variable perceptron''' has been developed for them as well, based on Collins' [[structured perceptron]] algorithm.&lt;ref name=&quot;lvperceptron&quot;/&gt; These models find applications in [[computer vision]], specifically [[gesture recognition]] from video streams&lt;ref name=&quot;morency&quot;/&gt; and [[shallow parsing]].&lt;ref name=&quot;lvperceptron&quot;/&gt;

== Software ==

This is a partial list of software that implement generic CRF tools.
* [https://github.com/zhongkaifu/RNNSharp RNNSharp] CRFs based on recurrent neural networks ([[C Sharp (programming language)|C#]], [[.NET Framework|.NET]])
* [http://klcl.pku.edu.cn/member/sunxu/code.htm CRF-ADF] Linear-chain CRFs with fast online ADF training ([[C Sharp (programming language)|C#]], [[.NET Framework|.NET]])
* [https://github.com/zhongkaifu/CRFSharp CRFSharp] Linear-chain CRFs ([[C Sharp (programming language)|C#]], [[.NET Framework|.NET]])
* [http://vision.csd.uwo.ca/code/ GCO] CRFs with submodular energy functions ([[C++]], [[Matlab]])
* [http://research.project-10.de/dgm DGM] General CRFs ([[C++]])
* [http://mallet.cs.umass.edu/grmm/index.php GRMM] General CRFs ([[Java (programming language)|Java]])
* [http://factorie.cs.umass.edu/ factorie] General CRFs ([[Scala (programming language)|Scala]])
* [http://www.cs.ubc.ca/~murphyk/Software/CRFall.zip CRFall] General CRFs ([[MATLAB|Matlab]])
* [http://crf.sourceforge.net/ Sarawagi's CRF] Linear-chain CRFs ([[Java (programming language)|Java]])
* [http://sourceforge.net/projects/hcrf/ HCRF library] Hidden-state CRFs ([[C++]], [[MATLAB|Matlab]])
* [http://accord-framework.net Accord.NET] Linear-chain CRF, HCRF and HMMs ([[C Sharp (programming language)|C#]], [[.NET Framework|.NET]])
* [http://wapiti.limsi.fr/ Wapiti] Fast linear-chain CRFs ([[C (programming language)|C]])&lt;ref&gt;T. Lavergne, O. Cappé and F. Yvon (2010). [http://acl.eldoc.ub.rug.nl/mirror/P/P10/P10-1052.pdf Practical very large scale CRFs] {{webarchive|url=https://web.archive.org/web/20130718001211/http://acl.eldoc.ub.rug.nl/mirror/P/P10/P10-1052.pdf |date=2013-07-18 }}. Proc. 48th Annual Meeting of the [[Association for Computational Linguistics|ACL]], pp. 504-513.&lt;/ref&gt;
* [http://www.chokkan.org/software/crfsuite/ CRFSuite] Fast restricted linear-chain CRFs ([[C (programming language)|C]])
* [https://web.archive.org/web/20100421020327/http://crfpp.sourceforge.net/ CRF++] Linear-chain CRFs ([[C++]])
* [http://flexcrfs.sourceforge.net/ FlexCRFs] First-order and second-order Markov CRFs ([[C++]])
* [http://hackage.haskell.org/package/crf-chain1 crf-chain1] First-order, linear-chain CRFs ([[Haskell (programming language)|Haskell]])
* [http://www.cs.rochester.edu/~bhole/code/crf/ imageCRF] CRF for segmenting images and image volumes ([[C++]])
* [http://mallet.cs.umass.edu/ MALLET] Linear-chain for sequence tagging ([[Java (programming language)|Java]])
* [https://pystruct.github.io/ PyStruct] Structured Learning in Python ([[Python (programming language)|Python]])
* [https://github.com/scrapinghub/python-crfsuite Pycrfsuite] A python binding for crfsuite ([[Python (programming language)|Python]])
* [https://github.com/p2t2/figaro Figaro] Probabilistic programming language capable of defining CRFs and other graphical models ([[Scala (programming language)|Scala]])
* [https://cran.r-project.org/web/packages/CRF/CRF.pdf CRF] Modeling and computational tools for CRFs and other undirected graphical models ([[R (programming language)|R]])
* [http://hciweb2.iwr.uni-heidelberg.de/opengm/index.php OpenGM] Library for discrete factor graph models and distributive operations on these models ([[C++]])
* [https://github.com/jotaraul/upgmpp UPGMpp]&lt;ref name=&quot;Rui:Gal:Gon15&quot;/&gt; Library for building, training, and performing inference with Undirected Graphical Models ([[C++]])

This is a partial list of software that implement CRF related tools.
* [http://www.broadinstitute.org/annotation/conrad Conrad] CRF based gene predictor ([[Java (programming language)|Java]])
* [http://nlp.stanford.edu/software/CRF-NER.shtml Stanford NER] Named Entity Recognizer ([[Java (programming language)|Java]])
* [https://web.archive.org/web/20100707042144/http://cbioc.eas.asu.edu/banner/ BANNER] Named Entity Recognizer ([[Java (programming language)|Java]])

== See also ==
* [[Hammersley–Clifford theorem]]
* [[Graphical model]]
* [[Markov random field]]
* [[Maximum entropy Markov model]] (MEMM)

== References ==
{{reflist|30em}}

==Further reading==
* McCallum, A.: Efficiently inducing features of conditional random fields. In: ''Proc. 19th Conference on Uncertainty in Artificial Intelligence''. (2003)
* Wallach, H.M.: [http://www.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf Conditional random fields: An introduction]. Technical report MS-CIS-04-21, University of Pennsylvania (2004)
* Sutton, C., McCallum, A.: An Introduction to Conditional Random Fields for Relational Learning. In &quot;Introduction to Statistical Relational Learning&quot;. Edited by [[Lise Getoor]] and Ben Taskar. MIT Press. (2006) [http://www.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf Online PDF]
* Klinger, R., Tomanek, K.: Classical Probabilistic Models and Conditional Random Fields. Algorithm Engineering Report TR07-2-013, Department of Computer Science, Dortmund University of Technology, December 2007. ISSN 1864-4503. [https://ls11-www.cs.uni-dortmund.de/_media/techreports/tr07-13.pdf Online PDF]


</text>
      <sha1>4isjzhn7lhqxyuozw00igm1soy0iko2</sha1>
    </revision>
  </page>
  <page>
    <title>Training, test, and validation sets</title>
    <ns>0</ns>
    <id>1514392</id>
    <revision>
      <id>812937200</id>
      <parentid>812937108</parentid>
      <timestamp>2017-11-30T19:46:18Z</timestamp>
      <contributor>
        <ip>24.30.235.138</ip>
      </contributor>
      <comment>/* Validation dataset */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12881">{{multiple issues|
{{refimprove|date=December 2012}}
{{notability|date=June 2012}}
}}

In [[machine learning]], the study and construction of algorithms that can learn from and make predictions on [[data]]&lt;ref&gt;{{cite journal |title=Glossary of terms |author1=Ron Kohavi |author2=Foster Provost |journal=[[Machine Learning (journal)|Machine Learning]] |volume=30 |pages=271–274 |year=1998 |url=http://ai.stanford.edu/~ronnyk/glossary.html}}&lt;/ref&gt; is a common task. Such algorithms work by making data-driven predictions or decisions,&lt;ref name=&quot;bishop&quot;&gt;Machine learning and pattern recognition &quot;can be viewed as two facets of the same field.&quot;&lt;/ref&gt;{{rp|2}} through building a [[mathematical model]] from input data.

The data used to build the final model usually comes from multiple [[dataset]]s. In particular, three data sets are commonly used in different stages of the creation of the model.

The model is initially fit on a '''training dataset''',&lt;ref name=&quot;James 2013 176&quot;&gt;{{cite book|last=James|first=Gareth|title=An Introduction to Statistical Learning: with Applications in R|date=2013|publisher=Springer|isbn=978-1461471370|page=176|url=http://www-bcf.usc.edu/~gareth/ISL/}}&lt;/ref&gt; that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in [[artificial neural networks]]) of the model.&lt;ref name=&quot;Ripley 1996 354&quot;&gt;{{cite book|last=Ripley|first=Brian|title=Pattern Recognition and Neural Networks|date=1996|publisher=Cambridge University Press|isbn=978-0521717700|page=354|url=https://www.amazon.com/Pattern-Recognition-Neural-Networks-Ripley/dp/0521717701}}&lt;/ref&gt; The model (e.g. a [[neural net]] or a [[naive Bayes classifier]]) is trained on the training dataset using a [[supervised learning]] method (e.g. [[gradient descent]] or [[stochastic gradient descent]]). In practice, the training dataset often consist of pairs of an input [[Array data structure|vector]] and the corresponding ''answer'' vector or scalar, which is commonly denoted as the ''target''. The current model is run with the training dataset and produces a result, which is then compared with the ''target'', for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both [[feature selection|variable selection]] and parameter [[estimation theory|estimation]].

Successively, the fitted model is used to predict the responses for the observations in a second dataset called the '''validation dataset'''.&lt;ref name=&quot;James 2013 176&quot;/&gt; The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's [[Hyperparameter (machine learning)|hyperparameters]] &lt;ref name=&quot;Brownlee&quot;&gt;{{cite web |url=https://machinelearningmastery.com/difference-test-validation-datasets/|title=What is the Difference Between Test and Validation Datasets?|last=Brownlee|first=Jason|access-date=12 October 2017}}&lt;/ref&gt; (e.g. the number of hidden units in a neural network&lt;ref name=&quot;Ripley 1996 354&quot;/&gt;). Validation datasets can be used for [[Regularization (mathematics)|regularization]] by [[early stopping]]: stop training when the error on the validation dataset increases, as this is a sign of [[overfitting]] to the training dataset.&lt;ref name=&quot;prechelt_early_2012&quot;&gt;{{Cite book
| publisher = Springer Berlin Heidelberg
| isbn = 978-3-642-35289-8
| pages = 53–67
| editor = Grégoire Montavon
| editor2 = Klaus-Robert Müller
| editor2-link = Klaus-Robert Müller
| last = Prechelt
| first = Lutz
|author2=Geneviève B. Orr
| title = Neural Networks: Tricks of the Trade
| chapter = Early Stopping — But When?
| series = Lecture Notes in Computer Science
| accessdate = 2013-12-15
| date = 2012-01-01
| chapterurl = https://link.springer.com/chapter/10.1007/978-3-642-35289-8_5
}}&lt;/ref&gt;
This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.&lt;ref name=&quot;prechelt_early_2012&quot;/&gt;

Finally, the '''test dataset''' is a dataset used to provide an unbiased evaluation of a ''final'' model fit on the training dataset.&lt;ref name=&quot;Brownlee&quot;/&gt;

== Training dataset ==

A training dataset is a [[dataset]] of examples used for learning, that is to fit the parameters (e.g., weights) of, for example, a [[Classifier (machine learning)|classifier]].&lt;ref name=&quot;Ripley, B.D. 1996 p. 354&quot;&gt;Ripley, B.D. (1996) ''Pattern Recognition and Neural Networks'', Cambridge: Cambridge University Press, p. 354&lt;/ref&gt;&lt;ref name=&quot;cann-faq&quot;&gt;&quot;[ftp://ftp.sas.com/pub/neural/FAQ.html#A_data Subject: What are the population, sample, training set, design set, validation set, and test set?]&quot;, [ftp://ftp.sas.com/pub/neural/FAQ.html Neural Network FAQ, part 1 of 7: Introduction] ([ftp://ftp.sas.com/pub/neural/FAQ1.txt txt]), comp.ai.neural-nets, Sarle, W.S., ed. (1997, last modified 2002-05-17)&lt;/ref&gt;

Most approaches that search through training data for empirical relationships tend to [[overfit]] the data, meaning that they can identify apparent relationships in the training data that do not hold in general.

== Test dataset ==

A test dataset is a [[dataset]] that is [[independence (probability theory)|independent]] of the training dataset, but that follows the same [[probability distribution]] as the training dataset. If a model fit to the training dataset also fits the test dataset well, minimal [[overfitting]] has taken place (see figure below). A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.

A test set is therefore a set of examples used only to assess the performance (i.e. generalization) of a fully specified classifier.&lt;ref name=&quot;Ripley, B.D. 1996 p. 354&quot;/&gt;&lt;ref name=&quot;cann-faq&quot;/&gt;

[[Image:traintest.svg|center|700px|thumb|A training set (left) and a test set (right) from the same statistical population are shown as blue points.  Two predictive models are fit to the training data.  Both fitted models are plotted with both the training and test sets.  In the training set, the [[mean squared error|MSE]] of the fit shown in orange is 4 whereas the MSE for the fit shown in green is 9.  In the test set, the MSE for the fit shown in orange is 15 and the MSE for the fit shown in green is 13.  The orange curve severely overfits the training data, since its MSE increases by almost a factor of four when comparing the test set to the training set.  The green curve overfits the training data much less, as its MSE increases by less than a factor of 2.]]

== Validation dataset ==

A validation dataset is a set of examples used to tune the [[Hyperparameter (machine learning)|hyperparameter]]s (i.e. the architecture) of a classifier.   In [[artificial neural networks]], an hyperparameter is, for example, the number of hidden units.&lt;ref name=&quot;Ripley, B.D. 1996 p. 354&quot;/&gt;&lt;ref name=&quot;cann-faq&quot;/&gt; It, as well as the testing set (as mentioned above), should follow the same probability distribution as the training dataset.

In order to avoid overfitting, when any [[Statistical classification|classification]] parameter needs to be adjusted, it is necessary to have a validation dataset in addition to the training and test datasets.  For example, if the most suitable classifier for the problem is sought, the training dataset is used to train the candidate algorithms, the validation dataset is used to compare their performances and decide which one to take and, finally, the test dataset is used to obtain{{Citation needed|date=October 2017}} the performance characteristics such as [[accuracy]], [[sensitivity and specificity|sensitivity]], [[Sensitivity and specificity|specificity]], [[Precision and recall#F-measure|F-measure]], and so on. The validation dataset functions as a hybrid: it is training data used by testing, but neither as part of the low-level training nor as part of the final testing {{Citation needed|date=October 2017}}.

The basic process of using a validation dataset for model selection (as part of training dataset, validation dataset, and test dataset) is:&lt;ref name=&quot;cann-faq&quot;/&gt;&lt;ref&gt;Bishop, C.M. (1995), ''Neural Networks for Pattern Recognition'', Oxford: Oxford University Press, p. 372&lt;/ref&gt;
{{quote|
Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the ''hold out'' method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.}}

An application of this process is in [[early stopping]], where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).

=== Selection of a validation dataset ===

==== Holdout method ====

Most simply, part of the training dataset can be set aside and used as a validation set: this is known as the '''holdout method'''{{Citation needed|date=October 2017}}. Common proportions are 70%/30% training/validation{{Citation needed|date=October 2017}}.

==== Cross-validation ====

Alternatively, the ''hold out'' process can be repeated, repeatedly partitioning the original training dataset into a training dataset and a validation dataset: this is known as [[cross-validation (statistics)|cross-validation]]. These repeated partitions can be done in various ways, such as dividing into 2 equal datasets and using them as training/validation, and then validation/training, or repeatedly selecting a random subset as a validation dataset{{Citation needed|date=October 2017}}.

==Hierarchical classification==
Another example of parameter adjustment is '''hierarchical classification''' (sometimes referred to as '''instance space decomposition''' &lt;ref&gt;Cohen S, Rokach L., Maimon O. Decision-tree instance-space decomposition with grouped gain-ratio In J. Information Sciences, vol. 177, issue 17, pp. 3592–3612. Elsevier. 2007.&lt;/ref&gt;), which splits a complete multi-class problem into a set of smaller classification problems. It serves for learning more accurate concepts due to simpler classification boundaries in subtasks and individual feature selection procedures for subtasks. When doing classification decomposition, the central choice is the order of combination of smaller classification steps, called the classification path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example,&lt;ref&gt;Sidorova, J., Badia, T. &quot;ESEDA: tool for enhanced speech emotion detection and analysis&quot;. The 4th International Conference on Automated Solutions for Cross Media Content and Multi-Channel Distribution (AXMEDIS 2008). Florence, November, 17-19, pp. 257–260. IEEE press.&lt;/ref&gt; on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.

==See also==
* [[Cross-validation (statistics)]]
* [[Machine learning]]
* [[Statistical classification]]

== References ==
{{Reflist}}

==External links==
*[ftp://ftp.sas.com/pub/neural/FAQ.html#A_data FAQ: What are the population, sample, training set, design set, validation set, and test set?]
*[https://machinelearningmastery.com/difference-test-validation-datasets/ What is the Difference Between Test and Validation Datasets? ]
*[https://www.quora.com/What-is-training-validation-and-testing-data-sets-scenario-in-machine-learning What is training, validation, and testing data-sets scenario in machine learning?]
*[https://stackoverflow.com/q/13610074/3924118 Is there a rule-of-thumb for how to divide a dataset into training and validation sets?]



[[Category:Validity (statistics)]]</text>
      <sha1>hkw43uk4va1tfmgtsxvmgni3uzpizy4</sha1>
    </revision>
  </page>
  <page>
    <title>Cross-entropy method</title>
    <ns>0</ns>
    <id>5767980</id>
    <revision>
      <id>812277504</id>
      <parentid>812277368</parentid>
      <timestamp>2017-11-27T01:17:37Z</timestamp>
      <contributor>
        <username>Msuzen</username>
        <id>370887</id>
      </contributor>
      <minor/>
      <comment>reflist inserted</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7329">
The '''cross-entropy''' ('''CE''') '''method''' developed by [[Reuven Rubinstein]] is a general [[Monte Carlo method|Monte Carlo]] approach to
[[Combinatorial optimization|combinatorial]] and [[Continuous optimization|continuous]] multi-extremal [[Optimization (mathematics)|optimization]] and [[importance sampling]].
The method originated from the field of ''rare event simulation'', where
very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems.
The CE method can be applied to static and noisy combinatorial optimization problems such as the [[traveling salesman problem]], the [[quadratic assignment problem]], [[Sequence alignment|DNA sequence alignment]], the [[Maxcut|max-cut]] problem and the buffer allocation problem, as well as continuous [[global optimization]] problems with many local [[extremum|extrema]].

In a nutshell, the CE method consists of two phases &lt;ref&gt; Rubinstein, R.Y. and  Kroese, D.P. (2004), The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning, Springer-Verlag, New York {{ISBN|978-0-387-21240-1}}.&lt;/ref&gt;

#Generate a random data sample (trajectories, vectors, etc.) according to a specified mechanism.
#Update the parameters of the random mechanism based on the data to produce a &quot;better&quot; sample in the next iteration. This step involves minimizing the [[cross entropy|''cross-entropy'']] or  [[Kullback–Leibler divergence]].

==Estimation via importance sampling==
Consider the general problem of estimating the quantity

:&lt;math&gt;\ell = \mathbb{E}_{\mathbf{u}}[H(\mathbf{X})] = \int H(\mathbf{x})\, f(\mathbf{x}; \mathbf{u})\, \textrm{d}\mathbf{x}&lt;/math&gt;,

where &lt;math&gt;H&lt;/math&gt; is some ''performance function'' and &lt;math&gt;f(\mathbf{x};\mathbf{u})&lt;/math&gt; is a member of some [[parametric family]] of distributions. Using [[importance sampling]] this quantity can be estimated as

:&lt;math&gt;\hat{\ell} = \frac{1}{N} \sum_{i=1}^N H(\mathbf{X}_i) \frac{f(\mathbf{X}_i; \mathbf{u})}{g(\mathbf{X}_i)}&lt;/math&gt;,

where &lt;math&gt;\mathbf{X}_1,\dots,\mathbf{X}_N&lt;/math&gt; is a random sample from &lt;math&gt;g\,&lt;/math&gt;. For positive &lt;math&gt;H&lt;/math&gt;, the theoretically ''optimal'' importance sampling [[probability density function|density]] (pdf) is given by

:&lt;math&gt; g^*(\mathbf{x}) = H(\mathbf{x}) f(\mathbf{x};\mathbf{u})/\ell&lt;/math&gt;.

This, however, depends on the unknown &lt;math&gt;\ell&lt;/math&gt;. The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the [[Kullback–Leibler divergence|Kullback–Leibler]] sense) to the optimal PDF &lt;math&gt;g^*&lt;/math&gt;.

==Generic CE algorithm==
# Choose initial parameter vector &lt;math&gt;\mathbf{v}^{(0)}&lt;/math&gt;; set t = 1.
# Generate a random sample &lt;math&gt;\mathbf{X}_1,\dots,\mathbf{X}_N&lt;/math&gt; from &lt;math&gt;f(\cdot;\mathbf{v}^{(t-1)})&lt;/math&gt;
# Solve for &lt;math&gt;\mathbf{v}^{(t)}&lt;/math&gt;, where&lt;br&gt;&lt;math&gt;\mathbf{v}^{(t)} = \mathop{\textrm{argmax}}_{\mathbf{u}} \frac{1}{N} \sum_{i=1}^N H(\mathbf{X}_i)\frac{f(\mathbf{X}_i;\mathbf{u})}{f(\mathbf{X}_i;\mathbf{v}^{(t-1)})} \log f(\mathbf{X}_i;\mathbf{v}^{(t-1)})&lt;/math&gt;
# If convergence is reached then '''stop'''; otherwise, increase t by 1 and reiterate from step 2.

In several cases, the solution to step 3 can be found ''analytically''.  Situations in which this occurs are
* When &lt;math&gt;f\,&lt;/math&gt; belongs to the [[Exponential family|natural exponential family]]
* When &lt;math&gt;f\,&lt;/math&gt; is [[discrete space|discrete]] with finite [[Support (mathematics)|support]]
* When &lt;math&gt;H(\mathbf{X}) = \mathrm{I}_{\{\mathbf{x}\in A\}}&lt;/math&gt; and &lt;math&gt;f(\mathbf{X}_i;\mathbf{u}) = f(\mathbf{X}_i;\mathbf{v}^{(t-1)})&lt;/math&gt;, then &lt;math&gt;\mathbf{v}^{(t)}&lt;/math&gt; corresponds to the [[Maximum likelihood|maximum likelihood estimator]] based on those &lt;math&gt;\mathbf{X}_k \in A&lt;/math&gt;.

== Continuous optimization&amp;mdash;example==
The same CE algorithm can be used for optimization, rather than estimation.
Suppose the problem is to maximize some function &lt;math&gt;S(x)&lt;/math&gt;, for example,
&lt;math&gt;S(x) = \textrm{e}^{-(x-2)^2} + 0.8\,\textrm{e}^{-(x+2)^2}&lt;/math&gt;.
To apply CE, one considers first the ''associated stochastic problem'' of estimating
&lt;math&gt;\mathbb{P}_{\boldsymbol{\theta}}(S(X)\geq\gamma)&lt;/math&gt;
for a given ''level'' &lt;math&gt;\gamma\,&lt;/math&gt;, and parametric family &lt;math&gt;\left\{f(\cdot;\boldsymbol{\theta})\right\}&lt;/math&gt;, for example the 1-dimensional
[[Gaussian distribution]],
parameterized by its mean &lt;math&gt;\mu_t\,&lt;/math&gt; and variance &lt;math&gt;\sigma_t^2&lt;/math&gt; (so &lt;math&gt;\boldsymbol{\theta} = (\mu,\sigma^2)&lt;/math&gt; here).
Hence, for a given &lt;math&gt;\gamma\,&lt;/math&gt;, the goal is to find &lt;math&gt;\boldsymbol{\theta}&lt;/math&gt; so that
&lt;math&gt;D_{\mathrm{KL}}(\textrm{I}_{\{S(x)\geq\gamma\}}\|f_{\boldsymbol{\theta}})&lt;/math&gt;
is minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above.
It turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and
parametric family are the sample mean and sample variance corresponding to the ''elite samples'', which are those samples that have objective function value &lt;math&gt;\geq\gamma&lt;/math&gt;.
The worst of the elite samples is then used as the level parameter for the next iteration.
This yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an [[estimation of distribution algorithm]].

===Pseudo-code===
 1. mu:=-6; sigma2:=100; t:=0; maxits=100;    // Initialize parameters
 2. N:=100; Ne:=10;                           //
 3. while t &lt; maxits and sigma2 &gt; epsilon     // While maxits not exceeded and not converged
 4.  X = SampleGaussian(mu,sigma2,N);         // Obtain N samples from current sampling distribution
 5.  S = exp(-(X-2)^2) + 0.8 exp(-(X+2)^2);   // Evaluate objective function at sampled points
 6.  X = sort(X,S);                           // Sort X by objective function values (in descending order)
 7.  mu = mean(X(1:Ne)); sigma2=var(X(1:Ne)); // Update parameters of sampling distribution
 8.  t = t+1;                                 // Increment iteration counter
 9. return mu                                 // Return mean of final sampling distribution as solution

==Related methods==
*[[Simulated annealing]]
*[[Genetic algorithms]]
*[[Harmony search]]
*[[Estimation of distribution algorithm]]
*[[Tabu search]]

==See also==
*[[Cross entropy]]
*[[Kullback–Leibler divergence]]
*[[Randomized algorithm]]
*[[Importance sampling]]

== Journal Papers ==
* De Boer, P-T., Kroese, D.P, Mannor, S. and Rubinstein, R.Y. (2005). A Tutorial on the Cross-Entropy Method. ''Annals of Operations Research'', '''134''' (1), 19–67.[http://www.maths.uq.edu.au/~kroese/ps/aortut.pdf]
*Rubinstein, R.Y. (1997). Optimization of Computer simulation Models with Rare Events, ''European Journal of Operations Research'', '''99''', 89–112.

==Software Implementations==
*[https://cran.r-project.org/web/packages/CEoptim/index.html '''CEoptim''' R package]

==References==
{{reflist}}




</text>
      <sha1>1iic62zsm0be52rzq2j5cf1s7jmbjbl</sha1>
    </revision>
  </page>
  <page>
    <title>Concept drift</title>
    <ns>0</ns>
    <id>3118600</id>
    <revision>
      <id>811288306</id>
      <parentid>801070120</parentid>
      <timestamp>2017-11-20T18:03:52Z</timestamp>
      <contributor>
        <username>Dawnseeker2000</username>
        <id>1544984</id>
      </contributor>
      <comment>[[WP:AWB/T|Typo fixing]], , [[WP:AWB/T|typo(s) fixed]]: Therefore → Therefore, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15989">In [[predictive analytics]] and [[machine learning]], the '''concept drift''' means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.

The term ''concept'' refers to the quantity to be predicted. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable.

==Examples==
In a [[fraud detection]] application the target concept may be a [[Binary numeral system|binary]] attribute FRAUDULENT with values &quot;yes&quot; or &quot;no&quot; that indicates whether a given transaction is fraudulent. Or, in a [[weather prediction]] application, there may be several target concepts such as TEMPERATURE, PRESSURE, and HUMIDITY.

The behavior of the customers in an [[online shop]] may change over time. For example, if weekly merchandise sales are to be predicted, and a [[predictive modelling|predictive model]] has been developed that works satisfactorily. The model may use inputs such as the amount of money spent on [[advertising]], [[Promotion (marketing)|promotions]] being run, and other metrics that may affect sales. The model is likely to become less and less accurate over time – this is concept drift. In the merchandise sales application, one reason for concept drift may be seasonality, which means that shopping behavior changes seasonally. Perhaps there will be higher sales in the winter holiday season than during the summer, for example.

==Possible remedies==

To prevent deterioration in [[prediction]] accuracy because of concept drift, both active and passive solutions can be adopted.  Active solutions rely on triggering mechanisms, e.g., change-detection tests (Basseville and Nikiforov 1993; Alippi and Roveri, 2007) to explicitly detect concept drift as a change in the statistics of the data-generating process. In stationary conditions, any fresh information made available can be integrated to improve the model. Differently, when concept drift is detected, the current model is no more up-to-date and must be substituted with a new one to maintain the prediction accuracy (Gama et al., 2004; Alippi et al., 2011). On the contrary, in passive solutions the model is continuously updated, e.g., by retraining the model on the most recently observed samples (Widmer and Kubat, 1996), or enforcing an ensemble of classifiers (Elwell and Polikar 2011).

Contextual information, when available, can be used to better explain the causes of the concept drift: for instance, in the sales prediction application, concept drift might be compensated by adding information about the season to the model. By providing information about the time of the year, the rate of deterioration of your model is likely to decrease, concept drift is unlikely to be eliminated altogether. This is because actual shopping behavior does not follow any static, [[finite model]]. New factors may arise at any time that influence shopping behavior, the influence of the known factors or their interactions may change.

Concept drift cannot be avoided for complex phenomenon that are not governed by fixed [[Physical law|laws of nature]]. All processes that arise from human activity, such as [[socioeconomic]] processes, and [[biological processes]] are likely to experience concept drift. Therefore, periodic retraining, also known as refreshing, of any model is necessary.

==Software==
* [[RapidMiner]] (formerly YALE (Yet Another Learning Environment)): free open-source software for knowledge discovery, data mining, and machine learning also featuring data stream mining, learning time-varying concepts, and tracking drifting concept (if used in combination with its data stream mining plugin (formerly: concept drift plugin))
* EDDM ([https://web.archive.org/web/20070322063617/http://iaia.lcc.uma.es/Members/mbaena/papers/eddm/ EDDM (Early Drift Detection Method)]): free open-source implementation of drift detection methods in [[Weka (machine learning)]].
* [[MOA (Massive Online Analysis)]]: free open-source software specific for mining data streams with concept drift. It contains a prequential evaluation method, the EDDM concept drift methods, a reader of ARFF real datasets, and artificial stream generators as SEA concepts, STAGGER, rotating hyperplane, random tree, and random radius based functions. MOA supports bi-directional interaction with [[Weka (machine learning)]].

==Datasets==

===Real===
* '''Airline''', approximately 116 million flight arrival and departure records (cleaned and sorted) compiled by E.Ikonomovska. Reference: Data Expo 2009 Competition [http://stat-computing.org/dataexpo/2009/]. [http://kt.ijs.si/elena_ikonomovska/data.html Access]
* '''Chess.com''' (online games) and '''Luxembourg''' (social survey) datasets compiled by I.Zliobaite. [https://sites.google.com/site/zliobaite/resources-1 Access]
* '''ECUE spam''' 2 datasets each consisting of more than 10,000 emails collected over a period of approximately 2 years by an individual. [http://www.comp.dit.ie/sjdelany/Dataset.htm Access] from S.J.Delany webpage
* '''Elec2''', electricity demand, 2 classes, 45312 instances. Reference: M.Harries, Splice-2 comparative evaluation: Electricity pricing, Technical report, The University of South Wales, 1999. [http://www.inescporto.pt/~jgama/ales/ales_5.html Access] from J.Gama webpage. [http://arxiv.org/pdf/1301.3524v1.pdf Comment on applicability].
* '''PAKDD'09 competition''' data represents the credit evaluation task. It is collected over a five-year period. Unfortunately, the true labels are released only for the first part of the data. [http://sede.neurotech.com.br/PAKDD2009/ Access]
* '''Sensor stream''' and '''Power supply stream''' datasets are available from X. Zhu's Stream Data Mining Repository.  [http://www.cse.fau.edu/~xqzhu/stream.html Access]
* '''SMEAR''' is a benchmark data stream with a lot of missing values. Environment observation data over 7 years. Predict cloudiness. [https://github.com/zliobaite/paper-missing-values Access]
* '''Text mining''', a collection of text mining datasets with concept drift, maintained by I.Katakis. [https://web.archive.org/web/20100704072013/http://mlkd.csd.auth.gr/concept_drift.html Access]
* '''Gas Sensor Array Drift Dataset''', a collection of 13910 measurements from 16 chemical sensors utilized for drift compensation in a discrimination task of 6 gases at various levels of concentrations. [http://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset Access]

===Other===
* '''KDD'99 competition''' data contains ''simulated'' intrusions in a military network environment. It is often used as a benchmark to evaluate handling concept drift. [http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html Access]

===Synthetic===
* '''Extreme verification latency benchmark''', Souza, V.M.A.; Silva, D.F.; Gama, J.; Batista, G.E.A.P.A.  : Data Stream Classification Guided by Clustering on Nonstationary Environments and Extreme Verification Latency.  SIAM International Conference on Data Mining (SDM), pp.&amp;nbsp;873–881, 2015. [https://sites.google.com/site/nonstationaryarchive/ Access] from Nonstationary Environments – Archive.
* '''Sine, Line, Plane, Circle and Boolean Data Sets''', L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift,  IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp.&amp;nbsp;730–742, 2010. [http://www.cs.le.ac.uk/people/llm11/opensource/ArtificialConceptDriftDataSets.zip Access] from L.Minku webpage.
* '''SEA concepts''', N.W.Street, Y.Kim, A streaming ensemble algorithm (SEA) for large-scale classification, KDD'01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, 2001. [https://web.archive.org/web/20080315131143/http://www.liaad.up.pt/~jgama/ales/ales_5.html Access] from J.Gama webpage.
* '''STAGGER''', J.C.Schlimmer, R.H.Granger, Incremental Learning from Noisy Data, Mach. Learn., vol.1, no.3, 1986.
* '''Mixed''', J.Gama, P.Medas, G.Castillo, P.Rodrigues, Learning with drift detection, 2004.

===Data generation frameworks===
* L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift,  IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp.&amp;nbsp;730–742, 2010. [http://www.cs.le.ac.uk/people/llm11/opensource/DriftsGenerator.zip Download] from L.Minku webpage.
* Lindstrom P, SJ Delany &amp; B MacNamee (2008) Autopilot: Simulating Changing Concepts in Real Data In: Proceedings of the 19th Irish Conference on Artificial Intelligence &amp; Cognitive Science, D Bridge, K Brown, B O'Sullivan &amp; H Sorensen (eds.) p272-263 [http://www.comp.dit.ie/sjdelany/publications/aics08-pl.pdf PDF]
* Narasimhamurthy A., L.I. Kuncheva, A framework for generating data to simulate changing environments, Proc. IASTED, Artificial Intelligence and Applications, Innsbruck, Austria, 2007, 384–389 [http://www.bangor.ac.uk/~mas00a/papers/anlkAIA07.pdf PDF]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }} [http://pages.bangor.ac.uk/~mas00a/EPSRC_simulation_framework/changing_environments_stage1a.htm Code]

==Projects==
* [http://www.infer.eu/ INFER]: Computational Intelligence Platform for Evolving and Robust Predictive Systems (2010–2014), Bournemouth University (UK), Evonik Industries (Germany), Research and Engineering Centre (Poland)
* [http://www.win.tue.nl/~mpechen/projects/hacdais/ HaCDAIS]: Handling Concept Drift in Adaptive Information Systems (2008–2012), Eindhoven University of Technology (the Netherlands)
* [http://www.liaad.up.pt/~kdus/ KDUS]: Knowledge Discovery from Ubiquitous Streams, INESC Porto and Laboratory of Artificial Intelligence and Decision Support (Portugal)
* [http://www.cs.man.ac.uk/~gbrown/adept/ ADEPT]: Adaptive Dynamic Ensemble Prediction Techniques, University of Manchester (UK), University of Bristol (UK)
* [https://web.archive.org/web/20090309132402/http://www.aladdinproject.org/ ALADDIN]: autonomous learning agents for decentralised data and information networks (2005–2010)

==Meetings==
*2014
** [http://www.ieee-wcci2014.org/accepted-ss.htm] Special Session on &quot;Concept Drift, Domain Adaptation &amp; Learning in Dynamic Environments&quot; @IEEE IJCNN 2014
*2013
** [https://sites.google.com/site/realstream2013/ RealStream] Real-World Challenges for Data Stream Mining Workshop-Discussion at the [[ECML PKDD]] 2013, Prague, Czech Republic.
** [http://aiai2013.cut.ac.cy/leaps-2013/ LEAPS 2013] The 1st International Workshop on Learning stratEgies and dAta Processing in nonStationary environments
*2011
** [http://www.icmla-conference.org/icmla11/LEE.htm LEE 2011] Special Session on Learning in evolving environments and its application on real-world problems at ICMLA'11
** [http://wwwis.win.tue.nl/hacdais2011/ HaCDAIS 2011] The 2nd International Workshop on Handling Concept Drift in Adaptive Information Systems
** [https://web.archive.org/web/20101031152019/http://icais.uni-klu.ac.at/cfp.php ICAIS 2011] Track on Incremental Learning
** [https://web.archive.org/web/20110128002602/http://www.ijcnn2011.org/special_section.php IJCNN 2011] Special Session on Concept Drift and Learning Dynamic Environments
** [http://www.soft-computing.de/CIDUE2011.html CIDUE 2011] Symposium on Computational Intelligence in Dynamic and Uncertain Environments
*2010
** [http://wwwis.win.tue.nl/hacdais2010/ HaCDAIS 2010] International Workshop on Handling Concept Drift in Adaptive Information Systems: Importance, Challenges and Solutions
** [http://www.icmla-conference.org/icmla10/CFP_SpecialSession9.html ICMLA10] Special Session on Dynamic learning in non-stationary environments
** [https://web.archive.org/web/20100425011804/http://www.liaad.up.pt/~jgama/SAC10/ SAC 2010] Data Streams Track at ACM Symposium on Applied Computing
** [https://web.archive.org/web/20100418214526/http://www.ornl.gov/sci/knowledgediscovery/SensorKDD-2010/ SensorKDD 2010] International Workshop on Knowledge Discovery from Sensor Data
** [http://lyle.smu.edu/cse/dbgroup/IDA/StreamKDD2010 StreamKDD 2010] Novel Data Stream Pattern Mining Techniques
** Concept Drift and Learning in Nonstationary Environments at [http://www.wcci2010.org/ IEEE World Congress on Computational Intelligence]
** [http://cig.iet.unipi.it/isda2010/files/MLMD.pdf MLMDS’2010] Special Session on Machine Learning Methods for Data Streams at the 10th International Conference on Intelligent Design and Applications, ISDA’10

==Mailing list==

Announcements, discussions, job postings related to the topic of concept drift
in data mining / machine learning. Posts are moderated.

To subscribe go to the group home page: https://groups.google.com/group/conceptdrift

== Bibliographic references ==
Many papers have been published describing algorithms for concept drift detection. Only reviews, surveys and overviews are here:

===Reviews===

* Krawczyk, B., Minku, L.L., Gama, J., Stefanowski, J., Wozniak, M. (2017). &quot;Ensemble Learning for Data Stream Analysis: a survey&quot;, Information Fusion, Vol 37, pp.&amp;nbsp;132–156,  [https://dx.doi.org/10.1016/j.inffus.2017.02.004 Access]
* Dal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C., &amp; Bontempi, G. (2015). Credit card fraud detection and concept-drift adaptation with delayed supervised information. In 2015 International Joint Conference on Neural Networks (IJCNN) (pp.&amp;nbsp;1–8). IEEE. [http://www.ulb.ac.be/di/map/adalpozz/pdf/IJCNN2015_final.pdf PDF]
* C.Alippi, &quot;Learning in Nonstationary and Evolving Environments&quot;, Chapter in ''Intelligence for Embedded Systems.'' Springer, 2014, 283pp, {{ISBN|978-3-319-05278-6}}.
* C.Alippi, R.Polikar, Special Issue on Learning In Nonstationary and Evolving Environments, IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 25, NO. 1, JANUARY 2014
* Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., &amp; Bontempi, G. (2014). Learned lessons in credit card fraud detection from a practitioner perspective. Expert systems with applications, 41(10), 4915–4928. [http://www.ulb.ac.be/di/map/adalpozz/pdf/FraudDetectionPaper_8.pdf PDF]
* Zliobaite, I., Learning under Concept Drift: an Overview. Technical Report. 2009, Faculty of Mathematics and Informatics, Vilnius University: Vilnius, Lithuania. [http://zliobaite.googlepages.com/Zliobaite_CDoverview.pdf PDF]
* Jiang, J., A Literature Survey on Domain Adaptation of Statistical Classifiers. 2008. [http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.pdf PDF]
* Kuncheva L.I. Classifier ensembles for detecting concept change in streaming data: Overview and perspectives, Proc. 2nd Workshop SUEMA 2008 (ECAI 2008), Patras, Greece, 2008, 5–10, [http://www.bangor.ac.uk/~mas00a/papers/lkSUEMA2008.pdf PDF]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }}
* Gaber, M, M., Zaslavsky, A., and Krishnaswamy, S., Mining Data Streams: A Review, in ACM SIGMOD Record, Vol. 34, No. 1, June 2005, {{ISSN|0163-5808}}
* Kuncheva L.I., Classifier ensembles for changing environments, Proceedings 5th International Workshop on Multiple Classifier Systems, MCS2004, Cagliari, Italy, in F. Roli, J. Kittler and T. Windeatt (Eds.), Lecture Notes in Computer Science, Vol 3077, 2004, 1–15, [http://www.bangor.ac.uk/~mas00a/papers/lkMCS04.pdf PDF]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }}.
* Tsymbal, A., The problem of concept drift: Definitions and related work. Technical Report. 2004, Department of Computer Science, Trinity College: Dublin, Ireland. [https://www.cs.tcd.ie/publications/tech-reports/reports.04/TCD-CS-2004-15.pdf PDF]

==See also==
* [[Data stream mining]]
* [[Data mining]]
* [[Machine learning]]


</text>
      <sha1>03i5a9mq54vrf9sn670m4b237iunujt</sha1>
    </revision>
  </page>
  <page>
    <title>Accuracy paradox</title>
    <ns>0</ns>
    <id>3771060</id>
    <revision>
      <id>814373733</id>
      <parentid>814371756</parentid>
      <timestamp>2017-12-08T12:12:04Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{No footnotes}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4516">The '''accuracy paradox''' for [[predictive analytics]] states that [[predictive modelling|predictive models]] with a given level of [[accuracy]] may have greater [[predictive power]] than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as [[precision and recall]].

Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless.

In an example predictive model for an [[insurance fraud]] application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the [[validity (statistics)|validation]] sample have been carefully checked and it is known which cases are fraudulent. A [[table of confusion]] assists analyzing the quality of the model.  The definition of accuracy, the table of confusion for model M&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;, and the calculation of accuracy for model M&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt; is shown below.

&lt;math&gt;\mathrm{A}(M) = \frac{TN + TP}{TN + FP + FN + TP}&lt;/math&gt;
where
: TN is the number of true negative cases
: FP is the number of false positive cases
: FN is the number of false negative cases
: TP is the number of true positive cases

''Formula 1: Definition of Accuracy''

{| class=&quot;wikitable&quot;
!
!Predicted Negative
!Predicted Positive
|-
|Negative Cases||9,700||150
|-
|Positive Cases||50||100
|}

''Table 1: Table of Confusion for Fraud Model M&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;.''

&lt;math&gt;\mathrm A (M) = \frac{9,700 + 100}{9,700 + 150 + 50 + 100} = 98.0\%&lt;/math&gt;

''Formula 2: Accuracy for model M&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;''

With an accuracy of 98.0% model M&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt; appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting &quot;no fraud&quot;. The table of confusion and the accuracy for this trivial “always predict negative” model M&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt; and the accuracy of this model are shown below.

{| class=&quot;wikitable&quot;
!
!Predicted Negative
!Predicted Positive
|-
|Negative Cases||9,850||0
|-
|Positive Cases||150||0
|}

''Table 2: Table of Confusion for Fraud Model M&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;.''

&lt;math&gt;\mathrm{A}(M) = \frac{9,850 + 0}{9,850 + 150 + 0 + 0} = 98.5\%&lt;/math&gt;

''Formula 3: Accuracy for model M&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;''

Model M&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;reduces the rate of inaccurate predictions from 2% to 1.5%. This is an apparent improvement of 25%. The new model M&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt; shows fewer incorrect predictions and markedly improved accuracy, as compared to the original model M&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt;, but is obviously useless.

The alternative model M&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;Fraud&lt;/sup&gt; does not offer any value to the company for preventing fraud. The less accurate model is more useful than the more accurate model.

Caution is advised when using accuracy in the evaluation of predictive models; it is appropriate only if the cost of a false positive (false alarm) is equal to the cost of a false negative (missed prediction).  Otherwise, a more appropriate [[loss function]] should be determined.

==See also==
*[[Receiver operating characteristic]] for other measures of how good model predictions are.

==References==
{{no footnotes|date=December 2017}}
{{Reflist}}

==General references==
{{refbegin}}
* {{citation |last=Zhu |first=Xingquan |title=Knowledge Discovery and Data Mining: Challenges and Realities |publisher=IGI Global | url=https://books.google.com/books?id=zdJQAAAAMAAJ&amp;q=data+mining+challenges+and+realities&amp;dq=data+mining+challenges+and+realities |year=2007 |isbn=978-1-59904-252-7 |pages=118–119}}
* {{doi|10.1117/12.785623}}
* {{cite web | pages=86–87 | url=http://www.utwente.nl/ewi/trese/graduation_projects/2009/Abma.pdf | title=Evaluation of requirements management tools with support for traceability-based change impact analysis | first=B.J.M. | last=Abma | date=September 10, 2009 | work=Master's thesis | publisher=[[University of Twente]] | location=The Netherlands }}
{{refend}}



</text>
      <sha1>gqoqjwdp61gp0ur63ft8c4j0q46kwzm</sha1>
    </revision>
  </page>
  <page>
    <title>Concept learning</title>
    <ns>0</ns>
    <id>6968451</id>
    <revision>
      <id>814605984</id>
      <parentid>814377940</parentid>
      <timestamp>2017-12-09T21:05:02Z</timestamp>
      <contributor>
        <ip>46.193.1.146</ip>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23843">{{multiple issues|
{{disputed|date=March 2014}}
{{contradict|date=March 2014}}
{{unfocused|date=March 2014}}
{{confusing|date=March 2014}}
{{expert needed|date=May 2011}}
{{Refimprove|date=April 2009}}
}}

'''Concept learning''', also known as '''category learning''', '''concept attainment''', and '''concept formation''', is defined by [[Jerome Bruner|Bruner]], Goodnow, &amp; Austin (1967) as &quot;the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories&quot;. More simply put, concepts are the mental categories that help us classify objects, events, or ideas, building on the understanding that each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features.

Concept learning also refers to a learning task in which a human or machine learner is trained to classify objects by being shown a set of example objects along with their class labels. The learner simplifies what has been observed by condensing it in the form of an example. This simplified version of what has been learned is then applied to future examples. Concept learning may be simple or complex because learning takes place over many areas. When a concept is difficult, it is less likely that the learner will be able to simplify, and therefore will be less likely to learn. Colloquially, the task is known as ''learning from examples.'' Most theories of concept learning are based [[Exemplar theory|on the storage of exemplars]] and avoid summarization or overt abstraction of any kind.

*Concept Learning: Inferring a Boolean-valued function from training examples of its input and output.
*A concept is an idea of something formed by combining all its features or attributes which construct the given concept. Every concept has two components:
*Attributes: features that one must look for to decide whether a data instance is a positive one of the concept.
*A rule: denotes what conjunction of constraints on the attributes will qualify as a positive instance of the concept.

== Types of concepts ==
Concept learning must be distinguished from learning by reciting something from memory (recall) or discriminating between two things that differ (discrimination). However, these issues are closely related, since memory recall of facts could be considered a &quot;trivial&quot; conceptual process where prior exemplars representing the concept are invariant. Similarly, while discrimination is not the same as initial concept learning, discrimination processes are involved in refining concepts by means of the repeated presentation of exemplars.

'''Concrete or Perceptual Concepts vs Abstract Concepts'''

'''Defined (or Relational) and Associated Concepts'''

'''Complex Concepts'''. Constructs such as a [[Schema (psychology)|schema]] and a script are examples of complex concepts. A schema is an organization of smaller concepts (or features) and is revised by situational information to assist in comprehension. A script on the other hand is a list of actions that a person follows in order to complete a desired goal. An example of a script would be the process of buying a CD. There are several actions that must occur before the actual act of purchasing the CD and a script provides a sequence of the necessary actions and proper order of these actions in order to be successful in purchasing the CD.

== Methods of learning a concept ==
'''Discovery''' – Every baby discovers concepts for itself, such as discovering that each of its fingers can be individually controlled or that care givers are individuals.  Although this is perception driven, formation of the concept is more than memorizing perceptions.

'''Examples''' – Supervised or unsupervised generalizing from examples may lead to learning a new concept, but concept formation is more than generalizing from examples.

'''Words''' – Hearing or reading new words leads to learning new concepts, but forming a new concept is more than learning a dictionary definition.  A person may have previously formed a new concept before encountering the word or phrase for it.

'''Exemplars comparison and contrast''' – An efficient way to learn new categories and to induce new categorization rules is by comparing a few example objects while being informed about their categorical relation. Comparing two exemplars while being informed that the two are from the same category allows identifying the attributes shared by the category members, as it exemplifies variability within this category. On the other hand, contrasting two exemplars while being informed that the two are from different categories may allow identifying attributes with diagnostic value. Within category comparison and between categories contrast are not similarly useful for category learning (Hammer et al., 2008), and the capacity to use these two forms of comparison-based learning changes at childhood (Hammer et al., 2009).

'''Invention''' – When prehistoric people who lacked tools used their fingernails to scrape food from killed animals or smashed melons, they noticed that a broken stone sometimes had a sharp edge like a fingernail and was therefore suitable for scraping food. Inventing a stone tool to avoid broken fingernails was a new concept.

== Theoretical issues ==

In general, the theoretical issues underlying concept learning are those underlying [[Inductive reasoning|induction]]. These issues are addressed in many diverse publications, including literature on subjects like [[Version Spaces]], [[Statistical Learning Theory]], [[PAC Learning]], [[Information Theory]], and [[Algorithmic Information Theory]]. Some of the broad theoretical ideas are also discussed by Watanabe (1969,1985), Solomonoff (1964a,1964b), and Rendell (1986); see the reference list below.

== Modern psychological theories ==

It is difficult to make any general statements about human (or animal) concept learning without already assuming a particular psychological theory of concept learning. Although the classical views of [[concept]]s and concept learning in philosophy speak of a process of [[abstraction]], [[data compression]], simplification, and summarization, currently popular psychological theories of concept learning diverge on all these basic points. The history of psychology has seen the rise and fall of many theories about concept learning.  [[Classical conditioning]] (as defined by [[Ivan Pavlov|Pavlov]]) created the earliest experimental technique.  [[Reinforcement learning]] as described by [[John B. Watson|Watson]] and elaborated by [[Clark Hull]] created a lasting paradigm in [[behavioral psychology]].  [[Cognitive psychology]] emphasized a computer and information flow metaphor for concept formation.  [[Neural network]] models of concept formation and the structure of knowledge have opened powerful hierarchical models of knowledge organization such as [[George Armitage Miller|George Miller]]'s [[Wordnet]].  Neural networks are based on computational models of learning using [[factor analysis]] or [[convolution]].  Neural networks also are open to [[neuroscience]] and [[psychophysiological]] models of learning following [[Karl Lashley]] and [[Donald Hebb]].

=== Rule-based===
Rule-based theories of concept learning began with [[cognitive psychology]] and early computer models of learning that might be implemented in a high level computer language with computational statements such as [[Conditional (computer programming)#If–then(–else)|if:then]] production rules.  They take classification data and a rule-based theory as input which are the result of a rule-based learner with the hopes of producing a more accurate model of the data (Hekenaho 1997). The majority of rule-based models that have been developed are heuristic, meaning that rational analyses have not been provided and the models are not related to statistical approaches to induction. A rational analysis for rule-based models could presume that concepts are represented as rules, and would then ask to what degree of belief a rational agent should be in agreement with each rule, with some observed examples provided (Goodman, Griffiths, Feldman, and Tenenbaum). Rule-based theories of concept learning are focused more so on [[perceptual learning]] and less on definition learning. Rules can be used in learning when the stimuli are confusable, as opposed to simple. When rules are used in learning, decisions are made based on properties alone and rely on simple criteria that do not require a lot of memory ( Rouder and Ratcliff, 2006).

Example of rule-based theory:

&quot;A radiologist using rule-based categorization would observe
whether specific properties of an X-ray image meet certain
criteria; for example, is there an extreme difference in brightness
in a suspicious region relative to other regions? A decision is
then based on this property alone.&quot; (see Rouder and Ratcliff 2006)

=== Prototype ===

The prototype view of concept learning holds that people abstract out the central tendency (or prototype) of the examples experienced and use this as a basis for their categorization decisions.

The prototype view of concept learning holds that people categorize based on one or more central examples of a given category followed by a penumbra of decreasingly typical examples. This implies that people do not categorize based on a list of things that all correspond to a definition, but rather on a hierarchical inventory based on semantic similarity to the central example(s).

To illustrate, imagine the following [[mental representation]]s of the category: Sports

The first illustration demonstrates a mental representation if we were to categorize by definition:

Definition of Sports: an athletic activity requiring skill or physical prowess and often of a competitive nature.

                                     Basketball   Football    Bowling
                          Baseball                                     Skiing
 	         Track and field	                             	   Snowboarding
             Lacrosse                                                                    rugby
                 Soccer				  Sports                       Skateboarding
 		     Golf				                    Bike-Racing
                        Hockey				                Surfing
                                   Weightlifting              Tennis

The second illustration demonstrates a mental representation that prototype theory would predict:

1. Baseball&lt;br&gt;
2. Football&lt;br&gt;
3. Basketball&lt;br&gt;
4. Soccer&lt;br&gt;
5. Hockey&lt;br&gt;
6. Tennis&lt;br&gt;
7. Golf&lt;br&gt;
...&lt;br&gt;
15. Bike-racing&lt;br&gt;
16. Weightlifting&lt;br&gt;
17. Skateboarding&lt;br&gt;
18. Snowboarding&lt;br&gt;
19. Boxing&lt;br&gt;
20. Wrestling&lt;br&gt;
...&lt;br&gt;
32. Fishing&lt;br&gt;
33. Hunting&lt;br&gt;
34. Hiking&lt;br&gt;
35. Sky-diving&lt;br&gt;
36. Bungee-jumping&lt;br&gt;
...&lt;br&gt;
62. Cooking&lt;br&gt;
63. Walking&lt;br&gt;
...&lt;br&gt;
82. Gatorade&lt;br&gt;
83. Water&lt;br&gt;
84. Protein&lt;br&gt;
85. Diet

It is evident that prototype theory hypothesizes a more continuous (less discrete) way of categorization in which the list of things that match the category's definition is not limited.

=== Exemplar ===

Exemplar theory is the storage of specific instances (exemplars), with new objects evaluated only with respect to how closely they resemble specific known members (and nonmembers) of the category. This theory hypothesizes that learners store examples ''verbatim''. This theory views concept learning as highly simplistic. Only individual properties are represented. These individual properties are not abstract and they do not create rules. An example of what exemplar theory might look like is, &quot;water is wet&quot;. It is simply known that some (or one, or all) stored examples of water have the property wet. Exemplar based theories have become more empirically popular over the years with some evidence suggesting that human learners use exemplar based strategies only in early learning, forming prototypes and generalizations later in life. An important result of exemplar models in psychology literature has been a de-emphasis of complexity in concept learning. One of the best known exemplar theories of concept learning is the Generalized Context Model (GCM).

A problem with exemplar theory is that exemplar models critically depend on two measures: similarity between exemplars, and having a rule to determine group membership. Sometimes it is difficult to attain or distinguish these measures.

=== Multiple-prototype===

More recently, cognitive psychologists have begun to explore the idea that the prototype and exemplar models form two extremes. It has been suggested that people are able to form a multiple prototype representation, besides the two extreme representations. For example, consider the category 'spoon'. There are two distinct subgroups or conceptual clusters: spoons tend to be either large and wooden, or small and made of metal. The prototypical spoon would then be a medium-size object made of a mixture of metal and wood, which is clearly an unrealistic proposal. A more natural representation of the category 'spoon' would instead consist of multiple (at least two) prototypes, one for each cluster. A number of different proposals have been made in this regard (Anderson, 1991; Griffiths, Canini, Sanborn &amp; Navarro, 2007; Love, Medin &amp; Gureckis, 2004; Vanpaemel &amp; Storms, 2008). These models can be regarded as providing a compromise between exemplar and prototype models.

=== Explanation-based ===

The basic idea of explanation-based learning suggests that a new concept is acquired by experiencing examples of it and forming a basic outline.{{ref|1|1}} Put simply, by observing or receiving the qualities of a thing the mind forms a concept which possesses and is identified by those qualities.

The original theory, proposed by Mitchell, Keller, and Kedar-Cabelli in 1986 and called explanation-based generalization, is that learning occurs through progressive generalizing.{{ref|2|2}} This theory was first developed to program machines to learn. When applied to human cognition, it translates as follows: the mind actively separates information that applies to more than one thing and enters it into a broader description of a category of things.  This is done by identifying sufficient conditions for something to fit in a category, similar to schematizing.

The revised model revolves around the integration of four mental processes – generalization, chunking, operationalization, and analogy{{ref|3|3}}.

* Generalization is the process by which the characteristics of a concept which are fundamental to it are recognized and labeled. For example, birds have feathers and wings. Anything with feathers and wings will be identified as ‘bird’.
* When information is grouped mentally, whether by similarity or relatedness, the group is called a chunk. Chunks can vary in size from a single item with parts or many items with many parts.{{ref|4|4}}
* A concept is operationalized when the mind is able to actively recognize examples of it by characteristics and label it appropriately.{{ref|5|5}}
* Analogy is the recognition of similarities among potential examples.{{ref|6|6}}

This particular theory of concept learning is relatively new and more research is being conducted to test it.

=== Bayesian ===

Bayes' theorem is important because it provides a powerful tool for understanding, manipulating and controlling data&lt;sup&gt;5&lt;/sup&gt; that takes a larger view that is not limited to data analysis alone&lt;sup&gt;6&lt;/sup&gt;.  The approach is subjective, and this requires the assessment of prior probabilities&lt;sup&gt;6&lt;/sup&gt;, making it also very complex.  However, if Bayesians show that the accumulated evidence and the application of Bayes' law are sufficient, the work will overcome the subjectivity of the inputs involved&lt;sup&gt;7&lt;/sup&gt;.  Bayesian inference can be used for any honestly collected data and has a major advantage because of its scientific focus&lt;sup&gt;6&lt;/sup&gt;.

One model that incorporates the Bayesian theory of concept learning is the [[ACT-R]] model, developed by [[John Robert Anderson (psychologist)|John R. Anderson]]&lt;!--there is no such thing as a bayesian theory of concept learning in the basic ACT-R system. Source?--&gt;.{{Citation needed|date=April 2009}} The ACT-R model is a programming language that defines the basic cognitive and perceptual operations that enable the human mind by producing a step-by-step simulation of human behavior.  This theory exploits the idea that each task humans perform consists of a series of discrete operations.  The model has been applied to learning and memory, higher level cognition, natural language, perception and attention, human-computer interaction, education, and computer generated forces.{{Citation needed|date=April 2009}}

In addition to John R. Anderson, [[Joshua Tenenbaum]] has been a contributor to the field of concept learning; he studied the computational basis of human learning and inference using behavioral testing of adults, children, and machines from Bayesian statistics and probability theory, but also from geometry, graph theory, and linear algebra.  Tenenbaum is working to achieve a better understanding of human learning in computational terms and trying to build computational systems that come closer to the capacities of human learners.

=== Component display theory ===

M. D. Merrill's component display theory (CDT) is a cognitive matrix that focuses on the interaction between two dimensions: the level of performance expected from the learner and the types of content of the material to be learned.  Merrill classifies a learner's level of performance as: find, use, remember, and material content as: facts, concepts, procedures, and principles.  The theory also calls upon four primary presentation forms and several other secondary presentation forms.  The primary presentation forms include: rules, examples, recall, and practice.  Secondary presentation forms include: prerequisites, objectives, helps, mnemonics, and feedback.  A complete lesson includes a combination of primary and secondary presentation forms, but the most effective combination varies from learner to learner and also from concept to concept. Another significant aspect of the CDT model is that it allows for the learner to control the instructional strategies used and adapt them to meet his or her own learning style and preference. A major goal of this model was to reduce three common errors in concept formation: over-generalization, under-generalization and misconception.

== See also ==
* [[Sample exclusion dimension]]

== References ==
{{Reflist}}

* {{cite journal
  | last = Rouder
  | first = Jeffrey
  | title = Comparing Exemplar and Rule-Based Theories of Categorization
  | journal = Current Directions in Psychological Science
  | volume = 15
  | pages = 9–13
  | year = 2006
  | doi = 10.1111/j.0963-7214.2006.00397.x
  | last2 = Ratcliff
  | first2 = Roger}}
* {{cite web
|url= http://www.mit.edu/~ndg/papers/op322-goodman.pdf
|title= A Rational Analysis of Rule-based Concept Learning|
accessdate=2007-12-04 |format=PDF}}
*{{cite web
|url= http://citeseer.ist.psu.edu/85278.html
|title= GA-based Rule Enhancement in Concept Learning|
accessdate=2007-12-04 }}
* Bruner, J., Goodnow, J. J., &amp; Austin, G. A. (1967). A study of thinking. New York: Science Editions.
*{{cite journal
  | last = Feldman
  | first = Jacob
  | title = The Simplicity Principle in Human Concept Learning
  | journal = Current Directions in Psychological Science
  | volume = 12
  | pages = 227–232
  | year = 2003
  | doi=10.1046/j.0963-7214.2003.01267.x}}
*{{cite journal
  | last = Rendell
  | first = Larry
  | title = A general framework for induction and a study of selective induction
  | journal = Machine Learning
  | volume = 1
  | pages = 177–226
  | year = 1986
  | doi = 10.1007/BF00114117
  | issue = 2}}
*{{cite journal
  | last = Hammer
  | first = Rubi
  | title = Comparison processes in category learning: From theory to behavior
  | journal = Brain Research
  | volume = 1225
  | pages = 102–118
  | year = 2008
  | doi = 10.1016/j.brainres.2008.04.079
  | issue = 15
  | pmid=18614160}}
*{{cite journal
  | last = Hammer
  | first = Rubi
  | title = The development of category learning strategies: What makes the difference?
  | journal = Cognition
  | volume = 112
  | pages = 105–119
  | year = 2009
  | doi = 10.1016/j.cognition.2009.03.012
  | issue = 1
  | pmid=19426967}}
*{{cite book
  | last = Watanabe
  | first = Satosi
  | title = Knowing and Guessing: A Quantitative Study of Inference and Information
  | publisher = Wiley
  | year = 1969
  | location = New York}}
*{{cite book
  | last = Watanabe
  | first = Satosi
  | title = Pattern Recognition: Human and Mechanical
  | publisher = Wiley
  | year = 1985
  | location = New York}}
*{{cite journal
  | last = Solomonoff
  | first = R. J.
  | title = A formal theory of inductive inference. Part I
  | journal = Information and Control
  | volume = 7
  | issue = 1
  | pages = 1–22
  | year = 1964
  | doi = 10.1016/S0019-9958(64)90223-2}}
*{{cite journal
  | last = Solomonoff
  | first = R. J.
  | title = A formal theory of inductive inference. Part II
  | journal = Information and Control
  | volume = 7
  | issue = 2
  | pages = 224–254
  | year = 1964
  | doi = 10.1016/S0019-9958(64)90131-7}}
*{{cite web
  |url= http://web.mit.edu/bcs/people/tenenbaum.shtml
  |title= Brain and Cognitive Sciences
  |accessdate= 2007-11-23
  |publisher= Massachusetts Institute of Technology}}
*{{cite web
  | last = Kearsley
  | first = Greg
  | title = Component Display Theory (M.D. Merrill)
  | year = Copyright 1994–2007
  | url = http://icebreakerideas.com/learning-theories/#Component_Display_Theory_MD_Merrill
  | accessdate = 2007-12-04 }}
*{{cite web
  | last = Kearsley
  | first = Greg
  | title = Concept
  | year = Copyright 1994–2007
  | url = http://tip.psychology.org/concept.html
  | accessdate = 2007-12-04 }}
*{{cite web
  | title = Component Display Theory
  | date = 2007-04-10
  | url = http://moogl.wordpress.com/2007/04/10/component-display-theory/
  | accessdate = 2007-12-04 }}
*{{cite web
  | title = Concept Attainment
  | year = 1999
  | url = http://www.lovinlearning.org/concept/
  | accessdate = 2007-12-04 }}
*{{cite web
  | title = Concept Learning
  | date = 2007-11-07
  | url = http://edutechwiki.unige.ch/en/Concept_learning
  | accessdate = 2007-12-04 }}
*{{cite web
  | title = Concept Formation
  | year = 2007
  | url = http://edutechwiki.unige.ch/en/Concept_learning
  | publisher = The McGraw-Hill Companies
  | accessdate = 2007-12-04 }}
*&lt;sup&gt;6&lt;/sup&gt;{{cite journal
  | last= Berry
  | first= Donald A.
  | title= Teaching Elementary Bayesian Statistics with Real Applications in Science
  | journal= The American Statistician
  | volume= 5
  | issue= 3
  | pages= 241–246
  | date= 1997–1998
  | doi=10.1080/00031305.1997.10473970}}
*&lt;sup&gt;7&lt;/sup&gt;{{cite journal
  | last= Brown
  | first= Harold I.
  | title= Reason, Judgment and Bayes's Law
  | journal= Philosophy of Science
  | volume= 61
  | issue= 3
  | pages= 351–369
  | year= 1994
  | doi= 10.1086/289808}}
*&lt;sup&gt;5&lt;/sup&gt;{{cite journal
  | last= Lindley
  | first= Dennis V.
  | title= Theory and Practice of Bayesian Statistics
  | journal= The Statistician
  | volume= 32
  | issue= 1/2
  | pages= 1–11
  | year= 1983
  | doi= 10.2307/2987587
  | publisher= Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 32, No. 1
  | jstor= 2987587}}

[[Category:Learning theory (education)]]
</text>
      <sha1>0hnntez0iis5jdmwk471kt10ixrsmvf</sha1>
    </revision>
  </page>
  <page>
    <title>Robot learning</title>
    <ns>0</ns>
    <id>3290880</id>
    <revision>
      <id>813843054</id>
      <parentid>803586632</parentid>
      <timestamp>2017-12-05T14:52:00Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v477)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9527">'''Robot learning''' is a research field at the intersection of [[machine learning]] and [[robotics]]. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).

Example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization,  as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation.

Robot learning can be closely related to [[adaptive control]], [[reinforcement learning]] as well as [[developmental robotics]] which considers the problem of autonomous lifelong acquisition of repertoires of skills.
While [[machine learning]] is frequently used by [[computer vision]] algorithms employed in the context of robotics, these applications are usually not referred to as &quot;robot learning&quot;.

&lt;!--
==Methods==
See: http://www.allaboutcircuits.com/news/google-tasks-robots-with-learning-skills-from-one-another-via-cloud-robotic/
---&gt;

==Projects==
{{Expand section|date=January 2017}}
Maya Cakmak, assistant professor of computer science and engineering at the [[University of Washington]], is trying to create a robot that learns by imitating - a technique called &quot;programming by demonstration&quot;. A researcher shows it a cleaning technique for the robot's vision system and it generalizes the cleaning motion from the human demonstration as well as identifying the &quot;state of dirt&quot; before and after cleaning.&lt;ref&gt;{{cite web|last1=Rosenblum|first1=Andrew|title=The robot you want most is far from reality|url=https://www.technologyreview.com/s/602128/the-robot-you-want-most-is-far-from-reality/|publisher=MIT Technology Review|accessdate=4 January 2017}}&lt;/ref&gt;

Similarly the [[Baxter (robot)|Baxter]] industrial robot can be taught how to do something by grabbing its arm and showing it the desired movements.&lt;ref&gt;{{cite web|title=Hands-on with Baxter, the factory robot of the future|url=https://arstechnica.com/gadgets/2014/06/hands-on-with-baxter-the-factory-robot-of-the-future/|publisher=Ars Technica|accessdate=4 January 2017}}&lt;/ref&gt; It can also use deep learning to teach itself to grasp an unknown object.&lt;ref&gt;{{cite web|title=Deep-Learning Robot Takes 10 Days to Teach Itself to Grasp|url=https://www.technologyreview.com/s/542076/deep-learning-robot-takes-10-days-to-teach-itself-to-grasp/|publisher=MIT Technology Review|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref name=technologyreview1/&gt;

==Sharing learned skills and knowledge==
{{Further|Cloud robotics}}
In [[Telex]]'s &quot;Million Object Challenge&quot; the goal is robots that learn how to spot and handle simple items and upload their data to the cloud to allow other robots to analyze and use the information.&lt;ref name=&quot;technologyreview1&quot;&gt;{{cite web|last1=Schaffer|first1=Amanda|title=10 Breakthrough Technologies 2016: Robots That Teach Each Other|url=https://www.technologyreview.com/s/600768/10-breakthrough-technologies-2016-robots-that-teach-each-other/|publisher=MIT Technology Review|accessdate=4 January 2017}}&lt;/ref&gt;

[[RoboBrain]] is a knowledge engine for robots which can be freely accessed by any device wishing to carry out a task. The database gathers new information about tasks as robots perform them, by searching the Internet, interpreting natural language text, images, and videos, [[object recognition]] as well as interaction. The project is led by Ashutosh Saxena at [[Stanford University]].&lt;ref&gt;{{cite web|title=RoboBrain: The World's First Knowledge Engine For Robots|url=https://www.technologyreview.com/s/533471/robobrain-the-worlds-first-knowledge-engine-for-robots/|publisher=MIT Technology Review|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Hernandez|first1=Daniela|title=The Plan to Build a Massive Online Brain for All the World’s Robots|url=https://www.wired.com/2014/08/robobrain/|publisher=WIRED|accessdate=4 January 2017}}&lt;/ref&gt;

[[RoboEarth]] is a project that has been described as a &quot;[[World Wide Web]]&lt;!--and [[Wikipedia]]--&gt; for robots&quot; − it is a network and database repository where robots can share information and learn from each other and a cloud for outsourcing heavy computation tasks. The project brings together researchers from five major universities in Germany, the Netherlands and Spain and is backed by the [[European Union]].&lt;ref&gt;{{cite web|title=Europe launches RoboEarth: 'Wikipedia for robots'|url=https://www.usatoday.com/story/tech/2014/01/17/robot-robotics-roboearth-europe-munich-netherlands/4575021/|publisher=USA TODAY|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=European researchers have created a hive mind for robots and it's being demoed this week|url=https://www.engadget.com/2014/01/14/roboearth-demo/|publisher=Engadget|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Robots test their own world wide web, dubbed RoboEarth|url=http://www.bbc.com/news/technology-25727110|publisher=BBC News|accessdate=4 January 2017|date=14 January 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title='Wikipedia for robots': Because bots need an Internet too|url=https://www.cnet.com/news/wikipedia-for-robots-because-bots-need-an-internet-too/|publisher=CNET|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=New Worldwide Network Lets Robots Ask Each Other Questions When They Get Confused|url=http://www.popsci.com/technology/article/2013-03/new-cloud-engine-robots-can-learn-each-other|publisher=Popular Science|accessdate=4 January 2017}}&lt;/ref&gt;

Google Research, [[DeepMind]], and [[Google X]] have decided to allow their robots share their experiences.&lt;ref&gt;{{cite web|title=Google Tasks Robots with Learning Skills from One Another via Cloud Robotics|url=http://www.allaboutcircuits.com/news/google-tasks-robots-with-learning-skills-from-one-another-via-cloud-robotic/|website=allaboutcircuits.com|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Tung|first1=Liam|title=Google's next big step for AI: Getting robots to teach each other new skills {{!}} ZDNet|url=http://www.zdnet.com/article/googles-next-big-step-for-ai-getting-robots-to-teach-each-other-new-skills/|publisher=ZDNet|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=How Robots Can Acquire New Skills from Their Shared Experience|url=https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html|publisher=Google Research Blog|accessdate=4 January 2017}}&lt;/ref&gt;

==See also==
{{Portal|Robotics|AI}}
* [[Developmental robotics]]
* [[Cognitive robotics]]
* [[Evolutionary robotics]]
* [[Institute of Robotics in Scandinavia AB]]

==References==
{{reflist}}

==External links==
* [http://www.ieee-ras.org/robot-learning IEEE RAS Technical Committee on Robot Learning (official IEEE website)]
* [http://learning-robots.de IEEE RAS Technical Committee on Robot Learning (TC members website)]
* [http://robot-learning.de  Robot Learning at the Max Planck Institute for Intelligent Systems and the Technical University Darmstadt]
* [http://www-clmc.usc.edu Robot Learning at the Computational Learning and Motor Control lab]
* [http://www.cns.atr.jp/ccc/en/ Humanoid Robot Learning at the Advanced Telecommunication Research Center (ATR)] {{en icon}} {{jp icon}}
* [http://lasa.epfl.ch Learning Algorithms and Systems Laboratory at EPFL (LASA)]
* [http://www.idsia.ch/~juergen/learningrobots.html Robot Learning] at the [http://www.idsia.ch/~juergen/cogbotlab.html Cognitive Robotics Lab] of [[Juergen Schmidhuber]] at [[IDSIA]] and [[Technical University of Munich]]
* [http://humanoid.fy.chalmers.se/ The Humanoid Project]: [[Peter Nordin]], [[Chalmers University of Technology]]
* [http://flowers.inria.Fr Inria and Ensta ParisTech FLOWERS team, France]: Autonomous lifelong learning in developmental robotics
* [https://www.cit-ec.de/ CITEC at University of Bielefeld, Germany]
* [http://www.er.ams.eng.osaka-u.ac.jp/asadalab/index_en.html Asada Laboratory], Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan
* [http://www-robotics.cs.umass.edu/index.php The Laboratory for Perceptual Robotics], [[University of Massachusetts Amherst]] Amherst, USA
* [http://www.tech.plym.ac.uk/SOCCE/CRNS/ Centre for Robotics and Neural Systems], [http://www.plymouth.ac.uk/ Plymouth University] Plymouth, United Kingdom
* [http://www.cs.cmu.edu/~rll/ Robot Learning Lab] at [[Carnegie Mellon University]]
* [http://www.nimbro.net Project Learning Humanoid Robots] at [[University of Bonn]]
* [http://www.skilligent.com/ Skilligent Robot Learning and Behavior Coordination System (commercial product)]
* [http://www.cs.cornell.edu/Courses/cs4758/ Robot Learning class] at [[Cornell University]]
* [http://www.iit.it/en/advr-labs/learning-and-interaction.html Robot Learning and Interaction Lab] at [[Italian Institute of Technology]]
* [http://www.dcsc.tudelft.nl/~robotics/media.html Reinforcement learning for robotics] at [[Delft University of Technology]]

{{Robotics}}

[[Category:Robot control|Learning]]

</text>
      <sha1>psguct90b2y4q9a4joajhzdi1vfhpz9</sha1>
    </revision>
  </page>
  <page>
    <title>Version space learning</title>
    <ns>0</ns>
    <id>7578809</id>
    <revision>
      <id>808610513</id>
      <parentid>808589103</parentid>
      <timestamp>2017-11-03T23:25:32Z</timestamp>
      <contributor>
        <username>JohnBlackburne</username>
        <id>3082175</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/77.176.192.210|77.176.192.210]] ([[User talk:77.176.192.210|talk]]): No rationale given for changing, to a template still a WIP. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7404">[[Image:Version space.png|thumb|right|300px|Version space for a &quot;rectangle&quot; hypothesis language in two dimensions.  Green pluses are positive examples, and red circles are negative examples.  GB is the maximally '''general''' positive hypothesis boundary, and SB is the maximally '''specific''' positive hypothesis boundary.  The intermediate (thin) rectangles represent the hypotheses in the version space.]]
'''Version space learning''' is a [[Symbolic artificial intelligence|logical]] approach to [[machine learning]], specifically [[binary classification]]. Version space learning algorithms search a predefined space of [[hypothesis|hypotheses]], viewed as a set of [[Sentence (logic)|logical sentences]]. Formally, the hypothesis space is a [[Logical disjunction|disjunction]]&lt;ref name=&quot;aima&quot;&gt;{{Cite AIMA|2|pages=683–686}}&lt;/ref&gt;

:&lt;math&gt;H_1 \lor H_2 \lor ... \lor H_n&lt;/math&gt;

(i.e., either hypothesis 1 is true, or hypothesis 2, or any subset of the hypotheses 1 through {{mvar|n}}). A version space learning algorithm is presented with examples, which it will use to restrict its hypothesis space; for each example {{mvar|x}}, the hypotheses that are [[Consistency|inconsistent]] with {{mvar|x}} are removed from the space.&lt;ref name=&quot;Mitchel-1982&quot;/&gt; This iterative refining of the hypothesis space is called the '''candidate elimination''' algorithm, the hypothesis space maintained inside the algorithm its ''version space''.{{r|aima}}

==The version space algorithm==
In settings where there is a generality-ordering on hypotheses, it is possible to represent the version space by two sets of hypotheses: (1) the '''most specific''' consistent hypotheses, and (2) the '''most general''' consistent hypotheses, where &quot;consistent&quot; indicates agreement with observed data.

The most specific hypotheses (i.e., the specific boundary '''SB''') cover the observed positive training examples, and as little of the remaining feature space as possible.  These hypotheses, if reduced any further, ''exclude'' a ''positive'' training example, and hence become inconsistent.  These minimal hypotheses essentially constitute a (pessimistic) claim that the true concept is defined just by the ''positive'' data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be negative.  (I.e., if data has not previously been ruled in, then it's ruled out.)

The most general hypotheses (i.e., the general boundary '''GB''') cover the observed positive training examples, but also cover as much of the remaining feature space without including any negative training examples.  These, if enlarged any further, ''include'' a ''negative'' training example, and hence become inconsistent.  These maximal hypotheses essentially constitute a (optimistic) claim  that the true concept is defined just by the ''negative'' data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be positive. (I.e., if data has not previously been ruled out, then it's ruled in.)

Thus, during learning, the version space (which itself is a set – possibly infinite – containing ''all'' consistent hypotheses) can be represented by just its lower and upper bounds (maximally general and maximally specific hypothesis sets), and learning operations can be performed just on these representative sets.

After learning, classification can be performed on unseen examples by testing the hypothesis learned by the algorithm. If the example is consistent with multiple hypotheses, a majority vote rule can be applied.{{r|aima}}

==Historical background==
The notion of version spaces was introduced by Mitchell in the early 1980s{{r|Mitchel-1982}} as a framework for understanding the basic problem of supervised learning within the context of [[state space search|solution search]].  Although the basic &quot;'''candidate elimination'''&quot; search method that accompanies the version space framework is not a popular learning algorithm, there are some practical implementations that have been developed (e.g., Sverdlik &amp; Reynolds 1992, Hong &amp; Tsang 1997, Dubois &amp; Quafafou 2002).

A major drawback of version space learning is its inability to deal with noise: any pair of inconsistent examples can cause the version space to ''collapse'', i.e., become empty, so that classification becomes impossible.{{r|aima}}

==See also==
*[[Formal concept analysis]]
*[[Inductive logic programming]]
*[[Rough set]]. [The rough set framework focuses on the case where ambiguity is introduced by an impoverished '''feature set'''. That is, the target concept cannot be decisively described because the available feature set fails to disambiguate objects belonging to different categories.  The version space framework focuses on the (classical induction) case where the ambiguity is introduced by an impoverished '''data set'''.  That is, the target concept cannot be decisively described because the available data fails to uniquely pick out a hypothesis.  Naturally, both types of ambiguity can occur in the same learning problem.]
* {{Harvtxt|Rendell|1986}}&lt;ref name=&quot;Rendell 1986&quot;/&gt; provides an interesting discussion of the general problem of induction.
* Mill (1843/2002) is the classic source on [[inductive logic]].

==References==
&lt;references&gt;
&lt;ref name=&quot;Mitchel-1982&quot;&gt;
{{cite journal
  | last = Mitchell
  | first = Tom M.
  | title = Generalization as search
  | journal = Artificial Intelligence
  | volume = 18
  | issue = 2
  | pages = 203–226
  | year = 1982
  | doi = 10.1016/0004-3702(82)90040-6}}
&lt;/ref&gt;
&lt;ref name=&quot;Rendell 1986&quot;&gt;
{{cite journal
  | last = Rendell
  | first = Larry
  | title = A general framework for induction and a study of selective induction
  | journal = Machine Learning
  | volume = 1
  | pages = 177–226
  | year = 1986
  | doi = 10.1007/BF00114117
  | issue = 2}}
&lt;/ref&gt;
&lt;/references&gt;

*{{cite conference
  | first = Vincent
  | last = Dubois
  |author2=Quafafou, Mohamed
   | title = Concept learning with approximation: Rough version spaces
  | booktitle = Rough Sets and Current Trends in Computing: Proceedings of the Third International Conference, RSCTC 2002
  | pages = 239–246
  | year = 2002
  | location = Malvern, Pennsylvania}}
*{{cite journal
  | last = Hong
  | first = Tzung-Pai
  |author2=Shian-Shyong Tsang
   | title = A generalized version space learning algorithm for noisy and uncertain data
  | journal = IEEE Transactions on Knowledge and Data Engineering
  | volume = 9
  | issue = 2
  | pages = 336–340
  | year = 1997
  | doi = 10.1109/69.591457}}
*{{cite book
  | last = Mill
  | first = John Stuart
  | title = A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Principles of Evidence and the Methods of Scientific Investigation
  | publisher = University Press of the Pacific
  | date = 1843/2002
  | location = Honolulu, HI}}
*{{cite book
  | last = Mitchell
  | first = Tom M.
  | title = Machine Learning
  | publisher = McGraw-Hill
  | year = 1997
  | location = Boston}}
*{{cite conference
  | first = W.
  | last = Sverdlik
  |author2=Reynolds, R.G.
   | title = Dynamic version spaces in machine learning
  | booktitle = Proceedings, Fourth International Conference on Tools with Artificial Intelligence (TAI '92)
  | pages = 308–315
  | year = 1992
  | location = Arlington, VA}}

</text>
      <sha1>c9epbu2g775oqn8w3dbyi5q85ncss14</sha1>
    </revision>
  </page>
  <page>
    <title>Evolvability (computer science)</title>
    <ns>0</ns>
    <id>8416103</id>
    <revision>
      <id>744178661</id>
      <parentid>744178328</parentid>
      <timestamp>2016-10-13T16:12:20Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <comment>/* top */ tidy</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6295">{{multiple|
{{primary sources|date=October 2016}}
{{Orphan|date=July 2011}}
}}
The term '''evolvability''' is used for a recent framework of computational learning introduced by [[Leslie Valiant]] in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of [[Probably approximately correct learning|PAC learning]] and learning from statistical queries.

==General Framework==

Let &lt;math&gt;F_n\,&lt;/math&gt; and &lt;math&gt;R_n\,&lt;/math&gt; be collections of functions on &lt;math&gt;n\,&lt;/math&gt; variables. Given an ''ideal function'' &lt;math&gt;f \in F_n&lt;/math&gt;, the goal is to find by local search a ''representation'' &lt;math&gt;r \in R_n&lt;/math&gt; that closely approximates &lt;math&gt;f\,&lt;/math&gt;. This closeness is measured by the ''performance'' &lt;math&gt;\operatorname{Perf}(f,r)&lt;/math&gt; of &lt;math&gt;r\,&lt;/math&gt; with respect to &lt;math&gt;f\,&lt;/math&gt;.

As is the case in the biological world, there is a difference between genotype and phenotype. In general, there can be multiple representations (genotypes) that correspond to the same function (phenotype). That is, for some &lt;math&gt;r,r' \in R_n&lt;/math&gt;, with &lt;math&gt;r \neq r'\,&lt;/math&gt;, still &lt;math&gt;r(x) = r'(x)\,&lt;/math&gt; for all &lt;math&gt;x \in X_n&lt;/math&gt;. However, this need not be the case. The goal then, is to find a representation that closely matches the phenotype of the ideal function, and the spirit of the local search is to allow only small changes in the genotype. Let the ''neighborhood'' &lt;math&gt;N(r)\,&lt;/math&gt; of a representation &lt;math&gt;r\,&lt;/math&gt; be the set of possible mutations of &lt;math&gt;r\,&lt;/math&gt;.

For simplicity, consider Boolean functions on &lt;math&gt;X_n = \{-1,1\}^n\,&lt;/math&gt;, and let &lt;math&gt;D_n\,&lt;/math&gt; be a probability distribution on &lt;math&gt;X_n\,&lt;/math&gt;. Define the performance in terms of this. Specifically,
:&lt;math&gt; \operatorname{Perf}(f,r) = \sum_{x \in X_n} f(x) r(x) D_n(x). &lt;/math&gt;
Note that &lt;math&gt;\operatorname{Perf}(f,r) = \operatorname{Prob}(f(x)=r(x)) - \operatorname{Prob}(f(x) \neq r(x)).&lt;/math&gt; In general, for non-Boolean functions, the performance will not correspond directly to the probability that the functions agree, although it will have some relationship.

Throughout an organism's life, it will only experience a limited number of environments, so its performance cannot be determined exactly. The ''empirical performance'' is defined by
&lt;math&gt; \operatorname{Perf}_s(f,r) = \frac{1}{s} \sum_{x \in S} f(x)r(x), &lt;/math&gt;
where &lt;math&gt;S\,&lt;/math&gt; is a multiset of &lt;math&gt;s\,&lt;/math&gt; independent selections from &lt;math&gt;X_n\,&lt;/math&gt; according to &lt;math&gt;D_n\,&lt;/math&gt;. If &lt;math&gt;s\,&lt;/math&gt; is large enough, evidently &lt;math&gt;\operatorname{Perf}_s(f,r)&lt;/math&gt; will be close to the actual performance &lt;math&gt;\operatorname{Perf}(f,r)&lt;/math&gt;.

Given an ideal function &lt;math&gt;f \in F_n&lt;/math&gt;, initial representation &lt;math&gt;r \in R_n&lt;/math&gt;, ''sample size'' &lt;math&gt;s\,&lt;/math&gt;, and ''tolerance'' &lt;math&gt;t\,&lt;/math&gt;, the ''mutator'' &lt;math&gt;\operatorname{Mut}(f,r,s,t)&lt;/math&gt; is a random variable defined as follows. Each &lt;math&gt;r' \in N(r)&lt;/math&gt; is classified as beneficial, neutral, or deleterious, depending on its empirical performance. Specifically,
* &lt;math&gt;r'\,&lt;/math&gt; is a beneficial mutation if &lt;math&gt;\operatorname{Perf}_s(f,r') - \operatorname{Perf}_s(f,r) \geq t&lt;/math&gt;;
* &lt;math&gt;r'\,&lt;/math&gt; is a neutral mutation if &lt;math&gt;-t &lt; \operatorname{Perf}_s(f,r') - \operatorname{Perf}_s(f,r) &lt; t&lt;/math&gt;;
* &lt;math&gt;r'\,&lt;/math&gt; is a deleterious mutation if &lt;math&gt;\operatorname{Perf}_s(f,r') - \operatorname{Perf}_s(f,r) \leq -t&lt;/math&gt;.

If there are any beneficial mutations, then &lt;math&gt;\operatorname{Mut}(f,r,s,t)&lt;/math&gt; is equal to one of these at random. If there are no beneficial mutations, then &lt;math&gt;\operatorname{Mut}(f,r,s,t)&lt;/math&gt; is equal to a random neutral mutation. In light of the similarity to biology, &lt;math&gt;r\,&lt;/math&gt; itself is required to be available as a mutation, so there will always be at least one neutral mutation.

The intention of this definition is that at each stage of evolution, all possible mutations of the current genome are tested in the environment. Out of the ones who thrive, or at least survive, one is chosen to be the candidate for the next stage. Given &lt;math&gt;r_0 \in R_n&lt;/math&gt;, we define the sequence &lt;math&gt;r_0,r_1,r_2,\ldots&lt;/math&gt; by &lt;math&gt;r_{i+1} = \operatorname{Mut}(f,r_i,s,t)&lt;/math&gt;. Thus &lt;math&gt;r_g\,&lt;/math&gt; is a random variable representing what &lt;math&gt;r_0\,&lt;/math&gt; has evolved to after &lt;math&gt;g\,&lt;/math&gt; ''generations''.

Let &lt;math&gt;F\,&lt;/math&gt; be a class of functions, &lt;math&gt;R\,&lt;/math&gt; be a class of representations, and &lt;math&gt;D\,&lt;/math&gt; a class of distributions on &lt;math&gt;X\,&lt;/math&gt;. We say that &lt;math&gt;F\,&lt;/math&gt; is ''evolvable by &lt;math&gt;R\,&lt;/math&gt; over &lt;math&gt;D\,&lt;/math&gt;'' if there exists polynomials &lt;math&gt;p(\cdot,\cdot)&lt;/math&gt;, &lt;math&gt;s(\cdot,\cdot)&lt;/math&gt;, &lt;math&gt;t(\cdot,\cdot)&lt;/math&gt;, and &lt;math&gt;g(\cdot,\cdot)&lt;/math&gt; such that for all &lt;math&gt;n\,&lt;/math&gt; and all &lt;math&gt;\epsilon &gt; 0\,&lt;/math&gt;, for all ideal functions &lt;math&gt;f \in F_n&lt;/math&gt; and representations &lt;math&gt;r_0 \in R_n&lt;/math&gt;, with probability at least &lt;math&gt;1 - \epsilon\,&lt;/math&gt;,
:&lt;math&gt; \operatorname{Perf}(f,r_{g(n,1/\epsilon)}) \geq 1-\epsilon, &lt;/math&gt;
where the sizes of neighborhoods &lt;math&gt;N(r)\,&lt;/math&gt; for &lt;math&gt;r \in R_n\,&lt;/math&gt; are at most &lt;math&gt;p(n,1/\epsilon)\,&lt;/math&gt;, the sample size is &lt;math&gt;s(n,1/\epsilon)\,&lt;/math&gt;, the tolerance is &lt;math&gt;t(1/n,\epsilon)\,&lt;/math&gt;, and the generation size is &lt;math&gt;g(n,1/\epsilon)\,&lt;/math&gt;.

&lt;math&gt;F\,&lt;/math&gt; is ''evolvable over &lt;math&gt;D\,&lt;/math&gt;'' if it is evolvable by some &lt;math&gt;R\,&lt;/math&gt; over &lt;math&gt;D\,&lt;/math&gt;.

&lt;math&gt;F\,&lt;/math&gt; is ''evolvable'' if it is evolvable over all distributions &lt;math&gt;D\,&lt;/math&gt;.

==Results==
The class of conjunctions and the class of disjunctions are evolvable over the uniform distribution for short conjunctions and disjunctions, respectively.

The class of parity functions (which evaluate to the parity of the number of true literals in a given subset of literals) are not evolvable, even for the uniform distribution.

Evolvability implies [[Probably approximately correct learning|PAC learnability]].

==References==
#{{citation|first=L. G.|last=Valiant|authorlink=Leslie Valiant|title=Evolvability|year=2006|id={{ECCC|2006|06|120}}}}.

</text>
      <sha1>p3kqw5s8r1qucgx25eaus3yyo0338vw</sha1>
    </revision>
  </page>
  <page>
    <title>Prior knowledge for pattern recognition</title>
    <ns>0</ns>
    <id>6881120</id>
    <revision>
      <id>799308468</id>
      <parentid>720988484</parentid>
      <timestamp>2017-09-06T22:36:48Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>adding a link using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5351">[[Pattern recognition]] is a very active field of research intimately bound to [[machine learning]]. Also known as classification or [[statistical classification]], pattern recognition aims at building a [[classifier (mathematics)|classifier]] that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs &lt;math&gt;(\boldsymbol{x}_i,y_i)&lt;/math&gt; that form the training data (or training set). Nonetheless, in real world applications such as [[character recognition]], a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.

== Prior Knowledge ==

Prior knowledge&lt;ref&gt;B. Scholkopf and A. Smola, &quot;Learning with Kernels&quot;, MIT Press 2002.&lt;/ref&gt; refers to all information about the problem available in addition to the training data. However, in this most general form, determining a [[Model (abstract)|model]] from a finite set of samples without prior knowledge is an [[ill-posed]] problem, in the sense that a unique model may not exist. Many classifiers incorporate the general smoothness assumption that a test pattern similar to one of the training samples tends to be assigned to the same class.

The importance of prior knowledge in machine learning is suggested by its role in search and optimization. Loosely, the [[No free lunch in search and optimization|no free lunch theorem]] states that all search algorithms have the same average performance over all problems, and thus implies that to gain in performance on a certain application one must use a specialized algorithm that includes some prior knowledge about the problem. &lt;!-- This sentence is still not right. Read the &quot;no free lunch&quot; article to see why.
David Wolpert actually published NFL-like results for machine learning before moving to
optimization with Bill Macready. Check his web site at NASA for a list of his publications.--&gt;

The different types of prior knowledge encountered in pattern recognition are now regrouped under two main categories: class-invariance and knowledge on the data.

== Class-invariance ==

A very common type of prior knowledge in pattern recognition is the invariance of the class (or the output of the classifier) to a [[Transformation (geometry)|transformation]] of the input pattern. This type of knowledge is referred to as '''transformation-invariance'''. The mostly used transformations used in image recognition are:

* [[Translation (geometry)|translation]];
* [[Rotation (mathematics)|rotation]];
* [[skewing]];
* [[Scaling (geometry)|scaling]].

Incorporating the invariance to a transformation &lt;math&gt;T_{\theta}: \boldsymbol{x} \mapsto T_{\theta}\boldsymbol{x}&lt;/math&gt; parametrized in &lt;math&gt;\theta&lt;/math&gt;  into a classifier of output &lt;math&gt;f(\boldsymbol{x})&lt;/math&gt; for an input pattern &lt;math&gt;\boldsymbol{x}&lt;/math&gt; corresponds to enforcing the equality

:&lt;math&gt;
f(\boldsymbol{x}) = f(T_{\theta}\boldsymbol{x}), \quad \forall \boldsymbol{x}, \theta .&lt;/math&gt;

Local invariance can also be considered for a transformation centered at &lt;math&gt;\theta=0&lt;/math&gt;, so that &lt;math&gt;T_0\boldsymbol{x} = \boldsymbol{x}&lt;/math&gt;, by using the constraint

:&lt;math&gt;
  \left.\frac{\partial}{\partial \theta}\right|_{\theta=0} f(T_{\theta} \boldsymbol{x}) = 0 .
&lt;/math&gt;

The function &lt;math&gt;f&lt;/math&gt; in these equations can be either the decision function of the classifier or its real-valued output.

Another approach is to consider class-invariance with respect to a &quot;domain of the input space&quot; instead of a transformation. In this case, the problem becomes finding &lt;math&gt;f&lt;/math&gt; so that

:&lt;math&gt;
	f(\boldsymbol{x}) = y_{\mathcal{P}},\ \forall \boldsymbol{x}\in \mathcal{P} ,
&lt;/math&gt;

where &lt;math&gt;y_{\mathcal{P}}&lt;/math&gt; is the membership class of the region &lt;math&gt;\mathcal{P}&lt;/math&gt; of the input space.

A different type of class-invariance found in pattern recognition is '''permutation-invariance''', i.e. invariance of the class to a permutation of elements in a structured input. A typical application of this type of prior knowledge is a classifier invariant to permutations of rows of the matrix inputs.

== Knowledge of the data ==

Other forms of prior knowledge than class-invariance concern the data more specifically and are thus of particular interest for real-world applications. The three particular cases that most often occur when gathering data are:
* '''Unlabeled samples''' are available with supposed class-memberships;
* '''Imbalance''' of the training set due to a high proportion of samples of a class;
* '''Quality of the data''' may vary from a sample to another.

Prior knowledge of these can enhance the quality of the recognition if included in the learning. Moreover, not taking into account the poor quality of some data or a large imbalance between the classes can mislead the decision of a classifier.

== Notes ==

&lt;references/&gt;

== References ==

* E. Krupka and N. Tishby, &quot;[http://www.jmlr.org/proceedings/papers/v2/krupka07a.html Incorporating Prior Knowledge on Features into Learning]&quot;, Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS 07)


</text>
      <sha1>eefduy0k9kw67jcfm9nxitd6xbeobwa</sha1>
    </revision>
  </page>
  <page>
    <title>Granular computing</title>
    <ns>0</ns>
    <id>1041204</id>
    <revision>
      <id>800587236</id>
      <parentid>767360044</parentid>
      <timestamp>2017-09-14T13:22:37Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>/* Variable aggregation */ l</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="40501">'''Granular computing''' (GrC) is an emerging computing paradigm of [[information processing]].  It concerns the processing of complex information entities called information granules, which arise in the process of data abstraction and derivation of knowledge from information or data.  Generally speaking, information granules are collections of entities that usually originate at the numeric level and  are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like.

At present, granular computing is more a ''theoretical perspective'' than a coherent set of methods or principles.  As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales.  In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented.

== Types of granulation ==
[[File:Catarina 26 mar 2004 1310Z.jpg|thumb|upright=1.10|Satellite view of cyclone.]]
[[File:NASA Manhattan.jpg|thumb|upright=1.10|Satellite view of Manhattan.]]
As mentioned above, ''granular computing'' is not an algorithm or process; there is no particular method that is called &quot;granular computing&quot;.  It is rather an approach  to looking at data that recognizes how different and interesting regularities in the data can appear at different levels of granularity, much as different features become salient in [[satellite images]] of greater or lesser resolution.  On a low-resolution satellite image, for example, one might notice interesting cloud patterns representing [[cyclones]] or other large-scale weather phenomena, while in a higher-resolution image, one misses these large-scale atmospheric phenomena but instead notices smaller-scale phenomena, such as the interesting pattern that is the streets of [[Manhattan]].  The same is generally true of all data: At different resolutions or granularities, different features and relationships emerge.  The aim of granular computing is ultimately simply to try to take advantage of this fact in designing more-effective machine-learning and reasoning systems.

There are several types of granularity that are often encountered in [[data mining]] and [[machine learning]], and we review them below:

=== Value granulation (discretization/quantization) ===
One type of granulation is the [[Quantization (signal processing)|quantization]] of variables.  It is very common that in data mining or machine-learning applications the resolution of variables needs to be ''decreased'' in order to extract meaningful regularities.  An example of this would be a variable such as &quot;outside temperature&quot; (&lt;math&gt;temp&lt;/math&gt;), which in a given application might be recorded to several decimal places of [[Arithmetic precision|precision]] (depending on the sensing apparatus).  However, for purposes of extracting relationships between &quot;outside temperature&quot; and, say, &quot;number of health-club applications&quot; (&lt;math&gt;club &lt;/math&gt;), it will generally be advantageous to quantize &quot;outside temperature&quot; into a smaller number of intervals.

==== Motivations ====
There are several interrelated reasons for granulating variables in this fashion:
* Based on prior domain knowledge, there is no expectation that minute variations in temperature (e.g., the difference between {{convert|80|-|80.7|°F|C|1}}) could have an influence on behaviors driving the number of health-club applications.  For this reason, any &quot;regularity&quot; which our learning algorithms might detect at this level of resolution would have to be ''spurious'', as an artifact of overfitting.  By coarsening the temperature variable into intervals the difference between which we ''do'' anticipate (based on prior domain knowledge) might influence  number of health-club applications, we eliminate the possibility of detecting these spurious  patterns.  Thus, in this case, reducing resolution is a method of controlling [[overfitting]].
* By reducing the number of intervals in the temperature variable (i.e., increasing its ''grain size''), we increase the amount of sample data indexed by each interval designation.  Thus, by coarsening the variable, we increase sample sizes and achieve better statistical estimation.  In this sense, increasing granularity provides an antidote to the so-called ''[[curse of dimensionality]]'', which relates to the exponential decrease in statistical power with increase in number of dimensions or variable cardinality.
*Independent of prior domain knowledge, it is often the case that meaningful regularities (i.e., which can be detected by a given learning methodology, representational language, etc.) may exist at one level of resolution and not at another.

[[File:Value granulation.png|thumb|200 px|Benefits of value granulation: Implications here exist at the resolution of &lt;math&gt;\{X_i,Y_j\}&lt;/math&gt; that do not exist at the higher resolution of &lt;math&gt;\{x_i,y_j\}&lt;/math&gt;; in particular, &lt;math&gt;\forall x_i,y_j: x_i \not\to y_j&lt;/math&gt;, while at the same time, &lt;math&gt;\forall X_i \exists Y_j: X_i \leftrightarrow Y_j&lt;/math&gt;.]]
For example, a simple learner or pattern recognition system may seek to extract regularities satisfying a [[conditional probability]] threshold such as &lt;math&gt;p(Y=y_j|X=x_i) \ge \alpha &lt;/math&gt;.  In the special case where &lt;math&gt;\alpha = 1 &lt;/math&gt;, this recognition system is essentially detecting ''[[logical implication]]'' of the form &lt;math&gt;X=x_i \rightarrow Y=y_j &lt;/math&gt; or, in words, &quot;if &lt;math&gt;X=x_i&lt;/math&gt;, then &lt;math&gt;Y=y_j &lt;/math&gt;&quot;.  The system's ability to recognize such implications (or, in general, conditional probabilities exceeding threshold)  is partially contingent on the resolution with which the system analyzes the variables.

As an example of this last point, consider the feature space shown to the right.  The variables may each be regarded at two different resolutions.  Variable &lt;math&gt;X&lt;/math&gt; may be regarded at a high (quaternary) resolution wherein it takes on the four values &lt;math&gt;\{x_1, x_2, x_3, x_4\}&lt;/math&gt; or at a lower (binary) resolution wherein it takes on the two values &lt;math&gt;\{X_1, X_2\}&lt;/math&gt;.  Similarly, variable &lt;math&gt;Y&lt;/math&gt; may be regarded at a high (quaternary) resolution or at a lower (binary) resolution, where it takes on the values &lt;math&gt;\{y_1, y_2, y_3, y_4\}&lt;/math&gt; or &lt;math&gt;\{Y_1, Y_2\}&lt;/math&gt;, respectively.  It will be noted that at the high resolution, there are '''no''' detectable implications of the form &lt;math&gt;X=x_i \rightarrow Y=y_j &lt;/math&gt;, since every &lt;math&gt;x_i&lt;/math&gt; is associated with more than one &lt;math&gt;y_j&lt;/math&gt;, and thus, for all &lt;math&gt;x_i&lt;/math&gt;, &lt;math&gt;p(Y=y_j|X=x_i) &lt; 1 &lt;/math&gt;.  However, at the low (binary) variable resolution, two bilateral implications become detectable:    &lt;math&gt;X=X_1 \leftrightarrow Y=Y_1 &lt;/math&gt; and &lt;math&gt;X=X_2 \leftrightarrow Y=Y_2 &lt;/math&gt;, since every &lt;math&gt;X_1&lt;/math&gt; occurs ''iff'' &lt;math&gt;Y_1&lt;/math&gt; and &lt;math&gt;X_2&lt;/math&gt; occurs ''iff'' &lt;math&gt;Y_2&lt;/math&gt;.  Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the   higher quaternary variable resolution.

====Issues and methods====
It is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results.  Instead, the feature space must be preprocessed (often by an [[information entropy|entropy]] analysis of some kind) so that some guidance can be given as to how the discretization process should proceed.  Moreover, one cannot generally achieve good results by naively analyzing and discretizing each variable independently, since this may obliterate the very interactions that we had hoped to discover.

A sample of papers that address the problem of variable discretization in general, and multiple-variable discretization in particular, is as follows: {{Harvtxt|Chiu|Wong|Cheung|1991}}, {{Harvtxt|Bay|2001}},  {{Harvtxt|Liu|Hussain|Tan|Dasii|2002}}, {{Harvtxt|Wang|Liu|1998}}, {{Harvtxt|Zighed|Rabaséda|Rakotomalala|1998}}, {{Harvtxt|Catlett|1991}}, {{Harvtxt|Dougherty|Kohavi|Sahami|1995}}, {{Harvtxt|Monti|Cooper|1999}}, {{Harvtxt|Fayyad|Irani|1993}}, {{Harvtxt|Chiu|Cheung|Wong|1990}}, {{Harvtxt|Nguyen|Nguyen|1998}}, {{Harvtxt|Grzymala-Busse|Stefanowski|2001}}, {{Harvtxt|Ting|1994}}, {{Harvtxt|Ludl|Widmer|2000}}, {{Harvtxt|Pfahringer|1995}}, {{Harvtxt|An|Cercone|1999}},
{{Harvtxt|Chiu|Cheung|1989}}, {{Harvtxt|Chmielewski|Grzymala-Busse|1996}}, {{Harvtxt|Lee|Shin|1994}}, {{Harvtxt|Liu|Wellman|2002}}, {{Harvtxt|Liu|Wellman|2004}}.

=== Variable granulation (clustering/aggregation/transformation) ===
Variable granulation is a term that could describe a variety of techniques, most of which are aimed at reducing dimensionality, redundancy, and storage requirements.  We briefly describe some of the ideas here, and present pointers to the literature.

====Variable transformation====
A number of classical methods, such as [[principal component analysis]], [[multidimensional scaling]], [[factor analysis]], and [[structural equation modeling]], and their relatives, fall under the genus of &quot;variable transformation.&quot;  Also in this category are more modern areas of study such as [[dimensionality reduction]], [[projection pursuit]], and [[independent component analysis]].  The common goal of these methods in general is to find a representation of the data in terms of new variables, which are a linear or nonlinear transformation of the original variables, and in which important statistical relationships emerge. The resulting variable sets are almost always smaller than the original variable set, and hence these methods can be loosely said to impose a granulation on the feature space.  These dimensionality reduction methods are all reviewed in the standard texts, such as {{Harvtxt|Duda|Hart|Stork|2001}}, {{Harvtxt|Witten|Frank|2005}}, and {{Harvtxt|Hastie|Tibshirani|Friedman|2001}}.

====Variable aggregation====
A different class of variable granulation methods derive more from [[data clustering]] methodologies than from the linear systems theory informing the above methods.  It was noted fairly early that one may consider &quot;clustering&quot; related variables in just  the same way that one considers clustering related data.  In data clustering, one identifies a group of similar entities (using a &quot;[[measure of similarity]]&quot; suitable to the domain), and then in some sense ''replaces'' those entities with a prototype of some kind.  The prototype may be the simple average of the data in the identified cluster, or some other representative measure.  But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to ''stand in'' for the much larger set of exemplars.   These prototypes are generally such as to capture most of the information of interest concerning the entities.

[[File:Kraskov tree.png|thumb|400 px|A Watanabe-Kraskov variable agglomeration tree. Variables are agglomerated (or &quot;unitized&quot;) from the bottom-up, with each merge-node representing a (constructed) variable having entropy equal to the joint entropy of the agglomerating variables. Thus, the agglomeration of two m-ary variables &lt;math&gt;X_1&lt;/math&gt; and &lt;math&gt;X_2&lt;/math&gt; having individual entropies &lt;math&gt;H(X_1)&lt;/math&gt; and &lt;math&gt;H(X_2)&lt;/math&gt; yields a single &lt;math&gt;m^2&lt;/math&gt;-ary variable &lt;math&gt;X_{1,2}&lt;/math&gt; with entropy &lt;math&gt;H(X_{1,2})=H(X_1,X_2)&lt;/math&gt;. When &lt;math&gt;X_1&lt;/math&gt; and &lt;math&gt;X_2&lt;/math&gt; are highly dependent (i.e., redundant) and have large mutual information &lt;math&gt;I(X_1;X_2)&lt;/math&gt;, then &lt;math&gt;H(X_{1,2})&lt;/math&gt; &amp;#x226A; &lt;math&gt;H(X_1)+H(X_2)&lt;/math&gt; because &lt;math&gt;H(X_1,X_2)=H(X_1)+H(X_2)-I(X_1;X_2)&lt;/math&gt;, and this would be considered a parsimonious unitization or aggregation.]]
Similarly, it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of ''prototype'' variables that capture the most salient relationships between the variables.    Although variable clustering methods based on [[linear correlation]] have been proposed ({{Harvnb|Duda|Hart|Stork|2001}};{{Harvnb|Rencher|2002}}), more powerful methods of variable clustering are based on the [[mutual information]] between variables. Watanabe has shown ({{Harvnb|Watanabe|1960}};{{Harvnb|Watanabe|1969}}) that for any set of variables one can construct a ''polytomic'' (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate &quot;total&quot; correlation  among the complete variable set is the sum of the &quot;partial&quot; correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts &quot;... as if they were looking for a natural division or a hidden crack.&quot;

One practical approach to building such a tree is to successively choose for agglomeration the two variables (either atomic variables or previously agglomerated variables) which have the highest pairwise mutual information {{Harv|Kraskov|Stögbauer|Andrzejak|Grassberger|2003}}. The product of each agglomeration is a new (constructed) variable that reflects the local [[joint distribution]] of the two agglomerating variables, and thus possesses an entropy equal to their [[joint entropy]].
(From a procedural standpoint, this agglomeration step involves replacing two columns in the attribute-value table—representing the two agglomerating variables—with a single column that has a unique value for every unique combination of values in the replaced columns {{Harv|Kraskov|Stögbauer|Andrzejak|Grassberger|2003}}.  No information is lost by such an operation; however, it should be noted that if one is exploring the data for inter-variable relationships, it would generally ''not'' be desirable to merge redundant variables in this way, since in such a context it is likely to be precisely the redundancy or ''dependency'' between variables that is of interest;  and once redundant variables are merged, their relationship to one another can no longer be studied.

=== System granulation (aggregation) ===

In [[database systems]], aggregations (see e.g. [[OLAP|OLAP aggregation]] and [[Business intelligence]] systems) result in transforming original data tables (often called information systems) into the tables with different semantics of rows and columns, wherein the rows correspond to the groups (granules) of original tuples and the columns express aggregated information about original values within each of the groups. Such aggregations are usually based on SQL and its extensions. The resulting granules usually correspond to the groups of original tuples with the same values (or ranges) over some pre-selected original columns.

There are also other approaches wherein the groups are defined basing on, e.g., physical adjacency of rows. For example, [[Infobright]] implemented a database engine wherein data was partitioned onto ''rough rows'', each consisting of 64K of physically consecutive (or almost consecutive) rows. Rough rows were automatically labeled with compact information about their values on data columns, often involving multi-column and multi-table relationships. It resulted in a higher layer of granulated information where objects corresponded to rough rows and attributes - to various aspects of rough information. Database operations could be efficiently supported within such a new framework, with an access to the original data pieces still available {{Slezak|Synak|Wojna|Wroblewski|2013}}.

=== Concept granulation (component analysis) ===
The origins of the ''granular computing'' ideology are to be found in the [[rough sets]] and [[fuzzy sets]] literatures.  One of the key insights of rough set research—although by no means unique to it—is that, in general, the selection of different sets of features or variables will yield different ''concept'' granulations.  Here, as in elementary rough set theory, by &quot;concept&quot; we mean a set of entities that are ''indistinguishable'' or ''indiscernible'' to the observer (i.e., a simple concept), or a set of entities that is composed from such simple concepts (i.e., a complex concept).  To put it in other words, by projecting a data set ([[value-attribute system]]) onto different sets of variables, we recognize alternative sets of equivalence-class &quot;concepts&quot; in the data, and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities.

====Equivalence class granulation====
We illustrate with an example.  Consider the attribute-value system below:

:{| class=&quot;wikitable&quot; style=&quot;text-align:center; width:30%&quot; border=&quot;1&quot;
|+ Sample Information System
! Object !! &lt;math&gt;P_{1}&lt;/math&gt; !! &lt;math&gt;P_{2}&lt;/math&gt; !! &lt;math&gt;P_{3}&lt;/math&gt; !! &lt;math&gt;P_{4}&lt;/math&gt; !! &lt;math&gt;P_{5}&lt;/math&gt;
|-
! &lt;math&gt;O_{1}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{2}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{3}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{4}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 1
|-
! &lt;math&gt;O_{5}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 1
|-
! &lt;math&gt;O_{6}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 2
|-
! &lt;math&gt;O_{7}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{8}&lt;/math&gt;
| 0 || 1 || 2 || 2 || 1
|-
! &lt;math&gt;O_{9}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 2
|-
! &lt;math&gt;O_{10}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|}

When the full set of attributes &lt;math&gt;P = \{P_{1},P_{2},P_{3},P_{4},P_{5}\}&lt;/math&gt; is considered, we see that we have the following seven equivalence classes or primitive (simple) concepts:

:&lt;math&gt;
\begin{cases}
\{O_{1},O_{2}\} \\
\{O_{3},O_{7},O_{10}\} \\
\{O_{4}\} \\
\{O_{5}\} \\
\{O_{6}\} \\
\{O_{8}\} \\
\{O_{9}\} \end{cases}
&lt;/math&gt;

Thus, the two objects within the first equivalence class, &lt;math&gt;\{O_{1},O_{2}\}&lt;/math&gt;,  cannot be distinguished from one another based on the available attributes, and the three objects within the second equivalence class, &lt;math&gt;\{O_{3},O_{7},O_{10}\}&lt;/math&gt;, cannot be distinguished from one another based on the available attributes.  The remaining five objects are each discernible from all other objects.  Now, let us imagine a projection of the attribute value system onto attribute &lt;math&gt;P_{1}&lt;/math&gt; alone, which would represent, for example, the  view from an observer which is only capable of detecting this single attribute. Then we obtain the following much coarser equivalence class structure.

:&lt;math&gt;
\begin{cases}
\{O_{1},O_{2}\} \\
\{O_{3},O_{5},O_{7},O_{9},O_{10}\} \\
\{O_{4},O_{6},O_{8}\} \end{cases}
&lt;/math&gt;

This is in a certain regard the same structure as before, but at a lower degree of resolution (larger grain size).  Just as in the case of [[#Value granulation (discretization/quantization)|value granulation (discretization/quantization)]], it is possible that relationships (dependencies) may emerge at one level of granularity that are not present at another.  As an example of this, we can consider the effect of concept granulation on the measure known as ''attribute dependency'' (a simpler relative of the [[mutual information]]).

To establish this notion of dependency (see also [[rough sets]]), let &lt;math&gt;[x]_Q = \{Q_1, Q_2, Q_3, \dots, Q_N \}&lt;/math&gt; represent a particular concept granulation, where each &lt;math&gt;Q_i&lt;/math&gt; is an equivalence class from the concept structure induced by attribute set &lt;math&gt;Q&lt;/math&gt;.  For example, if the  attribute set &lt;math&gt;Q&lt;/math&gt; consists of attribute &lt;math&gt;P_{1}&lt;/math&gt; alone, as above,  then the concept structure &lt;math&gt;[x]_Q&lt;/math&gt; will be composed of    &lt;math&gt;Q_1 = \{O_{1},O_{2}\}&lt;/math&gt;, &lt;math&gt;Q_2 = \{O_{3},O_{5},O_{7},O_{9},O_{10}\}&lt;/math&gt;, and &lt;math&gt;Q_3 = \{O_{4},O_{6},O_{8}\}&lt;/math&gt;.  The '''dependency''' of attribute set &lt;math&gt;Q&lt;/math&gt; on another attribute set &lt;math&gt;P&lt;/math&gt;, &lt;math&gt;\gamma_{P}(Q)&lt;/math&gt;, is given by

:&lt;math&gt;
\gamma_{P}(Q) =  \frac{\left | \sum_{i=1}^N {\underline P}Q_i \right |} {\left | \mathbb{U} \right |} \leq 1
&lt;/math&gt;

That is, for each equivalence class &lt;math&gt;Q_i&lt;/math&gt; in &lt;math&gt;[x]_Q&lt;/math&gt;, we add up the size of its &quot;lower approximation&quot; (see [[rough sets]]) by the attributes in &lt;math&gt;P&lt;/math&gt;, i.e., &lt;math&gt;{\underline P}Q_i&lt;/math&gt;.  More simply, this approximation  is the number of objects which on attribute set &lt;math&gt;P&lt;/math&gt; can be positively identified as belonging to target set &lt;math&gt;Q_i&lt;/math&gt;.  Added across all equivalence classes in &lt;math&gt;[x]_Q&lt;/math&gt;, the numerator above represents the total number of objects which—based on attribute set &lt;math&gt;P&lt;/math&gt;—can be positively categorized according to the classification induced by  attributes &lt;math&gt;Q&lt;/math&gt;.  The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects, in a sense capturing the &quot;synchronization&quot; of the two concept structures &lt;math&gt;[x]_Q&lt;/math&gt; and &lt;math&gt;[x]_P&lt;/math&gt;.  The dependency &lt;math&gt;\gamma_{P}(Q)&lt;/math&gt; &quot;can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in &lt;math&gt;P&lt;/math&gt; to determine the values of attributes in &lt;math&gt;Q&lt;/math&gt;&quot; (Ziarko &amp; Shan 1995).

Having gotten definitions now out of the way, we can make the simple observation that the choice of concept granularity (i.e., choice of attributes) will influence the detected dependencies among attributes.  Consider again the attribute value table from above:

:{| class=&quot;wikitable&quot; style=&quot;text-align:center; width:30%&quot; border=&quot;1&quot;
|+ Sample Information System
! Object !! &lt;math&gt;P_{1}&lt;/math&gt; !! &lt;math&gt;P_{2}&lt;/math&gt; !! &lt;math&gt;P_{3}&lt;/math&gt; !! &lt;math&gt;P_{4}&lt;/math&gt; !! &lt;math&gt;P_{5}&lt;/math&gt;
|-
! &lt;math&gt;O_{1}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{2}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{3}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{4}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 1
|-
! &lt;math&gt;O_{5}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 1
|-
! &lt;math&gt;O_{6}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 2
|-
! &lt;math&gt;O_{7}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{8}&lt;/math&gt;
| 0 || 1 || 2 || 2 || 1
|-
! &lt;math&gt;O_{9}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 2
|-
! &lt;math&gt;O_{10}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|}

Let us consider the dependency of attribute set  &lt;math&gt;Q = \{P_4, P_5\}&lt;/math&gt;
on attribute set &lt;math&gt;P = \{P_2, P_3\}&lt;/math&gt;.  That is, we wish to know what proportion of objects can be correctly classified into classes of &lt;math&gt;[x]_Q&lt;/math&gt; based on knowledge of &lt;math&gt;[x]_P&lt;/math&gt;.  The equivalence classes of &lt;math&gt;[x]_Q&lt;/math&gt; and of &lt;math&gt;[x]_P&lt;/math&gt; are shown below.

:{| class=&quot;wikitable&quot;
|-
! &lt;math&gt;[x]_Q&lt;/math&gt;
! &lt;math&gt;[x]_P&lt;/math&gt;
|-
| &lt;math&gt;
\begin{cases}
\{O_{1},O_{2}\} \\
\{O_{3},O_{7},O_{10}\} \\
\{O_{4},O_{5},O_{8}\} \\
\{O_{6},O_{9}\}\end{cases}
&lt;/math&gt;
| &lt;math&gt;
\begin{cases}
\{O_{1},O_{2}\} \\
\{O_{3},O_{7},O_{10}\} \\
\{O_{4},O_{6}\} \\
\{O_{5},O_{9}\} \\
\{O_{8}\}\end{cases}
&lt;/math&gt;
|}

The objects that can be ''definitively'' categorized according to concept structure &lt;math&gt;[x]_Q&lt;/math&gt; based on &lt;math&gt;[x]_P&lt;/math&gt; are those in the set &lt;math&gt;\{O_{1},O_{2},O_{3},O_{7},O_{8},O_{10}\}&lt;/math&gt;, and since there are six of these, the dependency of &lt;math&gt;Q&lt;/math&gt; on &lt;math&gt;P&lt;/math&gt;,  &lt;math&gt;\gamma_{P}(Q) = 6/10&lt;/math&gt;.  This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired.

We might then consider the dependency of the smaller attribute set  &lt;math&gt;Q = \{P_4\}&lt;/math&gt;
on the attribute set &lt;math&gt;P = \{P_2, P_3\}&lt;/math&gt;.  The move from &lt;math&gt;Q = \{P_4, P_5\}&lt;/math&gt; to &lt;math&gt;Q = \{P_4\}&lt;/math&gt; induces a coarsening of the class structure &lt;math&gt;[x]_Q&lt;/math&gt;, as will be seen shortly.  We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of &lt;math&gt;[x]_Q&lt;/math&gt; based on knowledge of &lt;math&gt;[x]_P&lt;/math&gt;.  The equivalence classes of the new &lt;math&gt;[x]_Q&lt;/math&gt; and of &lt;math&gt;[x]_P&lt;/math&gt; are shown below.

:{| class=&quot;wikitable&quot;
|-
! &lt;math&gt;[x]_Q&lt;/math&gt;
! &lt;math&gt;[x]_P&lt;/math&gt;
|-
| &lt;math&gt;
\begin{cases}
\{O_{1},O_{2},O_{3},O_{7},O_{10}\} \\
\{O_{4},O_{5},O_{6},O_{8},O_{9}\} \end{cases}
&lt;/math&gt;
| &lt;math&gt;
\begin{cases}
\{O_{1},O_{2}\} \\
\{O_{3},O_{7},O_{10}\} \\
\{O_{4},O_{6}\} \\
\{O_{5},O_{9}\} \\
\{O_{8}\}\end{cases}
&lt;/math&gt;
|}

Clearly, &lt;math&gt;[x]_Q&lt;/math&gt; has a coarser granularity than it did earlier.  The objects that can now be ''definitively'' categorized according to the concept structure &lt;math&gt;[x]_Q&lt;/math&gt; based on &lt;math&gt;[x]_P&lt;/math&gt; constitute the complete universe &lt;math&gt;\{O_{1},O_{2},\ldots,O_{10}\}&lt;/math&gt;, and thus  the dependency of &lt;math&gt;Q&lt;/math&gt; on &lt;math&gt;P&lt;/math&gt;,  &lt;math&gt;\gamma_{P}(Q) = 1&lt;/math&gt;.  That is, knowledge of membership according to category set  &lt;math&gt;[x]_P&lt;/math&gt; is adequate to determine category membership in &lt;math&gt;[x]_Q&lt;/math&gt; with complete certainty; In this case we might say that &lt;math&gt;P \rightarrow Q&lt;/math&gt;.  Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency.  However, we also note that the classes induced in &lt;math&gt;[x]_Q&lt;/math&gt; from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number; as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of &lt;math&gt;[x]_Q&lt;/math&gt;.

In general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence.  Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and [[Lotfi Zadeh]] listed in the [[#References]] below.

====Component granulation====
Another perspective on concept granulation may be obtained from work on parametric models of categories.  In [[mixture model]] learning, for example, a set of data is explained as a mixture of distinct [[Gaussian distribution|Gaussian]] (or other) distributions.  Thus, a large amount of data is &quot;replaced&quot; by a small number of distributions.  The choice of the number of these distributions, and their size, can again be viewed as a problem of ''concept granulation''.  In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately  ''coarsening'' the concept resolution.  Finding the &quot;right&quot; concept resolution is a tricky problem for which many methods have been proposed (e.g., [[Akaike information criterion|AIC]], [[Bayesian information criterion|BIC]], [[Minimum description length|MDL]], etc.), and these are frequently  considered under the rubric of &quot;[[model regularization]]&quot;.

==Different interpretations of granular computing==
Granular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving.  In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation.  By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities, it may be possible to develop a general theory for problem solving.

In a more philosophical sense, granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity (i.e., abstraction) in order to abstract and consider only those things that serve a specific interest and to switch among different granularities. By focusing on different levels of granularity, one can obtain different levels of knowledge, as well as a greater understanding of the inherent knowledge structure.  Granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems.

== See also ==
* [[Rough set|Rough Sets]], [[Discretization]]
* [[Type-2 Fuzzy Sets and Systems]]

== References ==
{{refbegin|2}}
*{{Citation | last=An| first=Aijun |last2=Cercone| first2=Nick|year= 1999| chapter=Discretization of continuous attributes for learning classification rules | editor=Ning Zhong |editor2=Lizhu Zhou | title=Methodologies for Knowledge Discovery and Data Mining: Proceedings of the Third Pacific-Asia Conference, PAKDD-99 | edition= | publisher= | place=[[Beijing, China]] |doi=10.1007/3-540-48912-6_69 |pages=509–514}}.
*Bargiela, A. and Pedrycz, W. (2003) ''Granular Computing. An introduction'', Kluwer Academic Publishers
*{{Citation | last=Bay| first=Stephen D. | author-link= | title=Multivariate discretization for set mining | journal=Knowledge and Information Systems |volume=3 |issue=4 | year=2001| pages=491–512 |doi=10.1007/PL00011680}}.
*{{Citation | last=Catlett| first=J.|year= 1991| chapter=On changing continuous attributes into ordered discrete attributes | editor=Y. Kodratoff | title=Machine Learning—EWSL-91: European Working Session on Learning | edition= | publisher= | place=[[Porto, Portugal]] | url=http://portal.acm.org/citation.cfm?coll=GUIDE&amp;dl=GUIDE&amp;id=112164 | accessdate=|pages=164–178}}.
*{{Citation | last=Chiu| first=David K. Y. | last2=Cheung| first2=Benny |year= 1989| chapter=Hierarchical maximum entropy discretization | editor=Ryszard Janicki |editor2=Waldemar W. Koczkodaj | title=Computing and Information: Proceedings of the International Conference on Computing and Information (ICCI '89) | edition= | publisher=North-Holland | place=[[Toronto|Toronto, Ontario]], [[Canada]] |pages=237–242}}.
*{{Citation | last=Chiu| first=David K. Y. | last3=Wong| first3=Andrew K. C.|last2=Cheung| first2=Benny | author-link= | title=Information synthesis based on hierarchical maximum entropy discretization | journal=[[Journal of Experimental and Theoretical Artificial Intelligence]] |volume=2 |issue= | year=1990| pages=117–129 | doi=10.1080/09528139008953718}}.
*{{Citation | last=Chiu| first=David K. Y. | last2=Wong| first2=Andrew K. C.|last3=Cheung| first3=Benny |year= 1991| chapter=Information discovery through hierarchical maximum entropy discretization and synthesis | editor=Gregory Piatetsky-Shapiro |editor2=William J. Frawley | title=Knowledge Discovery in Databases | edition= | publisher=MIT Press | place=[[Cambridge, MA]] | url= | accessdate=|pages=126–140}}.
*{{Citation | last=Chmielewski| first=Michal R. | last2=Grzymala-Busse| first2=Jerzy W.| author-link= | title=Global discretization of continuous attributes as preprocessing for machine learning | journal=International Journal of Approximate Reasoning |volume=15 |issue= | year=1996| pages=319–331 | url=http://kuscholarworks.ku.edu/dspace/bitstream/1808/412/1/j36-draft.pdf | doi=10.1016/s0888-613x(96)00074-6}}.
*{{Citation | last=Dougherty| first=James | last2=Kohavi| first2=Ron | last3=Sahami| first3=Mehran| year=1995|chapter=Supervised and unsupervised discretization of continuous features | editor=Armand Prieditis |editor2=Stuart Russell | title=Machine Learning: Proceedings of the Twelfth International Conference (ICML 1995) | edition= | publisher=Morgan Kaufmann | place=[[Tahoe City, CA]] | url=http://citeseer.ist.psu.edu/dougherty95supervised.html | accessdate= |pages=194–202}}.
*{{Citation | last=Duda| first=Richard O.| last2=Hart| first2=Peter E. | last3=Stork| first3=David G. |title=Pattern Classification| publisher=John Wiley &amp; Sons| place=[[New York City]] | year=2001| edition=2nd |isbn=978-0-471-05669-0}}
*{{Citation | last=Fayyad| first=Usama M.| last2=Irani| first2=Keki B.|  year=1993|chapter=Multi-interval discretization of continuous-valued attributes for classification learning | editor=edited volume | title=Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93) | place=[[Chambéry, France]] |pages=1022–1027}}.
*{{Citation | last=Grzymala-Busse| first=Jerzy W. | last2=Stefanowski| first2=Jerzy| author-link= | title=Three discretization methods for rule induction | journal=International Journal of Intelligent Systems |volume=16 |issue=1 | year=2001| pages=29–38 | doi=10.1002/1098-111X(200101)16:1&lt;29::AID-INT4&gt;3.0.CO;2-0}}.
*{{Citation | last=Hastie| first=Trevor|authorlink1=Trevor Hastie|last2=Tibshirani| first2=Robert |authorlink2=Robert Tibshirani| last3=Friedman| first3=Jerome |title=The Elements of Statistical Learning: Data Mining, Inference, and Prediction| publisher=Springer| place=[[New York City]] | year=2001| isbn=978-0-387-84857-0}}
*{{Citation
 | last=Kraskov | first=Alexander | last2=Stögbauer | first2=Harald
 | last3=Andrzejak| first3=Ralph G. | last4=Grassberger | first4=Peter
 | title=Hierarchical clustering based on mutual information
 | year=2003| page= | arxiv=q-bio/0311039}}.
*{{Citation
 | last=Lee | first=Changhwan | last2=Shin | first2=Dong-Guk
 | year=1994
 | chapter=A context-sensitive discretization of numeric attributes for classification learning
 | editor=A. G. Cohn
 | title=Proceedings of the 11th European Conference on Artificial Intelligence (ECAI 94)
 | publisher= | place=[[Netherlands|NL]]
 | url= | accessdate= |pages=428–432}}.
*{{Citation
 | last=Liu | first=Chao-Lin | last2=Wellman | first2=Michael
 | year=2002
 | title=Evaluation of Bayesian networks with flexible state-space abstraction methods
 | journal=International Journal of Approximate Reasoning
 | volume=30 | issue=1 | pages=1–39 | doi=10.1016/S0888-613X(01)00067-6}}.
*{{Citation
 | last=Liu | first=Chao-Lin | last2=Wellman | first2=Michael
 | year=2004
 | title= Bounding probabilistic relationships in Bayesian networks using qualitative influences: Methods and applications
 | journal=International Journal of Approximate Reasoning
 | volume=36 | issue=1 | pages=31–73 | doi=10.1016/j.ijar.2003.06.002}}.
*{{Citation
 | last=Liu | first=Huan | last2=Hussain | first2=Farhad
 | last3=Tan| first3=Chew Lim | last4=Dasii | first4=Manoranjan
 | title=Discretization: An enabling technique
 | journal=Data Mining and Knowledge Discovery
 | volume=6 |issue=4 | year=2002| pages=393–423
 | doi=10.1023/A:1016304305535}}.
*{{Citation
 | last=Ludl | first=Marcus-Christopher | last2=Widmer | first2=Gerhard
 | year=2000
 | chapter=Relative unsupervised discretization for association rule mining
 | editor=Djamel A. Zighed |editor2=Jan Komorowski |editor3=Jan Zytkow
 | title=Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery (PKDD 2000)
 | publisher= | place=[[Lyon, France]]
 | doi=10.1007/3-540-45372-5_15 |pages=148–158}}.
*{{Citation
 | last=Monti | first=Stefano | last2=Cooper | first2=Gregory F.
 | year=1999
 | chapter=A latent variable model for multivariate discretization
 | editors=edited volume
 | title=Uncertainty 99: The 7th International Workshop on Artificial Intelligence and Statistics
 | publisher= | place=[[Fort Lauderdale, FL]]
 | url=http://citeseer.ist.psu.edu/monti99latent.html | accessdate= }}.
*{{Citation
 | last=Nguyen | first=Hung Son | last2=Nguyen | first2=Sinh Hoa
 | year=1998 | chapter=Discretization methods in data mining
 | editor=Lech Polkowski |editor2=Andrzej Skowron
 | title=Rough Sets in Knowledge Discovery 1: Methodology and Applications
 | publisher=Physica-Verlag | place=[[Heidelberg]]
 | url= | accessdate= | pages=451–482}}.
*{{Citation
 | last=Pfahringer | first=Bernhard | year=1995
 | chapter=Compression-based discretization of continuous attributes
 | editor=Armand Prieditis |editor2=Stuart Russell
 | title=Machine Learning: Proceedings of the Twelfth International Conference (ICML 1995)
 | publisher=Morgan Kaufmann | place=[[Tahoe City, CA]]
 | url=http://citeseer.ist.psu.edu/pfahringer95compressionbased.html
 | accessdate= |pages=456–463}}.
*{{Citation
 | last=Rencher | first=Alvin C.
 | title=Methods of Multivariate Analysis
 | publisher=Wiley | place=[[New York City]] | year=2002| url=}}.
*{{Citation
 | last=Simon | first=Herbert A. | last2=Ando | first2=Albert
 | year= 1963
 | chapter=Aggregation of variables in dynamic systems
 | editor=Albert Ando |editor2=Franklin M. Fisher |editor3=Herbert A. Simon
 | title=Essays on the Structure of Social Science Models
 | publisher=MIT Press | place=Cambridge, MA | pages=64–91
 | url=
 | accessdate=
}}
*{{Citation
 | last=Simon | first=Herbert A.
 | year= 1996
 | chapter=The architecture of complexity: Hierarchic systems
 | editor=Herbert A. Simon
 | title=The Sciences of the Artificial
 | edition=2nd | publisher=MIT Press | place=Cambridge, MA
 | url=
 | accessdate=
 | pages=183–216}}
*{{Citation | last=Slezak| first=Dominik | last2=Synak| first2=Piotr| last3=Wojna| first3=Arkadiusz| last4=Wroblewski| first4=Jakub| author-link= | title=Two Database Related Interpretations of Rough Approximations: Data Organization and Query Execution | journal=Fundamenta Informaticae |volume=127 |issue=1-4 | year=2013| pages=445-459 | doi=10.3233/FI-2013-920}}.
*{{Citation
 | last=Ting| first=Kai Ming
 | title=Discretization of continuous-valued attributes and instance-based learning (Technical Report No.491)
 | publisher=Basser Department of Computer Science
 | place=[[Sydney]] | year=1994
 | url=http://citeseer.ist.psu.edu/145651.html }}.
*{{Citation
 | last=Wang | first=Ke | last2=Liu| first2=Bing
 | year=1998 | chapter=Concurrent discretization of multiple attributes
 | editor=Springer
 | title=Proceedings of the 5th Pacific Rim International Conference on Artificial Intelligence
 | publisher=Springer-Verlag | place=[[London]]
 | url=http://citeseer.ist.psu.edu/wang98concurrent.html
 | accessdate= |pages=250–259}}.
*{{Citation
 | last=Watanabe| first=Satosi
 | authorlink=Satosi Watanabe
 | title=Information theoretical analysis of multivariate correlation
 | journal=IBM Journal of Research and Development
 | volume=4 |issue=1 | year=1960| pages=66–82 | url= | doi=10.1147/rd.41.0066}}.
*{{Citation
 | last=Watanabe| first=Satosi
 | authorlink=Satosi Watanabe
 | title=Knowing and Guessing: A Quantitative Study of Inference and Information
 | publisher=Wiley | place=[[New York City]] | year=1969| url=}}.
*{{Citation
 | last=Witten | first=Ian H. | last2=Frank| first2=Eibe
 | title=Data Mining: Practical Machine Learning Tools and Techniques
 | publisher=Morgan Kaufmann | place=[[Amsterdam]] | edition=2
 | year=2005 | url=http://www.cs.waikato.ac.nz/~ml/weka/book.html}}
*Yao, Y.Y. (2004) &quot;A Partition Model of Granular Computing&quot;, Lecture Notes in Computer Science (to appear)
*{{cite conference
  | first = Y. Y. | last = Yao
  | title = On modeling data mining with granular computing
  | booktitle = Proceedings of the 25th Annual International Computer Software and Applications Conference (COMPSAC 2001)
  | pages = 638–643
  | year = 2001
  | url = http://portal.acm.org/citation.cfm?id=675398}}
*{{cite conference
  | first = Yiyu | last = Yao
  | title = Granular computing for data mining
  | booktitle = Proceedings of the SPIE Conference on Data Mining, Intrusion Detection, Information Assurance, and Data Networks Security
  | year = 2006
  | editor = [[Belur V. Dasarathy|Dasarathy, Belur V.]]
  | url = http://www2.cs.uregina.ca/~yyao/PAPER_PDF/grcfordm06.pdf}}
*{{cite conference
  | first = J. T. | last = Yao
  |author2=Yao, Y. Y.
  | title = Induction of classification rules by granular computing
  | booktitle = Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC'02)
  | pages = 331–338
  | publisher = Springer-Verlag
  | year = 2002
  | location = London, UK
  | url = http://www2.cs.uregina.ca/~jtyao/Papers/53_RSCTC02.pdf}}
*Zadeh, L.A. (1997) &quot;Toward a Theory of Fuzzy Information Granulation and its Centrality in Human Reasoning and Fuzzy Logic&quot;'', Fuzzy Sets and Systems'', 90:111-127
*{{Citation
 | last=Zighed | first=D. A. | last2=Rabaséda | first2=S.
 | last3=Rakotomalala | first3=R.
 | title=FUSINTER: A method for discretization of continuous attributes
 | journal=[[International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems]]
 | volume=6 | issue=3 | year=1998 | pages=307–326
 | url=http://portal.acm.org/citation.cfm?id=353472 | doi=10.1142/s0218488598000264}}.
{{refend}}


</text>
      <sha1>bspdsl4l6ggsr4qpb0s8vlawei5zv9b</sha1>
    </revision>
  </page>
  <page>
    <title>Probability matching</title>
    <ns>0</ns>
    <id>9731945</id>
    <revision>
      <id>739554769</id>
      <parentid>737505527</parentid>
      <timestamp>2016-09-15T11:11:06Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed ; added [[Category:Decision-making]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1890">{{no footnotes|date=February 2015}}
'''Probability matching''' is a [[decision strategy]] in which predictions of class membership are proportional to the class [[base rates]].  Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a ''probability-matching'' strategy will predict (for unlabeled examples) a class label of &quot;positive&quot; on 60% of instances, and a class label of &quot;negative&quot; on 40% of instances.

The optimal [[Bayesian decision theory|Bayesian decision strategy]] (to maximize the number of correct predictions, see {{Harvtxt|Duda|Hart|Stork|2001}}) in such a case is to always predict &quot;positive&quot; (i.e., predict the majority category in the absence of other information), which has 60% chance of winning rather than matching which has 52% of winning  (where ''p'' is the probability of positive realization, the result of matching would be &lt;math&gt;p^2+(1-p)^2&lt;/math&gt;, here &lt;math&gt;.6 \times .6+ .4 \times .4&lt;/math&gt;).  The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to [[Thompson sampling]]).

== References ==
*{{Citation
 | last=Duda| first=Richard O.| last2=Hart| first2=Peter E.
 | last3=Stork| first3=David G.
 | title=Pattern Classification | publisher=John Wiley &amp; Sons
 | place=[[New York City|New York]] | year=2001 | edition=2nd
 | url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html}}

* Shanks, D. R., Tunney, R. J., &amp; McCarthy, J. D. (2002). A re‐examination of probability matching and rational choice. ''Journal of Behavioral Decision Making'', 15(3), 233-250.



[[Category:Decision-making]]



{{statistics-stub}}</text>
      <sha1>2bu9pbgevt2ien4wyr89475fpj4e1hk</sha1>
    </revision>
  </page>
  <page>
    <title>Structural risk minimization</title>
    <ns>0</ns>
    <id>10704974</id>
    <revision>
      <id>746252986</id>
      <parentid>746252952</parentid>
      <timestamp>2016-10-26T06:49:36Z</timestamp>
      <contributor>
        <username>JWNoctis</username>
        <id>19212762</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/155.48.168.44|155.48.168.44]] ([[User talk:155.48.168.44|talk]]): Nonconstructive editing ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="982">{{otheruses|Minimisation (disambiguation){{!}}Minimisation}}
'''Structural risk minimization (SRM)''' is an inductive principle of use in [[machine learning]]. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of [[overfitting]] &amp;ndash; the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.

The SRM principle was first set out in a 1974 paper by [[Vladimir Vapnik]] and [[Alexey Chervonenkis]] and uses the [[VC dimension]].

==See also==
* [[Vapnik–Chervonenkis theory]]
* [[Support vector machines]]
* [[Model selection]]
* [[Occam Learning]]

==External links==

* [http://www.svms.org/srm/ Structural risk minimization] at the support vector machines website.

{{compu-sci-stub}}
</text>
      <sha1>poc2ajybg0cc32b8ndjxcou5lzeskao</sha1>
    </revision>
  </page>
  <page>
    <title>Lazy learning</title>
    <ns>0</ns>
    <id>10747879</id>
    <revision>
      <id>779234027</id>
      <parentid>779233595</parentid>
      <timestamp>2017-05-07T19:47:40Z</timestamp>
      <contributor>
        <ip>117.240.124.201</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1564">In [[machine learning]], '''lazy learning''' is a learning method in which generalization beyond the [[training data]] is delayed until a query is made to the system, as opposed to in [[eager learning]], where the system tries to generalize the training data before receiving queries.

The main advantage gained in employing a lazy learning method, such as [[case-based reasoning]], is that the target function will be approximated locally, such as in the [[k-nearest neighbor algorithm]]. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain.

The disadvantages with lazy learning include the large space requirement to store the entire training dataset. Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. Another disadvantage is that lazy learning methods are usually slower to evaluate, though this is coupled with a faster training phase.

Lazy classifiers are most useful for large datasets with few attributes.



==References==
* [https://cran.r-project.org/web/packages/lazy/ lazy: Lazy Learning for Local Regression], [[R (programming language)|R]] package with reference manual
* {{cite web|url=http://iridia0.ulb.ac.be/~lazy/|title=The Lazy Learning Package|archive-url=https://web.archive.org/web/20120216183916/http://iridia0.ulb.ac.be/~lazy/|archive-date=16 February 2012}}




{{Tech-stub}}</text>
      <sha1>jblmmwtq1nwzos2y0l3lpjmvguamcnv</sha1>
    </revision>
  </page>
  <page>
    <title>Eager learning</title>
    <ns>0</ns>
    <id>10747995</id>
    <revision>
      <id>802474837</id>
      <parentid>763815642</parentid>
      <timestamp>2017-09-26T11:16:48Z</timestamp>
      <contributor>
        <username>George Fernandez</username>
        <id>32014845</id>
      </contributor>
      <comment>added reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1517">In [[artificial intelligence]], '''eager learning''' is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to [[lazy learning]], where generalization beyond the training data is delayed until a query is made to the system. &lt;ref&gt;{{cite conference|url=https://books.google.com/books?id=GtcevX7n90wC&amp;pg=PA158 |title=Hybrid algorithms with Instance-Based Classification|author=Hendrickx, Iris|author2=Van den Bosch, Antal|author-link2=Antal van den Bosch|date=October 2005|publisher=Springer|booktitle=Machine Learning: ECML2005|pages=158–169}}&lt;/ref&gt;
The main advantage gained in employing an eager learning method, such as an [[artificial neural network]], is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the [[training data]]. Eager learning is an example of [[offline learning]], in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result.

The main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function.&lt;ref&gt;{{Cite book|title=INTRODUCTION TO KNOWLEDGE PROCESSING|last=|first=|publisher=|year=|isbn=|location=|pages=2}}&lt;/ref&gt;

==References==
{{reflist}}




{{compu-ai-stub}}</text>
      <sha1>18la1aezcc5bldn2g6cxvuhqv6yq1od</sha1>
    </revision>
  </page>
  <page>
    <title>Discriminative model</title>
    <ns>0</ns>
    <id>12155912</id>
    <revision>
      <id>797192781</id>
      <parentid>796351678</parentid>
      <timestamp>2017-08-25T14:04:54Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <minor/>
      <comment>Fixing or [[Category:Pages with ISBN errors|ISBN error]] or other ISBN error using [[WP:AutoEd|AutoEd]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2836">'''Discriminative models''', also called '''conditional models''', are a class of models used in [[machine learning]] for modeling the dependence of unobserved (target) variables &lt;math&gt;y&lt;/math&gt; on observed variables &lt;math&gt;x&lt;/math&gt;. Within a probabilistic framework, this is done by modeling the [[conditional probability distribution]] &lt;math&gt;P(y|x)&lt;/math&gt;, which can be used for predicting &lt;math&gt;y&lt;/math&gt; from &lt;math&gt;x&lt;/math&gt;.

Discriminative models, as opposed to [[generative model]]s, do not allow one to generate samples from the [[joint distribution]] of observed and target variables.  However, for tasks such as [[classification (machine learning)|classification]] and [[regression analysis|regression]] that do not require the joint distribution, discriminative models can yield superior performance (in part because they have fewer variables to compute).&lt;ref&gt;{{Cite journal|last=Singla|first=Parag|last2=Domingos|first2=Pedro|date=2005|title=Discriminative Training of Markov Logic Networks|url=http://dl.acm.org/citation.cfm?id=1619410.1619472|journal=Proceedings of the 20th National Conference on Artificial Intelligence - Volume 2|series=AAAI'05|location=Pittsburgh, Pennsylvania|publisher=AAAI Press|pages=868–873|isbn=157735236X}}&lt;/ref&gt;&lt;ref&gt;J. Lafferty, A. McCallum, and F. Pereira. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ''ICML'', 2001.&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.9829|title=On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes|last=Ng|first=Andrew Y.|last2=Jordan|first2=Michael I.|date=2001}}&lt;/ref&gt;  On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks.  In addition, most discriminative models are inherently [[supervised learning|supervised]] and cannot easily support [[unsupervised learning]]. Application-specific details ultimately dictate the suitability of selecting a discriminative versus generative model.

==Types==
{{prose|date=February 2012}}
Examples of discriminative models used in machine learning include:
*[[Logistic regression]], a type of [[generalized linear model|generalized linear regression]] used for predicting [[Bernoulli distribution|binary]] or [[categorical distribution|categorical]] outputs (also known as [[maximum entropy classifier]]s)
*[[Support vector machines]]
*[[Boosting (meta-algorithm)]]
*[[Conditional random field]]s
*[[Linear regression]]
*[[Neural network]]s
*[[Random Forest|Random forest]]s

== See also ==
* [[Generative model]]

==References==
{{reflist|30em}}




{{Portal|Statistics}}
{{Statistics|state=expanded}}

{{statistics-stub}}
{{comp-sci-stub}}</text>
      <sha1>8vf0w35xm2rts7v5k73480i67dm6wy6</sha1>
    </revision>
  </page>
  <page>
    <title>Data pre-processing</title>
    <ns>0</ns>
    <id>12386904</id>
    <revision>
      <id>815387669</id>
      <parentid>815387581</parentid>
      <timestamp>2017-12-14T15:01:02Z</timestamp>
      <contributor>
        <username>Larry.europe</username>
        <id>11040783</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2234">'''Data pre-processing''' is an important step in the [[data mining]] process. The phrase [[GIGO|&quot;garbage in, garbage out&quot;]] is particularly applicable to data mining and [[machine learning]] projects. Data-gathering methods are often loosely controlled, resulting in [[range error|out-of-range]] values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), [[missing values]], etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and [[data quality|quality of data]] is first and foremost before running an analysis.&lt;ref&gt;Pyle, D., 1999. ''Data Preparation for Data Mining.'' Morgan Kaufmann Publishers, [[Los Altos, California]].&lt;/ref&gt;
Often, data pre-processing is the most important phase of a [[machine learning]] project, especially in [[computational biology]].&lt;ref&gt;{{cite journal
| vauthors = Chicco D
| title = Ten quick tips for machine learning in computational biology
| journal = BioData Mining
| volume = 10
| issue =  35
| pages = 1-17
| date = December 2017
| pmid = 29234465
| doi = 10.1186/s13040-017-0155-3
| pmc= 5721660}}&lt;/ref&gt;

If there is much irrelevant and redundant information present or noisy and unreliable data, then [[knowledge discovery]] during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data pre-processing includes [[Data cleaning|cleaning]], [[Instance selection]], [[data normalization|normalization]], [[data transformation|transformation]], [[feature extraction]] and [[Feature selection|selection]], etc. The product of data pre-processing is the final [[training set]]. Kotsiantis et al. (2006) present a well-known algorithm for each step of data pre-processing.&lt;ref&gt;S. Kotsiantis, D. Kanellopoulos, P. Pintelas, &quot;Data Preprocessing for Supervised Learning&quot;, ''International Journal of Computer Science'', 2006, Vol 1 N. 2, pp 111–117.&lt;/ref&gt;

==See also==
*[[Data cleansing]]
*[[Data editing]]
*[[Data reduction]]
*[[Data wrangling]]

==References==
{{reflist}}

==External links==
*[http://dataprocessing.aixcape.org Online Data Processing Compendium]

{{data}}

</text>
      <sha1>ntsyq7mpmwrhc01wcijl0jpayfacvk9</sha1>
    </revision>
  </page>
  <page>
    <title>Predictive state representation</title>
    <ns>0</ns>
    <id>11360852</id>
    <revision>
      <id>783667875</id>
      <parentid>766039393</parentid>
      <timestamp>2017-06-03T21:44:22Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor/>
      <comment>/* top */ adjust bold, punct.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2771">{{notability|date=March 2011}}

In [[computer science]], a '''predictive state representation''' ('''PSR''') is a way to model a state of controlled [[dynamical system]] from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system.&lt;ref&gt;{{Cite journal|last=James|first=Michael R.|last2=Singh|first2=Satinder|date=2004-01-01|title=Learning and Discovery of Predictive State Representations in Dynamical Systems with Reset|url=http://doi.acm.org/10.1145/1015330.1015359|journal=Proceedings of the Twenty-first International Conference on Machine Learning|series=ICML '04|location=New York, NY, USA|publisher=ACM|pages=53–|doi=10.1145/1015330.1015359|isbn=1581138385}}&lt;/ref&gt; A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities.  This is in contrast to other models of dynamical systems, such as [[partially observable Markov decision process]]es (POMDPs) where the state of the system is represented as a [[probability distribution]] over unobserved nominal states.&lt;ref&gt;{{Cite web|url=https://www.semanticscholar.org/paper/A-Planning-Algorithm-for-Predictive-State-Izadi-Precup/b0bb9a5a8acd36692c13992151dfd812df24da81/pdf|title=A Planning Algorithm for Predictive State Representations (PDF) - Semantic Scholar|website=www.semanticscholar.org|language=en-US|access-date=2016-07-14}}&lt;/ref&gt;

==References==
{{Reflist}}
* {{cite conference
  | last = Littman | first = Michael L. | authorlink = Michael L. Littman |author2=[[Richard S. Sutton]] |author3=Satinder Singh
  | title = Predictive Representations of State
  | booktitle = Advances in Neural Information Processing Systems 14 (NIPS)
  | pages = 1555–1561
  | year = 2002
  | url = http://www.eecs.umich.edu/~baveja/Papers/psr.pdf}}

* {{cite conference
  | last =Singh | first = Satinder |author2=Michael R. James |author3=Matthew R. Rudary
  | title = Predictive State Representations: A New Theory for Modeling Dynamical Systems
  | booktitle = Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference (UAI)
  | pages = 512–519
  | year = 2004
  | url = http://www.eecs.umich.edu/~baveja/Papers/uai2004psr.pdf}}

* {{Citation
  | last = Wiewiora | first = Eric Walter
  | title = Modeling Probability Distributions with Predictive State Representations
  | year = 2008
  | url = http://cseweb.ucsd.edu/~ewiewior/dissertation.pdf}}





{{Compu-AI-stub}}</text>
      <sha1>afiyx1a0u559sfk7jt2ax4fqn2d5njp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Kernel methods for machine learning</title>
    <ns>14</ns>
    <id>12535256</id>
    <revision>
      <id>588588547</id>
      <parentid>524635569</parentid>
      <timestamp>2013-12-31T23:26:08Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor/>
      <comment>Adding Commons category link to [[:Commons:Category:Kernel methods for machine learning|category with the same name]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="208">{{Commons category|Kernel methods for machine learning}}
This page lists categories and articles related to [[kernel methods]] for [[machine learning]].


</text>
      <sha1>7w3dg0stcjnjueytik0pu4mc7xr2d06</sha1>
    </revision>
  </page>
  <page>
    <title>Expectation propagation</title>
    <ns>0</ns>
    <id>14923880</id>
    <revision>
      <id>696623248</id>
      <parentid>604292753</parentid>
      <timestamp>2015-12-24T12:11:30Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor/>
      <comment>/* References */Remove redundant |year= parameter from CS1 citations; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1077">'''Expectation propagation (EP)''' is a technique in [[Bayesian inference|Bayesian machine learning]].

EP finds approximations to a [[probability distribution]]. It uses an [[iterative]] approach that leverages the factorization structure of the target distribution.  It differs from other Bayesian approximation approaches such as [[Variational Bayesian methods]].

==References==
*{{cite book|author=[[Thomas Minka]]|chapter=Expectation Propagation for Approximate Bayesian Inference|url=http://research.microsoft.com/en-us/um/people/minka/papers/ep/minka-ep-uai.pdf|editor=Jack S. Breese, Daphne Koller|title=UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence|location=University of Washington, Seattle, Washington, USA|date=August 2–5, 2001|pages=362–369}}

==External links==
* [http://research.microsoft.com/~minka/papers/ep/ Minka's EP papers]
* [http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html List of papers using EP].





{{compsci-stub}}</text>
      <sha1>6ne3u53q8cte0sn9k13kw39r7vs71j3</sha1>
    </revision>
  </page>
  <page>
    <title>Multiple-instance learning</title>
    <ns>0</ns>
    <id>14082194</id>
    <revision>
      <id>809539753</id>
      <parentid>774102934</parentid>
      <timestamp>2017-11-09T19:46:47Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <minor/>
      <comment>/* References */[[User:JCW-CleanerBot#Logic|task]], replaced: Sci Rep. → Sci. Rep. using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7213">{{merge to|Multiple_instance_learning|discuss=Talk:Multiple_instance_learning#Merger proposal|date=September 2016}}
In [[machine learning]], '''multiple-instance learning''' (MIL) is a variation on [[supervised learning]].  Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled ''bags'', each containing many instances. In the simple case of multiple-instance [[binary classification]], a bag may be labeled negative if all the instances in it are negative.  On the other hand, a bag is labeled positive if there is at least one instance in it which is positive.  From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.

Take image classification for example in {{harvtxt|Amores|2013}}. Given an image, we want to know its target class based on its visual content. For instance, the target class might be &quot;beach&quot;, where the image contains both &quot;sand&quot; and &quot;water&quot;. In '''MIL''' terms, the image is described as a ''bag'' &lt;math&gt;X = \{X_1,..,X_N\}&lt;/math&gt;, where each&lt;math&gt;X_i&lt;/math&gt; is the feature vector (called ''instance'') extracted from the corresponding i-th region in the image and N is the total regions (instances) partitioning the image. The bag is labeled ''positive'' (&quot;beach&quot;) if it contains both &quot;sand&quot; region instances and &quot;water&quot; region instances.

Multiple-instance learning was originally proposed under this name by {{harvtxt|Dietterich|Lathrop|Lozano-Pérez|1997}}, but earlier examples of similar research exist, for instance in the work on [[handwriting|handwritten]] [[Numerical digit|digit]] [[optical character recognition|recognition]] by {{harvtxt|Keeler|Rumelhart|Leow|1990}}. Recent reviews of the MIL literature include {{harvtxt|Amores|2013}}, which provides an extensive review and comparative study of the different paradigms, and {{harvtxt|Foulds|Frank|2010}}, which provides a thorough review of the different assumptions used by different paradigms in the literature.

Examples of where MIL is applied are:

* Molecule activity
* Predicting binding sites of [[Calmodulin]] binding proteins &lt;ref&gt;{{cite journal|last1=Minhas|first1=Fayyaz|title=Multiple instance learning of Calmodulin binding sites,|journal=Bioinformatics|date=2012|volume=28|issue=18|page=i416-i422|url=http://bioinformatics.oxfordjournals.org/content/28/18/i416.short|doi=10.1093/bioinformatics/bts416|pmid=22962461|pmc=3436843}}&lt;/ref&gt;
* Predicting function for alternatively spliced isoforms {{harvtxt|Li|Menon|et al.|2014}},{{harvtxt|Eksi|Li|Menon|et al.|2013}}
* Image classification {{harvtxt|Maron|Ratan|1998}}
* Text or document categorization {{harvtxt|Kotzias et al.| 2015}}
* Predicting functional binding sites of MicroRNA targets {{harvtxt|Bandyopadhyay|Ghosh|et al.|2015}}
* Medical image classification {{harvtxt|Zhu et al.| 2016}}

Numerous researchers have worked on adapting classical classification techniques, such as [[support vector machines]] or [[Boosting (meta-algorithm)|boosting]], to work within the context of multiple-instance learning.

==See also==
* [[Multiple instance learning]]
* [[Multi-label classification]]

==References==
{{Reflist}}
*{{citation
 | first1 = Thomas G. | last1 = Dietterich
 | first2 = Richard H. | last2 = Lathrop
 | first3 = Tomás | last3 = Lozano-Pérez
 | title = Solving the multiple instance problem with axis-parallel rectangles
 | journal = Artificial Intelligence
 | volume = 89 | issue = 1–2 | year = 1997 | pages = 31–71 | doi = 10.1016/S0004-3702(96)00034-3}}.

*{{citation
 | first1 = Francisco | last1 = Herrera
 | first2 = Sebastián. | last2 =Ventura
 | first3 = Rafael | last3 = Bello
 | first4 = Chris | last4 = Cornelis
 | first5 = Amelia | last5 = Zafra
 | first6 = Dánel | last6 = Sánchez-Tarragó
 | first7 = Sarah | last7 = Vluymans
 | title = Multiple Instance Learning - Foundations and Algorithms
 | publisher = Springer
 | year = 2016 | doi = 10.1007/978-3-319-47759-6}}.

*{{citation
 | first1 = Jaume| last1 = Amores
 | title = Multiple instance classification: Review, taxonomy and comparative study
 | journal = Artificial Intelligence
 | volume = 201 | year = 2013 | pages = 81–105 | doi = 10.1016/j.artint.2013.06.003}}.

*{{citation
 | first1 = James | last1 = Foulds
 | first2 = Eibe | last2 = Frank
 | title = A Review of Multi-Instance Learning Assumptions
 | journal = Knowledge Engineering Review
 | volume = 25 | issue = 1 | year = 2010 | pages = 1–25 | doi = 10.1017/S026988890999035X}}.

*{{citation
 | first1 = James D. | last1 = Keeler
 | first2 = David E. | last2 = Rumelhart
 | first3 = Wee-Kheng | last3 = Leow
 | title = Integrated segmentation and recognition of hand-printed numerals
 | year = 1990 | pages = 557–563
 | work  = Proceedings of the 1990 Conference on Advances in Neural Information Processing Systems (NIPS 3)}}.

*{{citation
 | first1 = H.D. | last1 = Li
 | first2 = R. | last2 = Menon
 | title= The emerging era of genomic data integration for analyzing splice isoform function
 | year= 2014 | id = pii S0168-9525(14)00085-7
 | journal = Trends in Genetics
 | doi = 10.1016/j.tig.2014.05.005 | pmid=24951248 | volume=30 | pages=340–347|display-authors=etal}}.

*{{citation
 | first1 = R. | last1 = Eksi
 | first2 = H.D. | last2 = Li
 | first3 = R. | last3 = Menon
 | title= Systematically differentiating functions for alternatively spliced isoforms through integrating RNA-seq data
 | year= 2013 | pages = Nov;9(11):e1003314
 | journal = PLoS Comput Biol
 | doi = 10.1371/journal.pcbi.1003314 | pmid=24244129 | pmc=3820534 | volume=9|display-authors=etal}}.

*{{citation
 | first1 = O. | last1 = Maron
 | first2 = A.L. | last2 = Ratan
 | title= Multiple-instance learning for natural scene classification
 |  year= 1998 | pages = 341–349
 | work= Proceedings of the Fifteenth International Conference on Machine Learning}}.

*{{citation
 | first1 = Dimitrios | last1 = Kotzias
 | first2 = Misha  | last2 = Denil
 | first3 = Nando  | last3 = De Freitas
 | first4 = Padhraic  | last4 = Smyth
 | title= From Group to Instance Labels, using Deep Features
 |  year= 2015 | pages = 597–606
| doi = 10.1145/2783258.2783380
| work= Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
}}.

*{{cite conference
 |last1=Ray |first1=Soumya
 |first2=David |last2=Page
 |title=Multiple instance regression
 |conference=ICML
 |year=2001
 |url=http://pages.cs.wisc.edu/~sray/papers/mip.reg.icml01.pdf}}.

*{{citation
 | first1 = S. | last1 = Bandyopadhyay
 | first2 = D. | last2 = Ghosh
 | first3 = R. | last3 = Mitra
 | first4 = Z. | last4 = Zhao
 | title = MBSTAR: multiple instance learning for predicting specific functional binding sites in microRNA targets.
 | journal=Sci. Rep.
 | volume=5|year=2015
 | pmid=25614300
 | doi=10.1038/srep08004 | pages=8004 | pmc=4648438}}
*Zhu, Wentao; Lou, Qi; Vang, Yeeleng Scott; Xie, Xiaohui (2016), &quot;[https://arxiv.org/abs/1612.05968 Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification]&quot;, ''arXiv preprint arXiv:1612.05968''.

</text>
      <sha1>nds9zy10p9lk7oj1o0zczw3ydgmo718</sha1>
    </revision>
  </page>
  <page>
    <title>Ugly duckling theorem</title>
    <ns>0</ns>
    <id>5077439</id>
    <revision>
      <id>814945503</id>
      <parentid>814720902</parentid>
      <timestamp>2017-12-11T20:48:20Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>/* Mathematical formula */ adapt caption to new image (uses only observable properties)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11127">{{refimprove|date=October 2011}}

The '''Ugly Duckling theorem''' is an [[argument]] asserting that classification is impossible without some sort of [[bias (statistics)|bias]]. More particularly, it assumes finitely many properties combinable by [[logical connective]]s, and finitely many objects; it asserts that any two [[Identity of indiscernibles|different]] objects share the same number of ([[Extension (predicate logic)|extensional]]) properties. The theorem is named for [[Hans Christian Andersen]]'s story &quot;[[The Ugly Duckling]]&quot;, because it shows that a [[duckling]] is just as similar to a [[swan]] as two duckling are to each other. It was proposed by [[Satosi Watanabe]] in 1969.&lt;ref name=&quot;Watanabe.1969&quot;&gt;{{cite book
| lccn=68-56165
| isbn=0-471-92130-0
| url=
| author=Satosi Watanabe
| title=Knowing and Guessing: A Quantitative Study of Inference and Information
| location=New York
| publisher=Wiley
| edition=
| year=1969 }}&lt;/ref&gt;{{rp|376–377}}

==Mathematical formula==
[[File:Watanabe UglyDucklingTheorem svg.svg|thumb|400px|Watanabe's example, using objects ''A'', ''B'', ''C'', and properties F (&quot;first&quot;), W (&quot;white&quot;). &quot;0&quot;, &quot;1&quot;, &quot;[[negation|¬]]&quot; , &quot;[[conjunction (logic)|∧]]&quot;, &quot;[[disjunction (logic)|∨]]&quot;, and &quot;[[exclusive or|⊕]]&quot; denote &quot;''false''&quot;, &quot;''true''&quot;, &quot;''[[negation|not]]''&quot;, &quot;''[[conjunction (logic)|and]]''&quot;, &quot;''[[disjunction (logic)|or]]''&quot;, and &quot;''[[exclusive or]]''&quot;, respectively. Since F happens to imply W, each predicate that can be formed from F and W coincides with another one, hence there are only 8 [[extension (semantics)|extensionally]] distinct possible predicates. The white ducklings ''A'' and ''B'' agree on 4 of them (line 2, 3, 4, 8), but so do ''A'' and ''C'', too (line 3, 5, 7, 8), and so do ''B'' and ''C'' (line 1, 3, 6, 8).&lt;ref name=&quot;Watanabe.1969&quot;/&gt;{{rp|368}}&lt;ref&gt;Watanabe's ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;, ''y''&lt;sub&gt;1&lt;/sub&gt;, and ''y''&lt;sub&gt;2&lt;/sub&gt;, correspond to ''C'', ''B'', ''A'', F, and W, respectively.&lt;/ref&gt;]]

Suppose there are &lt;var&gt;n&lt;/var&gt; things in the universe, and one wants to put them into classes or categories. One has no preconceived ideas or [[bias]]es about what sorts of categories are &quot;natural&quot; or &quot;normal&quot; and what are not. So one has to consider all the possible classes that could be, all the possible ways of making sets out of the &lt;var&gt;n&lt;/var&gt; objects. There are &lt;math&gt;2^n&lt;/math&gt; such ways, the size of the [[power set]] of &lt;var&gt;n&lt;/var&gt; objects. One can use that to measure the similarity between two objects: and one would see how many sets they have in common. However one can not. Any two objects have exactly the same number of classes in common if we can form any possible class, namely &lt;math&gt;2^{n-1}&lt;/math&gt; (half the total number of classes there are). To see this is so, one may imagine each class is a represented by an &lt;var&gt;n&lt;/var&gt;-bit [[bit array|string]] (or [[binary numeral system|binary encoded]] integer), with a zero for each element not in the class and a one for each element in the class. As one finds, there are &lt;math&gt;2^n&lt;/math&gt; such strings.

As all possible choices of zeros and ones are there, any two bit-positions will agree exactly half the time. One may pick two elements and reorder the bits so they are the first two, and imagine the numbers sorted lexicographically. The first &lt;math&gt;2^n/2&lt;/math&gt; numbers will have bit #1 set to zero, and the second &lt;math&gt;2^n/2&lt;/math&gt; will have it set to one. Within each of those blocks, the top &lt;math&gt;2^n/4&lt;/math&gt; will have bit #2 set to zero and the other &lt;math&gt;2^n/4&lt;/math&gt; will have it as one, so they agree on two blocks of &lt;math&gt;2^n/4&lt;/math&gt; or on half of all the cases. No matter which two elements one picks. So if we have no preconceived bias about which categories are better, everything is then equally similar (or equally dissimilar). The number of [[propositional function|predicates]] simultaneously satisfied by two non-identical elements is constant over all such pairs and is the same{{citation needed|date=March 2011}} as the number of those satisfied by one.&lt;!-- I calculated these as 2^{n-2} and 2^{n-1} respectively. --&gt; Thus, some kind of inductive{{citation needed|date=March 2011}} bias is needed to make judgements; i.e. to prefer certain categories over others.

===Boolean functions===
Let &lt;math&gt;x_1, x_2, \dots, x_n&lt;/math&gt; be a set of vectors of &lt;math&gt;k&lt;/math&gt; booleans each.  The ugly duckling is the vector which is least like the others.  Given the booleans, this can be computed using [[Hamming distance]].

However, the choice of boolean features to consider could have been somewhat arbitrary.  Perhaps there were features derivable from the original features that were important for identifying the ugly duckling.  The set of booleans in the vector can be extended with new features computed as boolean functions of the &lt;math&gt;k&lt;/math&gt; original features.  The only canonical way to do this is to extend it with ''all'' possible Boolean functions.  The resulting completed vectors have &lt;math&gt;2^k&lt;/math&gt; features.  The Ugly Duckling Theorem states that there is no ugly duckling because any two completed vectors will either be equal or differ in exactly half of the features.

Proof.  Let x and y be two vectors.  If they are the same, then their completed vectors must also be the same because any Boolean function of x will agree with the same Boolean function of y.  If x and y are different, then there exists a coordinate &lt;math&gt;i&lt;/math&gt; where the &lt;math&gt;i&lt;/math&gt;-th coordinate of &lt;math&gt;x&lt;/math&gt; differs from the &lt;math&gt;i&lt;/math&gt;-th coordinate of &lt;math&gt;y&lt;/math&gt;.  Now the completed features contain every Boolean function on &lt;math&gt;k&lt;/math&gt; Boolean variables, with each one exactly once.  Viewing these Boolean functions as polynomials in &lt;math&gt;k&lt;/math&gt; variables over GF(2), segregate the functions into pairs &lt;math&gt;(f,g)&lt;/math&gt; where &lt;math&gt;f&lt;/math&gt; contains the &lt;math&gt;i&lt;/math&gt;-th coordinate as a linear term and &lt;math&gt;g&lt;/math&gt; is &lt;math&gt;f&lt;/math&gt; without that linear term.  Now, for every such pair &lt;math&gt;(f,g)&lt;/math&gt;, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; will agree on exactly one of the two functions.
If they agree on one, they must disagree on the other and vice versa.  (This proof is believed to be due to Watanabe.)

==Discussion==

{{clarify span|A solution to the Ugly Ducking Theorem|A problem may have a 'solution', but a theorem may not.|date=December 2017}} would be to introduce a constraint on how similarity is measured by limiting the properties involved in classification, say between A and B. However Medin et al. (1993) point out that this does not actually resolve the arbitrariness or bias problem since in what respects A is similar to B: “varies with the stimulus context and task, so that there is no unique answer, to the question of how similar is one object to another”.&lt;ref&gt;{{cite journal | url= | author=Douglas L. Medin and R.L. Goldstone and Dedre Gentner | title=Respects for similarity | journal=Psychological Review | volume=100 | number=2 | pages=254&amp;mdash;278 | month= | year=1993 }}&lt;/ref&gt;&lt;ref&gt;The philosopher [[Nelson Goodman]] (1972){{cn|date=December 2017}} came to the same conclusion: &quot;But importance is a highly volatile matter, varying with every shift of context and interest, and quite incapable of supporting the fixed distinctions that philosophers so often seek to rest upon it&quot;.&lt;/ref&gt; For example, &quot;a barberpole and a zebra would be more similar than a horse and a zebra if the feature ''striped'' had sufficient weight. Of course, if these feature weights were fixed, then these similarity relations would be constrained&quot;. Yet the property &quot;striped&quot; as a weight 'fix' or constraint is arbitrary itself, meaning: &quot;unless one can specify such criteria, then the claim that categorization is based on attribute matching is almost entirely vacuous&quot;.

Stamos (2003) has attempted to solve the Ugly Ducking Theorem by showing some judgments of overall similarity are non-arbitrary in the sense they are useful:

{{Quote|&quot;Presumably, people's perceptual and conceptual processes have evolved that information that matters to human needs and goals can be roughly approximated by a similarity heuristic... If you are in the jungle and you see a tiger but you decide not to stereotype (perhaps because you believe that similarity is a false friend), then you will probably be eaten. In other words, in the biological world stereotyping based on veridical judgments of overall similarity statistically results in greater survival and reproductive success.&quot;&lt;ref&gt;Stamos, D. N. (2003). ''The Species Problem''. Lexington Books. p. 344.&lt;/ref&gt;}}

Unless some properties are considered more salient, or ‘weighted’ more important than others, everything will appear equally similar, hence Watanabe (1986) wrote: “any objects, in so far as they are distinguishable, are equally similar&quot;.&lt;ref&gt;{{cite journal | url= | author=Satosi Watanabe | title=Epistemological Relativity | journal=Annals of the Japan Association for Philosophy of Science | volume=7 | number=1 | pages=1&amp;mdash;14 | month= | year=1986 }}&lt;/ref&gt;

&lt;!---commented out (wrong):---Watanabe came to realize there is an unquantifiable number of shared properties between all objects, making any classification biased.---&gt;
In a weaker setting that assumes infinitely many properties, Murphy and Medin (1985) give an example of two putative classified things, plums and lawnmowers:

{{Quote|&quot;Suppose that one is to list the attributes that plums and lawnmowers have in common in order to judge their similarity. It is easy to see that the list could be infinite: Both weigh less than 10,000 kg (and less than 10,001 kg), both did not exist 10,000,000 years ago (and 10,000,001 years ago), both cannot hear well, both can be dropped, both take up space, and so on. Likewise, the list of differences could be infinite… any two entities can be arbitrarily similar or dissimilar by changing the criterion of what counts as a relevant attribute.&quot;&lt;ref&gt;{{cite journal | url=http://matt.colorado.edu/teaching/highcog/spr10/readings/mm85.pdf | author=Gregory L. Murphy and Douglas L. Medin | title=The Role of Theories in Conceptual Coherence | journal=Psychological Review | volume=92 | number=3 | pages=289&amp;mdash;316 | date=Jul 1985 }}&lt;/ref&gt;}}
&lt;!---commented out (doesn't refer to Watanabe's setting, no indication that is refers to Murphy and Medin's setting, either):---However, since there is an unlimited number of properties to choose from, it remains an arbitrary choice what properties to select/deselect. This makes classification biased.---&gt;
&lt;!---commented out (has been said in the lead):---Watanabe named this the &quot;Ugly Duckling theorem&quot; because a swan is as similar to a duckling as to another swan (there are no constraints or fixes on what constitutes similarity).---&gt;

==See also==
* [[No free lunch in search and optimization]]
* [[No free lunch theorem]]

==Notes==
&lt;references/&gt;

{{The Ugly Duckling}}






</text>
      <sha1>kbhi5b6fhvq84f5oe08jsc7zy6z590i</sha1>
    </revision>
  </page>
  <page>
    <title>Rademacher complexity</title>
    <ns>0</ns>
    <id>14529261</id>
    <revision>
      <id>812558065</id>
      <parentid>807714456</parentid>
      <timestamp>2017-11-28T14:51:16Z</timestamp>
      <contributor>
        <username>Amit Moscovich</username>
        <id>1377490</id>
      </contributor>
      <comment>/* Definitions */ Removed absolute value in the definition of the empirical Rademacher complexity of F given S and the passive-aggressive comment following it. The current definition is compatible with Eq. (26.4) of Shalev-Shwartz et al.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10974">In [[computational learning theory]] ([[machine learning]] and [[theory of computation]]), '''Rademacher complexity''', named after [[Hans Rademacher]], measures richness of a class of real-valued functions with respect to a [[probability distribution]].

== Definitions ==
=== Rademacher complexity of a set ===
Given a set &lt;math&gt;A\subseteq \mathbb{R}^m&lt;/math&gt;, the '''Rademacher complexity of A''' is defined as follows:&lt;ref name=b11&gt;{{cite web|last1=Balcan|first1=Maria-Florina|title=Machine Learning Theory - Rademacher Complexity|url=http://www.cs.cmu.edu/~ninamf/ML11/lect1117.pdf|accessdate=10 December 2016|date=November 15–17, 2011}}&lt;/ref&gt;&lt;ref name=book14&gt;Chapter 26 in {{Cite Shai Shai 2014}}&lt;/ref&gt;{{rp|326}}
:&lt;math&gt;
\operatorname{Rad}(A)
:=
\frac{1}{m}
   \mathbb{E}_\sigma \left[
   \sup_{a \in A}
      \sum_{i=1}^m \sigma_i a_i
\right]
&lt;/math&gt;

where &lt;math&gt;\sigma_1, \sigma_2, \dots, \sigma_m&lt;/math&gt; are independent random variables drawn from the [[Rademacher distribution]] i.e.
&lt;math&gt;\Pr(\sigma_i = +1) = \Pr(\sigma_i = -1) = 1/2&lt;/math&gt; for &lt;math&gt;i=1,2,\dots,m&lt;/math&gt;.

=== Rademacher complexity of a function class ===
Given a sample &lt;math&gt;S=(z_1, z_2, \dots, z_m) \in Z^m&lt;/math&gt;, and a class &lt;math&gt;F&lt;/math&gt; of real-valued functions defined on a domain space &lt;math&gt;Z&lt;/math&gt;, the '''empirical Rademacher complexity''' of &lt;math&gt;F&lt;/math&gt; given &lt;math&gt;S&lt;/math&gt; is defined as:
:&lt;math&gt;
\operatorname{Rad}_S(F)
=
\frac{1}{m}
   \mathbb{E}_\sigma \left[
   \sup_{f \in F}
   \sum_{i=1}^m \sigma_i f(z_i)
\right]
&lt;/math&gt;

This can also be written using the previous definition:&lt;ref name=book14/&gt;{{rp|326}}
:&lt;math&gt;\operatorname{Rad}_S(F) = \operatorname{Rad}(F \circ S) &lt;/math&gt;
where &lt;math&gt;F \circ S&lt;/math&gt; denotes [[function composition]], i.e:
:&lt;math&gt;F \circ S := \{ (f(z_1),\ldots,f(z_m))| f\in F\}&lt;/math&gt;

Let &lt;math&gt;P&lt;/math&gt; be a probability distribution over &lt;math&gt;Z&lt;/math&gt;.
The '''Rademacher complexity''' of the function class &lt;math&gt;F&lt;/math&gt; with respect to &lt;math&gt;P&lt;/math&gt; for sample size &lt;math&gt;m&lt;/math&gt; is:

:&lt;math&gt;
\operatorname{Rad}_{P,m}(F)
:=
\mathbb{E}_{S\sim P^m} \left[ \operatorname{Rad}_S(F) \right]
&lt;/math&gt;

where the above expectation is taken over an [[identically independently distributed]] (i.i.d.) sample &lt;math&gt;S=(z_1, z_2, \dots, z_m)&lt;/math&gt; generated according to &lt;math&gt;P&lt;/math&gt;.

== Examples ==
1. &lt;math&gt;A&lt;/math&gt; contains a single vector, e.g, &lt;math&gt;A = \{(a,b)\} \subset \mathbb{R}^2&lt;/math&gt;. Then:
:&lt;math&gt;\operatorname{Rad}(A) = {1\over 2}\cdot ({1\over 4}\cdot(a+b) + {1\over 4}\cdot(a-b) + {1\over 4}\cdot(-a+b) + {1\over 4}\cdot(-a-b)) = 0&lt;/math&gt;
The same is true for every singleton hypothesis class.&lt;ref name=book12&gt;{{Cite Mehryar Afshin Ameet 2012}}&lt;/ref&gt;{{rp|56}}

2. &lt;math&gt;A&lt;/math&gt; contains two vectors, e.g, &lt;math&gt;A = \{(1,1),(1,2)\} \subset \mathbb{R}^2&lt;/math&gt;. Then:
:&lt;math&gt;\operatorname{Rad}(A) = {1\over 2}\cdot ({1\over 4}\cdot\max(1+1,1+2) + {1\over 4}\cdot\max(1-1,1-2) + {1\over 4}\cdot\max(-1+1,-1+2) + {1\over 4}\cdot\max(-1-1,-1-2)) &lt;/math&gt;
:&lt;math&gt;= {1\over 8}(3+0+1-2) = {1\over 4}&lt;/math&gt;

== Using the Rademacher complexity ==
The Rademacher complexity can be used to derive data-dependent upper-bounds on the [[learnability]] of function classes. Intuitively, a function-class with smaller Rademacher complexity is easier to learn.

=== Bounding the representativeness ===
In [[machine learning]], it is desired to have a [[training set]] that represents the true distribution of samples. This can be quantified using the notion of '''representativeness'''. Denote by P the [[probability distribution]] from which the samples are drawn. Denote by &lt;math&gt;H&lt;/math&gt; the set of hypotheses (potential classifiers) and denote by &lt;math&gt;F&lt;/math&gt; the corresponding set of error functions, i.e, for every &lt;math&gt;h\in H&lt;/math&gt;, there is a function &lt;math&gt;f_h\in F&lt;/math&gt;, that maps each training sample (features,label) to the error of the classifier &lt;math&gt;h&lt;/math&gt; on that sample (for example, if we do binary classification and the error function is the simple 0-1 loss, then &lt;math&gt;f_h&lt;/math&gt; is a function that returns 0 for each training sample on which &lt;math&gt;h&lt;/math&gt; is correct and 1 for each training sample on which &lt;math&gt;h&lt;/math&gt; is wrong). Define:
:&lt;math&gt;L_P(f) := E_{z\sim P}[f(z)]&lt;/math&gt; - the expected error on the real distribution;
:&lt;math&gt;L_S(f) := {1\over m} \sum_{i=1}^m f(z_i)&lt;/math&gt; - the estimated error on the sample.
The representativeness of the sample &lt;math&gt;S&lt;/math&gt;, with respect to &lt;math&gt;P&lt;/math&gt; and &lt;math&gt;F&lt;/math&gt;, is defined as:
:&lt;math&gt;Rep_P(F,S) := \sup_{f\in F} (L_P(f) - L_S(f))&lt;/math&gt;
Smaller representativeness is better, since it means that the empirical error of a classifier on the training set is not much lower than its true error.

The expected representativeness of a sample can be bounded by the expected Rademacher complexity of the function class:&lt;ref name=book14/&gt;{{rp|326}}
:&lt;math display=block&gt;E_{S\sim P^m} [Rep_P(F,S)] \leq 2 \cdot E_{S\sim P^m} [\operatorname{Rad}(F\circ S)]&lt;/math&gt;

=== Bounding the generalization error ===
When the Rademacher complexity is small, it is possible to learn the hypothesis class H using [[empirical risk minimization]].

For example (with binary error function),&lt;ref name=book14/&gt;{{rp|328}} for every &lt;math&gt;\delta&gt;0&lt;/math&gt;, with probability at least &lt;math&gt;1-\delta&lt;/math&gt;, for every hypothesis &lt;math&gt;h\in H&lt;/math&gt;:
:&lt;math&gt;L_P(h) - L_S(h) \leq 2 \operatorname{Rad}(F\circ S) + 4 \sqrt{2\ln(4/\delta)\over m}&lt;/math&gt;

== Bounding the Rademacher complexity ==
Since smaller Rademacher complexity is better, it is useful to have upper bounds on the Rademacher complexity of various function sets. The following rules can be used to upper-bound the Rademacher complexity of a set &lt;math&gt;A \subset \mathbb{R}^m&lt;/math&gt;.&lt;ref name=book14/&gt;{{rp|329-330}}

1. If all vectors in &lt;math&gt;A&lt;/math&gt; are translated by a constant vector &lt;math&gt;a_0 \in \mathbb{R}^m&lt;/math&gt;, then Rad(A) does not change.

2. If all vectors in &lt;math&gt;A&lt;/math&gt; are multiplied by a scalar &lt;math&gt;c\in \mathbb{R}&lt;/math&gt;, then Rad(A) is multiplied by &lt;math&gt;|c|&lt;/math&gt;.

3. Rad(A+B) = Rad(A) + Rad(B). &lt;ref name=book12/&gt;{{rp|56}}

4. (Kakade&amp;Tewari Lemma) If all vectors in &lt;math&gt;A&lt;/math&gt; are operated by a [[Lipschitz function]], then Rad(A) is (at most) multiplied by the [[Lipschitz constant]] of the function. In particular, if all vectors in &lt;math&gt;A&lt;/math&gt; are operated by a [[contraction mapping]], then Rad(A) strictly decreases.

5. The Rademacher complexity of the [[convex hull]] of &lt;math&gt;A&lt;/math&gt; equals Rad(A).

6. (Massart Lemma) The Rademacher complexity of a finite set grows logarithmically with the set size. Formally, let &lt;math&gt;A&lt;/math&gt; be a set of &lt;math&gt;N&lt;/math&gt; vectors in &lt;math&gt;\mathbb{R}^m&lt;/math&gt;, and let &lt;math&gt;\bar{a}&lt;/math&gt; be the mean of the vectors in &lt;math&gt;A&lt;/math&gt;. Then:
:&lt;math&gt;\operatorname{Rad}(A) \leq \max_{a\in A} ||a-\bar{a}|| \cdot {\sqrt{2\log{N}}\over m}&lt;/math&gt;
In particular, if &lt;math&gt;A&lt;/math&gt; is a set of binary vectors, the norm is at most &lt;math&gt;\sqrt{m}&lt;/math&gt;, so:
:&lt;math&gt;\operatorname{Rad}(A) \leq  {\sqrt{2\log{N} \over m}}&lt;/math&gt;

=== Bounds related to the VC dimension ===
Let &lt;math&gt;H&lt;/math&gt; be a [[set family]] whose  [[VC dimension]]  is &lt;math&gt;d&lt;/math&gt;. It is known that the [[growth function]] of &lt;math&gt;H&lt;/math&gt; is bounded as:
:for all &lt;math&gt;m&gt;d+1&lt;/math&gt;: &lt;math&gt;Growth(H,m)\leq (em/d)^d&lt;/math&gt;
This means that, for every set &lt;math&gt;h&lt;/math&gt; with at most &lt;math&gt;m&lt;/math&gt; elements, &lt;math&gt;|H\cap h|\leq (em/d)^d&lt;/math&gt;. The set-family &lt;math&gt;H\cap h&lt;/math&gt; can be considered as a set of binary vectors over &lt;math&gt;\mathbb{R}^m&lt;/math&gt;. Substituting this in Massart's lemma gives:
:&lt;math&gt;\operatorname{Rad}(H\cap h) \leq {\sqrt{2 d \log{(em/d)} \over m}}&lt;/math&gt;

With more advanced techniques ([[Dudley's theorem|Dudley's entropy bound]] and Haussler's upper bound&lt;ref&gt;Bousquet, O. (2004). Introduction to Statistical Learning Theory. ''Biological Cybernetics'', ''3176''(1), 169–207. &lt;nowiki&gt;http://doi.org/10.1007/978-3-540-28650-9_8&lt;/nowiki&gt;&lt;/ref&gt;) one can show, for example, that there exists a constant &lt;math&gt;C&lt;/math&gt;, such that any class of &lt;math&gt;\{0,1\}&lt;/math&gt;-indicator functions with [[Vapnik-Chervonenkis dimension]] &lt;math&gt;d&lt;/math&gt; has Rademacher complexity upper-bounded by &lt;math&gt;C\sqrt{\frac{d}{m}}&lt;/math&gt;.

=== Bounds related to linear classes ===
The following bounds are related to linear operations on &lt;math&gt;S&lt;/math&gt; - a constant set of &lt;math&gt;m&lt;/math&gt; vectors in &lt;math&gt;\mathbb{R}^n&lt;/math&gt;. &lt;ref name=book14/&gt;{{rp|332-333}}

1. Define &lt;math&gt;A_2 = \{(w\cdot x_1,\ldots,w\cdot x_m) | ||w||_2\leq 1\} = &lt;/math&gt; the set of dot-products of the vectors in &lt;math&gt;S&lt;/math&gt; with vectors in the [[unit ball]]. Then:
:&lt;math&gt;\operatorname{Rad}(A_2) \leq {\max_i||x_i||_2 \over \sqrt{m}}&lt;/math&gt;

2. Define &lt;math&gt;A_1 = \{(w\cdot x_1,\ldots,w\cdot x_m) | ||w||_1\leq 1\} = &lt;/math&gt; the set of dot-products of the vectors in &lt;math&gt;S&lt;/math&gt; with vectors in the unit ball of the 1-norm. Then:
:&lt;math&gt;\operatorname{Rad}(A_1) \leq \max_i||x_i||_\infty\cdot \sqrt{2\log(2n) \over m}&lt;/math&gt;

=== Bounds related to covering numbers ===
The following bound relates the Rademacher complexity of a set &lt;math&gt;A&lt;/math&gt; to its external [[covering number]] - the number of balls of a given radius &lt;math&gt;r&lt;/math&gt; whose union contains &lt;math&gt;A&lt;/math&gt;. The bound is attributed to Dudley.&lt;ref name=book14/&gt;{{rp|338}}

Suppose &lt;math&gt;A\subset \mathbb{R}^m&lt;/math&gt; is a set of vectors whose length (norm) is at most &lt;math&gt;c&lt;/math&gt;. Then, for every integer &lt;math&gt;M&gt;0&lt;/math&gt;:
:&lt;math&gt;
\operatorname{Rad}(A) \leq
{c\cdot 2^{-M}\over \sqrt{m}}
+
{6c \over m}\cdot
\sum_{i=1}^M 2^{-i}\sqrt{\log(N^{\text{ext}}_{c\cdot 2^{-i}}(A))}
&lt;/math&gt;

In particular, if &lt;math&gt;A&lt;/math&gt; lies in a ''d''-dimensional subspace of &lt;math&gt;\mathbb{R}^m&lt;/math&gt;, then:
:&lt;math&gt;\forall r&gt;0: N^{\text{ext}}_r(A) \leq (2 c \sqrt{d}/r)^d&lt;/math&gt;
Substituting this in the previous bound gives the following bound on the Rademacher complexity:
:&lt;math&gt;
\operatorname{Rad}(A) \leq
{6c \over m}\cdot
\bigg(\sqrt{d\log(2\sqrt{d})} + 2\sqrt{d}\bigg)
=
O\bigg({c\sqrt{d\log(d)}\over m}\bigg)
&lt;/math&gt;

== Gaussian complexity ==
'''Gaussian complexity''' is a similar complexity with similar physical meanings, and can be obtained from the Rademacher complexity using the random variables &lt;math&gt;g_i&lt;/math&gt; instead of &lt;math&gt;\sigma_i&lt;/math&gt;, where &lt;math&gt;g_i&lt;/math&gt; are [[Normal distribution|Gaussian]] [[Independent and identically distributed random variables|i.i.d.]] random variables with zero-mean and variance 1, i.e. &lt;math&gt;g_i \sim \mathcal{N} \left( 0, 1 \right)&lt;/math&gt;.

==References==
{{reflist}}
* Peter L. Bartlett, Shahar Mendelson (2002) ''Rademacher and Gaussian Complexities: Risk Bounds and Structural Results''. Journal of Machine Learning Research 3 463-482
* Giorgio Gnecco, Marcello Sanguineti (2008) ''Approximation Error Bounds via Rademacher's Complexity''. Applied Mathematical Sciences, Vol. 2, 2008, no. 4, 153 - 176


</text>
      <sha1>8c4dshekukt5iruhv29wqnfcd0r9co3</sha1>
    </revision>
  </page>
  <page>
    <title>Curse of dimensionality</title>
    <ns>0</ns>
    <id>787776</id>
    <revision>
      <id>814761123</id>
      <parentid>813259525</parentid>
      <timestamp>2017-12-10T19:49:45Z</timestamp>
      <contributor>
        <username>EvilxFish</username>
        <id>20295023</id>
      </contributor>
      <comment>Undid revision 813259525 by [[Special:Contributions/76.206.40.123|76.206.40.123]] ([[User talk:76.206.40.123|talk]]) This is not referring to a sparse matrix but rather the difficultly in obtaining data to explore high dimensional space (I believe).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15295">The '''curse of dimensionality''' refers to various phenomena that arise when analyzing and organizing data in [[high-dimensional space]]s (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the [[three-dimensional space|three-dimensional]] [[physical space]] of everyday experience.
The expression was coined by [[Richard E. Bellman]] when considering problems in [[dynamic optimization]].&lt;ref&gt;{{Cite book|author1=Richard Ernest Bellman|author2=Rand Corporation|title=Dynamic programming|url=https://books.google.it/books?id=wdtoPwAACAAJ|year=1957|publisher=Princeton University Press|isbn=978-0-691-07951-6|postscript=}},&lt;br /&gt;Republished: {{Cite book|author=Richard Ernest Bellman|title=Dynamic Programming|url=https://books.google.com/books?id=fyVtp3EMxasC|year=2003|publisher=Courier Dover Publications|isbn=978-0-486-42809-3}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|author=Richard Ernest Bellman|title=Adaptive control processes: a guided tour|url=https://books.google.com/books?id=POAmAAAAMAAJ|year=1961|publisher=Princeton University Press}}&lt;/ref&gt;

There are multiple phenomena referred to by this name in domains such as [[numerical analysis]], [[Sampling (statistics)|sampling]], [[combinatorics]], [[machine learning]], [[data mining]], and [[database]]s. The common theme of these problems is that when the dimensionality increases, the [[volume]] of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.
{{toclimit|3}}
== In different domains ==

=== Combinatorics ===
In some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities.  Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the [[combinatorial explosion]]. Even in the simplest case of &lt;math&gt;d&lt;/math&gt; binary variables, the number of possible combinations already is &lt;math&gt;O(2^d)&lt;/math&gt;, exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations.

=== Sampling ===
There is an exponential increase in volume associated with adding extra dimensions to a [[Space (mathematics)|mathematical space]].  For example, 10&lt;sup&gt;2&lt;/sup&gt;=100 evenly spaced sample points suffice to sample a [[unit interval]] (a &quot;1-dimensional cube&quot;) with no more than 10&lt;sup&gt;−2&lt;/sup&gt;=0.01 distance between points; an equivalent sampling of a 10-dimensional [[unit hypercube]] with a lattice that has a spacing of 10&lt;sup&gt;−2&lt;/sup&gt;=0.01 between adjacent points would require 10&lt;sup&gt;20&lt;/sup&gt;[=(10&lt;sup&gt;2&lt;/sup&gt;)&lt;sup&gt;10&lt;/sup&gt;] sample points. In general, with a spacing distance of 10&lt;sup&gt;−n&lt;/sup&gt; the 10-dimensional hypercube appears to be a factor of 10&lt;sup&gt;n(10-1)&lt;/sup&gt;[=(10&lt;sup&gt;n&lt;/sup&gt;)&lt;sup&gt;10&lt;/sup&gt;/(10&lt;sup&gt;n&lt;/sup&gt;)] &quot;larger&quot; than the 1-dimensional hypercube, which is the unit interval. In the above example n=2: when using a sampling distance of 0.01 the 10-dimensional hypercube appears to be 10&lt;sup&gt;18&lt;/sup&gt; &quot;larger&quot; than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below.

=== Optimization ===
When solving dynamic [[optimization (mathematics)|optimization]] problems by numerical [[backward induction]], the objective function must be computed for each combination of values.  This is a significant obstacle when the dimension of the &quot;state variable&quot; is large.

=== Machine learning ===
In [[machine learning]] problems that involve learning a &quot;state-of-nature&quot; (maybe an infinite distribution) from a finite number of data samples in a high-dimensional [[feature space]] with each feature having a number of possible values, an enormous amount of training data is required to ensure that there are several samples with each combination of values.  With a fixed number of training samples, the predictive power reduces as the dimensionality increases, and this is known as ''Hughes phenomenon'' (named after Gordon F. Hughes).&lt;ref&gt;{{cite journal |last=Hughes |first=G.F. |title=On the mean accuracy of statistical pattern recognizers |journal=IEEE Transactions on Information Theory |volume=14 |issue=1 |pages=55–63 |date=January 1968 |doi=10.1109/TIT.1968.1054102 |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=1054102}}&lt;/ref&gt;

=== Distance functions ===
When a measure such as a [[Euclidean distance]] is defined using many coordinates, there is little difference in the distances between different pairs of samples.

One way to illustrate the &quot;vastness&quot; of high-dimensional Euclidean space is to compare the proportion of an inscribed [[hypersphere]] with radius &lt;math&gt;r&lt;/math&gt; and dimension &lt;math&gt;d&lt;/math&gt;, to that of a [[hypercube]] with edges of length &lt;math&gt;2r&lt;/math&gt;.
The volume of such a sphere is &lt;math&gt;\frac{2r^d\pi^{d/2}}{d \; \Gamma(d/2)}&lt;/math&gt;, where [[Gamma function|&lt;math&gt;\Gamma&lt;/math&gt;]] is the [[gamma function]], while the volume of the cube is &lt;math&gt;(2r)^d&lt;/math&gt;.
As the dimension &lt;math&gt;d&lt;/math&gt; of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube.  This can clearly be [[:commons:File:Ball-cube-volume-ratio-semilog.png|seen]] by comparing the proportions as the dimension &lt;math&gt;d&lt;/math&gt; goes to infinity:
:&lt;math&gt;\frac{V_{hypersphere}}{V_{hypercube}}=\frac{\pi^{d/2}}{d2^{d-1}\Gamma(d/2)}\rightarrow 0&lt;/math&gt; as &lt;math&gt;d \rightarrow \infty&lt;/math&gt;.

Furthermore, the distance between the center and the corners is &lt;math&gt;r\sqrt{d}&lt;/math&gt;, which increases without bound for fixed r.
In this sense, nearly all of the high-dimensional space is &quot;far away&quot; from the centre. To put it another way, the high-dimensional unit hypercube can be said to consist almost entirely of the &quot;corners&quot; of the hypercube, with almost no &quot;middle&quot;.

This also helps to understand the [[chi-squared distribution]]. Indeed, the (non-central) chi-squared distribution associated to a random point in the interval [-1, 1] is the same as the distribution of the length-squared of a random point in the ''d''-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around ''d'' times the standard deviation squared (σ&lt;sup&gt;2&lt;/sup&gt;) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the ''d''-cube concentrates near the surface of a sphere of radius {{sqrt|''d''}}''σ''.

A further development of this phenomenon is as follows. Any fixed distribution on ''[[Real number|ℝ]]'' induces a product distribution on points in ''ℝ&lt;sup&gt;d&lt;/sup&gt;''. For any fixed ''n'', it turns out that the minimum and the maximum distance between a random reference point ''Q'' and a list of ''n'' random data points ''P&lt;sub&gt;1&lt;/sub&gt;,...,P&lt;sub&gt;n&lt;/sub&gt;'' become indiscernible compared to the minimum distance:&lt;ref&gt;{{Cite journal | doi = 10.1007/3-540-49257-7_15 | title = When is &quot;Nearest Neighbor&quot; Meaningful? | year = 1999 | last1 = Beyer | first1 = K. | last2 = Goldstein | first2 = J. | series = LNCS | last3 = Ramakrishnan | first3 = R. | last4 = Shaft | first4 = U. | volume = 1540 | journal = Proc. 7th International Conference on Database Theory - ICDT'99| pages = 217–235| isbn = 978-3-540-65452-0}}&lt;/ref&gt;
:&lt;math&gt;\lim_{d \to \infty} E\left(\frac{\operatorname{dist}_{\max} (d) - \operatorname{dist}_{\min} (d)}{\operatorname{dist}_{\min} (d)}\right)
\to 0&lt;/math&gt;.
This is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions ''ℝ'' are [[Independent and identically distributed random variables|independent and identically distributed]].&lt;ref name=&quot;survey&quot; /&gt; When attributes are correlated, data can become easier and provide higher distance contrast and the [[signal-to-noise ratio]] was found to play an important role, thus  [[feature selection]] should be used.&lt;ref name=&quot;survey&quot; /&gt;

=== Nearest neighbor search ===
The effect complicates [[nearest neighbor search]] in high dimensional space.  It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions.&lt;ref&gt;{{cite journal |first1=R.B. |last1=Marimont |first2=M.B. |last2=Shapiro |title=Nearest Neighbour Searches and the Curse of Dimensionality |journal=IMA J Appl Math |volume=24 |issue=1 |pages=59–70 |year=1979 |doi=10.1093/imamat/24.1.59 |url=http://imamat.oxfordjournals.org/content/24/1/59.short}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |first1=Edgar |last1=Chávez |first2=Gonzalo |last2=Navarro |first3=Ricardo |last3=Baeza-Yates |first4=José Luis |last4=Marroquín |title=Searching in Metric Spaces |journal=ACM Computing Surveys |volume=33 |issue=3 |pages=273–321 |year=2001 |doi=10.1145/502807.502808 |citeseerx = 10.1.1.100.7845 }}&lt;/ref&gt;

However, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties,&lt;ref name=&quot;houle-ssdbm10&quot;&gt;{{Cite conference | last1 = Houle | first1 = M. E. | last2 = Kriegel | first2 = H. P. | authorlink2=Hans-Peter Kriegel | last3 = Kröger | first3 = P.| last4 = Schubert | first4 = E. | last5 = Zimek | first5 = A.| title = Can Shared-Neighbor Distances Defeat the Curse of Dimensionality? | doi = 10.1007/978-3-642-13818-8_34 | conference = Scientific and Statistical Database Management | series = Lecture Notes in Computer Science | volume = 6187 | pages = 482 | year = 2010 | isbn = 978-3-642-13817-1 | pmid =  | pmc = | url = http://www.dbs.ifi.lmu.de/~zimek/publications/SSDBM2010/SNN-SSDBM2010-preprint.pdf| format = PDF}}&lt;/ref&gt; since ''relevant'' additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant (&quot;noise&quot;) dimensions, however, reduce the contrast in the manner described above. In [[time series analysis]], where the data are inherently high-dimensional, distance functions also work reliably as long as the [[signal-to-noise ratio]] is high enough.&lt;ref name=&quot;houle-sstd11&quot;&gt;{{Cite conference | last1 = Bernecker | first1 = T. | last2 = Houle | first2 = M. E. | last3 = Kriegel | first3 = H. P. | authorlink3=Hans-Peter Kriegel| last4 = Kröger | first4 = P. | last5 = Renz | first5 = M. | last6 = Schubert | first6 = E. | last7 = Zimek | first7 = A. | title = Quality of Similarity Rankings in Time Series | doi = 10.1007/978-3-642-22922-0_25 | conference = Symposium on Spatial and Temporal Databases| series = Lecture Notes in Computer Science | volume = 6849 | pages = 422 | year = 2011 | isbn = 978-3-642-22921-3 }}&lt;/ref&gt;

====''k''-nearest neighbor classification====
Another effect of high dimensionality on distance functions concerns ''k''-nearest neighbor (''k''-NN) [[Graph (discrete mathematics)|graphs]] constructed from a [[data set]] using a distance function. As the dimension increases, the [[indegree]] distribution of the ''k''-NN [[directed graph|digraph]] becomes [[Skewness|skewed]] with a peak on the right because of the emergence of a disproportionate number of '''hubs''', that is, data-points that appear in many more ''k''-NN lists of other data-points than the average. This phenomenon can have a considerable impact on various techniques for [[Classification (machine learning)|classification]] (including the [[K-nearest neighbor algorithm|''k''-NN classifier]]), [[semi-supervised learning]], and [[Cluster analysis|clustering]],&lt;ref&gt;{{Cite journal
 | last1=Radovanović | first1=Miloš
 | last2=Nanopoulos | first2=Alexandros
 | last3=Ivanović | first3=Mirjana
 | year=2010
 | title=Hubs in space: Popular nearest neighbors in high-dimensional data
 | journal=Journal of Machine Learning Research
 | volume=11
 | pages=2487–2531
 | url=http://www.jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf
 | format=PDF
}}&lt;/ref&gt; and it also affects [[information retrieval]].&lt;ref&gt;{{Cite conference| last1 = Radovanović | first1 = M. | last2 = Nanopoulos | first2 = A. | last3 = Ivanović | first3 = M. | doi = 10.1145/1835449.1835482 | title = On the existence of obstinate results in vector space models | conference = 33rd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '10 | pages = 186 | year = 2010 | isbn = 9781450301534 }}&lt;/ref&gt;

=== Anomaly detection ===

In a recent survey, Zimek et al. identified the following problems when searching for [[anomaly detection|anomalies]] in high-dimensional data:&lt;ref name=&quot;survey&quot;&gt;{{Cite journal | last1 = Zimek | first1 = A. | last2 = Schubert | first2 = E.| last3 = Kriegel | first3 = H.-P. | authorlink3=Hans-Peter Kriegel| title = A survey on unsupervised outlier detection in high-dimensional numerical data | doi = 10.1002/sam.11161 | journal = Statistical Analysis and Data Mining | volume = 5 | issue = 5 | pages = 363–387| year = 2012 | pmid =  | pmc = }}&lt;/ref&gt;

# Concentration of scores and distances: derived values such as distances become numerically similar
# Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant
# Definition of reference sets: for local methods, reference sets are often nearest-neighbor based
# Incomparable scores for different dimensionalities: different subspaces produce incomparable scores
# Interpretability of scores: the scores often no longer convey a semantic meaning
# Exponential search space: the search space can no longer be systematically scanned
# [[Data snooping]] bias: given the large search space, for every desired significance a hypothesis can be found
# Hubness: certain objects occur more frequently in neighbor lists than others.

Many of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions.

==See also==
{{Too many see alsos|date=April 2017}}
{{Div col|cols=3}}
*[[Bellman equation]]
*[[Backwards induction]]
*[[Cluster analysis]]
*[[Clustering high-dimensional data]]
*[[Combinatorial explosion]]
*[[Concentration of measure]]
*[[Dimension reduction]]
*[[Model Order Reduction]]
*[[Dynamic programming]]
*[[Fourier-related transforms]]
*[[High-dimensional space]]
*[[Linear least squares (mathematics)|Linear least squares]]
*[[Multilinear principal component analysis|Multilinear PCA]]
*[[Multilinear subspace learning]]
*[[Principal component analysis]]
*[[Quasi-random]]
*[[Singular value decomposition]]
*[[Stereology]]
*[[Time series]]
*[[Wavelet]]
{{Div col end}}

==References==
{{Reflist|30em}}



</text>
      <sha1>lnat0xlk6ymij8jbnrkueq2w7xm8jyg</sha1>
    </revision>
  </page>
  <page>
    <title>Uncertain data</title>
    <ns>0</ns>
    <id>19058043</id>
    <revision>
      <id>800365739</id>
      <parentid>765544217</parentid>
      <timestamp>2017-09-13T02:42:19Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>/* References */clean up spacing around punctuation, replaced: ,W → , W using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4402">In [[computer science]], '''uncertain data''' is data that contains [[measurement error|noise]] that makes it deviate from the correct, intended or original values. In the age of [[big data]], uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out '''managing uncertain data at scale''' in its '''global technology outlook''' report&lt;ref&gt;{{cite report|url=http://www.zurich.ibm.com/pdf/isl/infoportal/GTO_2012_Booklet.pdf|title=Global Technology Outlook|year=2012}}&lt;/ref&gt; that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.

Uncertain data is found in the area of [[sensor networks]]; text where [[noisy text]] is found in abundance on social media, web and within enterprises where the structured and unstructured data may be old, outdated, or plain incorrect; in modeling where the [[mathematical model]] may only be an approximation of the actual process. When representing such data in a [[database]], some indication of the [[probability]] of the correctness of the various values also needs to be estimated.

There are three main models of uncertain data in databases. In '''attribute uncertainty''', each uncertain attribute in a tuple is subject to its own independent [[probability distribution]].&lt;ref name=&quot;orion&quot;&gt;{{cite journal|last=Prabhakar|first=Sunil |title=ORION: Managing Uncertain (Sensor) Data|url=http://mobisensors.cs.pitt.edu/files/papers/prabhakar.pdf}}&lt;/ref&gt; For example, if readings are taken of temperature and wind speed, each would be described by its own probability distribution, as knowing the reading for one measurement would not provide any information about the other.

In '''correlated uncertainty''', multiple attributes may be described by a [[joint probability distribution]].&lt;ref name=&quot;orion&quot;/&gt; For example, if readings are taken of the position of an object, and the ''x''- and ''y''-coordinates stored, the probability of different values may depend on the distance from the recorded coordinates. As distance depends on both coordinates, it may be appropriate to use a joint distribution for these coordinates, as they are not [[Statistical independence|independent]].

In '''tuple uncertainty''', all the attributes of a [[tuple]] are subject to a joint probability distribution. This covers the case of correlated uncertainty, but also includes the case where there is a probability of a tuple not belonging in the relevant relation, which is indicated by all the probabilities not summing to one.&lt;ref name=&quot;orion&quot;/&gt; For example, assume we have the following tuple from a [[probabilistic database]]:
{| class=&quot;wikitable&quot; border=&quot;1&quot;
|-
|| (a, 0.4) | (b, 0.5)
|}

Then, the tuple has 10% chance of not existing in the database.

==References==
{{reflist}}

*{{cite conference|first=Habich|last=Volk|author2=Clemens Utzny, Ralf Dittmann, Wolfgang Lehner|publisher=IEEE|accessdate=2008-08-01|title=Error-Aware Density-Based Clustering of Imprecise Measurement Values|booktitle=Seventh IEEE International Conference on Data Mining Workshops, 2007. ICDM Workshops 2007. }}
*{{cite conference|first=Volk|last=Rosentahl|author2=Martin Hahmann, Dirk Habich, Wolfgang Lehner|publisher=IEEE|accessdate=2008-08-01|title=Clustering Uncertain Data With Possible Worlds|booktitle=Proceedings of the 1st Workshop on Management and mining Of Uncertain Data in conjunction with the 25th International Conference on Data Engineering, 2009.}}






{{compu-sci-stub}}</text>
      <sha1>kx26fv9s4xnuv88vnafewrd4medjc9x</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge integration</title>
    <ns>0</ns>
    <id>4144848</id>
    <revision>
      <id>710817190</id>
      <parentid>705762078</parentid>
      <timestamp>2016-03-19T06:00:35Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>Remove blank line(s) between list items per  to fix an accessibility issue for users of [[screen reader]]s. Do  and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3782">'''Knowledge integration''' is the process of synthesizing multiple [[knowledge model]]s (or representations) into a common model (representation).

Compared to [[information integration]], which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.

For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.

The [http://wise.berkeley.edu Web-based Inquiry Science Environment (WISE)], from the [[University of California at Berkeley]] has been developed along the lines of knowledge integration theory.

'''Knowledge integration''' has also been studied as the process of incorporating new information into a body of existing knowledge with an [[interdisciplinary]] approach.  This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.

A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps.  By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.

The [[machine learning]] program KI, developed by Murray and Porter at the [[University of Texas at Austin]], was created to study the use of automated and semi-automated knowledge integration to assist [[knowledge engineers]] constructing a large [[knowledge base]].

A possible technique which can be used is [[semantic matching]]. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

The [[University of Waterloo]] operates a Bachelor of Knowledge Integration [[undergraduate degree]] program as an academic major or minor. The program started in 2008.

==See also==
* [[Knowledge value chain]]

==References==
{{Reflist}}&lt;!--added under references heading by script-assisted edit--&gt;

==Further reading==
* Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In ''The Cambridge Handbook of the Learning Sciences.'' Cambridge, MA. Cambridge University Press
* Murray, K. S. (1996) KI: A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence
* Murray, K. S. (1995) [http://www.ai.sri.com/pubs/files/1636.pdf Learning as Knowledge Integration], Technical Report TR-95-41, The University of Texas at Austin
* Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society
* Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration: Initial Results. International Journal for Man-Machine Studies, volume 33
* Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference



</text>
      <sha1>9ta9v9xiakskoh5sz5stga0082mvhjv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Learning in computer vision</title>
    <ns>14</ns>
    <id>19314112</id>
    <revision>
      <id>604225892</id>
      <parentid>604225655</parentid>
      <timestamp>2014-04-14T23:19:15Z</timestamp>
      <contributor>
        <username>Deva Ramanan</username>
        <id>21191864</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="695">Learning-based methods in [[computer vision]] make use of training data to build systems for visual analysis. For example, one may train a system for detecting faces using training images of faces. Training data is often given in the forms of image or video collections, together with target labels. Such data is often fed into a [[machine learning]] algorithm, that will learn to predict such labels given novel images or video. Learning-based methods have been used for a variety of computer vision tasks, including low-level problems such as image-denoising, and high-level tasks such as object recognition and scene classification.


</text>
      <sha1>svxifbvd4z6r02uvef3yvxuqo2z5ddt</sha1>
    </revision>
  </page>
  <page>
    <title>Offline learning</title>
    <ns>0</ns>
    <id>10748030</id>
    <revision>
      <id>789607333</id>
      <parentid>714911929</parentid>
      <timestamp>2017-07-08T11:00:05Z</timestamp>
      <contributor>
        <username>Marko Tscherepanow</username>
        <id>30960457</id>
      </contributor>
      <minor/>
      <comment>Added link to &quot;Incremental learning&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="484">{{unreferenced|date=July 2015}}
In [[machine learning]], systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed. These systems are also typically examples of [[eager learning]].

==See also==
* [[online machine learning|online learning]], the opposite model
* [[incremental learning]], a learning model for the incremental extension of knowledge




{{compu-stub}}</text>
      <sha1>17n6tqiixc61hu4ynxw3alz0o0gxot6</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Ensemble learning</title>
    <ns>14</ns>
    <id>3985352</id>
    <revision>
      <id>786890664</id>
      <parentid>243007847</parentid>
      <timestamp>2017-06-22T05:58:02Z</timestamp>
      <contributor>
        <username>Rentier</username>
        <id>454738</id>
      </contributor>
      <comment>rm (dead) external link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="322">'''Ensemble learning''' is a type of [[machine learning]] that studies [[algorithms]] and architectures that build collections, or ''ensembles'', of [[statistical classification|statistical classifiers]] that are more accurate than a single classifier.


</text>
      <sha1>355g8efhuyhe8cb9y3zsl07mszneeiw</sha1>
    </revision>
  </page>
  <page>
    <title>Neural modeling fields</title>
    <ns>0</ns>
    <id>19208664</id>
    <revision>
      <id>800634632</id>
      <parentid>800433200</parentid>
      <timestamp>2017-09-14T19:22:25Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>clean up spacing around punctuation, replaced: ,E → , E, ,M → , M (2), ,n → , n (5), ,w → , w (3) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22744">'''Neural modeling field (NMF)''' is a mathematical framework for [[machine learning]] which combines ideas from [[neural networks]], [[fuzzy logic]], and [[model based recognition]]. It has also been referred to as '''modeling fields''',  '''modeling fields theory''' (MFT),  '''Maximum likelihood artificial neural networks''' (MLANS).
&lt;ref&gt;[http://www.oup.com/us/catalog/he/subject/Engineering/ElectricalandComputerEngineering/ComputerEngineering/NeuralNetworks/?view=usa&amp;ci=9780195111620]: Perlovsky, L.I. 2001. Neural Networks and Intellect: using model based concepts. New York: Oxford University Press&lt;/ref&gt;
&lt;ref&gt;Perlovsky, L.I. (2006). Toward Physics of the Mind: Concepts, Emotions, Consciousness, and Symbols. Phys. Life Rev. 3(1), pp.22-55.&lt;/ref&gt;
&lt;ref&gt;[http://ieeexplore.ieee.org/xpl/absprintf.jsp?arnumber=713700&amp;page=FREE]: Deming, R.W.,
Automatic buried mine detection using the maximum likelihoodadaptive neural system (MLANS), in Proceedings of ''Intelligent Control (ISIC)'', 1998. Held jointly with ''IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA), Intelligent Systems and Semiotics (ISAS)''&lt;/ref&gt;
&lt;ref&gt;[http://www.mdatechnology.net/techprofile.aspx?id=227 ]: MDA Technology Applications Program web site&lt;/ref&gt;
&lt;ref&gt;[http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4274797]: Cangelosi, A.; Tikhanoff, V.; Fontanari, J.F.; Hourdakis, E., Integrating Language and Cognition: A Cognitive Robotics Approach, Computational Intelligence Magazine, IEEE, Volume 2,  Issue 3,  Aug. 2007 Page(s):65 - 70&lt;/ref&gt;
&lt;ref&gt;[http://spie.org/x648.xml?product_id=521387&amp;showAbstracts=true&amp;origin_id=x648]: Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense III (Proceedings Volume), Editor(s): Edward M. Carapezza, Date: 15 September 2004,{{ISBN|978-0-8194-5326-6}}, See Chapter: ''Counter-terrorism threat prediction architecture''&lt;/ref&gt;
This framework has been developed by [[Leonid Perlovsky]] at the [[AFRL]].  NMF is interpreted as a mathematical description of mind’s mechanisms, including [[concept]]s, [[emotions]], [[instincts]], [[imagination]], [[thinking]], and [[understanding]].  NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.

==Concept models and similarity measures==
In the general case, NMF system consists of multiple processing levels. At each level, output signals are the concepts recognized in (or formed from) input, bottom-up signals. Input signals are associated with (or recognized, or grouped into) concepts according to the models and at this level.   In the process of learning the concept-models are adapted for better representation of the input signals so that similarity between the concept-models and signals increases. This increase in similarity can be interpreted as satisfaction of an instinct for knowledge, and is felt as [[aesthetic emotions]].

Each hierarchical level consists of N &quot;neurons&quot; enumerated by index n=1,2..N. These neurons receive input, bottom-up signals, '''X(n)''', from lower levels in the processing hierarchy. '''X'''(n) is a field of bottom-up neuronal synaptic activations, coming from neurons at a lower level.  Each neuron has a number of synapses; for generality, each neuron activation is described as a set of numbers,

:&lt;math&gt; \vec X(n) = \{ X_d(n) \}, d = 1..D.&lt;/math&gt;

, where D is the number or dimensions necessary to describe individual neuron's activation.

Top-down, or priming signals to these neurons are sent by concept-models, '''M'''&lt;sub&gt;m&lt;/sub&gt;('''S'''&lt;sub&gt;m&lt;/sub&gt;,n)

:&lt;math&gt; \vec M_m(\vec S_m, n), m = 1..M.&lt;/math&gt;

, where  M is the number of models.  Each model is characterized by its parameters, '''S&lt;sub&gt;m&lt;/sub&gt;'''; in the neuron structure of the brain they are encoded by strength of synaptic connections, mathematically, they are given by a set of numbers,

:&lt;math&gt; \vec S_m = \{ S_m^a \}, a = 1..A.&lt;/math&gt;

, where A is the number of dimensions necessary to describe invividual model.

Models represent signals in the following way.  Suppose that signal '''X(''n'')''' is coming from sensory neurons n activated by object m, which is characterized by parameters '''S&lt;sub&gt;m&lt;/sub&gt;'''. These parameters may include position, orientation, or lighting of an object m. Model '''M&lt;sub&gt;m&lt;/sub&gt;'''('''S&lt;sub&gt;m&lt;/sub&gt;''',n) predicts a value '''X'''(n) of a signal at neuron n.  For example, during visual perception, a neuron n in the visual cortex receives a signal '''X'''(n) from retina and a [[Priming (psychology)|priming]] signal '''M&lt;sub&gt;m&lt;/sub&gt;'''('''S&lt;sub&gt;m&lt;/sub&gt;''',n) from an object-concept-model ''m''.  Neuron ''n'' is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong.  Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level.  Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects; in cognition models correspond to relationships and situations.

Learning is an essential part of perception and cognition, and in NMF theory it is driven by the dynamics that increase a [[similarity measure]] between the sets of models and signals, L({'''X'''},{'''M'''}).  The similarity measure is a function of model parameters and associations between the input bottom-up signals and top-down, concept-model signals. In constructing a mathematical description of the similarity measure, it is important to acknowledge two principles:
:''First'', the visual field content is unknown before perception occurred
:''Second'', it may contain any of a number of objects. Important information could be contained in any bottom-up signal;

Therefore, the similarity measure is constructed so that it accounts for all bottom-up signals, ''X''(''n''),

:&lt;math&gt; L( \{\vec X(n)\}, \{\vec M_m( \vec S_m, n)\} ) = \prod_{n=1}^N{l(\vec X(n))}.&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; (1)

This expression contains a product of partial similarities, l('''X'''(n)), over all bottom-up signals; therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied); this is a reflection of the first principle.  Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each input neuron signal.  Its constituent elements are conditional partial similarities between signal '''X'''(n) and model '''M&lt;sub&gt;m&lt;/sub&gt;''', l('''X'''(n)|m). This measure is “conditional” on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows:

:&lt;math&gt; L( \{\vec X(n)\}, \{\vec M_m( \vec S_m, n)\} ) = \prod_{n=1}^N{ \sum_{m=1}^M { r(m) l(\vec X(n) | m) } }.&lt;/math&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;(2)

The structure of the expression above follows standard principles of the probability theory: a summation is taken over alternatives, m, and various pieces of evidence, n, are multiplied. This expression is not necessarily a probability, but it has a probabilistic structure. If learning is successful, it approximates probabilistic description and leads to near-optimal Bayesian decisions. The name “conditional partial similarity” for l('''X'''(n)|m) (or simply l(n|m)) follows the probabilistic terminology. If learning is successful, l(n|m) becomes a conditional probability density function, a probabilistic measure that signal in neuron n originated from object m.  Then L is a total likelihood of observing signals {'''X'''(n)} coming from objects described by concept-model {'''M&lt;sub&gt;m&lt;/sub&gt;'''}. Coefficients r(m), called priors in probability theory, contain preliminary biases or expectations, expected objects m have relatively high r(m) values; their true values are usually unknown and should be learned, like other parameters '''S&lt;sub&gt;m&lt;/sub&gt;'''.

Note that in probability theory, a product of probabilities usually assumes that evidence is independent. Expression for L contains a product over n, but it does not assume independence among various signals '''X'''(n). There is a dependence among signals due to concept-models: each model '''M&lt;sub&gt;m&lt;/sub&gt;'''('''S&lt;sub&gt;m&lt;/sub&gt;''',n) predicts expected signal values in many neurons n.

During the learning process, concept-models are constantly modified. Usually, the functional forms of models, '''M&lt;sub&gt;m&lt;/sub&gt;'''('''S&lt;sub&gt;m&lt;/sub&gt;''',n),  are all fixed and learning-adaptation involves only model parameters, '''S&lt;sub&gt;m&lt;/sub&gt;'''.  From time to time a system forms a new concept, while retaining an old one as well; alternatively, old concepts are sometimes merged or eliminated. This requires a modification of the similarity measure L; The reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a “skeptic penalty function,” ([[Penalty method]]) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M) = exp(-N&lt;sub&gt;par&lt;/sub&gt;/2), where N&lt;sub&gt;par&lt;/sub&gt; is a total number of adaptive parameters in all models (this penalty function is known as [[Akaike information criterion]], see (Perlovsky 2001) for further discussion and references).

==Learning in NMF using dynamic logic algorithm==

The learning process consists of estimating model parameters '''S''' and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models  are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in M&lt;sup&gt;N&lt;/sup&gt; items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of [[Perlovsky|dynamic logic]],.&lt;ref&gt;Perlovsky, L.I. (1996). Mathematical Concepts of Intellect. Proc. World Congress on Neural Networks, San Diego, CA; Lawrence Erlbaum Associates, NJ, pp.1013-16&lt;/ref&gt;&lt;ref&gt;Perlovsky, L.I.(1997). Physical Concepts of Intellect. Proc. Russian Academy of Sciences, 354(3), pp. 320-323.&lt;/ref&gt; An important aspect of dynamic logic is ''matching vagueness or fuzziness of similarity measures to the uncertainty of models''. Initially, parameter values are not known, and uncertainty of models is high; so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases.

The maximization of similarity L is done as follows. First, the unknown parameters {'''S'''&lt;sub&gt;m&lt;/sub&gt;} are randomly initialized. Then the association variables f(m|n) are computed,

:&lt;math&gt; f(m|n) = \frac{r(m) l( \vec X(n|m)) }{ \sum_{m'=1}^M { r(m') l( \vec X(n|m')) } } &lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; (3).

Equation for f(m|n) looks like the Bayes formula for a posteriori probabilities; if l(n|m) in the result of learning become conditional likelihoods, f(m|n) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows:

:&lt;math&gt; \frac{d \vec S_m }{dt} = \sum_{n=1}^N { f(m|n) \frac{\partial{\ln l(n|m)} }{\partial{\vec M_m} } \frac{\partial{\vec M_m}}{\partial{\vec S_m}} } &lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; (4).

:&lt;math&gt; \frac{df(m|n)}{dt} = f(m|n)\sum_{m'=1}^M { [\delta_{mm'} - f(m'|n)] \frac{\partial{\ln l(n|m')}}{\partial{\vec M_{m'}}} } \frac{\partial{\vec M_{m'}}}{\partial{\vec S_{m'}}} \frac{d \vec S_{m'}}{dt} &lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; (5)

The following theorem has been proved (Perlovsky 2001):

''Theorem''. Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by max{S&lt;sub&gt;m&lt;/sub&gt;}L.

It follows that the stationary states of an MF system are the maximum similarity states.  When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters {'''S'''&lt;sub&gt;m&lt;/sub&gt;} are asymptotically unbiased and efficient estimates of these parameters.&lt;ref&gt;Cramer, H. (1946). Mathematical Methods of Statistics, Princeton University Press, Princeton NJ.&lt;/ref&gt; The computational complexity of dynamic logic is linear in N.

Practically, when solving the equations through successive iterations, f(m|n) can be recomputed at every iteration using (3), as opposed to incremental formula (5).

The proof of the above theorem contains a proof that similarity L increases at each iteration.  This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions: NMF-dynamic logic system emotionally enjoys learning.

==Example of dynamic logic operations==
Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy ‘smile’ and ‘frown’ patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M&lt;sup&gt;N&lt;/sup&gt; ~ 10&lt;sup&gt;6000&lt;/sup&gt;. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10&lt;sup&gt;32&lt;/sup&gt; to 10&lt;sup&gt;40&lt;/sup&gt; operations, still a prohibitive computational complexity.
To apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models  of expected patterns. The models and conditional partial similarities for this case are described in details in:&lt;ref&gt;Linnehan, R., Mutz, Perlovsky, L.I., C., Weijers, B., Schindler, J., Brockett, R. (2003). Detection of Patterns Below Clutter in Images. Int. Conf. On Integration of Knowledge Intensive Multi-Agent Systems, Cambridge, MA Oct.1-3, 2003.&lt;/ref&gt; a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for ‘smiles’ and ‘frowns’. The number of computer operations in this example was about 10&lt;sup&gt;10&lt;/sup&gt;. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.

During an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals: the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true ‘smile’ and ‘frown’ patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between –2dB and –0.7dB); (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the ‘best’ fit.

There are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob models and parabolic models; their number, location, and curvature are estimated from the data. Until about stage (g) the algorithm used simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (h), when similarity stopped increasing.

[[File:ExampleOfApplicationOfDynamicLogicToNoisyImage.JPG|center |frame| Fig.1. Finding ‘smile’ and ‘frown’ patterns in noise, an example of dynamic logic operation: (a) true ‘smile’ and ‘frown’ patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between –2dB and –0.7dB); (c) an initial fuzzy blob-model, the fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Between stages (d) and (e) the algorithm tried to fit the data with more than one model and decided, that it needs three blob-models to ‘understand’ the content of the data. There are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob-models and parabolic models, which number, location, and curvature are estimated from the data. Until about stage (g) the algorithm ‘thought’ in terms of simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (m), when similarity L stopped increasing. This example is discussed in more details in (Linnehan et al. 2003).]]

==Neural modeling fields hierarchical organization==

Above, a single processing level in a hierarchical NMF system was described. At each level of hierarchy there are input signals from lower levels, models, similarity measures (L), emotions, which are defined as changes in similarity, and actions; actions include adaptation, behavior satisfying the knowledge instinct – maximization of similarity. An input to each level is a set of signals '''X'''(n), or in neural terminology, an input field of neuronal activations. The result of signal processing at a given level are activated models, or concepts m recognized in the input signals n; these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level.

The activated models initiate other actions. They serve as input signals to the next processing level, where more general concept-models are recognized or created. Output signals from a given level, serving as input to the next level, are the model activation signals, a&lt;sub&gt;m&lt;/sub&gt;, defined as

a&lt;sub&gt;m&lt;/sub&gt; =  ∑&lt;sub&gt;n=1..N&lt;/sub&gt; f(m|n).

The hierarchical NMF system is illustrated in Fig. 2. Within the hierarchy of the mind, each concept-model finds its “mental” meaning and purpose at a higher level (in addition to other purposes). For example, consider a concept-model “chair.” It has a “behavioral” purpose of initiating sitting behavior (if sitting is required by the body), this is the “bodily” purpose at the same hierarchical level. In addition, it has a “purely mental” purpose at a higher level in the hierarchy, a purpose of helping to recognize a more general concept, say of a “concert hall,” a model of which contains rows of chairs.

[[File:NMF Hierarchy.JPG|center |frame| Fig.2. Hierarchical NMF system. At each level of a hierarchy there are models, similarity measures, and actions (including adaptation, maximizing the knowledge instinct - similarity). High levels of partial similarity measures correspond to concepts recognized at a given level. Concept activations are output signals at this level and they become input signals to the next level, propagating knowledge up the hierarchy.]]

From time to time a system forms a new concept or eliminates an old one. At every level, the NMF system always keeps a reserve of vague (fuzzy) inactive concept-models. They are inactive in that their parameters are not adapted to the data; therefore their similarities to signals are low. Yet, because of a large vagueness (covariance) the similarities are not exactly zero. When a new signal does not fit well into any of the active models, its similarities to inactive models automatically increase (because first, every piece of data is accounted for, and second, inactive models are vague-fuzzy and potentially can “grab” every signal that does not fit into more specific, less fuzzy, active models. When the activation signal a&lt;sub&gt;m&lt;/sub&gt; for an inactive model, m, exceeds a certain threshold, the model is activated. Similarly, when an activation signal for a particular model falls below a threshold, the model is deactivated. Thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level (prior information, system resources, numbers of activated models of various types, etc.). Activation signals for active models at a particular level { a&lt;sub&gt;m&lt;/sub&gt; } form a “neuronal field,” which serve as input signals to the next level, where more abstract and more general concepts are formed.

==References==
{{Reflist}}

==Related==
* [[Leonid Perlovsky]]


</text>
      <sha1>mf4i15qi6bqy7z0d6rqap91rzp12i6s</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic analysis (machine learning)</title>
    <ns>0</ns>
    <id>14271782</id>
    <revision>
      <id>809394321</id>
      <parentid>795555584</parentid>
      <timestamp>2017-11-08T20:50:53Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:FrescoBot/Links|link syntax]] and minor changes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="857">{{Unreferenced|date=June 2011}}
In [[machine learning]], '''semantic analysis''' of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents.

[[Latent semantic analysis]] (sometimes latent semantic indexing), is a class of techniques where documents are represented as [[linear algebra|vectors]] in term space. A prominent example is [[Probabilistic latent semantic indexing|PLSI]].

[[Latent Dirichlet allocation]] involves attributing document terms to topics.

[[n-gram]]s and [[hidden Markov models]] work by representing the term stream as a [[markov chain]] where each term is derived from the few terms before it.

== See also ==
* [[Semantic analysis (knowledge representation)]]



{{Compsci-stub}}</text>
      <sha1>rqqn9up1sti1iq15m4nkjxlk18ic1vp</sha1>
    </revision>
  </page>
  <page>
    <title>Algorithmic inference</title>
    <ns>0</ns>
    <id>20890511</id>
    <revision>
      <id>800577526</id>
      <parentid>726672453</parentid>
      <timestamp>2017-09-14T11:55:02Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* References */[[User:JCW-CleanerBot#Logic|task]], replaced: Proceedings of London Mathematical Society → Proceedings of the London Mathematical Society using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17447">'''Algorithmic inference''' gathers new developments in the [[statistical inference]] methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are [[computational learning theory]], [[granular computing]], [[bioinformatics]], and, long ago, structural probability {{harv|Fraser|1966}}.
The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the [[probability distribution|distribution laws]] to the functional properties of the [[statistics]], and the interest of computer scientists from the algorithms for processing data to the [[information]] they process.

== The Fisher parametric inference problem ==
Concerning the identification of the parameters of a distribution law, the mature reader may recall lengthy disputes in the mid 20th century about the interpretation of their variability in terms of [[fiducial distribution]] {{harv|Fisher|1956}}, structural probabilities {{harv|Fraser|1966}}, priors/posteriors {{harv|Ramsey|1925}}, and so on. From an [[epistemologic|epistemology]] viewpoint, this entailed a companion dispute as to the nature of [[probability]]: is it a physical feature of phenomena to be described through [[random variables]] or a way of synthesizing data about a phenomenon? Opting for the latter, Fisher defines a ''fiducial distribution'' law of parameters of a given random variable that he deduces from a sample of its specifications. With this law he computes, for instance “the probability that μ (mean of a [[Normal distribution|Gaussian variable]] – our note) is less than any assigned value, or the probability that it lies between any assigned values, or, in short, its probability distribution, in the light of the sample observed”.

== The classic solution ==
Fisher fought hard to defend the difference and superiority of his notion of parameter distribution in comparison to
analogous notions, such as  Bayes' [[posterior distribution]], Fraser's constructive probability and Neyman's [[confidence intervals]]. For half a century, Neyman's confidence intervals  won out for all practical purposes, crediting the phenomenological nature of probability. With this perspective, when you deal with a Gaussian variable, its mean μ is fixed by the physical features of the phenomenon you are observing, where the observations are random operators, hence the observed values are specifications of a [[random sample]]. Because of their randomness, you may compute from the sample specific intervals containing the fixed μ with a given probability that you denote ''confidence''.

=== Example ===
Let ''X'' be a Gaussian variable&lt;ref&gt;By default, capital letters (such as ''U'', ''X'') will denote random variables and small letters (''u'', ''x'') their corresponding specifications.&lt;/ref&gt; with parameters &lt;math&gt;\mu&lt;/math&gt; and &lt;math&gt;\sigma^2&lt;/math&gt;
and &lt;math&gt;\{X_1,\ldots,X_m\}&lt;/math&gt; a sample drawn from it. Working with statistics

: &lt;math&gt;S_\mu =\sum_{i=1}^m X_i&lt;/math&gt;

and

: &lt;math&gt;S_{\sigma^2}=\sum_{i=1}^m (X_i-\overline X)^2,\text{ where }\overline X = \frac{S_{\mu}}{m} &lt;/math&gt;

is the sample mean, we recognize that

: &lt;math&gt;T=\frac{S_{\mu}-m\mu}{\sqrt{S_{\sigma^2}}}\sqrt\frac{m-1}{m}=\frac{\overline X-\mu}{\sqrt{S_{\sigma^2}/(m(m-1))}}&lt;/math&gt;

follows a [[Student's t distribution]] {{harv|Wilks|1962}} with parameter (degrees of freedom) ''m''&amp;nbsp;−&amp;nbsp;1, so that

: &lt;math&gt;f_T(t)=\frac{\Gamma(m/2)}{\Gamma((m-1)/2)}\frac{1}{\sqrt{\pi(m-1)}}\left(1 + \frac{t^2}{m-1}\right)^{m/2}.&lt;/math&gt;

Gauging ''T'' between two quantiles and inverting its expression as a function of &lt;math&gt;\mu&lt;/math&gt; you obtain confidence intervals for &lt;math&gt;\mu&lt;/math&gt;.

With the  sample specification:

:&lt;math&gt;\mathbf x=\{7.14, 6.3, 3.9, 6.46, 0.2, 2.94, 4.14, 4.69, 6.02, 1.58\}&lt;/math&gt;

having size ''m'' = 10, you compute the  statistics &lt;math&gt;s_\mu = 43.37&lt;/math&gt; and &lt;math&gt;s_{\sigma^2}=46.07&lt;/math&gt;, and obtain a 0.90 confidence interval for &lt;math&gt;\mu&lt;/math&gt; with extremes (3.03,&amp;nbsp;5.65).
{{clear}}

== Inferring functions with the help of a computer ==
From a modeling perspective the entire dispute looks like a chicken-egg dilemma: either fixed data by first and probability distribution of their properties as a consequence, or fixed properties by first and probability distribution of the observed data as a corollary.
The classic solution has one benefit and one drawback. The former was appreciated particularly back  when people still did computations with  sheet and pencil. Per se, the task of computing a Neyman  confidence interval for the fixed parameter θ is hard: you don’t know θ, but you look for disposing around it an interval with a possibly very low probability of failing. The analytical solution is allowed for a very limited number of theoretical cases.  ''Vice versa'' a large variety of instances may be quickly solved in an ''approximate way'' via the [[central limit theorem]] in terms of confidence interval around a Gaussian distribution – that's the benefit.
The drawback is that the central limit theorem is applicable when the sample size is sufficiently large. Therefore, it is less and less applicable with the sample involved in modern inference instances. The fault is not in the sample size on its own part. Rather, this size is not sufficiently large because of the [[complexity]] of the inference problem.

With the availability of large computing facilities,  scientists refocused from isolated parameters inference to complex functions inference, i.e. re sets of highly nested parameters identifying functions. In these cases we speak about ''learning of functions'' (in terms for instance of [[regression analysis|regression]], [[Neuro-fuzzy|neuro-fuzzy system]] or [[computational learning theory|computational learning]]) on the basis of highly informative samples. A first effect of having a complex structure linking data is the reduction of the number of sample [[Degrees of freedom (statistics)|degrees of freedom]], i.e. the burning of a part of sample points, so that the effective sample size to be considered in the central limit theorem is too small. Focusing  on the sample size ensuring a limited learning error with a given [[confidence level]], the consequence is that the  lower bound on this size grows with [[complexity index|complexity indices]] such as [[VC dimension]] or [[Complexity index#Detail|detail of a class]] to which the function we want to learn belongs.

=== Example ===
A sample of 1,000 independent bits is enough to ensure an absolute error of at most 0.081 on the estimation of the parameter ''p'' of the underlying Bernoulli variable with a confidence of at least 0.99. The same size cannot guarantee a threshold less than 0.088 with the same confidence 0.99 when the error is identified with the probability that a 20-year-old man living in New York does not fit the ranges of height, weight and waistline observed on 1,000 Big Apple inhabitants. The accuracy shortage occurs because both the VC dimension and the detail of the class of parallelepipeds, among which the one observed from the 1,000 inhabitants' ranges falls, are equal to 6.
{{clear}}

== The general inversion problem solving the Fisher question ==
With insufficiently large samples, the approach: ''fixed sample – random properties'' suggests inference procedures in three steps:
{|
|- valign=&quot;top&quot;
|{{Anchor|Sampling mechanism}}1. || '''Sampling mechanism'''. It consists of a pair &lt;math&gt;(Z, g_{\boldsymbol\theta})&lt;/math&gt;, where the seed ''Z'' is a random variable without unknown parameters, while the explaining function &lt;math&gt;g_{\boldsymbol\theta}&lt;/math&gt; is a function mapping from samples of ''Z'' to samples of the random variable ''X'' we are interested in. The parameter vector &lt;math&gt;\boldsymbol\theta&lt;/math&gt; is a specification of the random parameter &lt;math&gt;\mathbf\Theta&lt;/math&gt;. Its components  are  the parameters of the   ''X'' distribution law. The Integral Transform Theorem &lt;!-- {{harv|Mood|1962}} What's that? --&gt; ensures the existence of such a mechanism for each (scalar or vector) ''X'' when the seed coincides with the random variable ''U'' [[Uniform distribution (continuous)|uniformly]] distributed in &lt;math&gt;[0,1]&lt;/math&gt;.
{|
|- valign=&quot;top&quot;
|{{Anchor|Pareto Example}}''Example. ''|| For ''X'' following a [[Pareto distribution]] with parameters ''a'' and ''k'', i.e.

:&lt;math&gt;F_X(x)=\left(1-\frac{k}{x}^a\right) I_{[k,\infty)}(x),&lt;/math&gt;

a sampling mechanism &lt;math&gt;(U, g_{(a,k)})&lt;/math&gt; for ''X'' with seed ''U''  reads:

:&lt;math&gt;g_{(a,k)}(u)=k (1-u)^{-\frac{1}{a}},&lt;/math&gt;
or, equivalently, &lt;math&gt; g_{(a,k)}(u)=k u^{-1/a}. &lt;/math&gt;
|}
|- valign=&quot;top&quot;
| {{Anchor|Master equation}}2. || '''Master equations'''. The actual connection between the model and the observed data is tossed in terms of a set of relations between statistics on the data and unknown parameters that come as a corollary of the sampling mechanisms. We call these relations ''master equations''. Pivoting around the statistic &lt;math&gt;s=h(x_1,\ldots,x_m)= h(g_{\boldsymbol\theta}(z_1),\ldots, g_{\boldsymbol\theta}(z_m))&lt;/math&gt;, the general form of a master equation is:

:&lt;math&gt;s= \rho(\boldsymbol\theta;z_1,\ldots,z_m)&lt;/math&gt;.

With these relations we may inspect the values of the parameters that could have generated a sample with the observed statistic from a particular setting of the seeds representing the seed of the sample. Hence, to the population of sample seeds corresponds a population of parameters. In order to ensure this population clean properties, it is enough to draw randomly the seed values and involve either [[sufficient statistics]] or, simply,  [[well-behaved statistic]]s  w.r.t. the parameters, in the master equations.

For example, the statistics &lt;math&gt;s_1=\sum_{i=1}^m \log x_i&lt;/math&gt; and &lt;math&gt;s_2=\min_{i=1,\ldots,m} \{x_i\}&lt;/math&gt; prove to be sufficient for parameters ''a'' and ''k'' of a Pareto random variable ''X''. Thanks to the (equivalent form of the) sampling mechanism &lt;math&gt;g_{(a,k)}&lt;/math&gt; we may read them as
:&lt;math&gt;s_1=m\log k+1/a \sum_{i=1}^m \log u_i&lt;/math&gt;
:&lt;math&gt;s_2=\min_{i=1,\ldots,m} \{k u_i^{-\frac{1}{a}}\},&lt;/math&gt;
respectively.
|- valign=&quot;top&quot;
| 3. || '''Parameter population'''. Having fixed a set of master equations, you may map sample seeds into parameters either numerically through a [[bootstrapping populations|population bootstrap]], or analytically through a [[Twisting properties#twisting argument|twisting argument]]. Hence from a population of seeds you obtain a population of parameters.

{|
|- valign=&quot;top&quot;
|''Example. '' || From the above master equation we can draw a pair  of parameters, &lt;math&gt;( a, k)&lt;/math&gt;, ''compatible'' with the observed sample by solving the following system of equations:

:&lt;math&gt; a=\frac{\sum\log u_i-m\log \min \{u_i\}}{s_1-m\log s_2}.&lt;/math&gt;
:&lt;math&gt; k=\mathrm e^{\frac{ a s_1-\sum\log u_i}{m a}}&lt;/math&gt;

where &lt;math&gt;s_1&lt;/math&gt; and &lt;math&gt;s_2&lt;/math&gt; are the observed statistics and &lt;math&gt;u_1,\ldots,u_m&lt;/math&gt; a set of uniform seeds. Transferring to the parameters the probability (density) affecting the seeds,  you obtain the distribution law of the random  parameters ''A'' and ''K'' compatible with the statistics you have observed.
|}
Compatibility denotes parameters of compatible populations, i.e. of populations that ''could have generated'' a sample giving rise to the observed statistics. You may formalize this notion as follows:
|}

===Definition===
For a random variable and a sample drawn from it a {{Anchor|compatible distribution}}''compatible distribution'' is a distribution having the same [[Algorithmic inference#Sampling mechanism|sampling mechanism]] &lt;math&gt;\mathcal M_X=(Z,g_{\boldsymbol\theta})&lt;/math&gt; of ''X'' with a value &lt;math&gt;\boldsymbol\theta&lt;/math&gt; of the random parameter &lt;math&gt;\mathbf\Theta&lt;/math&gt; derived from a master equation rooted on a well-behaved statistic ''s''.

=== Example ===
[[Image:Parecdf.png|frame|thunbail|left|90px|Joint empirical cumulative distribution function of parameters &lt;math&gt;(A,K)&lt;/math&gt; of a Pareto random variable.]][[Image:Mucdf.png|frame|thunbail|right|90px|Cumulative distribution function of the mean ''M'' of a Gaussian random variable]]You may find the distribution law of the Pareto parameters ''A''&amp;nbsp;and ''K''&amp;nbsp;as an implementation example of the [[bootstrapping populations|population bootstrap]]&amp;nbsp;method as in the figure on the left.

Implementing the [[Twisting properties#twisting argument|twisting argument]]&amp;nbsp;method,  you get the distribution  law &lt;math&gt;F_M(\mu)&lt;/math&gt;&amp;nbsp;of the mean  ''M''&amp;nbsp;of a Gaussian variable ''X''&amp;nbsp;on the basis of the statistic &lt;math&gt;s_M=\sum_{i=1}^m x_i&lt;/math&gt;&amp;nbsp;when &lt;math&gt;\Sigma^2&lt;/math&gt;&amp;nbsp;is known to be equal to &lt;math&gt;\sigma^2&lt;/math&gt;&amp;nbsp;{{harv|Apolloni|Malchiodi|Gaito|2006}}. Its expression is:

:&lt;math&gt;F_M(\mu)=\Phi\left(\frac{m\mu-s_M}{\sigma\sqrt{m}}\right), &lt;/math&gt;

shown in the figure on the right, where &lt;math&gt;\Phi&lt;/math&gt; is the [[cumulative distribution function]] of  a [[standard normal distribution]].

[[Image:Muconfint.png|frame|thunbail|90px|left|Upper (purple curve) and lower (blue curve) extremes of a 90% confidence interval of the mean ''M'' of a Gaussian random variable for a fixed &lt;math&gt;\sigma&lt;/math&gt; and different values of the statistic ''s''&lt;sub&gt;''m''&lt;/sub&gt;.]] Computing a [[confidence interval]]&amp;nbsp;for ''M''&amp;nbsp;given its distribution function is straightforward: we need only find two quantiles (for instance &lt;math&gt;\delta/2&lt;/math&gt;&amp;nbsp;and &lt;math&gt;1-\delta/2&lt;/math&gt;&amp;nbsp;quantiles in case we are interested in a confidence interval of level δ symmetric in the tail's probabilities) as indicated  on the left in the diagram showing the behavior of the two bounds for different values of the statistic ''s''&lt;sub&gt;''m''&lt;/sub&gt;.

The Achilles heel of Fisher's approach lies in the joint distribution of more than one parameter, say mean and variance of a Gaussian distribution. On the contrary, with the last approach (and  above-mentioned methods: [[bootstrapping populations|population bootstrap]] and [[Twisting properties#twisting argument|twisting argument]]) we may learn the joint distribution of many parameters. For instance, focusing on the distribution of two or many more parameters, in the figures below we report two confidence regions where the function to be learnt falls with a confidence of 90%. The former concerns the probability with which an extended [[support vector machine]] attributes a binary label 1 to the points of the &lt;math&gt;(x,y)&lt;/math&gt; plane. The two surfaces are drawn on the basis of a set of sample points in turn labelled according to a specific distribution law {{harv|Apolloni|Bassis|Malchiodi|Witold|2008}}. The latter concerns the confidence region of the hazard rate of breast cancer recurrence computed from a censored sample {{harv|Apolloni|Malchiodi|Gaito|2006}}.
{|
| [[Image:Svmconf.png|frame|thunbail|100px|90% confidence region for the family of support vector machines endowed with hyperbolic tangent profile function]]
| [[Image:Hazardconf.png|frame|thunbail|100px|90% confidence region for the hazard function of breast cancer recurrence computed from the censored sample &lt;math&gt;t=(9, 13, &gt; 13, 18, 12, 23, 31, 34, &gt; 45, 48, &gt; 161),\, &lt;/math&gt;

with &gt;&amp;nbsp;''t'' denoting a censored time]]
|}

&lt;!--Referenze

Fraser, D.A.S.: Statistics. An Introduction. John Wiley &amp; Sons, London (1958)
Fisher, M.A.: The fiducial argument in statistical inference. Annals of Eugenics 6
(1935) 391–398
Vapnick
Valiant
M. Blanchette, T. Kunisaewa, D. Sankoff Parametric genome rearrangement, Gene 172 (1996) GC 11–17 Elsevier
L: Birkedal, M. Tofte, A constraint-based region inference algorithm --&gt;

== Notes ==

&lt;references /&gt;

{{more footnotes|date=July 2011}}

== References ==

*{{Citation
 | last = Fraser | first = D. A. S.
 | year = 1966
 | title = Structural probability and generalization
 | journal = Biometrika
 | volume = 53
 | issue = 1/2
 | pages = 1–9
 | ref = harv
 | postscript = .
 | doi=10.2307/2334048
}}
*{{Citation
 | last=Fisher |first=M. A.
 | title=Statistical Methods and Scientific Inference
 | publisher=Oliver and Boyd
 | location=Edinburgh and London
 | year=1956
 | ref=harv
}}
*{{Citation
 | last1=Apolloni |first1=B.
 | last2=Malchiodi | first2=D.
 | last3=Gaito | first3=S.
 | title=Algorithmic Inference in Machine Learning
 | publisher=Magill
 | series=International Series on Advanced Intelligence
 | location=Adelaide
 | volume=5
 | quote=Advanced Knowledge International
 | edition=2nd
 | year=2006
 | ref=harv
}}
*{{Citation
 | last1=Apolloni |first1=B.
 | last2=Bassis | first2=S.
 | last3=Malchiodi | first3=D.
 | last4=Witold | first4=P.
 | title=The Puzzle of Granular Computing
 | publisher=Springer
 | series=Studies in Computational Intelligence
 | location=Berlin
 | volume=138
 | year=2008
 | ref=harv
}}
*{{Citation
 | last= Ramsey |first= F. P.
 | title= The Foundations of Mathematics
 | year= 1925
 | journal=Proceedings of the London Mathematical Society
 | ref=harv
 | postscript= .
}}
*{{Citation
 | last=Wilks |first=S.S.
 | title=Mathematical Statistics
 | series=Wiley Publications in Statistics
 | publisher=John Wiley
 | location=New York
 | year=1962
 | ref=harv
}}

[[Category:Algorithmic inference| ]]
</text>
      <sha1>drf0m2jhpfaq6sxax0nlmd1eiy80cqf</sha1>
    </revision>
  </page>
  <page>
    <title>Decision list</title>
    <ns>0</ns>
    <id>19317802</id>
    <revision>
      <id>711324685</id>
      <parentid>628959016</parentid>
      <timestamp>2016-03-22T05:47:27Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Definition */ {{mvar}} for MathML bug</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2065">'''Decision lists''' are a representation for Boolean functions.&lt;ref&gt;{{cite journal|author=Ronald L. Rivest|authorlink=Ronald L. Rivest|title=Learning decision lists|journal=Machine Learning|volume=2|issue=3|pages=229–246|date=Nov 1987|doi=10.1023/A:1022607331053|url=http://people.csail.mit.edu/rivest/pubs/Riv87b.pdf}}&lt;/ref&gt;  Single term decision lists are more expressive than [[disjunctions]] and [[conjunctions]]; however, 1-term decision lists are less expressive than the general [[disjunctive normal form]] and the [[conjunctive normal form]].

The language specified by a k-length decision list includes as a subset the language specified by a k-depth [[decision tree]].

Learning decision lists can be used for [[attribute efficient learning]].&lt;ref&gt;Adam R. Klivans and Rocco A. Servedio, &quot;Toward Attribute Efficient Learning of Decision Lists and Parities&quot;, ''Journal of Machine Learning Research'' '''7''':12:587-602 [http://dl.acm.org/citation.cfm?id=1248567&amp;dl=ACM&amp;coll=DL&amp;CFID=344844478&amp;CFTOKEN=13074001 ACM Digital Library] [http://www.jmlr.org/papers/volume7/klivans06a/klivans06a.pdf full text]&lt;/ref&gt;

== Definition ==

A decision list (DL) of length {{mvar|r}} is of the form:

 '''if''' {{math|''f''&lt;sub&gt;1&lt;/sub&gt;}} '''then'''
   output {{math|''b''&lt;sub&gt;1&lt;/sub&gt;}}
 '''else if''' {{math|''f''&lt;sub&gt;2&lt;/sub&gt;}} '''then'''
   output {{math|''b''&lt;sub&gt;2&lt;/sub&gt;}}
 ...
 '''else if''' {{mvar|f&lt;sub&gt;r&lt;/sub&gt;}} '''then'''
   output {{mvar|b&lt;sub&gt;r&lt;/sub&gt;}}

where {{mvar|f&lt;sub&gt;i&lt;/sub&gt;}} is the {{mvar|i}}th formula and {{mvar|b&lt;sub&gt;i&lt;/sub&gt;}} is the {{mvar|i}}th [[Boolean data type|boolean]] for &lt;math&gt;i \in \{1...r\}&lt;/math&gt;.  The last if-then-else is the default case, which means formula {{mvar|f&lt;sub&gt;r&lt;/sub&gt;}} is always equal to true. A {{mvar|k}}-DL is a decision list where all of formulas have at most {{mvar|k}} terms.  Sometimes &quot;decision list&quot; is used to refer to a 1-DL, where all of the formulas are either a variable or its [[negation]].

==References==
&lt;references/&gt;





{{AI-stub}}</text>
      <sha1>399kt228w3fowkzf57t9n08i1wzbgms</sha1>
    </revision>
  </page>
  <page>
    <title>Rule induction</title>
    <ns>0</ns>
    <id>7517319</id>
    <revision>
      <id>799119079</id>
      <parentid>799119024</parentid>
      <timestamp>2017-09-05T19:11:56Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>fixing a typographical error</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1414">'''Rule induction''' is an area of [[machine learning]] in which formal rules are extracted from a set of observations.  The rules extracted may represent a full [[scientific model]] of the data, or merely represent local [[patterns]] in the data.

== Paradigms ==

Some major rule induction paradigms are:
*[[Association rule learning]] algorithms (e.g., Aggrawal)
*[[Decision rules|Decision rule]] algorithms (e.g., Quinlan 1987)
*[[Hypothesis testing]] algorithms (e.g., RULEX)
*[[Horn clause]] induction
*[[Version spaces]]
*[[Rough set]] rules
*[[Inductive Logic Programming]]
*Boolean decomposition (Feldman)

== Algorithms ==

Some rule induction algorithms are:
*Charade&lt;ref&gt;Sahami, Mehran. &quot;[https://pdfs.semanticscholar.org/9039/68adbb73916120b67d8098e5df95a0166eb6.pdf Learning classification rules using lattices].&quot; Machine learning: ECML-95 (1995): 343-346.&lt;/ref&gt;
*Rulex
*[[PROGOL|Progol]]
*[[CN2 algorithm | CN2]]

== References ==
*{{cite conference
  | first = J. R.
  | last = Quinlan
  | title = Generating production rules from decision trees
  | booktitle = Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87)
  | pages = 304–307
  | date = 1987
  | location = Milan, Italy
  | editor = McDermott, John
  | url = http://www.ijcai.org/Proceedings/87-1/Papers/063.pdf}}




{{Comp-sci-stub}}</text>
      <sha1>ax864f8rve5mdup4fuspi7epurvqqd8</sha1>
    </revision>
  </page>
  <page>
    <title>Instance-based learning</title>
    <ns>0</ns>
    <id>22589574</id>
    <revision>
      <id>798310995</id>
      <parentid>786014301</parentid>
      <timestamp>2017-09-01T04:55:15Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Bots/Requests for approval/KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3225">In [[machine learning]], '''instance-based learning''' (sometimes called '''memory-based learning'''&lt;ref&gt;{{cite book |author1=Walter Daelemans |authorlink1=Walter Daelemans |author2=Antal van den Bosch |authorlink2=Antal van den Bosch |year=2005 |title=Memory-Based Language Processing |publisher=Cambridge University Press}}&lt;/ref&gt;) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.

It is called instance-based because it constructs hypotheses directly from the training instances themselves.&lt;ref name='aima733'&gt;[[Stuart J. Russell|Stuart Russell]] and [[Peter Norvig]] (2003). ''[[Artificial Intelligence: A Modern Approach]]'', second edition, p. 733. Prentice Hall. {{ISBN|0-13-080302-2}}&lt;/ref&gt;
This means that the hypothesis complexity can grow with the data:&lt;ref name='aima733'/&gt; in the worst case, a hypothesis is a list of ''n'' training items and the computational complexity of [[Classification (machine learning)|classifying]] a single new instance is [[Big O notation|''O'']](''n''). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. Instance-based learners may simply store a new instance or throw an old instance away.

Examples of instance-based learning algorithm are the [[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]], [[kernel method|kernel machines]] and [[Radial basis function network|RBF networks]].&lt;ref&gt;{{cite book |author=Tom Mitchell |title=Machine Learning |year=1997 |publisher=McGraw-Hill}}&lt;/ref&gt;{{rp|ch. 8}} These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision.

To battle the memory complexity of storing all training instances, as well as the risk of [[overfitting]] to noise in the training set, ''instance reduction'' algorithms have been proposed.&lt;ref&gt;{{cite journal |title=Reduction techniques for instance-based learning algorithms |author1=D. Randall Wilson |author2=Tony R. Martinez |journal=[[Machine Learning (journal)|Machine Learning]] |publisher=Kluwer |year=2000}}&lt;/ref&gt;

Gagliardi&lt;ref name=Gagliardi2011&gt;{{cite journal|last=Gagliardi|first=F|title=Instance-based classifiers applied to medical databases: Diagnosis and knowledge extraction|journal=Artificial Intelligence in Medicine|year=2011|volume=52|issue=3|pages=123–139|doi=10.1016/j.artmed.2011.04.002|url=https://dx.doi.org/10.1016/j.artmed.2011.04.002}}&lt;/ref&gt; applies this family of classifiers in medical field as second-opinion [[Clinical decision support system|diagnostic tools]] and as tools for the knowledge extraction phase in the process of [[knowledge discovery in databases]].
One of these classifiers (called ''Prototype exemplar learning classifier'' ([[PEL-C]]) is able to extract a mixture of abstracted prototypical cases (that are [[syndrome]]s) and selected atypical clinical cases.

==See also==
*[[Analogical modeling]]

==References==
{{reflist|30em}}



{{AI-stub}}</text>
      <sha1>rqu4hbk54jtsr4d1dvujokp1azs8u2i</sha1>
    </revision>
  </page>
  <page>
    <title>Overfitting</title>
    <ns>0</ns>
    <id>173332</id>
    <revision>
      <id>812739901</id>
      <parentid>812729908</parentid>
      <timestamp>2017-11-29T16:06:08Z</timestamp>
      <contributor>
        <ip>86.134.138.174</ip>
      </contributor>
      <comment>/* Statistical inference */  wording</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16539">{{Refimprove|date=August 2017}}

[[Image:Overfitting.svg|thumb|300px|Figure 1.&amp;nbsp; The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data, compared to the black line.]]
[[Image:Overfitted Data.png|thumb|300px|Figure 2.&amp;nbsp; Noisy (roughly linear) data is fitted to a linear function and a [[polynomial]] function. Although the polynomial function is a perfect fit, the linear function can be expected to generalize better: if the two functions were used to extrapolate beyond the fit data, the linear function would make better predictions.]]

In statistics, '''overfitting''' is &quot;the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably&quot;.&lt;ref&gt;Definition of &quot;[https://en.oxforddictionaries.com/definition/overfitting overfitting]&quot; at [[OxfordDictionaries.com]]: this definition is specifically for Statistics.&lt;/ref&gt; An '''overfitted model''' is a [[statistical model]] that contains more [[parameter]]s than can be justified by the data.&lt;ref name=&quot;CDS&quot; /&gt; The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the [[Statistical noise|noise]]) as if that variation represented underlying model structure.&lt;ref name=&quot;BA2002&quot; /&gt;{{rp|45}}

'''Underfitting''' occurs when a statistical model cannot adequately capture the underlying structure of the data. An '''underfitted model''' is a model where some parameters or terms that would appear in a correctly specified model are missing.&lt;ref name=&quot;CDS&quot; /&gt; Underfitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.

Overfitting and underfitting can occur in [[machine learning]], in particular. In machine learning, the phenomena are sometimes called &quot;overtraining&quot; and &quot;undertraining&quot;.

The possibility of overfitting exists because the criterion used for [[model selection|selecting the model]] is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then overfitting occurs when a model begins to &quot;memorize&quot; training data rather than &quot;learning&quot; to generalize from a trend.

As an extreme example, if the number of parameters is the same as or greater than the number of observations, a simple model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure&amp;nbsp;2.) Such a model, though, will typically fail severely when making predictions.

The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data.{{Citation needed|date=September 2017}} Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as ''shrinkage'').&lt;ref name=&quot;CDS&quot;&gt;Everitt B.S., Skrondal A. (2010), ''Cambridge Dictionary of Statistics'', [[Cambridge University Press]].&lt;/ref&gt; In particular, the value of the [[coefficient of determination]] will [[Shrinkage (statistics)|shrink]] relative to the original data.

To lessen the chance of, or amount of, overfitting, several techniques are available (e.g. [[Model selection|model comparison]], [[cross-validation (statistics)|cross-validation]], [[regularization (mathematics)|regularization]], [[early stopping]], [[pruning (algorithm)|pruning]], [[Prior distribution|Bayesian priors]], or [[Dropout (neural networks)|dropout]]). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.

==Statistical inference==
{{expand section|date=October 2017}}
In statistics, an [[Statistical inference|inference]] is drawn from a [[statistical model]], which has been [[model selection|selected]] via some procedure. Burnham&amp;nbsp;&amp; Anderson, in their much-cited text on model selection, argue that to avoid overfitting, we should adhere to the &quot;[[Principle of Parsimony]]&quot;.&lt;ref name=&quot;BA2002&quot;&gt;{{Citation |last=Burnham |first=K. P. |last2=Anderson |first2=D. R. |year=2002 |title=Model Selection and Multimodel Inference |edition=2nd |publisher=Springer-Verlag }}. (This has over 38000 citations on [[Google Scholar]].)&lt;/ref&gt; The authors also state the following.&lt;ref name=&quot;BA2002&quot; /&gt;{{rp|32-33}}
{{quote|text= Overfitted models &amp;hellip; are often free of bias in the parameter estimators, but have estimated (and actual) sampling variances that are needlessly large (the precision of the estimators is poor, relative to what could have been accomplished with a more parsimonious model). Spurious treatment effects tend to be identified, and spurious variables are included with overfitted models. &amp;hellip; A best approximating model is achieved by properly balancing the errors of underfitting and overfitting.}}

Overfitting is more likely to be a serious concern when there is little theory is available to guide the analysis, in part because then there tend to be a large number of models to select from. The book ''Model Selection and Model Averaging'' (2008) puts it this way.&lt;ref&gt;{{citation|last1=Claeskens|first1=G.|author1-link= Gerda Claeskens |authorlink2=Nils Lid Hjort|last2=Hjort|first2=N.L.|year=2008|title=Model Selection and Model Averaging|publisher=[[Cambridge University Press]]}}.&lt;/ref&gt;
{{quote| text=Given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? With so many candidate models, overfitting is a real danger. Is the monkey who typed Hamlet actually a good writer?}}

===Regression===
In regression, overfitting occurs frequently.&lt;ref name=&quot;RMS&quot;&gt;{{citation| title= Regression Modeling Strategies | last= Harrell | first= F. E., Jr. | year= 2001 | publisher= Springer}}.&lt;/ref&gt; In the extreme case, if there are ''p'' variables in a [[linear regression]] with ''p'' data points, the fitted line will go exactly through every point.&lt;ref&gt;{{cite web
| url=http://www.ma.utexas.edu/users/mks/statmistakes/ovefitting.html
| title=Overfitting
| author=Martha K. Smith
| date=2014-06-13
| publisher=[[University of Texas at Austin]]
| accessdate=2016-07-31}}&lt;/ref&gt; A recent study suggests that two observations per independent variable are sufficient for [[linear regression]]&lt;ref name=&quot;Austin et al. (2015)&quot;&gt;{{cite journal |first=P. C. |last=Austin |first2=E. W. |last2=Steyerberg |year=2015 |title=The number of subjects per variable required in linear regression analyses |journal=Journal of Clinical Epidemiology |volume=68 |issue=6 |pages=627-636 |doi=10.1016/j.jclinepi.2014.12.014}}&lt;/ref&gt;. For [[logistic regression]] or Cox [[proportional hazards models]], there are a variety of rules of thumb (e.g. 5-9&lt;ref name=&quot;Vittinghoff et al. (2007)&quot;&gt;{{cite journal |first=E. |last=Vittinghoff |first2=C. E. |last2=McCulloch |year=2007 |title=Relaxing the Rule of Ten Events per Variable in Logistic and Cox Regression |journal=American Journal of Epidemiology |volume=165 |issue=6 |pages=710-718 |doi=10.1093/aje/kwk052}}&lt;/ref&gt;, 10&lt;ref&gt;{{cite book
| title = Applied Regression Analysis
| edition= 3rd
| last1 = Draper
| first1 = Norman R.
| last2 = Smith
| first2 = Harry
| publisher = [[John Wiley &amp; Sons|Wiley]]
| year = 1998
| isbn = 978-0471170822}}&lt;/ref&gt; and 10-15&lt;ref&gt;{{cite web
| url = http://blog.minitab.com/blog/adventures-in-statistics/the-danger-of-overfitting-regression-models
| title = The Danger of Overfitting Regression Models
| author = Jim Frost
| date = 2015-09-03
| accessdate = 2016-07-31}}&lt;/ref&gt; — the guideline of 10 observations per independent variable is known as the &quot;[[one in ten rule]]&quot;). In the process of regression model selection, the mean squared error of the random regression function can be split into random noise, approximation bias, and variance in the estimate of regression function, and [[bias–variance tradeoff]] is often used to overcome overfit models.

==Machine learning==
[[Image:Overfitting svg.svg|thumb|300px|Overfitting/overtraining in supervised learning (e.g., [[neural network]]). Training error is shown in blue, validation error in red, both as a function of the number of training cycles. If the validation error increases(positive slope) while the training error steadily decreases(negative slope) then a situation of overfitting may have occurred. The best predictive and fitted model would be where the validation error has its global minimum.]]

Usually a learning [[algorithm]] is trained using some set of &quot;training data&quot;: exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed &quot;validation data&quot; that was not encountered during its training.

Overfitting is the use of models or procedures that violate [[Occam's razor]], for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. For an example where there are too many adjustable parameters, consider a dataset where training data for {{mvar|y}} can be adequately predicted by a linear function of two dependent variables. Such a function requires only three parameters (the intercept and two slopes). Replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two dependent variables, carries a risk: Occam's razor implies that any given complex function is ''a priori'' less probable than any given simple function. If the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training-data fit to offset the complexity increase, then the new complex function &quot;overfits&quot; the data, and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset.&lt;ref name=hawkins&gt;Hawkins, Douglas M. (2004), &quot;The problem of overfitting&quot;, ''[[Journal of Chemical Information and Modeling]]'', 44.1: 1-12.&lt;/ref&gt;

When comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model; the expressivity of each parameter must be considered as well. For example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with {{mvar|m}} parameters to a regression model with {{mvar|n}} parameters.&lt;ref name=hawkins /&gt;

Overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data, that have no [[causal relation]] to the [[Function approximation|target function]]. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse.

As a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. It's easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes; but this model will not generalize at all to new data, because those past times will never occur again.

Generally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future and irrelevant information (&quot;noise&quot;). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. The problem is determining which part to ignore. A learning algorithm that can reduce the chance of fitting noise is called '''[[Robustness (computer science)#Robust machine learning|robust]]'''.

===Consequences===
The most obvious consequence of overfitting is poor performance on the validation dataset. Other negative consequences include:&lt;ref name=hawkins /&gt;

* A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data-entry.
* A more complex, overfitted function is likely to be less portable than a simple one. At one extreme, a one-variable linear regression is so portable that, if necessary, it could even be done by hand. At the other extreme are models that can be reproduced only by exactly duplicating the original modeler's entire setup, making reuse or scientific reproduction difficult.

==Underfitting==

Underfitting occurs when a statistical model or machine learning algorithm cannot adequately capture the underlying structure of the data. It occurs when the model or algorithm does not fit the data enough. Underfitting occurs if the model or algorithm shows low variance but high bias (to contrast the opposite, overfitting from high variance and low bias). It is often a result of an excessively simple model.&lt;ref&gt;{{cite web|first=Eric|last=Cai|title=Machine Learning Lesson of the Day – Overfitting and Underfitting|url=http://www.statsblogs.com/2014/03/20/machine-learning-lesson-of-the-day-overfitting-and-underfitting/|website=StatBlogs|date=2014-03-20}}&lt;/ref&gt;

Burnham&amp;nbsp;&amp; Anderson state the following.&lt;ref name=&quot;BA2002&quot;/&gt;{{rp|32}}
{{quote|text= &amp;hellip; an underfitted model would ignore some important replicable (i.e., conceptually replicable in most other samples) structure in the data and thus fail to identify effects that were actually supported by the data. In this case, bias in the parameter estimators is often substantial, and the sampling variance is underestimated, both factors resulting in poor confidence interval coverage. Underfitted models tend to miss important treatment effects in experimental settings.}}

== See also ==
* [[Bias–variance tradeoff]]
* [[Curve fitting]]
* [[Data dredging]]
* [[Freedman's paradox]]
* [[Model selection]]
* [[Occam's razor]]
* [[VC dimension]] - measures the complexity of a learning model: larger VC dimension implies larger risk of overfitting

== References ==

* {{Cite journal
| last1 = Leinweber
| first1 = D. J.
| title = Stupid Data Miner Tricks
| journal = [[The Journal of Investing]]
| volume = 16
| pages = 15–22
| year = 2007
| doi = 10.3905/joi.2007.681820}}

* {{cite journal
| last = Tetko
| first = I. V.
| last2 = Livingstone
| first2 = D. J.
| last3 = Luik
| first3 = A. I.
| title = Neural network studies. 1. Comparison of Overfitting and Overtraining
| journal = [[Journal of Chemical Information and Modeling|J. Chem. Inf. Comput. Sci.]]
| year = 1995
| volume = 35
| issue = 5
| pages = 826–833
| url = http://www.vcclab.org/articles/jcics-overtraining.pdf
| doi = 10.1021/ci00027a006 }}
== Notes ==
{{reflist}}

== External links ==
* [http://blog.lokad.com/journal/2009/4/22/overfitting-when-accuracy-measure-goes-wrong.html Overfitting: when accuracy measure goes wrong] - an introductory video tutorial.
* [http://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/node16.html The Problem of Overfitting Data]
* [http://courses.cs.washington.edu/courses/cse546/12wi/slides/cse546wi12LinearRegression.pdf CSE546: Linear Regression Bias / Variance Tradeoff]



</text>
      <sha1>lk9ij08loieomdsd7c95h5xjjfatfy0</sha1>
    </revision>
  </page>
  <page>
    <title>Feature vector</title>
    <ns>0</ns>
    <id>2085584</id>
    <revision>
      <id>803725215</id>
      <parentid>796125884</parentid>
      <timestamp>2017-10-04T06:42:15Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor/>
      <comment>/* See also */ alpha</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3640">{{Redirect|Feature space|feature spaces in kernel machines|Kernel method}}
In [[pattern recognition]] and [[machine learning]], a '''feature vector''' is an n-dimensional [[vector (geometric)|vector]] of numerical [[Features (pattern recognition)|feature]]s that represent some object. Many [[algorithm]]s in machine learning require a numerical representation of objects, since such representations facilitate processing and
statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of [[explanatory variable]]s used in [[statistics|statistical]] procedures such as [[linear regression]].  Feature vectors are often combined with weights using a [[dot product]] in order to construct a [[linear predictor function]] that is used to determine a score for making a prediction.

The [[vector space]] associated with these vectors is often called the '''feature space'''. In order to reduce the dimensionality of the feature space, a number of [[dimensionality reduction]] techniques can be employed.

Higher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases the feature 'Age' is useful and is defined as ''Age = 'Year of death' minus 'Year of birth' ''. This process is referred to as '''feature construction'''.&lt;ref name=Liu1998&gt;Liu, H., Motoda H. (1998) ''Feature Selection for Knowledge Discovery and Data Mining.'', Kluwer Academic Publishers. Norwell, MA, USA. 1998.&lt;/ref&gt;&lt;ref name=Piramithu2009&gt;Piramuthu, S., Sikora R. T. Iterative feature construction for improving inductive learning algorithms. In Journal of Expert Systems with Applications. Vol. 36 , Iss. 2 (March 2009), pp. 3401-3406, 2009&lt;/ref&gt; Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C)&lt;ref name=bloedorn1998&gt;Bloedorn, E., Michalski, R. Data-driven constructive induction: a methodology and its applications. IEEE Intelligent Systems, Special issue on Feature Transformation and Subset Selection, pp. 30-37, March/April, 1998&lt;/ref&gt; that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems.&lt;ref name=breinman1984&gt;Breiman, L. Friedman, T., Olshen, R., Stone, C. (1984) ''Classification and regression trees'', Wadsworth&lt;/ref&gt; Applications include studies of disease and [[emotion recognition]] from speech.&lt;ref name=Sidorova2009&gt;Sidorova, J., Badia T. Syntactic learning for ESEDA.1, tool for enhanced speech emotion detection and analysis. Internet Technology and Secured Transactions Conference 2009 (ICITST-2009), London, November 9–12. IEEE&lt;/ref&gt;

== See also ==
* [[Dimensionality reduction]]
* [[Feature engineering]]
* [[Feature extraction]]
* [[Feature selection]]
* [[Statistical classification]]

==References==
{{reflist}}


{{DEFAULTSORT:Feature Vector}}




{{mathapplied-stub}}</text>
      <sha1>jdliqm3zd0xcc8kzg83ravhqh32jqw2</sha1>
    </revision>
  </page>
  <page>
    <title>Uniform convergence in probability</title>
    <ns>0</ns>
    <id>22999791</id>
    <revision>
      <id>807738730</id>
      <parentid>807738257</parentid>
      <timestamp>2017-10-29T21:33:12Z</timestamp>
      <contributor>
        <username>Seppi333</username>
        <id>19008077</id>
      </contributor>
      <comment>add merge template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12888">{{merge to|Convergence of random variables|Proofs of convergence of random variables|discuss=Talk:Uniform convergence in probability#Merge to Convergence of random variables and Proofs of convergence of random variables|date=October 2017}}
{{other|uniform convergence}}
'''Uniform convergence in probability''' is a form of [[convergence in probability]] in [[Asymptotic theory (statistics)|statistical asymptotic theory]] and [[probability theory]]. It means that, under certain conditions, the ''empirical frequencies'' of all events in a certain event-family converge to their ''theoretical probabilities''.  Uniform convergence in probability has applications to [[statistics]] as well as [[machine learning]] as part of [[statistical learning theory]].

The [[law of large numbers]] says that, for each ''single'' event, its empirical frequency in a sequence of independent trials converges (with high probability) to its theoretical probability. But in some applications, we are interested not in a single event but in a whole ''family of events''. We would like to know whether the empirical frequency of every event in the family converges to its theoretical probability ''simultaneously''. The Uniform Convergence Theorem gives a sufficient condition for this convergence to hold. Roughly, if the event-family is sufficiently simple (its [[VC dimension]] is sufficiently small) then uniform convergence holds.

{{TOC limit|3}}

== Definitions ==
For a class of [[Predicate (mathematical logic)|predicates]] &lt;math&gt;H&lt;/math&gt; defined on a set &lt;math&gt;X&lt;/math&gt; and a set of samples &lt;math&gt;x=(x_1,x_2,\dots,x_m)&lt;/math&gt;, where &lt;math&gt;x_i\in X&lt;/math&gt;, the ''empirical frequency'' of &lt;math&gt;h\in H&lt;/math&gt; on &lt;math&gt;x&lt;/math&gt; is

: &lt;math&gt;\widehat{Q}_x(h)=\frac 1 m |\{i:1\leq i\leq m, h(x_i)=1\}|.&lt;/math&gt;

The ''theoretical probability'' of &lt;math&gt;h\in H&lt;/math&gt; is defined as &lt;math&gt;Q_P(h) = P\{y\in X : h(y)=1\}.&lt;/math&gt;

The Uniform Convergence Theorem states, roughly, that if &lt;math&gt;H&lt;/math&gt; is &quot;simple&quot; and we draw samples independently (with replacement) from &lt;math&gt;X&lt;/math&gt; according to any distribution &lt;math&gt;P&lt;/math&gt;, then [[with high probability]], the empirical frequency will be close to its [[expected value]], which is the theoretical probability.

Here &quot;simple&quot; means that the [[Vapnik–Chervonenkis dimension]] of the class &lt;math&gt;H&lt;/math&gt; is small relative to the size of the sample. In other words, a sufficiently simple collection of functions behaves roughly the same on a small random sample as it does on the distribution as a whole.

The Uniform Convergence Theorem was first proved by Vapnik and Chervonenkis&lt;ref name=vc&gt;{{Cite Vapnik Chervonenkis}}&lt;/ref&gt; using the concept of [[growth function]].

==Uniform convergence theorem ==

The statement of the uniform convergence theorem is as follows:&lt;ref name=&quot;books.google.com&quot;&gt;[https://books.google.com/books?id=OiSJYwp4lzYC&amp;dq=neural+network+learning+theoretical+foundations&amp;printsec=frontcover&amp;source=bn&amp;hl=en&amp;ei=kF32SZDNG8TgtgeXxpSfDw&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=4 Martin Anthony Peter, l. Bartlett. Neural Network Learning: Theoretical Foundations, pages 46–50. First Edition, 1999. Cambridge University Press] {{ISBN|0-521-57353-X}}&lt;/ref&gt;

If &lt;math&gt;H&lt;/math&gt; is a set of &lt;math&gt;\{0,1\}&lt;/math&gt;-valued functions defined on a set &lt;math&gt;X&lt;/math&gt; and  &lt;math&gt;P&lt;/math&gt; is a probability distribution on &lt;math&gt;X&lt;/math&gt; then for &lt;math&gt;\varepsilon&gt;0&lt;/math&gt; and &lt;math&gt;m&lt;/math&gt; a positive integer, we have:
: &lt;math&gt;P^m\{|Q_P(h)-\widehat{Q_x}(h)|\geq\varepsilon \text{ for some } h\in H\}\leq 4\Pi_H(2m)e^{-\varepsilon^2 m/8}.&lt;/math&gt;
: where, for any &lt;math&gt;x\in X^m,&lt;/math&gt;,
: &lt;math&gt;Q_P(h)=P\{(y\in X:h(y)=1\},&lt;/math&gt;
: &lt;math&gt;\widehat{Q}_x(h)=\frac 1 m |\{i:1\leq i\leq m,h(x_{i})=1\}|&lt;/math&gt;
: and &lt;math&gt;|x|=m&lt;/math&gt;. &lt;math&gt;P^m&lt;/math&gt; indicates that the probability is taken over &lt;math&gt;x&lt;/math&gt; consisting of &lt;math&gt;m&lt;/math&gt; i.i.d. draws from the distribution &lt;math&gt;P&lt;/math&gt;.

: &lt;math&gt;\Pi_H&lt;/math&gt; is defined as: For any &lt;math&gt;\{0,1\}&lt;/math&gt;-valued functions &lt;math&gt;H&lt;/math&gt; over &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;D\subseteq X &lt;/math&gt;,
: &lt;math&gt;\Pi_H(D)=\{h\cap D:h\in H\}.&lt;/math&gt;

And for any natural number &lt;math&gt;m&lt;/math&gt;, the [[shattering number]] &lt;math&gt;\Pi_H(m)&lt;/math&gt; is defined as:
: &lt;math&gt;\Pi_H(m)=\max|\{h\cap D:|D|=m,h\in H\}|.&lt;/math&gt;

From the point of Learning Theory one can consider &lt;math&gt;H&lt;/math&gt; to be the [[Concept class|Concept/Hypothesis]] class defined over the instance set &lt;math&gt;X&lt;/math&gt;. Before getting into the details of the proof of the theorem we will state Sauer's Lemma which we will need in our proof.

== Sauer–Shelah lemma ==
The [[Sauer–Shelah lemma]]&lt;ref&gt;[http://ttic.uchicago.edu/~tewari/lectures/lecture11.pdf Sham Kakade and Ambuj Tewari, CMSC 35900 (Spring 2008) Learning Theory, Lecture 11]&lt;/ref&gt; relates the shattering number &lt;math&gt;\Pi_{h}(m)&lt;/math&gt; to the VC Dimension.

'''Lemma:''' &lt;math&gt;\Pi_{H}(m)\leq\left( \frac{em}{d}\right)^{d}&lt;/math&gt;, where &lt;math&gt;d&lt;/math&gt; is the [[VC Dimension]] of the concept class &lt;math&gt;H&lt;/math&gt;.

'''Corollary:''' &lt;math&gt;\Pi_{H}(m)\leq m^{d}&lt;/math&gt;.

== Proof of uniform convergence theorem ==
&lt;ref name=vc/&gt; and &lt;ref name=&quot;books.google.com&quot;/&gt; are the sources of the proof below. Before we get into the details of the proof of the ''Uniform Convergence Theorem'' we will present a high level overview of the proof.

#''Symmetrization:'' We transform the problem of analyzing &lt;math&gt;|Q_{P}(h)-\widehat{Q}_{x}(h)|\geq\varepsilon&lt;/math&gt; into the problem of analyzing &lt;math&gt;|\widehat{Q}_{r}(h)-\widehat{Q}_{s}(h)|\geq\varepsilon/2&lt;/math&gt;, where &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt; are i.i.d samples of size &lt;math&gt;m&lt;/math&gt; drawn according to the distribution &lt;math&gt;P&lt;/math&gt;. One can view &lt;math&gt;r&lt;/math&gt; as the original randomly drawn sample of length &lt;math&gt;m&lt;/math&gt;, while &lt;math&gt;s&lt;/math&gt; may be thought as the testing sample which is used to estimate &lt;math&gt;Q_{P}(h)&lt;/math&gt;.
#''Permutation:'' Since &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt; are picked identically and independently, so swapping elements between them will not change the probability distribution on &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt;. So, we will try to bound the probability of &lt;math&gt;|\widehat{Q}_{r}(h)-\widehat{Q}_{s}(h)|\geq\varepsilon/2&lt;/math&gt; for some &lt;math&gt;h \in H&lt;/math&gt; by considering the effect of a specific collection of permutations of the joint sample &lt;math&gt;x=r||s&lt;/math&gt;. Specifically, we consider permutations &lt;math&gt;\sigma(x)&lt;/math&gt; which swap &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;x_{m+i}&lt;/math&gt; in some subset of &lt;math&gt;{1,2,...,m}&lt;/math&gt;. The symbol &lt;math&gt;r||s&lt;/math&gt; means the concatenation of &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt;.
#''Reduction to a finite class:'' We can now restrict the function class &lt;math&gt;H&lt;/math&gt; to a fixed joint sample and hence, if &lt;math&gt;H&lt;/math&gt; has finite VC Dimension, it reduces to the problem to one involving a finite function class.

We present the technical details of the proof.

=== Symmetrization ===

'''Lemma:''' Let &lt;math&gt;V=\{x\in X^m:|Q_P(h)-\widehat{Q}_x(h)|\geq\varepsilon \text{ for some } h\in H\}&lt;/math&gt; and
: &lt;math&gt;R=\{(r,s)\in X^m \times X^m:|\widehat{Q_r}(h)-\widehat{Q}_s(h) |\geq \varepsilon /2 \text{ for some } h\in H\}.&lt;/math&gt;

Then for &lt;math&gt;m\geq\frac 2 {\varepsilon^2}&lt;/math&gt;, &lt;math&gt;P^m(V)\leq 2P^{2m}(R)&lt;/math&gt;.

Proof:
By the triangle inequality,&lt;br&gt;
if &lt;math&gt;|Q_{P}(h)-\widehat{Q}_r(h)|\geq\varepsilon&lt;/math&gt; and &lt;math&gt;|Q_P(h)-\widehat{Q}_s (h)|\leq\varepsilon /2&lt;/math&gt; then &lt;math&gt;|\widehat{Q}_r(h)-\widehat{Q}_s (h)|\geq\varepsilon /2&lt;/math&gt;.

Therefore,

: &lt;math&gt;
\begin{align}
&amp; P^{2m}(R) \\[5pt]
\geq {} &amp; P^{2m}\{\exists h\in H,|Q_{P}(h)-\widehat{Q}_r(h)| \geq \varepsilon \text{ and } |Q_P(h)-\widehat{Q}_s(h)|\leq\varepsilon /2\} \\[5pt]
= {} &amp; \int_V P^m\{s:\exists h\in H,|Q_P(h)-\widehat{Q}_r(h)|\geq\varepsilon \text{ and } |Q_P(h)-\widehat{Q}_s(h)|\leq\varepsilon /2\} \, dP^m(r) \\[5pt]
= {} &amp; A
\end{align}
&lt;/math&gt;

since &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt; are independent.

Now for &lt;math&gt;r\in V&lt;/math&gt; fix an &lt;math&gt;h\in H&lt;/math&gt; such that &lt;math&gt;|Q_P(h)-\widehat{Q}_r(h)|\geq\varepsilon&lt;/math&gt;. For this &lt;math&gt;h&lt;/math&gt;, we shall show that

: &lt;math&gt;P^m \left\{ |Q_P(h)-\widehat{Q}_s(h)|\leq\frac \varepsilon 2\right\} \geq\frac 1 2. &lt;/math&gt;

Thus for any &lt;math&gt;r\in V&lt;/math&gt;, &lt;math&gt;A\geq\frac{P^m(V)}2&lt;/math&gt; and hence &lt;math&gt;P^{2m}(R)\geq\frac{P^m(V)}2&lt;/math&gt;. And hence we perform the first step of our high level idea.

Notice, &lt;math&gt;m\cdot \widehat{Q}_s(h)&lt;/math&gt; is a binomial random variable with expectation &lt;math&gt;m\cdot Q_{P}(h)&lt;/math&gt; and variance &lt;math&gt;m\cdot Q_P(h)(1-Q_P(h))&lt;/math&gt;. By [[Chebyshev's inequality]] we get

: &lt;math&gt;P^m \left\{|Q_P(h)-\widehat{Q_s(h)}| &gt; \frac \varepsilon 2\right\} \leq \frac{m\cdot Q_P(h)(1-Q_P(h))}{(\varepsilon m/2)^2} \leq \frac 1 {\varepsilon^2 m} \leq\frac 1 2 &lt;/math&gt;

for the mentioned bound on &lt;math&gt;m&lt;/math&gt;. Here we use the fact that &lt;math&gt;x(1-x)\leq 1/4&lt;/math&gt; for &lt;math&gt;x&lt;/math&gt;.

=== Permutations ===

Let &lt;math&gt;\Gamma_{m}&lt;/math&gt; be the set of all permutations of &lt;math&gt;\{1,2,3,\dots,2m\}&lt;/math&gt; that swaps &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;m+i&lt;/math&gt; &lt;math&gt;\forall i&lt;/math&gt; in some subset of &lt;math&gt;\{1,2,3,\ldots,2m\}&lt;/math&gt;.

'''Lemma:''' Let &lt;math&gt;R&lt;/math&gt; be any subset of &lt;math&gt;X^{2m}&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; any probability distribution on &lt;math&gt;X&lt;/math&gt;. Then,

: &lt;math&gt;P^{2m}(R)=E[\Pr[\sigma(x)\in R]]\leq \max_{x\in X^{2m}}(\Pr[\sigma(x)\in R]),&lt;/math&gt;

where the expectation is over &lt;math&gt;x&lt;/math&gt; chosen according to &lt;math&gt;P^{2m}&lt;/math&gt;, and the probability is over &lt;math&gt;\sigma&lt;/math&gt; chosen uniformly from &lt;math&gt;\Gamma_{m}&lt;/math&gt;.

Proof:
For any &lt;math&gt;\sigma\in\Gamma_m,&lt;/math&gt;

: &lt;math&gt; P^{2m}(R) = P^{2m}\{x:\sigma(x)\in R\} &lt;/math&gt;

(since coordinate permutations preserve the product distribution &lt;math&gt; P^{2m}&lt;/math&gt;.)

: &lt;math&gt;
\begin{align}
\therefore P^{2m}(R) = {} &amp; \int_{X^{2m}}1_{R}(x) \, dP^{2m}(x) \\[5pt]
= {} &amp; \frac{1}{|\Gamma_{m}|}\sum_{\sigma\in\Gamma_m} \int_{X^{2m}} 1_R(\sigma(x)) \, dP^{2m}(x) \\[5pt]
= {} &amp; \int_{X^{2m}} \frac 1 {|\Gamma_m|}\sum_{\sigma\in\Gamma_m} 1_R (\sigma(x)) \, dP^{2m}(x) \\[5pt]
&amp; \text{(because } |\Gamma_{m}| \text{ is finite)} \\[5pt]
= {} &amp; \int_{X^{2m}} \Pr[\sigma(x)\in R] \, dP^{2m}(x) \quad \text{(the expectation)} \\[5pt]
\leq {} &amp; \max_{x\in X^{2m}}(\Pr[\sigma(x)\in R]).
\end{align}
&lt;/math&gt;

The maximum is guaranteed to exist since there is only a finite set of values that probability under a random permutation can take.

=== Reduction to a finite class ===

'''Lemma:''' Basing on the previous lemma,
: &lt;math&gt;\max_{x\in X^{2m}}(\Pr[\sigma(x)\in R])\leq 4\Pi_H(2m)e^{-\varepsilon^2 m/8} &lt;/math&gt;.

Proof:
Let us define &lt;math&gt;x=(x_1,x_2,\ldots,x_{2m})&lt;/math&gt; and &lt;math&gt;t=|H|_x|&lt;/math&gt; which is at most &lt;math&gt;\Pi_H(2m)&lt;/math&gt;. This means there are functions &lt;math&gt;h_1,h_2,\ldots,h_t\in H&lt;/math&gt; such that for any &lt;math&gt;h\in H,\exists i&lt;/math&gt; between &lt;math&gt;1&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; with &lt;math&gt;h_i(x_k)=h(x_k)&lt;/math&gt; for &lt;math&gt;1\leq k\leq 2m. &lt;/math&gt;

We see that &lt;math&gt;\sigma(x)\in R&lt;/math&gt; iff for some &lt;math&gt;h&lt;/math&gt; in &lt;math&gt;H&lt;/math&gt; satisfies,
&lt;math&gt;|\frac{1}{m}|\{1\leq i\leq m:h(x_{\sigma_{i}})=1\}|-\frac{1}{m}|\{m+1\leq i\leq 2m:h(x_{\sigma_{i}})=1\}||\geq\frac{\varepsilon}{2}&lt;/math&gt;.
Hence if we define &lt;math&gt;w^{j}_{i}=1&lt;/math&gt; if &lt;math&gt;h_{j}(x_{i})=1&lt;/math&gt; and &lt;math&gt;w^{j}_{i}=0&lt;/math&gt; otherwise.

For &lt;math&gt;1\leq i\leq m&lt;/math&gt; and &lt;math&gt;1\leq j\leq t&lt;/math&gt;, we have that &lt;math&gt;\sigma(x)\in R&lt;/math&gt; iff for some &lt;math&gt;j&lt;/math&gt; in &lt;math&gt;{1,\ldots,t}&lt;/math&gt; satisfies &lt;math&gt;|\frac 1 m \left(\sum_i w^j_{\sigma(i)}-\sum_i w^j_{\sigma(m+i)}\right)|\geq\frac \varepsilon 2 &lt;/math&gt;. By union bound we get

: &lt;math&gt;\Pr[\sigma(x)\in R]\leq t\cdot \max\left(\Pr[|\frac 1 m \left(\sum_i w^j_{\sigma_i} - \sum_i w^j_{\sigma_{m+i}}\right)| \geq \frac \varepsilon 2]\right)&lt;/math&gt;

:&lt;math&gt;\leq \Pi_{H}(2m)\cdot \max\left(\Pr\left[ \left| \frac 1 m \left(\sum_i w^j_{\sigma_i}-\sum_i w^j_{\sigma_{m+i}}\right)\right| \geq \frac \varepsilon 2 \right] \right).&lt;/math&gt;

Since, the distribution over the permutations &lt;math&gt;\sigma&lt;/math&gt; is uniform for each &lt;math&gt;i&lt;/math&gt;, so &lt;math&gt;w^j_{\sigma_i}-w^j_{\sigma_{m+i}}&lt;/math&gt; equals &lt;math&gt;\pm |w^j_i-w^j_{m+i}|&lt;/math&gt;, with equal probability.

Thus,

: &lt;math&gt;\Pr\left[\left|\frac 1 m \left(\sum_i \left(w^j_{\sigma_i}-w^j_{\sigma_{m+i}}\right)\right)\right|\geq\frac \varepsilon 2\right] = \Pr\left[ \left| \frac 1 m \left( \sum_i|w^j_i-w^j_{m+i}|\beta_i\right)\right|\geq\frac \varepsilon 2\right],&lt;/math&gt;

where the probability on the right is over &lt;math&gt;\beta_{i}&lt;/math&gt; and both the possibilities are equally likely. By [[Hoeffding's inequality]], this is at most &lt;math&gt;2e^{-m\varepsilon^2/8}&lt;/math&gt;.

Finally, combining all the three parts of the proof we get the '''Uniform Convergence Theorem'''.

==References==
{{Reflist}}




</text>
      <sha1>a9mmgdmacr8njmulfnh8z1bl6pm1wql</sha1>
    </revision>
  </page>
  <page>
    <title>CBCL (MIT)</title>
    <ns>0</ns>
    <id>17114678</id>
    <revision>
      <id>793539761</id>
      <parentid>793317998</parentid>
      <timestamp>2017-08-02T11:22:08Z</timestamp>
      <contributor>
        <username>RHaworth</username>
        <id>161142</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/Blueclaw|Blueclaw]] ([[User talk:Blueclaw|talk]]): Well-established article - needs AfD if you insist. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2674">The '''Center for Biological &amp; Computational Learning''' is a research lab at the [[Massachusetts Institute of Technology]].

CBCL was established in 1992 with support from the [[National Science Foundation]]. It is based in the Department of Brain &amp; Cognitive Sciences at MIT, and is associated with the [[McGovern Institute for Brain Research]], and the [[MIT Computer Science and Artificial Intelligence Laboratory]].

It was founded with the belief that learning is at the very core of the problem of intelligence, both biological and artificial. Learning is thus the gateway to understanding how the human brain works and for making intelligent machines. CBCL studies the problem of learning within a multidisciplinary approach. Its main goal is to nurture serious research on the mathematics, the engineering and the neuroscience of learning.

Research is focused on the problem of learning in theory, engineering applications, and [[neuroscience]].

In [[computational neuroscience]], the center has developed a model of the [[ventral stream]] in the [[visual cortex]] which accounts for much of the physiological data, and [[Psychophysiology|psychophysical]] experiments in difficult object recognition tasks. The model performs at the level of the best computer vision systems{{Citation needed|date=April 2016}}.

==See also==
[[Tomaso Poggio]] director of CBCL

==External links==
*[http://cbcl.mit.edu/ The Center for Biological and Computational Learning (CBCL)]
*BBC:  [http://video.google.com/videoplay?docid=1900129797336249123&amp;q=serre+oliva+poggio&amp;pr=goog-sl Visions of the Future]  -  February 29, 2008 - This is part of the excellent BBC series entitled &quot;visions of the future&quot;. This short clip (3min) here shows work performed at CBCL (MIT) about a computational neuroscience model of the ventral stream of the visual cortex. The story here focuses on recent work by Serre, Oliva and Poggio on comparing the performance of the model to human observers during a rapid object categorization task.
*THE DISCOVERY CHANNEL  [Toronto, Canada] by Jennifer Scott  (June 17, 2002): Video:[http://cbcl.mit.edu/news/files/discovery-video.wmv Science, Lies &amp; Videotape]  -  Tony Ezzat and Tomaso Poggio.
*NBC TODAY SHOW with Katie Couric (May 20, 2002): Video:* [http://cbcl.mit.edu/news/files/100tdy_couric_mitvideo_020520.asf (100 kbit/s)] [http://cbcl.mit.edu/news/files/300tdy_couric_mitvideo_020520.asf (300 kbit/s)] - Tony Ezzat and Tomaso Poggio.






</text>
      <sha1>opjt4delxowh5e4qx5hbancml7redta</sha1>
    </revision>
  </page>
  <page>
    <title>Matthews correlation coefficient</title>
    <ns>0</ns>
    <id>12306500</id>
    <revision>
      <id>815433719</id>
      <parentid>806773109</parentid>
      <timestamp>2017-12-14T20:19:28Z</timestamp>
      <contributor>
        <username>Larry.europe</username>
        <id>11040783</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8001">The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]], introduced by biochemist [[Brian Matthews (biochemist)|Brian W. Matthews]] in 1975.&lt;ref name=&quot;Matthews1975&quot;&gt;{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442–451|doi=10.1016/0005-2795(75)90109-9}}&lt;/ref&gt; It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes{{Citation needed|date=October 2017}}. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &amp;minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &amp;minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 2×2 [[contingency table]]

: &lt;math&gt;|\text{MCC}| = \sqrt{\frac{\chi^2}{n}}&lt;/math&gt;

where ''n'' is the total number of observations.

While there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures.&lt;ref name=&quot;Powers2011&quot;/&gt; Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.

The MCC can be calculated directly from the [[confusion matrix]] using the formula:

: &lt;math&gt;
\text{MCC} = \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;

In this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.

The original formula as given by Matthews was:&lt;ref name=Matthews1975 /&gt;
: &lt;math&gt;
\text{N} = TN + TP + FN + FP
&lt;/math&gt;
: &lt;math&gt;
\text{S} = \frac{ TP + FN } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{P} = \frac{ TP + FP } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{MCC} = \frac{ TP / N - S \times P } {\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }
&lt;/math&gt;

This is equal to the formula given above. As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[Markedness]] (Δp) and [[Youden's J statistic]] ([[Informedness]] or Δp').&lt;ref name=&quot;Powers2011&quot;&gt;{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}&lt;/ref&gt;&lt;ref name=&quot;Perruchet2004&quot;&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt; [[Markedness]] and [[Informedness]] correspond to different directions of information flow and generalize [[Youden's J statistic]], the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.&lt;ref name=&quot;Powers2011&quot;/&gt;

Some scientists claim the Matthews correlation coefficient to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context.&lt;ref&gt;{{cite journal
| vauthors = Chicco D
| title = Ten quick tips for machine learning in computational biology
| journal = BioData Mining
| volume = 10
| issue =  35
| pages = 1-17
| date = December 2017
| pmid = 29234465
| doi = 10.1186/s13040-017-0155-3
| pmc= 5721660}}&lt;/ref&gt;

== Confusion matrix ==
{{main article|Confusion matrix}}

{{Confusion matrix terms|recall=}}

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

== Multiclass case ==
The Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the  &lt;math&gt;R_K&lt;/math&gt; statistic (for K different classes) by the author, and defined in terms of a &lt;math&gt;K\times K&lt;/math&gt; confusion matrix &lt;math&gt;C&lt;/math&gt;
&lt;ref name=&quot;gorodkin2004comparing&quot;&gt;{{cite journal|last=Gorodkin|first=Jan|title=Comparing two K-category assignments by a K-category correlation coefficient|journal=Computational biology and chemistry|date=2004|volume=28|number=5|pages=367–374|publisher=Elsevier}}&lt;/ref&gt;
.&lt;ref name=&quot;GorodkinRk2006&quot;&gt;{{cite web|last1=Gorodkin|first1=Jan|title=The Rk Page|url=http://rk.kvl.dk/introduction/index.html|website=The Rk Page|accessdate=28 December 2016}}&lt;/ref&gt;

:&lt;math&gt;
\text{MCC} = \frac{\sum_{k}\sum_{l}\sum_{m} C_{kk}C_{lm} - C_{kl}C_{mk}}{
\sqrt{
\sum_{k}(\sum_l C_{kl} )(\sum_{k' | k' \neq k}\sum_{l'} C_{k'l'})
}
\sqrt{
\sum_{k}(\sum_l C_{lk} )(\sum_{k' | k' \neq k}\sum_{l'} C_{l'k'})
}
}
&lt;/math&gt;

When there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1.

&lt;!--
TODO: potentially un-comment later, for now just stick with referenced version

This formula can be more easily understood by defining intermediate variables:
* &lt;math&gt;t_k=\sum_i C_{ik}&lt;/math&gt; the number of times class k truly occurred,
* &lt;math&gt;p_k=\sum_i C_{ki}&lt;/math&gt; the number of times class k was predicted,
* &lt;math&gt;c=\sum_{k} C_{kk}&lt;/math&gt; the total number of samples correctly predicted,
* &lt;math&gt;s=\sum_i \sum_j C_{ij}&lt;/math&gt; the total number of samples. This allows the formula to be expressed as:

:&lt;math&gt;
\text{MCC} = \frac{cs - \vec{t} \cdot \vec{p}}{
\sqrt{s^2 - \vec{p} \cdot \vec{p}}
\sqrt{s^2 - \vec{t} \cdot \vec{t}}
}
&lt;/math&gt;
--&gt;

== See also ==
* [[Cohen's kappa]]
* [[Cramér's V (statistics)|Cramér's V]], a similar measure of association between nominal variables.
* [[F1 score]]
* [[Phi coefficient]]

== References ==

{{Reflist}}

&lt;!--should reference in the main text  === General References ===
* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview&quot; ''Bioinformatics'' 2000, 16, 412&amp;ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]
* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]
--&gt;

{{DEFAULTSORT:Matthews Correlation Coefficient}}







[[Category:Summary statistics for contingency tables]]</text>
      <sha1>1sk22rlv9my8dg9u8yp9gcllboswtx7</sha1>
    </revision>
  </page>
  <page>
    <title>Learning with errors</title>
    <ns>0</ns>
    <id>23864530</id>
    <revision>
      <id>807015073</id>
      <parentid>804065529</parentid>
      <timestamp>2017-10-25T11:46:25Z</timestamp>
      <contributor>
        <username>Cdcdb</username>
        <id>31006392</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16866">'''Learning with errors (LWE)''' is a problem in [[machine learning]] that is conjectured to be hard to solve. Introduced&lt;ref name=&quot;regev05&quot; /&gt; by Oded Regev in 2005, it is a generalization of the [[parity learning]] problem. Regev showed, furthermore, that the LWE problem is as hard to solve as several worst-case [[lattice problems]]. The LWE problem has recently&lt;ref name=&quot;regev05&quot;&gt;Oded Regev, “On lattices, learning with errors, random linear codes, and cryptography,” in Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (Baltimore, MD, USA: ACM, 2005), 84-93, http://portal.acm.org/citation.cfm?id=1060590.1060603.&lt;/ref&gt;&lt;ref name=&quot;peikert09&quot;&gt;Chris Peikert, “Public-key cryptosystems from the worst-case shortest vector problem: extended abstract,” in Proceedings of the 41st annual ACM symposium on Theory of computing (Bethesda, MD, USA: ACM, 2009), 333-342, http://portal.acm.org/citation.cfm?id=1536414.1536461.&lt;/ref&gt; been used as a [[Computational hardness assumption|hardness assumption]] to create [[Public-key cryptography|public-key cryptosystems]], such as the [[ring learning with errors key exchange]] by Peikert.&lt;ref&gt;{{Cite book|title = Lattice Cryptography for the Internet|url = https://link.springer.com/chapter/10.1007/978-3-319-11659-4_12|publisher = Springer International Publishing|date = 2014-10-01|isbn = 978-3-319-11658-7|pages = 197–219|series = Lecture Notes in Computer Science|first = Chris|last = Peikert|editor-first = Michele|editor-last = Mosca}}&lt;/ref&gt;

An algorithm is said to solve the LWE problem if, when given access to samples &lt;math&gt;(x,y)&lt;/math&gt; where &lt;math&gt;x\in \mathbb{Z}_q^n&lt;/math&gt; (a [[Vector (mathematics and physics)|vector]] of &lt;math&gt;n&lt;/math&gt; integers [[Modular arithmetic|modulo]] &lt;math&gt;q&lt;/math&gt;) and &lt;math&gt;y \in \mathbb{Z}_q&lt;/math&gt;, with the assurance, for some fixed [[linear function]] &lt;math&gt;f:\mathbb{Z}_q^n \rightarrow \mathbb{Z}_q,&lt;/math&gt; that &lt;math&gt;y=f(x)&lt;/math&gt; with high probability and deviates from it according to some known noise model, the algorithm can recreate &lt;math&gt;f&lt;/math&gt; or some close approximation of it with high probability.

== Definition ==
Denote by &lt;math&gt;\mathbb{T}=\mathbb{R}/\mathbb{Z}&lt;/math&gt; the additive group on reals modulo one. Denote by &lt;math&gt;A_{\mathbf{s},\phi}&lt;/math&gt; the distribution on &lt;math&gt;\mathbb{Z}_q^n \times \mathbb{T}&lt;/math&gt; obtained by choosing a vector &lt;math&gt;\mathbf{a}\in \mathbb{Z}_q^n&lt;/math&gt; uniformly at random, choosing &lt;math&gt;e&lt;/math&gt; according to a probability distribution &lt;math&gt;\phi&lt;/math&gt;  on &lt;math&gt;\mathbb{T}&lt;/math&gt; and outputting &lt;math&gt;(\mathbf{a},\langle \mathbf{a},\mathbf{s} \rangle /q + e)&lt;/math&gt; for some fixed vector &lt;math&gt;\mathbf{s} \in \mathbb{Z}_q^n&lt;/math&gt;. Here &lt;math&gt;\textstyle \langle \mathbf{a},\mathbf{s} \rangle = \sum_{i=1}^n a_i s_i&lt;/math&gt; is the standard inner product &lt;math&gt; \mathbb{Z}_q^n \times \mathbb{Z}_q^n \longrightarrow \mathbb{Z}_q &lt;/math&gt;, the division is done in the [[field of reals]] (or more formally, this &quot;division by &lt;math&gt;q&lt;/math&gt;&quot; is notation for the group homomorphism &lt;math&gt; \mathbb{Z}_q \longrightarrow \mathbb{T}&lt;/math&gt; mapping &lt;math&gt; 1 \in \mathbb{Z}_q &lt;/math&gt; to &lt;math&gt; 1/q + \mathbb{Z} \in \mathbb{T}&lt;/math&gt;), and the final addition is in &lt;math&gt;\mathbb{T}&lt;/math&gt;.

The '''learning with errors problem''' &lt;math&gt;\mathrm{LWE}_{q,\phi}&lt;/math&gt; is to find &lt;math&gt;\mathbf{s} \in \mathbb{Z}_q^n&lt;/math&gt;, given access to polynomially many samples of choice from &lt;math&gt;A_{\mathbf{s},\phi}&lt;/math&gt;.

For every &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, denote by &lt;math&gt;D_\alpha&lt;/math&gt; the one-dimensional [[Normal distribution|Gaussian]] with density function &lt;math&gt;D_\alpha(x)=\rho_\alpha(x)/\alpha&lt;/math&gt; where &lt;math&gt;\rho_\alpha(x)=e^{-\pi(|x|/\alpha)^2}&lt;/math&gt;, and let &lt;math&gt;\Psi_\alpha&lt;/math&gt; be the distribution on &lt;math&gt;\mathbb{T}&lt;/math&gt; obtained by considering &lt;math&gt;D_\alpha&lt;/math&gt; modulo one.  The version of LWE considered in most of the results would be &lt;math&gt;\mathrm{LWE}_{q,\Psi_\alpha}&lt;/math&gt;

== Decision version ==

The '''LWE''' problem described above is the ''search'' version of the problem. In the ''decision'' version ('''DLWE'''), the goal is to distinguish between noisy inner products and uniformly random samples from &lt;math&gt;\mathbb{Z}_q^n \times \mathbb{T}&lt;/math&gt; (practically, some discretized version of it). Regev&lt;ref name=&quot;regev05&quot; /&gt; showed that the ''decision'' and ''search'' versions are equivalent when &lt;math&gt;q&lt;/math&gt; is a prime bounded by some polynomial in &lt;math&gt;n&lt;/math&gt;.

=== Solving decision assuming search ===
Intuitively, if we have a procedure for the search problem, the decision version can be solved easily: just feed the input samples for the decision problem to the solver for the search problem. Denote the given samples by &lt;math&gt;\{(\mathbf{a_i},\mathbf{b_i})\} \subset \mathbb{Z}^n_q \times \mathbb{T}&lt;/math&gt;. If the solver returns a candidate &lt;math&gt;\mathbf{s}&lt;/math&gt;, for all &lt;math&gt;i&lt;/math&gt;, calculate &lt;math&gt;\{\langle \mathbf{a_i}, \mathbf{s} \rangle - \mathbf{b_i} \} &lt;/math&gt;.  If the samples are from an LWE distribution, then the results of this calculation will be distributed according &lt;math&gt;\chi&lt;/math&gt;, but if the samples are uniformly random, these quantities will be distributed uniformly as well.

=== Solving search assuming decision ===
For the other direction, given a solver for the decision problem, the search version can be solved as follows: Recover &lt;math&gt;\mathbf{s}&lt;/math&gt; one coordinate at a time. To obtain the first coordinate, &lt;math&gt;\mathbf{s}_1&lt;/math&gt;, make a guess &lt;math&gt;k \in Z_q&lt;/math&gt;, and do the following. Choose a number &lt;math&gt;r \in \mathbb{Z}_q&lt;/math&gt; uniformly at random. Transform the given samples &lt;math&gt;\{(\mathbf{a_i},\mathbf{b_i})\} \subset \mathbb{Z}^n_q \times \mathbb{T}&lt;/math&gt; as follows. Calculate &lt;math&gt;\{(\mathbf{a_i}+(r,0,\ldots,0),\mathbf{b_i}+(r k)/q)\}&lt;/math&gt;.  Send the transformed samples to the decision solver.

If the guess &lt;math&gt;k&lt;/math&gt; was correct, the transformation takes the distribution &lt;math&gt;A_{\mathbf{s},\chi}&lt;/math&gt; to itself, and otherwise, since &lt;math&gt;q&lt;/math&gt; is prime, it takes it to the uniform distribution. So, given a polynomial-time solver for the decision problem that errs with very small probability, since &lt;math&gt;q&lt;/math&gt; is bounded by some polynomial in &lt;math&gt;n&lt;/math&gt;, it only takes polynomial time to guess every possible value for &lt;math&gt;k&lt;/math&gt; and use the solver to see which one is correct.

After obtaining &lt;math&gt;\mathbf{s}_1&lt;/math&gt;, we follow an analogous procedure for each other coordinate &lt;math&gt;\mathbf{s}_j&lt;/math&gt;.  Namely, we transform our &lt;math&gt;\mathbf{b_i}&lt;/math&gt; samples the same way, and transform our &lt;math&gt;\mathbf{a_i}&lt;/math&gt; samples by calculating &lt;math&gt;\mathbf{a_i} + (0, \ldots, r, \ldots, 0)&lt;/math&gt;, where the &lt;math&gt;r&lt;/math&gt; is in the &lt;math&gt;j^{th}&lt;/math&gt; coordinate.&lt;ref name=&quot;regev05&quot; /&gt;

Peikert&lt;ref name=&quot;peikert09&quot; /&gt; showed that this reduction, with a small modification, works for any &lt;math&gt;q&lt;/math&gt; that is a product of distinct, small (polynomial in &lt;math&gt;n&lt;/math&gt;) primes.  The main idea is if &lt;math&gt;q = q_1 q_2 \cdots q_t&lt;/math&gt;, for each &lt;math&gt;q_{\ell}&lt;/math&gt;, guess and check to see if &lt;math&gt;\mathbf{s}_j&lt;/math&gt; is congruent to &lt;math&gt;0 \mod q_{\ell}&lt;/math&gt;, and then use the [[Chinese remainder theorem]] to recover &lt;math&gt;\mathbf{s}_j&lt;/math&gt;.

=== Average case hardness ===
Regev&lt;ref name=&quot;regev05&quot; /&gt; showed the [[Random self-reducibility]] of the '''LWE''' and '''DLWE''' problems for arbitrary &lt;math&gt;q&lt;/math&gt; and &lt;math&gt;\chi&lt;/math&gt;.  Given samples &lt;math&gt;\{(\mathbf{a_i},\mathbf{b_i})\}&lt;/math&gt; from &lt;math&gt;A_{\mathbf{s},\chi}&lt;/math&gt;, it is easy to see that &lt;math&gt;\{(\mathbf{a_i},\mathbf{b_i} + \langle \mathbf{a_i}, \mathbf{t} \rangle)/q\}&lt;/math&gt; are samples from &lt;math&gt;A_{\mathbf{s} + \mathbf{t},\chi}&lt;/math&gt;.

So, suppose there was some set &lt;math&gt;\mathcal{S} \subset \mathbb{Z}_q^n&lt;/math&gt; such that &lt;math&gt;|\mathcal{S}|/|\mathbb{Z}_q^n| = 1/poly(n)&lt;/math&gt;, and for distributions &lt;math&gt;A_{\mathbf{s'},\chi}&lt;/math&gt;, with &lt;math&gt;\mathbf{s'} \leftarrow \mathcal{S}&lt;/math&gt;, '''DLWE''' was easy.

Then there would be some distinguisher &lt;math&gt;\mathcal{A}&lt;/math&gt;, who, given samples &lt;math&gt;\{(\mathbf{a_i},\mathbf{b_i}) \}&lt;/math&gt;, could tell whether they were uniformly random or from &lt;math&gt;A_{\mathbf{s'},\chi}&lt;/math&gt;.  If we need to distinguish uniformly random samples from &lt;math&gt;A_{\mathbf{s},\chi}&lt;/math&gt;, where &lt;math&gt;\mathbf{s}&lt;/math&gt; is chosen uniformly at random from &lt;math&gt;\mathbb{Z}_q^n&lt;/math&gt;, we could simply try different values &lt;math&gt;\mathbf{t} &lt;/math&gt; sampled uniformly at random from &lt;math&gt;\mathbb{Z}_q^n&lt;/math&gt;, calculate &lt;math&gt;\{(\mathbf{a_i},\mathbf{b_i} + \langle \mathbf{a_i}, \mathbf{t} \rangle)/q\}&lt;/math&gt; and feed these samples to &lt;math&gt;\mathcal{A}&lt;/math&gt;.  Since &lt;math&gt;\mathcal{S}&lt;/math&gt; comprises a large fraction of &lt;math&gt;\mathbb{Z}_q^n&lt;/math&gt;, with high probability, if we choose a polynomial number of values for &lt;math&gt;\mathbf{t}&lt;/math&gt;, we will find one such that &lt;math&gt;\mathbf{s} + \mathbf{t} \in \mathcal{S}&lt;/math&gt;, and &lt;math&gt;\mathcal{A}&lt;/math&gt; will successfully distinguish the samples.

Thus, no such &lt;math&gt;\mathcal{S}&lt;/math&gt; can exist, meaning '''LWE''' and '''DLWE''' are (up to a polynomial factor) as hard in the average case as they are in the worst case.

== Hardness results ==

=== Regev's result ===
For a n-dimensional lattice &lt;math&gt;L&lt;/math&gt;, let ''smoothing parameter'' &lt;math&gt;\eta_\epsilon(L)&lt;/math&gt; denote the smallest &lt;math&gt;s&lt;/math&gt; such that &lt;math&gt;\rho_{1/s}(L^*\setminus \{\mathbf{0}\}) \leq \epsilon &lt;/math&gt; where &lt;math&gt;L^*&lt;/math&gt; is the dual of &lt;math&gt;L&lt;/math&gt; and &lt;math&gt;\rho_\alpha(x)=e^{-\pi(|x|/\alpha)^2}&lt;/math&gt; is extended to sets by summing over function values at each element in the set. Let &lt;math&gt;D_{L,r}&lt;/math&gt; denote the discrete Gaussian distribution on &lt;math&gt;L&lt;/math&gt; of width &lt;math&gt;r&lt;/math&gt; for a lattice &lt;math&gt;L&lt;/math&gt; and real &lt;math&gt;r&gt;0&lt;/math&gt;. The probability of each &lt;math&gt;x \in L&lt;/math&gt; is proportional to &lt;math&gt;\rho_r(x)&lt;/math&gt;.

The ''discrete Gaussian sampling problem''(DGS) is defined as follows: An instance of &lt;math&gt;DGS_\phi&lt;/math&gt; is given by an &lt;math&gt;n&lt;/math&gt;-dimensional lattice &lt;math&gt;L&lt;/math&gt; and a number &lt;math&gt;r \geq \phi(L)&lt;/math&gt;. The goal is to output a sample from &lt;math&gt;D_{L,r}&lt;/math&gt;. Regev shows that there is a reduction from &lt;math&gt;GapSVP_{100\sqrt{n}\gamma(n)}&lt;/math&gt; to &lt;math&gt;DGS_{\sqrt{n}\gamma(n)/\lambda(L^*)}&lt;/math&gt; for any function &lt;math&gt;\gamma(n)&lt;/math&gt;.

Regev then shows that there exists an efficient quantum algorithm for &lt;math&gt;DGS_{\sqrt{2n}\eta_\epsilon(L)/\alpha}&lt;/math&gt; given access to an oracle for &lt;math&gt;LWE_{q,\Psi_\alpha}&lt;/math&gt; for integer &lt;math&gt;q&lt;/math&gt; and &lt;math&gt;\alpha \in (0,1)&lt;/math&gt; such that &lt;math&gt;\alpha q &gt; 2\sqrt{n}&lt;/math&gt;. This implies the hardness for &lt;math&gt;LWE&lt;/math&gt;. Although the proof of this assertion works for any &lt;math&gt;q&lt;/math&gt;, for creating a cryptosystem, the &lt;math&gt;q&lt;/math&gt; has to be polynomial in &lt;math&gt;n&lt;/math&gt;.

=== Peikert's result ===

Peikert proves&lt;ref name=&quot;peikert09&quot; /&gt; that there is a probabilistic polynomial time reduction from the [[Lattice problems#GapSVP|&lt;math&gt;GapSVP_{\zeta,\gamma}&lt;/math&gt;]] problem in the worst case to solving &lt;math&gt;LWE_{q,\Psi_\alpha}&lt;/math&gt; using &lt;math&gt;poly(n)&lt;/math&gt; samples for parameters &lt;math&gt;\alpha \in (0,1)&lt;/math&gt;, &lt;math&gt;\gamma(n)\geq n/(\alpha \sqrt{\log{n}})&lt;/math&gt;, &lt;math&gt;\zeta(n) \geq \gamma(n)&lt;/math&gt; and &lt;math&gt;q \geq (\zeta/\sqrt{n}) \omega \sqrt{\log{n}})&lt;/math&gt;.

== Use in Cryptography ==

The '''LWE''' problem serves as a versatile problem used in construction of several&lt;ref name=&quot;regev05&quot; /&gt;&lt;ref name=&quot;peikert09&quot; /&gt;&lt;ref&gt;Chris Peikert and Brent Waters, “Lossy trapdoor functions and their applications,” in Proceedings of the 40th annual ACM symposium on Theory of computing (Victoria, British Columbia, Canada: ACM, 2008), 187-196, http://portal.acm.org/citation.cfm?id=1374406.&lt;/ref&gt;&lt;ref&gt;Craig Gentry, Chris Peikert, and Vinod Vaikuntanathan, “Trapdoors for hard lattices and new cryptographic constructions,” in Proceedings of the 40th annual ACM symposium on Theory of computing (Victoria, British Columbia, Canada: ACM, 2008), 197-206, http://portal.acm.org/citation.cfm?id=1374407.&lt;/ref&gt; cryptosystems. In 2005, Regev&lt;ref name=&quot;regev05&quot; /&gt; showed that the decision version of LWE is hard assuming quantum hardness of the [[lattice problems]] &lt;math&gt;GapSVP_\gamma&lt;/math&gt; (for &lt;math&gt;\gamma&lt;/math&gt; as above) and &lt;math&gt;SIVP_{t}&lt;/math&gt; with t=Õ(n/&lt;math&gt;\alpha&lt;/math&gt;). In 2009, Peikert&lt;ref name=&quot;peikert09&quot; /&gt; proved a similar result assuming only the classical hardness of the related problem [[Lattice problems#GapSVP|&lt;math&gt;GapSVP_{\zeta,\gamma}&lt;/math&gt;]]. The disadvantage of Peikert's result is that it bases itself on a non-standard version of an easier (when compared to SIVP) problem GapSVP.

=== Public-key cryptosystem ===
Regev&lt;ref name=&quot;regev05&quot; /&gt; proposed a [[public-key cryptosystem]] based on the hardness of the '''LWE''' problem. The cryptosystem as well as the proof of security and correctness are completely classical. The system is characterized by &lt;math&gt;m,q&lt;/math&gt; and a probability distribution &lt;math&gt;\chi&lt;/math&gt; on &lt;math&gt;\mathbb{T}&lt;/math&gt;. The setting of the parameters used in proofs of correctness and security is
* &lt;math&gt;q \geq 2 &lt;/math&gt;, a prime number between &lt;math&gt;n^2&lt;/math&gt; and &lt;math&gt;2n^2&lt;/math&gt;.
* &lt;math&gt;m=(1+\epsilon)(n+1) \log{q}&lt;/math&gt; for an arbitrary constant &lt;math&gt;\epsilon&lt;/math&gt;
* &lt;math&gt;\chi=\Psi_{\alpha(n)}&lt;/math&gt; for &lt;math&gt;\alpha(n) \in o(1/\sqrt{n}\log{n})&lt;/math&gt;

The cryptosystem is then defined by:
* ''Private Key'': Private key is an &lt;math&gt;\mathbf{s}\in \mathbb{Z}^n_q&lt;/math&gt; chosen uniformly at random.
* ''Public Key'': Choose &lt;math&gt;m&lt;/math&gt; vectors &lt;math&gt;a_1,\ldots,a_m \in  \mathbb{Z}^n_q&lt;/math&gt; uniformly and independently. Choose error offsets &lt;math&gt;e_1,\ldots,e_m \in \mathbb{T}&lt;/math&gt; independently according to &lt;math&gt;\chi&lt;/math&gt;. The public key consists of &lt;math&gt;(a_i,b_i=\langle a_i,\mathbf{s} \rangle/q + e_i)^m_{i=1}&lt;/math&gt;
* ''Encryption'': The encryption of a bit &lt;math&gt;x \in \{0,1\}&lt;/math&gt; is done by choosing a random subset &lt;math&gt;S&lt;/math&gt; of &lt;math&gt;[m]&lt;/math&gt; and then defining &lt;math&gt;Enc(x)&lt;/math&gt; as &lt;math&gt;(\sum_{i \in S} a_i, x/2 + \sum_{i \in S} b_i)&lt;/math&gt;
* ''Decryption'': The decryption of &lt;math&gt;(a,b)&lt;/math&gt; is &lt;math&gt;0&lt;/math&gt; if &lt;math&gt;b-\langle a, \mathbf{s} \rangle/q&lt;/math&gt; is closer to &lt;math&gt;0&lt;/math&gt; than to &lt;math&gt;\frac{1}{2}&lt;/math&gt;, and &lt;math&gt;1&lt;/math&gt; otherwise.

The proof of correctness follows from choice of parameters and some probability analysis. The proof of security is by reduction to the decision version of '''LWE''': an algorithm for distinguishing between encryptions (with above parameters) of &lt;math&gt;0&lt;/math&gt; and &lt;math&gt;1&lt;/math&gt; can be used to distinguish between &lt;math&gt;A_{s,\chi}&lt;/math&gt; and the uniform distribution over &lt;math&gt;\mathbb{Z}^n_q \times \mathbb{T}&lt;/math&gt;

=== CCA-secure cryptosystem ===
{{Expand section|date=December 2009}}
Peikert&lt;ref name=&quot;peikert09&quot; /&gt; proposed a system that is secure even against any [[chosen-ciphertext attack]].

=== Key exchange ===
The idea of using LWE and Ring LWE for key exchange was proposed and filed at the University of Cincinnati in 2011 by Jintai Ding. The idea comes from the associativity of matrix multiplications, and the errors are used to provide the security. The paper&lt;ref&gt;{{Cite journal|last=Lin|first=Jintai Ding, Xiang Xie, Xiaodong|date=2012-01-01|title=A Simple Provably Secure Key Exchange Scheme Based on the Learning with Errors Problem|url=https://eprint.iacr.org/2012/688}}&lt;/ref&gt; appeared in 2012 after a provisional patent application was filed in 2012.

The security of the protocol is proven based on the hardness of solving LWE problem. In 2014, Peikert presented a key transport scheme&lt;ref&gt;{{Cite journal|last=Peikert|first=Chris|date=2014-01-01|title=Lattice Cryptography for the Internet|url=https://eprint.iacr.org/2014/070}}&lt;/ref&gt; following the same basic idea of Ding's, where the new idea of sending additional 1 bit signal for rounding in Ding's construction is also utilized. The &quot;new hope&quot; implementation&lt;ref&gt;{{Cite journal|last=Alkim|first=Erdem|last2=Ducas|first2=Léo|last3=Pöppelmann|first3=Thomas|last4=Schwabe|first4=Peter|date=2015-01-01|title=Post-quantum key exchange - a new hope|url=https://eprint.iacr.org/2015/1092}}&lt;/ref&gt; selected for Google's post quantum experiment,&lt;ref&gt;{{Cite news|url=https://security.googleblog.com/2016/07/experimenting-with-post-quantum.html|title=Experimenting with Post-Quantum Cryptography|newspaper=Google Online Security Blog|access-date=2017-02-08|language=en-US}}&lt;/ref&gt; uses Peikert's scheme with variation in the error distribution.

== See also ==
*[[Lattice-based cryptography]]
*[[Ring learning with errors key exchange|Ring Learning with Errors Key Exchange]]
*[[Short integer solution problem]]

==References==
&lt;references/&gt;



[[Category:Post-quantum cryptography]]</text>
      <sha1>520qvhq5haipach0h1w8ckoz7k1pf9d</sha1>
    </revision>
  </page>
  <page>
    <title>CIML community portal</title>
    <ns>0</ns>
    <id>22795783</id>
    <revision>
      <id>801403242</id>
      <parentid>723149667</parentid>
      <timestamp>2017-09-19T13:15:39Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>clean up spacing around punctuation, replaced: ,D → , D (2), ,e → , e using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3450">The [[computational intelligence]] and [[machine learning]] (CIML) community portal is an international multi-university initiative.  Its primary purpose is to help facilitate a [[virtual scientific community]] infrastructure for all those involved with, or interested in, computational intelligence and machine learning.  This includes CIML research-, education, and application-oriented resources residing at the portal and others that are linked from the CIML site.

== Overview ==
The CIML community portal was created to facilitate an online virtual scientific community wherein anyone interested in CIML can share research, obtain resources, or simply learn more. The effort is currently led by [[Jacek Zurada]] (principal investigator), with Rammohan Ragade and Janusz Wojtusiak, aided by a team of 25 volunteer researchers from 13 different countries.&lt;ref&gt;Jacek M. Zurada, Janusz Wojtusiak, Maciej A. Mazurowski, Devendra Mehta, Khalid Moidu, Steve Margolis, Toward Multidisciplinary Collaboration in the CIML Virtual Community, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 62–66&lt;/ref&gt;&lt;ref&gt;Jacek M. Zurada, Janusz Wojtusiak, Rommohan Ragade, James Gentle, Maciej A. Mazurowski, and Artur Abdullin, Building Virtual Community in Computational Intelligence and Machine Learning, ''IEEE Computational Intelligence Magazine'', February 2009, pp. 43–54&lt;/ref&gt;

The ultimate goal of the CIML community portal is to accommodate and cater to a broad range of users, including experts, students, the public, and outside researchers interested in using CIML methods and software tools.  Each community member and user will be guided through the portal resources and tools based on their respective CIML experience (e.g. expert, student, outside researcher) and goals (e.g. collaboration, education).  A preliminary version of the community's portal, with limited capabilities, is now operational and available for users.&lt;ref&gt;{{cite web|url=http://www.cimlcommunity.org/|title=Cimlcommunity.org|publisher=}}&lt;/ref&gt;  All electronic resources on the portal are peer-reviewed to ensure high quality and cite-ability for literature.

==Further reading==
*Jacek M. Zurada, Janusz Wojtusiak, Fahmida Chowdhury, James E. Gentle, Cedric J. Jeannot, and Maciej A. Mazurowski, Computational Intelligence Virtual Community: Framework and Implementation Issues, Proceedings of the IEEE World Congress on Computational Intelligence, Hong Kong, June 1–6, 2008.
* Jacek M. Zurada, Janusz Wojtusiak, Maciej A. Mazurowski, Devendra Mehta, Khalid Moidu, Steve Margolis, Toward Multidisciplinary Collaboration in the CIML Virtual Community, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp.&amp;nbsp;62–66
*Chris Boyle, Artur Abdullin, Rammohan Ragade, Maciej A. Mazurowski, Janusz Wojtusiak, Jacek M. Zurada, Workflow considerations in the emerging CI-ML virtual organization, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp.&amp;nbsp;67–70

==See also==
*[[Artificial Intelligence]]
*[[Computational Intelligence]]
*[[Machine Learning]]
*[[National Science Foundation]]

==References==
&lt;references /&gt;

== External links ==
* {{Official website|http://www.cimlcommunity.org/ }}


</text>
      <sha1>hmzj7ulj4um2yyr63pyy11ezrvhvhnz</sha1>
    </revision>
  </page>
  <page>
    <title>Learning to rank</title>
    <ns>0</ns>
    <id>25050663</id>
    <revision>
      <id>809633836</id>
      <parentid>808187873</parentid>
      <timestamp>2017-11-10T12:04:06Z</timestamp>
      <contributor>
        <username>Kiudee</username>
        <id>11577221</id>
      </contributor>
      <comment>/* List of methods */ Fix reference for LambdaMART</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28969">{{machine learning bar}}
'''Learning to rank'''&lt;ref name=&quot;liu&quot;&gt;{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|series=Foundations and Trends in Information Retrieval
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225–331
|journal=Foundations and Trends in Information Retrieval
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://wwwconference.org/www2009/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
&lt;/ref&gt; or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press {{ISBN|9780262018258}}.&lt;/ref&gt; [[Training data]] consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. &quot;relevant&quot; or &quot;not relevant&quot;) for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is &quot;similar&quot; to rankings in the training data in some sense.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], and [[online advertising]].

A possible architecture of a machine-learned search engine is shown in the figure to the right.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
&lt;!-- &quot;assessor&quot; is the more standard term, used e.g. by TREC conference --&gt;
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check the relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used — only the top few documents, retrieved by some existing ranking models are checked. &lt;!--
  TODO: write something about selection bias caused by pooling
--&gt; Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),&lt;ref name=&quot;Joachims2002&quot;&gt;{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}&lt;/ref&gt; ''query chains'',&lt;ref&gt;{{citation
 |author1=Joachims T. |author2=Radlinski F. | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
}}&lt;/ref&gt; or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.&lt;ref&gt;{{citation
 |author1=B. Cambazoglu |author2=H. Zaragoza |author3=O. Chapelle |author4=J. Chen |author5=C. Liao |author6=Z. Zheng |author7=J. Degenhardt. | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010.
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}&lt;/ref&gt; First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,&lt;ref&gt;{{citation
 |author1=Broder A. |author2=Carmel D. |author3=Herscovici M. |author4=Soffer A. |author5=Zien J. | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the twelfth international conference on Information and knowledge management
 | year=2003
 | pages=426–434
 | isbn=1-58113-723-0
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 }}&lt;/ref&gt; or [[Okapi BM25|BM25]]. This phase is called ''top-&lt;math&gt;k&lt;/math&gt; document retrieval'' and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.&lt;ref name=&quot;manning-q-eval&quot;&gt;{{citation
 |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]&lt;/ref&gt; In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;&lt;ref name=&quot;Duh09&quot;&gt;{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}&lt;/ref&gt;
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.&lt;ref name=&quot;Duh09&quot; /&gt;
* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.&lt;ref&gt;Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.&lt;/ref&gt;

== Feature vectors ==
For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such an approach is sometimes called ''bag of features'' and is analogous to the [[bag of words]] model and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features — those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.&lt;ref name=&quot;manning-q-eval&quot; /&gt;&lt;ref&gt;
{{cite conference
 | first=M. |last=Richardson |author2=Prakash, A. |author3=Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707–715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}&lt;/ref&gt;
* ''Query-dependent'' or ''dynamic'' features — those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:&lt;ref name=&quot;letor3&quot;&gt;[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]&lt;/ref&gt;
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
* [[Information retrieval#Mean average precision|Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where &quot;@''n''&quot; denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]]
* [[Spearman's rank correlation coefficient|Spearman's Rho]]

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.&lt;ref&gt;http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt&lt;/ref&gt; Other metrics such as MAP, MRR and precision, are defined only for binary judgments.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* [[Expected reciprocal rank]] (ERR);&lt;ref&gt;{{citation
|author1=Olivier Chapelle |author2=Donald Metzler |author3=Ya Zhang |author4=Pierre Grinspan |title=Expected Reciprocal Rank for Graded Relevance
|url=https://web.archive.org/web/20120224053008/http://research.yahoo.com/files/err.pdf
|journal=CIKM
|year=2009
|pages=
}}&lt;/ref&gt;
* [[Yandex]]'s pfound.&lt;ref&gt;{{citation
|author1=Gulin A. |author2=Karpovich P. |author3=Raskovalov D. |author4=Segalovich I. |title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163–168
}} (in Russian)&lt;/ref&gt;
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] has analyzed existing algorithms for learning to rank problems in his paper &quot;Learning to Rank for Information Retrieval&quot;.&lt;ref name=&quot;liu&quot; /&gt; He categorized them into three groups by their input representation and [[loss function]]: the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets &lt;ref name=&quot;Tax2015&quot;&gt;{{citation
 |author1=Tax, Niek |author2=Bockting, Sander |author3=Hiemstra, Djoerd | journal=Information Processing &amp; Management
 | title=A cross-benchmark comparison of 87 learning to rank methods
 | pages=757-772
 | year=2015
 | url=http://wwwhome.cs.utwente.nl/~hiemstra/papers/ipm2015.pdf
 | doi=10.1016/j.ipm.2015.07.002
}}&lt;/ref&gt;.

=== Pointwise approach ===
In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case, the learning-to-rank problem is approximated by a classification problem — learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize the average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class=&quot;wikitable sortable&quot;
! Year || Name || Type || Notes
|-
| 1989 || OPRF &lt;ref name=&quot;Fuhr1989&quot;&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183–204
 | year=1989
 | doi=10.1145/65943.65944
}}&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR &lt;ref name=&quot;Cooperetal1992&quot;&gt;{{citation
 |author1=Cooper, William S. |author2=Gey, Frederic C. |author3=Dabney, Daniel P. | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198–210
 | year=1992
 | doi=10.1145/133160.133199
}}&lt;/ref&gt;   || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pointwise || Staged logistic regression
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||  A more recent exposition is in,&lt;ref name=&quot;Joachims2002&quot; /&gt; which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.20.378 | title = Pranking }}&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;1&lt;/span&gt; pointwise || Ordinal regression.
|-
| 2003 &lt;!-- or 1998? --&gt; || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || pairwise/listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || &lt;span style=&quot;display:none&quot;&gt;1&lt;/span&gt; pointwise ||
|-
| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || RankGP&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.90.220 | title = RankGP }}&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
Regularized least-squares based ranking. The work is extended in
&lt;ref name=pahikkala2009efficient&gt;{{Citation|last=Pahikkala|first=Tapio |author2=Tsivtsivadze, Evgeni |author3=Airola, Antti |author4=Järvinen, Jouni |author5=Boberg, Jorma |title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129–165|doi=10.1007/s10994-008-5097-z|postscript=.}}&lt;/ref&gt; to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM&lt;sup&gt;map&lt;/sup&gt;] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || pairwise/listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.&lt;ref&gt;C. Burges. (2010). [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].&lt;/ref&gt;
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]&lt;ref&gt;Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]&lt;ref&gt;Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.&lt;/ref&gt;  || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]&lt;ref&gt;Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF &quot;SortNet: learning to rank by a neural-based sorting algorithm&quot;], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator.
|-
| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || A method combines Plackett-Luce Model and neural network to minimize the expected Bayes risk, related to NDCG, from the decision-making aspect.
|-
| 2010 || [https://people.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]&lt;ref&gt;Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise &amp; listwise ||
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pointwise &amp; pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|-
|2017 || [http://www.cs.nott.ac.uk/~psxoi/dls_sac2017.pdf ES-Rank] || listwise || Evolutionary Strategy Learning to Rank technique with 7 fitness evaluation metrics
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;&lt;ref name=&quot;Fuhr1992&quot;&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243–255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
}}&lt;/ref&gt; a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.&lt;ref name=&quot;Fuhr1989&quot; /&gt; Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 &lt;ref name=&quot;Cooperetal1992&quot; /&gt; and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.&lt;ref&gt;{{citation |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]&lt;/ref&gt;  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.&lt;ref&gt;Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]&lt;/ref&gt;&lt;ref&gt;{{US Patent|7197497}}&lt;/ref&gt;

[[Bing (search engine)|Bing]]'s search is said to be powered by [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] algorithm,&lt;ref&gt;[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]&lt;/ref&gt;{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced&lt;ref name=&quot;snezhinsk&quot;&gt;[http://webmaster.ya.ru/replies.xml?item_no=5707&amp;ncrnd=5118 Yandex corporate blog entry about new ranking model &quot;Snezhinsk&quot;] (in Russian)&lt;/ref&gt; that it had significantly increased its [[search quality]] due to deployment of a new proprietary [[MatrixNet]] algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.&lt;ref&gt;The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].&lt;/ref&gt; Recently they have also sponsored a machine-learned ranking competition &quot;Internet Mathematics 2009&quot;&lt;ref&gt;[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]&lt;/ref&gt; based on their own search engine's production data. Yahoo has announced a similar competition in 2010.&lt;ref&gt;[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]&lt;/ref&gt;

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.&lt;ref&gt;{{cite web
  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
  | archiveurl = https://www.webcitation.org/5sq8irWNM
  | archivedate = 2010-09-18
  | title = Are Machine-Learned Models Prone to Catastrophic Errors?
  | date = 2008-05-24
  | last = Rajaraman
  | first = Anand
  | authorlink = Anand Rajaraman}}&lt;/ref&gt; [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models &quot;learn what people say they like, not what people actually like&quot;.&lt;ref&gt;{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = https://www.webcitation.org/5sq7DX3Pj
  | archivedate = 2010-09-15
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}&lt;/ref&gt;

In January 2017 the technology was included in the [[Open-source software|open source]] search engine [[Apache Solr]]™,&lt;ref&gt;{{Cite news|url=https://www.techatbloomberg.com/blog/bloomberg-integrated-learning-rank-apache-solr/|title=How Bloomberg Integrated Learning-to-Rank into Apache Solr {{!}} Tech at Bloomberg|date=2017-01-23|work=Tech at Bloomberg|access-date=2017-02-28|language=en-US}}&lt;/ref&gt; thus making machine learned search rank widely accessible also for enterprise search.

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]
* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]
* [https://github.com/apache/lucene-solr/tree/master/solr/contrib/ltr Java implementation in the Apache Solr search engine]



</text>
      <sha1>9x1n0xhfjp4lnynea6a0vhvxfuwub5e</sha1>
    </revision>
  </page>
  <page>
    <title>Transduction (machine learning)</title>
    <ns>0</ns>
    <id>960361</id>
    <revision>
      <id>718723089</id>
      <parentid>718723076</parentid>
      <timestamp>2016-05-05T06:40:34Z</timestamp>
      <contributor>
        <username>The Evil IP address</username>
        <id>8337922</id>
      </contributor>
      <minor/>
      <comment>Section heading change: Example roblem → Example problem using a [[User:The Evil IP address/hdedit|script]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7890">{{No footnotes|date=April 2011}}

In [[logic]], [[statistical inference]], and [[supervised learning]],
'''transduction''' or '''transductive inference''' is [[reasoning]] from
observed, specific (training) cases to specific (test) cases. In contrast,
[[induction (philosophy)|induction]] is reasoning from observed training cases
to general rules, which are then applied to the test cases. The distinction is
most interesting in cases where the predictions of the transductive model are
not achievable by any inductive model. Note that this is caused by transductive
inference on different test sets producing mutually inconsistent predictions.

Transduction was introduced by [[Vladimir Vapnik]] in the 1990s, motivated by
his view that transduction is preferable to induction since, according to him, induction requires
solving a more general problem (inferring a function) before solving a more
specific problem (computing outputs for new cases): &quot;When solving a problem of
interest, do not solve a more general problem as an intermediate step. Try to
get the answer that you really need but not a more general one.&quot; A similar
observation had been made earlier by [[Bertrand Russell]]:
&quot;we shall reach the conclusion that Socrates is mortal with a greater approach to
certainty if we make our argument purely inductive than if we go by way of 'all men are mortal' and then use
deduction&quot; (Russell 1912, chap VII).

An example of learning which is not inductive would be in the case of binary
classification, where the inputs tend to cluster in two groups. A large set of
test inputs may help in finding the clusters, thus providing useful information
about the classification labels. The same predictions would not be obtainable
from a model which induces a function based only on the training cases.  Some
people may call this an example of the closely related [[semi-supervised learning]], since Vapnik's motivation is quite different. An example of an algorithm in this category is the Transductive [[Support Vector Machine]] (TSVM).

A third possible motivation which leads to transduction arises through the need
to approximate. If exact inference is computationally prohibitive, one may at
least try to make sure that the approximations are good at the test inputs. In
this case, the test inputs could come from an arbitrary distribution (not
necessarily related to the distribution of the training inputs), which wouldn't
be allowed in semi-supervised learning. An example of an algorithm falling in
this category is the [[Bayesian Committee Machine]] (BCM).

==Example problem==

The following example problem contrasts some of the unique properties of transduction against induction.

[[File:labels.png]]

A collection of points is given, such that some of the points are labeled (A, B, or C), but most of the points are unlabeled (?). The goal is to predict appropriate labels for all of the unlabeled points.

The inductive approach to solving this problem is to use the labeled points to train a [[supervised learning]] algorithm, and then have it predict labels for all of the unlabeled points. With this problem, however, the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model. It will certainly struggle to build a model that captures the structure of this data. For example, if a nearest-neighbor algorithm is used, then the points near the middle will be labeled &quot;A&quot; or &quot;C&quot;, even though it is apparent that they belong to the same cluster as the point labeled &quot;B&quot;.

Transduction has the advantage of being able to consider all of the points, not just the labeled points, while performing the labeling task. In this case, transductive algorithms would label the unlabeled points according to the clusters to which they naturally belong. The points in the middle, therefore, would most likely be labeled &quot;B&quot;, because they are packed very close to that cluster.

An advantage of transduction is that it may be able to make better predictions with fewer labeled points, because it uses the natural breaks found in the unlabeled points. One disadvantage of transduction is that it builds no predictive model. If a previously unknown point is added to the set, the entire transductive algorithm would need to be repeated with all of the points in order to predict a label. This can be computationally expensive if the data is made available incrementally in a stream. Further, this might cause the predictions of some of the old points to change (which may be good or bad, depending on the application). A supervised learning algorithm, on the other hand, can label new points instantly, with very little computational cost.

==Transduction algorithms==

Transduction algorithms can be broadly divided into two categories: those that seek to assign discrete labels to unlabeled points, and those that seek to regress continuous labels for unlabeled points. Algorithms that seek to predict discrete labels tend to be derived by adding partial supervision to a [[Cluster analysis|clustering]] algorithm. These can be further subdivided into two categories: those that cluster by partitioning, and those that cluster by agglomerating. Algorithms that seek to predict continuous labels tend to be derived by adding partial supervision to a [[manifold learning]] algorithm.

===Partitioning transduction===

Partitioning transduction can be thought of as top-down transduction. It is a semi-supervised extension of partition-based clustering. It is typically performed as follows:

 Consider the set of all points to be one large partition.
 While any partition P contains two points with conflicting labels:
   Partition P into smaller partitions.
 For each partition P:
   Assign the same label to all of the points in P.

Of course, any reasonable partitioning technique could be used with this algorithm. [[Max flow min cut]] partitioning schemes are very popular for this purpose.

===Agglomerative transduction===

Agglomerative transduction can be thought of as bottom-up transduction. It is a semi-supervised extension of agglomerative clustering. It is typically performed as follows:

 Compute the pair-wise distances, D, between all the points.
 Sort D in ascending order.
 Consider each point to be a cluster of size 1.
 For each pair of points {a,b} in D:
   If (a is unlabeled) or (b is unlabeled) or (a and b have the same label)
     Merge the two clusters that contain a and b.
     Label all points in the merged cluster with the same label.

===Manifold transduction===

Manifold-learning-based transduction is still a very young field of research.

==See also==

* [[Epilogism]]

==References==

* V. N. Vapnik. ''Statistical learning theory''. New York: Wiley, 1998. ''(See pages 339-371)''
* V. Tresp. ''A Bayesian committee machine'', Neural Computation, 12, 2000, [http://www.tresp.org/papers/bcm6.pdf pdf].
* B. Russell. ''The Problems of Philosophy'', Home University Library, 1912. [http://www.ditext.com/russell/rus7.html].

==External links==
* A Gammerman, V. Vovk, V. Vapnik (1998). &quot;[http://www1.cs.columbia.edu/~dplewis/candidacy/gammerman98learning.pdf Learning by Transduction].&quot; An early explanation of transductive learning.
* &quot;[http://www.kyb.mpg.de/ssl-book/discussion.pdf A Discussion of Semi-Supervised Learning and Transduction],&quot; Chapter 25 of ''Semi-Supervised Learning,'' Olivier Chapelle, Bernhard Schölkopf and Alexander Zien, eds. (2006). MIT Press. A discussion of the difference between SSL and transduction.
* [http://waffles.sourceforge.net Waffles] is an open source C++ library of machine learning algorithms, including transduction algorithms.
* [http://www.cs.cornell.edu/people/tj/svm_light/ SVMlight] is a general purpose SVM package that includes the transductive SVM option.

{{DEFAULTSORT:Transduction (Machine Learning)}}
</text>
      <sha1>ji9i7qje99ktyikjh8g0ih3t4o8nq7x</sha1>
    </revision>
  </page>
  <page>
    <title>Cross-validation (statistics)</title>
    <ns>0</ns>
    <id>416612</id>
    <revision>
      <id>813962022</id>
      <parentid>810455558</parentid>
      <timestamp>2017-12-06T04:22:17Z</timestamp>
      <contributor>
        <ip>2001:44B8:272:5700:6892:A619:74:FF22</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27215">{{Refimprove|date=August 2017}}

[[File:K-fold cross validation EN.jpg|thumb|Diagram of k-fold cross-validation with k=4.]]
'''Cross-validation''', sometimes called '''rotation estimation''',&lt;ref&gt;{{cite book |last=Geisser |first=Seymour |year=1993 |title=Predictive Inference |publisher=Chapman and Hall |location=New York, NY |isbn=0-412-03471-9 }}&lt;/ref&gt;&lt;ref name=&quot;Kohavi95&quot;&gt;{{cite journal |last=Kohavi |first=Ron |year=1995 |title=A study of cross-validation and bootstrap for accuracy estimation and model selection |journal=Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence |citeseerx = 10.1.1.48.529 |volume=2 |issue=12 |pages=1137–1143 | publisher = Morgan Kaufmann | location = San Mateo, CA }}&lt;/ref&gt;&lt;ref name=&quot;Devijver82&quot;&gt;{{cite book | last1 = Devijver | first1 = Pierre A. | last2 = Kittler | first2 = Josef | title = Pattern Recognition: A Statistical Approach | publisher = Prentice-Hall | location = London, GB | date = 1982 }}&lt;/ref&gt; is a [[model validation]] technique for assessing how the results of a [[statistics|statistical]] analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how [[accuracy|accurately]] a [[predictive modelling|predictive model]] will perform in practice.  In a prediction problem, a model is usually given a dataset of ''known data'' on which training is run (''training dataset''), and a dataset of ''unknown data'' (or ''first seen'' data) against which the model is tested (called the [[validation set|validation dataset]] or ''testing set'').&lt;ref name=&quot;Newbie question: Confused about train, validation and test data!&quot;&gt;{{cite web|url=http://www.heatonresearch.com/node/1823 |title=Newbie question: Confused about train, validation and test data! |accessdate=2013-11-14 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20150314221014/http://www.heatonresearch.com/node/1823 |archivedate=2015-03-14 |df= }}&lt;/ref&gt; The goal of cross validation is to define a dataset to &quot;test&quot; the model in the training phase (i.e., the ''validation set''), in order to limit problems like [[overfitting]] {{Citation needed|reason=Cross-validation does not prevent overfiting, this is not established in the literature |date=August 2017}}, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc.

One round of cross-validation involves [[partition of a set|partitioning]] a [[statistical sample|sample]] of [[data]] into [[Complement (set theory)|complementary]] subsets, performing the analysis on one subset (called the ''training set''), and validating the analysis on the other subset (called the ''validation set'' or ''testing set''). To reduce [[variance|variability]], multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to estimate a final predictive model.

One of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modelling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique.&lt;ref name=&quot;:0&quot;&gt;{{Cite book|title = Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions|last = Grossman,|first = Robert|publisher = Morgan &amp; Claypool|year = 2010|isbn = |location = |pages = |last2 = Seni|first2 = Giovanni|last3 = Elder|first3 = John|last4 = Agarwal|first4 = Nitin|last5 = Liu|first5 = Huan|doi = 10.2200/S00240ED1V01Y200912DMK002}}&lt;/ref&gt;

In summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance.&lt;ref name=&quot;:0&quot;/&gt;

==Purpose of cross-validation==

Suppose we have a [[statistical model|model]] with one or more unknown [[parameters]], and a data set to which the model can be fit (the training data set).  The fitting process [[optimization (mathematics)|optimizes]] the model parameters to make the model fit the training data as well as possible.  If we then take an [[independence (probability theory)|independent]] sample of validation data from the same [[statistical population|population]] as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data.  This is called [[overfitting]] {{Citation needed|reason=This definition is questionable. This is much more folklore than a published work. Citation is neeed. |date=August 2017}}, and is particularly likely to happen when the size of the training data set is small, or when the number of parameters in the model is large.  Cross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.

[[Linear regression]] provides a simple illustration of overfitting {{Citation needed|reason=This definition is questionable. This is much more folklore than a published work. Citation is neeed. |date=August 2017}}.  In linear regression we have [[real number|real]] ''response values'' ''y''&lt;sub&gt;1&lt;/sub&gt;, ..., ''y&lt;sub&gt;n&lt;/sub&gt;'', and ''n'' ''p''-dimensional [[Euclidean vector|vector]] ''covariates'' '''''x'''''&lt;sub&gt;1&lt;/sub&gt;, ..., '''''x&lt;sub&gt;''n''&lt;/sub&gt;'''''.  The components of the vectors '''''x'''''&lt;sub&gt;''i''&lt;/sub&gt; are denoted ''x''&lt;sub&gt;''i''1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''ip''&lt;/sub&gt;. If we use [[least squares]] to fit a function in the form of a [[hyperplane]] ''y'' = ''a'' + '''''β'''''&lt;sup&gt;T&lt;/sup&gt;'''''x''''' to the data ('''''x'''''&lt;sub&gt;''i''&lt;/sub&gt;, ''y''&lt;sub&gt;''i''&lt;/sub&gt;)&lt;sub&gt;&amp;nbsp;1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;≤&amp;nbsp;''n''&lt;/sub&gt;, we could then assess the fit using the [[mean squared error]] (MSE). The MSE for given estimated parameter values ''a'' and '''''β''''' on the training set ('''''x'''''&lt;sub&gt;''i''&lt;/sub&gt;, ''y''&lt;sub&gt;''i''&lt;/sub&gt;)&lt;sub&gt;&amp;nbsp;1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;≤&amp;nbsp;''n''&lt;/sub&gt; is

:&lt;math&gt;
\frac 1 n \sum_{i=1}^n (y_i - a - \boldsymbol\beta^T \mathbf{x}_i)^2 = \frac{1}{n}\sum_{i=1}^n (y_i - a - \beta_1x_{i1} - \dots - \beta_px_{ip})^2
&lt;/math&gt;

If the model is correctly specified, it can be shown under mild assumptions that the [[expected value]] of the MSE for the training set is (''n''&amp;nbsp;&amp;minus;&amp;nbsp;''p''&amp;nbsp;&amp;minus;&amp;nbsp;1)/(''n''&amp;nbsp;+&amp;nbsp;''p''&amp;nbsp;+&amp;nbsp;1)&amp;nbsp;&lt;&amp;nbsp;1 times the expected value of the MSE for the validation set&lt;ref&gt;{{Cite journal|last=Trippa|first=Lorenzo|last2=Waldron|first2=Levi|last3=Huttenhower|first3=Curtis|last4=Parmigiani|first4=Giovanni|date=March 2015|title=Bayesian nonparametric cross-study validation of prediction methods|url=http://projecteuclid.org/euclid.aoas/1430226098|journal=The Annals of Applied Statistics|language=EN|volume=9|issue=1|pages=402–428|doi=10.1214/14-AOAS798|issn=1932-6157}}&lt;/ref&gt; (the expected value is taken over the distribution of training sets).  Thus if we fit the model and compute the MSE on the training set, we will get an optimistically [[bias (statistics)|biased]] assessment of how well the model will fit an independent data set.  This biased estimate is called the ''in-sample'' estimate of the fit, whereas the cross-validation estimate is an ''out-of-sample'' estimate.

Since in linear regression it is possible to directly compute the factor (''n''&amp;nbsp;&amp;minus;&amp;nbsp;''p''&amp;nbsp;&amp;minus;&amp;nbsp;1)/(''n''&amp;nbsp;+&amp;nbsp;''p''&amp;nbsp;+&amp;nbsp;1) by which the training MSE underestimates the validation MSE, cross-validation is not practically useful in that setting (however, cross-validation remains useful in the context of linear regression in that it can be used to select an optimally regularized cost function).
In most other regression procedures (e.g. [[logistic regression]]), there is no simple formula to make such an adjustment. Cross-validation is, thus, a generally applicable way to predict the performance of a model on a validation set using computation in place of mathematical analysis.

==Common types of cross-validation==
Two types of cross-validation can be distinguished, exhaustive and non-exhaustive cross-validation.

===Exhaustive cross-validation ===
Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.

====Leave-p-out cross-validation====
Leave-''p''-out cross-validation ('''LpO CV''') involves using ''p'' observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of ''p'' observations and a training set.

LpO cross-validation requires training and validating the model &lt;math&gt;C^n_p&lt;/math&gt; times, where ''n'' is the number of observations in the original sample, and where &lt;math&gt;C^n_p&lt;/math&gt; is the [[binomial coefficient]].  For ''p'' &gt; 1 and for even moderately large ''n'', LpO CV can become computationally infeasible.  For example, with ''n'' = 100 and ''p'' = 30 = 30 percent of 100 (as suggested above), &lt;math&gt;C^{100}_{30} \approx 3\times 10^{25}.&lt;/math&gt;

====Leave-one-out cross-validation====&lt;!-- This section is linked from [[Data mining]] --&gt;
Leave-''one''-out cross-validation ('''LOOCV''') is a particular case of leave-''p''-out cross-validation with ''p''&amp;nbsp;=&amp;nbsp;1.  The process looks similar to [[Jackknife resampling|jackknife]]; however, with cross-validation you compute a statistic on the left-out sample(s), while with jackknifing you compute a statistic from the kept samples only.

LOO cross-validation does not have the same problem of excessive computation time as general LpO cross-validation because &lt;math&gt;C^n_1=n&lt;/math&gt;.

=== Non-exhaustive cross-validation ===
Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. Those methods are approximations of leave-''p''-out cross-validation.

====''k''-fold cross-validation====

In ''k''-fold cross-validation, the original sample is randomly partitioned into ''k'' equal sized subsamples.
Of the ''k'' subsamples, a single subsample is retained as the validation data for testing the model, and the remaining ''k''&amp;nbsp;−&amp;nbsp;1 subsamples are used as training data. The cross-validation process is then repeated ''k'' times (the ''folds''), with each of the ''k'' subsamples used exactly once as the validation data.  The ''k'' results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling  (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,&lt;ref name=&quot;McLachlan&quot;&gt;{{cite book |title=Analyzing microarray gene expression data |first= Geoffrey J. |last= McLachlan |first2=Kim-Anh |last2=Do|author2-link=Kim-Anh Do |first3=Christophe |last3=Ambroise |year=2004 |publisher= Wiley }}&lt;/ref&gt; but in general ''k'' remains an unfixed parameter.

For example, setting ''k''&amp;nbsp;=&amp;nbsp;''2'' results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets ''d''&lt;sub&gt;0&lt;/sub&gt; and ''d''&lt;sub&gt;1&lt;/sub&gt;, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on ''d''&lt;sub&gt;0&lt;/sub&gt; and validate on ''d''&lt;sub&gt;1&lt;/sub&gt;, followed by training on ''d''&lt;sub&gt;1&lt;/sub&gt; and validating on&amp;nbsp;''d''&lt;sub&gt;0&lt;/sub&gt;.

When ''k''&amp;nbsp;=&amp;nbsp;''n'' (the number of observations), the ''k''-fold cross-validation is exactly the leave-one-out cross-validation.

In ''stratified'' ''k''-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds.  In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.

====Holdout method====

In the holdout method, we randomly assign data points to two sets ''d''&lt;sub&gt;0&lt;/sub&gt; and ''d''&lt;sub&gt;1&lt;/sub&gt;, usually called the training set and the test set, respectively. The size of each of the sets is arbitrary although typically the test set is smaller than the training set. We then train on ''d''&lt;sub&gt;0&lt;/sub&gt; and test on ''d''&lt;sub&gt;1&lt;/sub&gt;.

In typical cross-validation, multiple runs are aggregated together; in contrast, the holdout method, in isolation, involves a single run. While the holdout method can be framed as &quot;the simplest kind of cross-validation&quot;,&lt;ref&gt;{{cite web|title=Cross Validation|url=http://www.cs.cmu.edu/~schneide/tut5/node42.html|accessdate=11 November 2012}}&lt;/ref&gt; many sources instead classify holdout as a type of simple validation, rather than a simple or degenerate form of cross-validation.&lt;ref&gt;Kohavi, Ron. &quot;A study of cross-validation and bootstrap for accuracy estimation and model selection.&quot; Ijcai. Vol. 14. No. 2. 1995.&lt;/ref&gt;&lt;ref&gt;Arlot, Sylvain, and Alain Celisse. &quot;A survey of cross-validation procedures for model selection.&quot; Statistics surveys 4 (2010): 40-79. &quot;In brief, CV consists in averaging several hold-out estimators of the risk corresponding to different data splits.&quot;&lt;/ref&gt;

====Repeated random sub-sampling validation====

This method, also known as Monte Carlo cross-validation,&lt;ref name=&quot;mccv&quot;&gt;{{Cite book|title = Fundamentals of data mining in genomics and proteomics|last = Dubitzky,|first = Werner|publisher = Springer Science &amp; Business Media|year = 2007|isbn = |location = |pages = 178|last2 = Granzow|first2 = Martin|last3 = Berrar|first3 = Daniel|doi = }}&lt;/ref&gt; randomly splits the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over ''k''-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap.  This method also exhibits [[Monte Carlo method|Monte Carlo]] variation, meaning that the results will vary if the analysis is repeated with different random splits.

As the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation.

In a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are [[dichotomous]] with an unbalanced representation of the two response values in the data.

==Measures of fit==

The goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model.  It can be used to estimate any quantitative measure of fit that is appropriate for the data and model.  For example, for [[binary classification]] problems, each case in the validation set is either predicted correctly or incorrectly.  In this situation the misclassification error rate can be used to summarize the fit, although other measures like [[positive predictive value]] could also be used.  When the value being predicted is continuously distributed, the [[mean squared error]], [[root mean squared error]] or [[median absolute deviation]] could be used to summarize the errors.

==Applications==

Cross-validation can be used to compare the performances of different predictive modeling procedures.  For example, suppose we are interested in [[optical character recognition]], and we are considering using either [[support vector machines]] (SVM) or [[k nearest neighbors|''k'' nearest neighbors]] (KNN) to predict the true character from an image of a handwritten character.  Using cross-validation, we could objectively compare these two methods in terms of their respective fractions of misclassified characters.  If we simply compared the methods based on their in-sample error rates, the KNN method would likely appear to perform better, since it is more flexible and hence more prone to [[overfitting]] {{Citation needed|reason=Under which conditions|date=August 2017}} compared to the SVM method.

Cross-validation can also be used in [[Feature selection|''variable selection'']].&lt;ref name=&quot;Picard84&quot;&gt;{{cite journal |last=Picard |first=Richard |last2=Cook |first2=Dennis |year=1984 |title=Cross-Validation of Regression Models |journal=Journal of the American Statistical Association |jstor=2288403 |volume=79 |pages=575–583 |doi=10.2307/2288403 |issue=387 }}&lt;/ref&gt; Suppose we are using the [[gene expression|expression]] levels of 20 [[proteins]] to predict whether a [[cancer]] patient will respond to a [[drug]]. A practical goal would be to determine which subset of the 20 features should be used to produce the best predictive model. For most modeling procedures, if we compare feature subsets using the in-sample error rates, the best performance will occur when all 20 features are used. However under cross-validation, the model with the best fit will generally include only a subset of the features that are deemed truly informative.

==Statistical properties==

Suppose we choose a measure of fit ''F'', and use cross-validation to produce an estimate ''F&lt;sup&gt;*&lt;/sup&gt;'' of the expected fit ''EF'' of a model to an independent data set drawn from the same population as the training data. If we imagine sampling multiple independent training sets following the same distribution, the resulting values for ''F&lt;sup&gt;*&lt;/sup&gt;'' will vary. The statistical properties of ''F&lt;sup&gt;*&lt;/sup&gt;'' result from this variation.

The cross-validation estimator ''F&lt;sup&gt;*&lt;/sup&gt;'' is very nearly unbiased for ''EF'' &lt;ref&gt;&quot;Thoughts on prediction and cross-validation.&quot; Ronald Christensen, Department of Mathematics and Statistics University of New Mexico, May 21, 2015. Retrieved from http://www.math.unm.edu/~fletcher/Prediction.pdf on May 31, 2017.&lt;/ref&gt;{{Citation needed|date=October 2016}}. The reason that it is slightly biased is that the training set in cross-validation is slightly smaller than the actual data set (e.g. for LOOCV the training set size is ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1 when there are ''n'' observed cases). In nearly all situations, the effect of this bias will be conservative in that the estimated fit will be slightly biased in the direction suggesting a poorer fit. In practice, this bias is rarely a concern.

The variance of ''F&lt;sup&gt;*&lt;/sup&gt;'' can be large.&lt;ref name=&quot;Efron97&quot;&gt;{{cite journal |last1=Efron |first1=Bradley |last2=Tibshirani |first2=Robert |year=1997 |title=Improvements on cross-validation: The .632 + Bootstrap Method |journal=[[Journal of the American Statistical Association]] |volume=92 |pages=548–560 |mr=1467848 |jstor=2965703 |doi=10.2307/2965703 |issue=438 }}&lt;/ref&gt;&lt;ref name=&quot;Stone77&quot;&gt;{{cite journal |last=Stone |first=Mervyn |year=1977 |title=Asymptotics for and against cross-validation |journal=[[Biometrika]] |volume=64 |number=1 |pages=29–35 |doi=10.1093/biomet/64.1.29 |mr=0474601 |jstor=2335766 }}&lt;/ref&gt; For this reason, if two statistical procedures are compared based on the results of cross-validation, it is important to note that the procedure with the better estimated performance may not actually be the better of the two procedures (i.e. it may not have the better value of ''EF'').  Some progress has been made on constructing confidence intervals around cross-validation estimates,&lt;ref name=&quot;Efron97&quot; /&gt; but this is considered a difficult problem.

==Computational issues==

Most forms of cross-validation are straightforward to implement as long as an implementation of the prediction method being studied is available.  In particular, the prediction method can be a &quot;black box&quot; – there is no need to have access to the internals of its implementation.  If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly.  In some cases such as [[least squares]] and [[kernel regression]], cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast &quot;updating rules&quot; such as the [[Sherman–Morrison formula]].  However one must be careful to preserve the &quot;total blinding&quot; of the validation set from the training procedure, otherwise bias may result.  An extreme example of accelerating cross-validation occurs in [[linear regression]], where the results of cross-validation have a [[closed-form expression]] known as the ''prediction residual error sum of squares'' ([[PRESS statistic|PRESS]]).

==Limitations and misuse==

Cross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled.

In many applications of predictive modeling, the structure of the system being studied evolves over time (i.e. it is &quot;non-stationary&quot;).  Both of these can introduce systematic differences between the training and validation sets.  For example, if a model for [[stock market prediction|predicting stock values]] is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population.  As another example, suppose a model is developed to predict an individual's risk for being [[medical diagnosis|diagnosed]] with a particular disease within the next year.  If the model is trained using data from a study involving only a specific population group (e.g. young people or males), but is then applied to the general population, the cross-validation results from the training set could differ greatly from the actual predictive performance.

In many applications, models also may be incorrectly specified and vary as a function of modeler biases and/or arbitrary choices. When this occurs, there may be an illusion that the system changes in external samples, whereas the reason is that the model has missed a critical predictor and/or included a confounded predictor.   New evidence is that cross-validation by itself is not very predictive of external validity, whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity.&lt;ref&gt;{{cite journal |last=Consortium |first=MAQC |year=2010 |title=The Microarray Quality Control (MAQC)-II study of common practices for the development and validation of microarray-based predictive models |journal=Nature Biotechnology  |volume=28 |pages=827–838 |publisher=Nature Publishing Group |location = London | doi = 10.1038/nbt.1665 |pmid=20676074 |pmc=3315840}}&lt;/ref&gt;  As defined by this large MAQC-II study across 30,000 models, swap sampling incorporates cross-validation in the sense that predictions are tested across independent training and validation samples. Yet, models are also developed across these independent samples and by modelers who are blinded to one another.  When there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently, MAQC-II shows that this will be much more predictive of poor external predictive validity than traditional cross-validation.

The reason for the success of the swapped sampling is a built-in control for human biases in model building.  In addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects, these are some other ways that cross-validation can be misused:

* By performing an initial analysis to identify the most informative [[features (pattern recognition)|features]] using the entire data set – if feature selection or model tuning is required by the modeling procedure, this must be repeated on every training set. Otherwise, predictions will certainly be upwardly biased.&lt;ref name=&quot;Bermingham-intro&quot;&gt;{{cite journal |title= Application of high-dimensional feature selection: evaluation for genomic prediction in man |first1=Mairead L. |last1=Bermingham |first2=Ricardo |last2=Pong-Wong | first3=Athina|last3=Spiliopoulou | first4=Caroline|last4=Hayward |first5=Igor|last5=Rudan |first6=Harry|last6=Campbell |first7=Alan F.|last7=Wright |first8=James F.|last8=Wilson |first9=Felix |last9=Agakov |first10=Pau|last10=Navarro |first11=Chris S.|last11=Haley |journal=[[Scientific Reports|Sci. Rep.]] |volume=5 |year=2015 |url= http://www.nature.com/srep/2015/150519/srep10312/full/srep10312.html}}&lt;/ref&gt;  If cross-validation is used to decide which features to use, an ''inner cross-validation'' to carry out the feature selection on every training set must be performed.&lt;ref&gt;{{Cite journal| doi = 10.1186/1471-2105-7-91| pmid = 16504092| pmc = 1397873| title = Bias in error estimation when using cross-validation for model selection| year = 2006| last1 = Varma | first1 = Sudhir| last2 = Simon | first2 = Richard| journal = BMC Bioinformatics| volume = 7| pages = 91}}&lt;/ref&gt;
* By allowing some of the training data to also be included in the test set – this can happen due to &quot;twinning&quot; in the data set, whereby some exactly identical or nearly identical samples are present in the data set.  Note that to some extent twinning always takes place even in perfectly independent training and validation samples. This is because some of the training sample observations will have nearly identical values of predictors as validation sample observations. And some of these will correlate with a target at better than chance levels in the same direction in both training and validation when they are actually driven by confounded predictors with poor external validity.  If such a cross-validated model is selected from a ''k''-fold set, human confirmation bias will be at work and determine that such a model has been validated. This is why traditional cross-validation needs to be supplemented with controls for human bias and confounded model specification like swap sampling and prospective studies.

==Cross validation for time-series models==
Since the order of the data is important, cross-validation might be problematic for Time-series models. A more appropriate approach might be to use forward chaining.

==See also==
{{Commons category|Cross-validation (statistics)}}
* [[Boosting (machine learning)]]
* [[Bootstrap aggregating]] (bagging)
* [[Bootstrapping (statistics)]]
* [[Model selection]]
* [[Resampling (statistics)]]
* [[Stability (learning theory)]]
* [[Validity (statistics)]]

==Notes and references==
{{reflist|30em}}

{{Statistics|hide}}

{{DEFAULTSORT:Cross-Validation (Statistics)}}


</text>
      <sha1>hdrkve0m5gls6ghne5peoksuemxtvyf</sha1>
    </revision>
  </page>
  <page>
    <title>Predictive learning</title>
    <ns>0</ns>
    <id>2291650</id>
    <revision>
      <id>538191324</id>
      <parentid>447127509</parentid>
      <timestamp>2013-02-14T08:47:58Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Removing {{Orphan}} ([[User talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="913">{{Unreferenced stub|auto=yes|date=December 2009}}

'''Predictive learning''' is a technique of [[machine learning]] in which an agent tries to build a model of its environment by trying out different actions in various circumstances. It uses knowledge of the effects its actions appear to have, turning them into planning operators. These allow the agent to act purposefully in its world. Predictive learning is one attempt to learn with a minimum of pre-existing mental structure. It may have been inspired by [[Jean Piaget|Piaget]]'s account of how children construct knowledge of the world by interacting with it. [[Gary Drescher]]'s book 'Made-up Minds' was seminal for the area.

Another more recent predictive learning theory is [[Jeff Hawkins]]' [[memory-prediction framework]], which is laid out in his [[On Intelligence]].

{{DEFAULTSORT:Predictive Learning}}


{{Tech-stub}}</text>
      <sha1>0f83dlkbwjk8g9ll6rs7ohn196lud9i</sha1>
    </revision>
  </page>
  <page>
    <title>Empirical risk minimization</title>
    <ns>0</ns>
    <id>1455062</id>
    <revision>
      <id>808373618</id>
      <parentid>808372473</parentid>
      <timestamp>2017-11-02T13:57:51Z</timestamp>
      <contributor>
        <username>Johndburger</username>
        <id>363970</id>
      </contributor>
      <comment>Added See Also section; Changed &quot;Literature&quot; section title to more standard &quot;Further Reading&quot;.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4553">{{machine learning bar}}
'''Empirical risk minimization''' (ERM) is a principle in [[statistical learning theory]] which defines a family of learning algorithms and is used to give theoretical bounds on the performance of [[machine learning|learning algorithms]].

== Background ==
Consider the following situation, which is a general setting of many [[supervised learning]] problems. We have two spaces of objects &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; and would like to learn a function &lt;math&gt;\ h: X \to Y&lt;/math&gt; (often called ''hypothesis'') which outputs an object &lt;math&gt;y \in Y&lt;/math&gt;, given &lt;math&gt;x \in X&lt;/math&gt;. To do so, we have at our disposal a ''training set'' of a few examples &lt;math&gt;\ (x_1, y_1), \ldots, (x_m, y_m)&lt;/math&gt; where &lt;math&gt;x_i \in X&lt;/math&gt; is an input and &lt;math&gt;y_i \in Y&lt;/math&gt; is the corresponding response that we wish to get from &lt;math&gt;\ h(x_i)&lt;/math&gt;.

To put it more formally, we assume that there is a [[joint probability distribution]] &lt;math&gt;P(x, y)&lt;/math&gt; over &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt;, and that the training set consists of &lt;math&gt;m&lt;/math&gt; instances &lt;math&gt;\ (x_1, y_1), \ldots, (x_m, y_m)&lt;/math&gt; drawn [[i.i.d.]] from &lt;math&gt;P(x, y)&lt;/math&gt;. Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because &lt;math&gt;y&lt;/math&gt; is not a deterministic function of &lt;math&gt;x&lt;/math&gt;, but rather a [[random variable]] with [[conditional distribution]] &lt;math&gt;P(y | x)&lt;/math&gt; for a fixed &lt;math&gt;x&lt;/math&gt;.

We also assume that we are given a non-negative real-valued [[loss function]] &lt;math&gt;L(\hat{y}, y)&lt;/math&gt; which measures how different the prediction &lt;math&gt;\hat{y}&lt;/math&gt; of a hypothesis is from the true outcome &lt;math&gt;y.&lt;/math&gt; The [[Risk (statistics)|risk]] associated with hypothesis &lt;math&gt;h(x)&lt;/math&gt; is then defined as the [[Expected value|expectation]] of the loss function:
: &lt;math&gt;R(h) = \mathbf{E}[L(h(x), y)] = \int L(h(x), y)\,dP(x, y).&lt;/math&gt;

A loss function commonly used in theory is the [[0-1 loss function]]: &lt;math&gt;L(\hat{y}, y) = I(\hat{y} \ne y)&lt;/math&gt;, where &lt;math&gt;I(\dots)&lt;/math&gt; is the [[indicator notation]].

The ultimate goal of a learning algorithm is to find a hypothesis &lt;math&gt; h^*&lt;/math&gt; among a fixed class of functions &lt;math&gt;\mathcal{H}&lt;/math&gt; for which the risk &lt;math&gt;R(h)&lt;/math&gt; is minimal:
: &lt;math&gt;h^* = \arg \min_{h \in \mathcal{H}} R(h).&lt;/math&gt;

== Empirical risk minimization ==
In general, the risk &lt;math&gt;R(h)&lt;/math&gt; cannot be computed because the distribution &lt;math&gt;P(x, y)&lt;/math&gt; is unknown to the learning algorithm (this situation is referred to as [[agnostic learning]]). However, we can compute an approximation, called ''empirical risk'', by averaging the loss function on the training set:
: &lt;math&gt;\! R_\text{emp}(h) = \frac{1}{m} \sum_{i=1}^m L(h(x_i), y_i).&lt;/math&gt;

''Empirical risk minimization'' principle states that the learning algorithm should choose a hypothesis &lt;math&gt;\hat{h}&lt;/math&gt; which minimizes the empirical risk:
: &lt;math&gt;\hat{h} = \arg \min_{h \in \mathcal{H}} R_{\text{emp}}(h).&lt;/math&gt;
Thus the learning algorithm defined by the ERM principle consists in solving the above [[Mathematical optimization|optimization]] problem.

== Properties ==
{{Expand section|date=February 2010}}

=== Computational complexity ===
Empirical risk minimization for a classification problem with [[0-1 loss function]] is known to be an [[NP-hard]] problem even for such relatively simple class of functions as [[linear classifier]]s.&lt;ref&gt;V. Feldman, V. Guruswami, P. Raghavendra and Yi Wu (2009). [https://arxiv.org/abs/1012.0729 ''Agnostic Learning of Monomials by Halfspaces is Hard.''] (See the paper and references therein)&lt;/ref&gt; Though, it can be solved efficiently when minimal empirical risk is zero, i.e. data is [[linearly separable]].

In practice, machine learning algorithms cope with that either by employing a convex approximation to 0-1 loss function (like [[hinge loss]] for [[Support vector machine|SVM]]), which is easier to optimize, or by posing assumptions on the distribution &lt;math&gt;P(x, y)&lt;/math&gt; (and thus stop being agnostic learning algorithms to which the above result applies).

==See also==

[[Maximum likelihood estimation]]

== References ==
{{Reflist}}

== Further reading ==
* {{cite book
    | last=Vapnik
    | first=V.
    | authorlink = Vladimir Vapnik
    | title=The Nature of Statistical Learning Theory
    | publisher = [[Springer-Verlag]]
    | series=Information Science and Statistics
    | year = 2000
    | isbn=978-0-387-98780-4}}

</text>
      <sha1>d0jmi8j859r5glquh2gldgmt5h2jssy</sha1>
    </revision>
  </page>
  <page>
    <title>Product of experts</title>
    <ns>0</ns>
    <id>24825162</id>
    <revision>
      <id>508558707</id>
      <parentid>508558688</parentid>
      <timestamp>2012-08-22T03:19:08Z</timestamp>
      <contributor>
        <username>Jojalozzo</username>
        <id>4328601</id>
      </contributor>
      <minor/>
      <comment>Jojalozzo moved page [[Product of Experts]] to [[Product of experts]]: Common noun phrase</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1198">'''Product of experts''' (PoE) is a [[machine learning]] technique. It models a probability distribution by combining the output from several simpler distributions.
It was proposed by [[Geoff Hinton]], along with an algorithm for training the parameters of such a system.

The core idea is to combine several probability distributions (&quot;experts&quot;) by multiplying their density functions—making the PoE classification similar to an &quot;and&quot; operation. This allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem.

This is related to (but quite different from) a [[mixture model]], where several probability distributions are combined via an &quot;or&quot; operation, which is a weighted sum of their density functions.

==External links==
*{{Cite journal|doi=10.1162/089976602760128018|last=Hinton |first=Geoffrey E.|year=2002|title=Training Products of Experts by Minimizing Contrastive Divergence|journal=Neural Computation|volume=14|issue=8|pages=1771–1800|url=http://www.cs.toronto.edu/~hinton/absps/nccd.pdf|accessdate=2009-10-25|pmid=12180402}}

{{DEFAULTSORT:Product Of Experts}}



{{Compu-stub}}</text>
      <sha1>5cttuecchiw2lhilajs0vwbvvpwsc4g</sha1>
    </revision>
  </page>
  <page>
    <title>Unsupervised learning</title>
    <ns>0</ns>
    <id>233497</id>
    <revision>
      <id>813828303</id>
      <parentid>806744572</parentid>
      <timestamp>2017-12-05T12:57:41Z</timestamp>
      <contributor>
        <ip>105.154.104.55</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9890">{{multiple issues|
{{Cleanup|date=May 2010}}
{{More footnotes|date=February 2010}}
}}
{{Machine learning bar}}

'''Unsupervised machine learning''' is the [[machine learning]] task of inferring a function to describe hidden structure from &quot;unlabeled&quot; data (a classification or categorization is not included in the observations). Since the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm—which is one way of distinguishing unsupervised learning from [[supervised learning]] and [[reinforcement learning]].

A central case of unsupervised learning is the problem of [[density estimation]] in [[statistics]],&lt;ref name=&quot;JordanBishop2004&quot;&gt;{{cite book |first1=Michael I. |last1=Jordan |first2=Christopher M. |last2=Bishop |chapter=Neural Networks |editor=Allen B. Tucker |title=Computer Science Handbook, Second Edition (Section VII: Intelligent Systems) |location=Boca Raton, FL |publisher=Chapman &amp; Hall/CRC Press LLC |year=2004 |ISBN=1-58488-360-X }}&lt;/ref&gt; though unsupervised learning encompasses many other problems (and solutions) involving summarizing and explaining key features of the data.

Approaches to unsupervised learning include:

* [[Data clustering|Clustering]]
** [[k-means]]
** [[mixture models]]
** [[hierarchical clustering]],&lt;ref&gt;{{cite book|last=Hastie, Trevor, Robert Tibshirani|first=Friedman, Jerome|title=The Elements of Statistical Learning: Data mining, Inference, and Prediction|date=2009|publisher=Springer|location=New York|isbn=978-0-387-84857-0|pages=485–586}}&lt;/ref&gt;
* [[Anomaly detection]]
* [[Artificial neural network|Neural Networks]]
**[[Hebbian Learning]]
**[[Generative Adversarial Networks]]
* Approaches for learning [[latent variable model]]s such as
** [[Expectation–maximization algorithm]] (EM)
** [[Method of moments (statistics)|Method of moments]]
** [[Blind signal separation]] techniques, e.g.,
*** [[Principal component analysis]],
*** [[Independent component analysis]],
*** [[Non-negative matrix factorization]],
*** [[Singular value decomposition]].&lt;ref&gt;[[Ranjan Acharyya|Acharyya, Ranjan]] (2008); ''A New Approach for Blind Source Separation of Convolutive Sources'', {{ISBN|978-3-639-07797-1}} (this book focuses on unsupervised learning with Blind Source Separation)&lt;/ref&gt;

== In neural networks ==

The classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by [[Donald Hebb]]'s principle, that is, neurons that fire together wire together. In [[Hebbian learning]], the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials ([[spike-timing-dependent plasticity]] or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.

Among [[Artificial neural network|neural network]] models, the [[self-organizing map]] (SOM) and [[adaptive resonance theory]] (ART) are commonly used unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the [[vigilance parameter]]. ART networks are also used for many pattern recognition tasks, such as [[automatic target recognition]] and seismic signal processing. The first version of ART was &quot;ART1&quot;, developed by Carpenter and Grossberg (1988).&lt;ref&gt;{{cite journal|author1=Carpenter, G.A.  |author2=Grossberg, S. |lastauthoramp=yes |year=1988|title=The ART of adaptive pattern recognition by a self-organizing neural network|journal=Computer|volume=21|pages=77–88|url=http://www.cns.bu.edu/Profiles/Grossberg/CarGro1988Computer.pdf|doi=10.1109/2.33}}
&lt;/ref&gt;

== Method of moments ==
One of the statistical approaches for unsupervised learning is the [[method of moments (statistics)|method of moments]]. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the [[mean]] vector, and the second order moment is the [[covariance]] matrix (when the mean is zero). Higher order moments are usually represented using [[tensors]] which are the generalization of matrices to higher orders as multi-dimensional arrays.

In particular, the method of moments is shown to be effective in learning the parameters of [[latent variable model]]s.&lt;ref name=&quot;TensorLVMs&quot;&gt;{{cite journal |last=Anandkumar |first=Animashree |last2=Ge |first2=Rong |last3=Hsu |first3=Daniel |last4=Kakade |first4=Sham |first5= Matus |last5=Telgarsky |date=2014 |title=Tensor Decompositions for Learning Latent Variable Models |url=http://www.jmlr.org/papers/volume15/anandkumar14b/anandkumar14b.pdf |journal=Journal of Machine Learning Research (JMLR) |volume=15 |pages=2773–2832}}&lt;/ref&gt;
Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the [[topic modeling]] which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.&lt;ref name=&quot;TensorLVMs&quot; /&gt;

The [[Expectation–maximization algorithm]] (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.&lt;ref name=&quot;TensorLVMs&quot; /&gt;

== Examples ==
{{Advert|section|type=|date=August 2017}}
Behavioral-based detection in network security has become a good application area for a combination of supervised- and unsupervised-machine learning. This is because the amount of data for a human security analyst to analyze is impossible (measured in terabytes per day) to review to find patterns and anomalies. According to Giora Engel, co-founder of LightCyber, in a ''Dark Reading'' article, &quot;The great promise machine learning holds for the security industry is its ability to detect advanced and unknown attacks -- particularly those leading to data breaches.&quot;&lt;ref&gt;{{Cite news|url=http://www.darkreading.com/threat-intelligence/3-flavors-of-machine-learning--who-what-and-where/a/d-id/1324278|title=3 Flavors of Machine Learning:  Who, What &amp; Where|last=Engel|first=Giora|date=February 11, 2016|work=|newspaper=Dark Reading|access-date=2016-11-21|via=}}&lt;/ref&gt; The basic premise is that a motivated attacker will find their way into a network (generally by compromising a user's computer or network account through phishing, social engineering or malware). The security challenge then becomes finding the attacker by their operational activities, which include reconnaissance, lateral movement, command &amp; control and exfiltration. These activities—especially reconnaissance and lateral movement—stand in contrast to an established baseline of &quot;normal&quot; or &quot;good&quot; activity for each user and device on the network. The role of machine learning is to create ongoing profiles for users and devices and then find meaningful anomalies.&lt;ref&gt;{{Cite web|url=http://blog.stephenwolfram.com/2017/03/the-rd-pipeline-continues-launching-version-11-1/|title=The R&amp;D Pipeline Continues: Launching Version 11.1—Stephen Wolfram|last=|first=|date=|website=blog.stephenwolfram.com|language=en|archive-url=|archive-date=|dead-url=|access-date=2017-03-22}}&lt;/ref&gt;

== See also ==
* [[Cluster analysis]]
* [[Anomaly detection]]
* [[Expectation–maximization algorithm]]
* [[Generative topographic map]]
* [[Multivariate analysis]]
* [[Radial basis function network]]
* [[Hebbian Theory]]

== Notes ==
{{reflist}}

== Further reading ==
* {{cite book |editor1=Bousquet, O. |editor3=Raetsch, G. |editor2=von Luxburg, U. |title=Advanced Lectures on Machine Learning |publisher=Springer-Verlag |year=2004 |isbn=978-3540231226}}
* {{cite book |author1=[[Richard O. Duda|Duda, Richard O.]] |author2=[[Peter E. Hart|Hart, Peter E.]] |author3=Stork, David G. |year=2001 |chapter=Unsupervised Learning and Clustering  |title=[[Pattern classification]] |edition=2nd |publisher=Wiley |isbn=0-471-05669-3}}
*{{cite book |first1=Trevor |last1=Hastie |first2=Robert |last2=Tibshirani |title=The Elements of Statistical Learning: Data mining, Inference, and Prediction |year=2009 |publisher=Springer| location=New York |isbn=978-0-387-84857-0 |pages=485–586 |doi=10.1007/978-0-387-84858-7_14}}
* {{cite book |editor1=[[Geoffrey Hinton|Hinton, Geoffrey]] |editor2=[[Terrence J. Sejnowski|Sejnowski, Terrence J.]] |year=1999 |title=Unsupervised Learning: Foundations of Neural Computation |publisher=[[MIT Press]] |isbn=0-262-58168-X}} (This book focuses on unsupervised learning in [[neural network]]s)

{{DEFAULTSORT:Unsupervised Learning}}
[[Category:Unsupervised learning| ]]
</text>
      <sha1>oeogfccewgl4xjf6fsp99xw07s7hfgs</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Loss functions</title>
    <ns>14</ns>
    <id>28004586</id>
    <revision>
      <id>740645154</id>
      <parentid>739999869</parentid>
      <timestamp>2016-09-22T10:45:53Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed , this is rather about decision theory</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="89">

</text>
      <sha1>7rco192043fhyqgwwii50538sfona3u</sha1>
    </revision>
  </page>
  <page>
    <title>Meta learning (computer science)</title>
    <ns>0</ns>
    <id>4615464</id>
    <revision>
      <id>815693237</id>
      <parentid>815692714</parentid>
      <timestamp>2017-12-16T13:19:55Z</timestamp>
      <contributor>
        <username>Yoshua.Bengio</username>
        <id>12787963</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7356">{{About|meta learning in machine learning|meta learning in social psychology|Meta learning|metalearning in neuroscience|Metalearning (neuroscience)}}
{{Refimprove|date=August 2010}}
'''Meta learning''' is a subfield of [[machine learning]] where automatic learning algorithms are applied on [[meta-data]] about machine learning experiments. Although different researchers hold different views as to what the term exactly means (see below), the main goal is to use such meta-data to understand how automatic learning can become flexible in solving different kinds of learning problems, hence to improve the performance of existing [[learning algorithms]] or to learn the learning algorithm itself, hence the alternative term '''Learning to learn'''.

Flexibility is very important because each learning algorithm is based on a set of assumptions about the data, its [[inductive bias]]. This means that it will only learn well if the bias matches the data in the learning problem. A learning algorithm may perform very well on one learning problem, but very badly on the next. From a non-expert point of view, this poses strong restrictions on the use of [[machine learning]] or [[data mining]] techniques, since the relationship between the learning problem (often some kind of [[database]]) and the effectiveness of different learning algorithms is not yet understood.

By using different kinds of meta-data, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of [[metaheuristic]], which can be said to be a related problem. A good analogy to meta-learning, and the inspiration for the early work by Bengio et al (1991),&lt;ref&gt;{{cite conference|last=Bengio|first=Yoshua|last2=Bengio|first2=Samy|last3=Cloutier|first3=Jocelyn|conference=IJCNN'91|url=http://bengio.abracadoudou.com/publications/pdf/bengio_1991_ijcnn.pdf|date=1991|title=Learning to learn a synaptic rule}}&lt;/ref&gt; considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain.

== Definition ==
A proposed definition&lt;ref&gt;{{Cite journal|last=Lemke|first=Christiane|last2=Budka|first2=Marcin|last3=Gabrys|first3=Bogdan|date=2013-07-20|title=Metalearning: a survey of trends and technologies|url=https://link.springer.com/article/10.1007/s10462-013-9406-y|journal=Artificial Intelligence Review|language=en|volume=44|issue=1|pages=117–130|doi=10.1007/s10462-013-9406-y|issn=0269-2821|pmc=4459543|pmid=26069389}}&lt;/ref&gt; for what qualifies as a meta learning system considers three requirements:
# The system must include a learning subsystem, which adapts with experience.
# Experience is gained by exploiting meta knowledge extracted
#* ...in a previous learning episode on a single dataset.
#* ...from different domains or problems.
# Learning bias must be chosen dynamically.
The term bias in the last point refers to the set of assumptions influencing the choice of hypotheses for explaining the data&lt;ref&gt;{{Cite book|url=https://link.springer.com/10.1007/978-3-540-73263-1|title=Metalearning - Springer|doi=10.1007/978-3-540-73263-1}}&lt;/ref&gt; and must not be confused with the notion of bias represented in the [[bias-variance dilemma]]. Meta learning is concerned with two aspects of learning bias; declarative bias specifies the representation of the space of hypotheses, and affects the size of the search space (i.e. represent hypotheses using linear functions only) while procedural bias imposes constraints on the ordering of the inductive hypotheses (i.e. preferring smaller hypotheses).

==Different views on meta learning==

These are some of the views on (and approaches to) meta learning, please note that there exist many variations on these general approaches:

* ''Discovering meta-knowledge'' works by inducing knowledge (e.g. rules) that expresses how each learning method will perform on different learning problems. The meta-data is formed by characteristics of the data (general, statistical, information-theoretic,... ) in the learning problem, and characteristics of the learning algorithm (type, parameter settings, performance measures,...). Another learning algorithm then learns how the data characteristics relate to the algorithm characteristics. Given a new learning problem, the data characteristics are measured, and the performance of different learning algorithms can be predicted. Hence, one can select the algorithms best suited for the new problem, at least if the induced relationship holds.
* ''Stacked generalisation'' works by combining a number of (different) learning algorithms. The meta-data is formed by the predictions of those different algorithms. Then another learning algorithm learns from this meta-data to predict which combinations of algorithms give generally good results. Given a new learning problem, the predictions of the selected set of algorithms are combined (e.g. by (weighted) voting) to provide the final prediction. Since each algorithm is deemed to work on a subset of problems, a combination is hoped to be more flexible and still able to make good predictions.
* ''[[Boosting (meta-algorithm)|Boosting]]'' is related to stacked generalisation, but uses the same algorithm multiple times, where the examples in the training data get different weights over each run. This yields different predictions, each focused on rightly predicting a subset of the data, and combining those predictions leads to better (but more expensive) results.
* ''Dynamic bias selection'' works by altering the inductive bias of a learning algorithm to match the given problem. This is done by altering key aspects of the learning algorithm, such as the hypothesis representation, heuristic formulae, or parameters. Many different approaches exist.
* ''[[Inductive transfer]]'' also called learning to learn, studies how the learning process can be improved over time. Meta-data consists of knowledge about previous learning episodes, and is used to efficiently develop an effective hypothesis for a new task. A related approach is called [[learning to learn]], in which the goal is to use acquired knowledge from one domain to help learning in other domains.
* Other approaches using meta-data to improve automatic learning are [[learning classifier system]]s, [[case-based reasoning]] and [[constraint satisfaction]].

==References==
{{Reflist}}

* Vilalta R. and Drissi Y. (2002). ''[http://axon.cs.byu.edu/Dan/478/misc/Vilalta.pdf A perspective view and survey of meta-learning]'', Artificial Intelligence Review, 18(2), 77—95.
* Giraud-Carrier, C., &amp; Keller, J. (2002). Dealing with the data flood, J. Meij (ed), chapter Meta-Learning. STT/Beweton, The Hague.
* Brazdil P., Giraud-Carrier C., Soares C., Vilalta R. (2009) [https://books.google.com/books?id=-Gsi_cxZGpcC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false Metalearning: applications to data mining], chapter Metalearning: Concepts and Systems, Springer.

==See also==
* [http://www.scholarpedia.org/article/Metalearning Metalearning] article in [[Scholarpedia]]

{{DEFAULTSORT:Meta Learning (Computer Science)}}
</text>
      <sha1>spqevd8ch0gkuj4gwe5mirldneduxb5</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-armed bandit</title>
    <ns>0</ns>
    <id>2854828</id>
    <revision>
      <id>813552333</id>
      <parentid>813160425</parentid>
      <timestamp>2017-12-04T01:21:12Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor/>
      <comment>refpunct., cap</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="50292">[[File:Las Vegas slot machines.jpg|thumb|right|A row of slot machines in Las Vegas.]]

In [[probability theory]], the '''multi-armed bandit problem''' (sometimes called the '''''K''-&lt;ref name=&quot;doi10.1023/A:1013689704352&quot;&gt;{{Cite journal | last1 = Auer | first1 = P. | last2 = Cesa-Bianchi | first2 = N. | last3 = Fischer | first3 = P. | journal = Machine Learning | volume = 47 | issue = 2/3 | pages = 235–256 | year = 2002 |title=Finite-time Analysis of the Multiarmed Bandit Problem| doi = 10.1023/A:1013689704352 | pmid =  | pmc = }}&lt;/ref&gt; or ''N''-armed bandit problem'''&lt;ref&gt;{{Cite journal | last1 = Katehakis | first1 = M. N. | last2 = Veinott | first2 = A. F. | doi = 10.1287/moor.12.2.262 | title = The Multi-Armed Bandit Problem: Decomposition and Computation | journal = Mathematics of Operations Research | volume = 12 | issue = 2 | pages = 262–268 | year = 1987 | pmid =  | pmc = }}&lt;/ref&gt;) is a problem in which a gambler at a row of [[slot machines]] (sometimes known as &quot;one-armed bandits&quot;) has to decide which machines to play, how many times to play each machine and in which order to play them.&lt;ref name=&quot;weber&quot;&gt;{{citation
 | last = Weber | first = Richard
 | issue = 4
 | journal = [[Annals of Applied Probability]]
 | pages = 1024–1033
 | title = On the Gittins index for multiarmed bandits
 | volume = 2
 | year = 1992
 | jstor=2959678
 | doi = 10.1214/aoap/1177005588}}&lt;/ref&gt; When played, each machine provides a random reward from a [[probability distribution]] specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.&lt;ref name=&quot;Gittins89&quot;/&gt;&lt;ref Name=&quot;BF&quot;/&gt;

[[Herbert Robbins]] in 1952, realizing the importance of the problem, constructed convergent population selection strategies in &quot;some aspects of the sequential design of experiments&quot;.&lt;ref&gt;{{Cite journal | last1 = Robbins | first1 = H. | title = Some aspects of the sequential design of experiments | doi = 10.1090/S0002-9904-1952-09620-8 | journal = Bulletin of the American Mathematical Society | volume = 58 | issue = 5 | pages = 527–535 | year = 1952 | pmid =  | pmc = }}&lt;/ref&gt;

A theorem, the [[Gittins index]], first published by [[John C. Gittins]], gives an optimal policy for maximizing the expected discounted reward.&lt;ref&gt;{{cite journal | author = J. C. Gittins | authorlink = John C. Gittins | year = 1979 | title = Bandit Processes and Dynamic Allocation Indices | journal = Journal of the Royal Statistical Society. Series B (Methodological)  | volume = 41 | issue = 2 | pages = 148–177 | doix =  | jstor = 2985029 | url =  | format =  | accessdate = }}&lt;/ref&gt;

In practice, multi-armed bandits have been used to model the problem of managing research projects in a large organization, like a science foundation or a [[Pharmaceutical industry|pharmaceutical company]]. Given a fixed budget, the problem is to allocate resources among the competing projects, whose properties are only partially known at the time of allocation, but which may become better understood as time passes.&lt;ref name=&quot;Gittins89&quot; /&gt;&lt;ref name=&quot;BF&quot;/&gt;

In early versions of the multi-armed bandit problem, the gambler has no initial knowledge about the machines. The crucial tradeoff the gambler faces at each trial is between &quot;exploitation&quot; of the machine that has the highest expected payoff and &quot;exploration&quot; to get more [[Bayes' theorem|information]] about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in [[reinforcement learning]].

==Empirical motivation==
[[File:The Jet Propulsion Laboratory (9416811752).jpg|thumb|How to distribute a given budget among these research departments to maximize results?]]
The multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called &quot;exploration&quot;) and optimize his or her decisions based on existing knowledge (called &quot;exploitation&quot;). The agent attempts to balance these competing tasks in order to maximize his total value over the period of time considered. There are many practical applications of the bandit model, for example:

* [[clinical trial]]s investigating the effects of different experimental treatments while minimizing patient losses,&lt;ref name=&quot;Gittins89&quot; /&gt;&lt;ref name=&quot;BF&quot;/&gt;&lt;ref name=&quot;WHP&quot;/&gt;&lt;ref name=&quot;KD&quot;&gt;Press (1986)&lt;/ref&gt;
* [[adaptive routing]] efforts for minimizing delays in a network,
* [[Portfolio (finance)|financial portfolio design]]&lt;ref name=&quot;BrochuHoffmandeFreitas&quot; /&gt;&lt;ref name=&quot;ShenWangJiangZha&quot; /&gt;

In these practical examples, the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the ''exploitation vs. exploration tradeoff'' in [[reinforcement learning]].

The model has also been used to control dynamic allocation of resources to different projects, answering the question of which project to work on, given uncertainty about the difficulty and payoff of each possibility.&lt;ref name=&quot;farias2011irrevocable&quot; /&gt;

Originally considered by Allied scientists in [[World War II]], it proved so intractable that, according to [[Peter Whittle (mathematician)|Peter Whittle]], the problem was proposed to be dropped over [[Germany]] so that German scientists could also waste their time on it.&lt;ref name=&quot;Whittle79&quot;/&gt;

The version of the problem now commonly analyzed was formulated by [[Herbert Robbins]] in 1952.

==The multi-armed bandit model==
The multi-armed bandit (short: ''bandit'' or MAB) can be seen as a set of real [[Probability distribution|distributions]] &lt;math&gt;B = \{R_1, \dots ,R_K\}&lt;/math&gt;, each distribution being  associated with the rewards delivered by one of the &lt;math&gt;K \in \mathbb{N}^+&lt;/math&gt; levers. Let &lt;math&gt;\mu_1, \dots , \mu_K&lt;/math&gt; be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon &lt;math&gt;H&lt;/math&gt; is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state [[Markov decision process]]. The [[Regret (decision theory)|regret]] &lt;math&gt;\rho&lt;/math&gt; after &lt;math&gt;T&lt;/math&gt; rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards: &lt;math&gt;\rho = T \mu^* - \sum_{t=1}^T \widehat{r}_t&lt;/math&gt;, where &lt;math&gt;\mu^*&lt;/math&gt; is the maximal reward mean, &lt;math&gt;\mu^* = \max_k \{ \mu_k \}&lt;/math&gt;, and &lt;math&gt;\widehat{r}_t&lt;/math&gt; is the reward at time ''t''.

A ''zero-regret strategy'' is a strategy whose average regret per round &lt;math&gt;\rho / T&lt;/math&gt; tends to zero with probability 1 when the number of played rounds tends to infinity.&lt;ref name=&quot;Vermorel2005&quot;/&gt; Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played.

==Variations==
A common formulation is the ''Binary multi-armed bandit'' or ''Bernoulli multi-armed bandit,'' which issues a reward of one with probability &lt;math&gt;p&lt;/math&gt;, and otherwise a reward of zero.

Another formulation of the multi-armed bandit has each arm representing an independent Markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the Markov state evolution probabilities. There is a reward depending on the current state of the machine. In a generalisation called the &quot;restless bandit problem&quot;, the states of non-played arms can also evolve over time.&lt;ref name=&quot;Whittle88&quot;/&gt; There has also been discussion of systems where the number of choices (about which arm to play) increases over time.&lt;ref name=&quot;Whittle81&quot;/&gt;

Computer science researchers have studied multi-armed bandits under worst-case assumptions, obtaining algorithms to minimize regret in both finite and infinite ([[asymptotic]]) time horizons for both stochastic&lt;ref name=&quot;doi10.1023/A:1013689704352&quot;/&gt; and non-stochastic&lt;ref&gt;{{Cite journal | last1 = Auer | first1 = P. | last2 = Cesa-Bianchi | first2 = N. | last3 = Freund | first3 = Y. | last4 = Schapire | first4 = R. E. | title = The Nonstochastic Multiarmed Bandit Problem | doi = 10.1137/S0097539701398375 | journal = [[SIAM Journal on Computing|SIAM J. Comput.]] | volume = 32 | issue = 1 | pages = 48–77 | year = 2002 | pmid =  | pmc = }}&lt;/ref&gt; arm payoffs.

==Bandit strategies==
A major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate  to the population with highest mean) in the work described below.

===Optimal solutions===
&lt;!-- [[File:1966-HerbertRobbins.jpg|thumb|Herbert Robbins]] --&gt;
In the paper &quot;Asymptotically efficient adaptive allocation rules&quot;, Lai and Robbins&lt;ref&gt;{{cite journal | last1 = Lai | first1 = T.L. | last2 = Robbins | first2 = H. | year = 1985 | title = Asymptotically efficient adaptive allocation rules | url = | journal = Advances in Applied Mathematics | volume = 6 | issue = 1| pages =4–22 | doi = 10.1016/0196-8858(85)90002-8  }}&lt;/ref&gt;  (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family.  Then, in [[Michael Katehakis|Katehakis]] and [[Herbert Robbins|Robbins]]&lt;ref&gt;{{cite journal | last1 = Katehakis | first1 = M.N. | last2 = Robbins | first2 = H.  | year = 1995 | title = Sequential choice from several populations | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 92 | issue = 19| pages =8584–5 | doi = 10.1073/pnas.92.19.8584 | pmid = 11607577 | pmc = 41010  }}&lt;/ref&gt; simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and [[Michael Katehakis|Katehakis]]  in the paper &quot;Optimal adaptive policies for sequential allocation problems&quot;,&lt;ref&gt;{{cite journal | last1 = Burnetas | first1 = A.N. | last2 = Katehakis | first2 = M.N. | year = 1996 | title = Optimal adaptive policies for sequential allocation problems | url = | journal = Advances in Applied Mathematics | volume = 17 | issue = 2| pages =122–142 | doi = 10.1006/aama.1996.0007  }}&lt;/ref&gt; where index based policies  with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., non-parametric) discrete, univariate distributions.

Later in &quot;Optimal adaptive policies for Markov decision processes&quot;&lt;ref&gt;{{cite journal | last1 = Burnetas | first1 = A.N. | last2 = Katehakis | first2 = M.N. | year = 1997 | title = Optimal adaptive policies for Markov decision processes | url = | journal = Math. Oper. Res. | volume = 22 | issue = 1| pages =222–255 | doi = 10.1287/moor.22.1.222  }}&lt;/ref&gt;  Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information,  where the transition law and/or the expected one period rewards may depend on unknown parameters. In this work the explicit form for a class of adaptive policies that possess uniformly maximum convergence rate  properties for the total expected finite horizon reward, were constructed under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett,&lt;ref&gt;{{cite journal | last1 = Tewari | first1 = A. | last2 = Bartlett | first2 = P.L. | year = 2008 | title = Optimistic linear programming gives logarithmic regret for irreducible MDPs | url = http://books.nips.cc/papers/files/nips20/NIPS2007_0673.pdf | format=PDF| journal = Advances in Neural Information Processing Systems | volume = 20 | issue = | pages =  | citeseerx=10.1.1.69.5482 }}&lt;/ref&gt; Ortner&lt;ref&gt;{{cite journal | last1 = Ortner | first1 = R. | year = 2010 | title = Online regret bounds for Markov decision processes with deterministic transitions | url = | journal = Theoretical Computer Science | volume = 411 | issue = 29| pages =2684–2695 | doi = 10.1016/j.tcs.2010.04.005  }}&lt;/ref&gt; Filippi,  Cappé, and Garivier,&lt;ref&gt;Filippi, S. and Cappé, O. and Garivier, A. (2010). &quot;Online regret bounds for Markov decision processes with deterministic transitions&quot;, ''Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on'', pp. 115--122&lt;/ref&gt; and Honda and Takemura.&lt;ref&gt;{{cite journal | last1=Honda | first1= J.|last2= Takemura  | first2= A. |year=2011|title=An asymptotically optimal policy for finite support models in the multi-armed bandit problem|journal=Machine learning|volume=85|issue=3|pages= 361–391 | arxiv=0905.2776 |doi=10.1007/s10994-011-5257-4}}&lt;/ref&gt;

===Approximate solutions===
Many strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below.

====Semi-uniform strategies====
Semi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a [[Greedy algorithm|greedy]] behavior where the ''best'' lever (based on previous observations) is always pulled except when a (uniformly) random action is taken.

* '''Epsilon-greedy strategy''':&lt;ref&gt;Sutton, R. S. &amp; Barto, A. G. 1998 Reinforcement learning: an introduction. Cambridge, MA: MIT Press.&lt;/ref&gt; The best lever is selected for a proportion &lt;math&gt;1 - \epsilon&lt;/math&gt; of the trials, and a lever is selected at random (with uniform probability) for a proportion &lt;math&gt;\epsilon&lt;/math&gt;. A typical parameter value might be &lt;math&gt;\epsilon = 0.1&lt;/math&gt;, but this can vary widely depending on circumstances and predilections.
* '''Epsilon-first strategy'''{{Citation needed|date=March 2015}}: A pure exploration phase is followed by a pure exploitation phase. For &lt;math&gt;N&lt;/math&gt; trials in total, the exploration phase occupies &lt;math&gt;\epsilon N&lt;/math&gt; trials and the exploitation phase &lt;math&gt;(1 - \epsilon) N&lt;/math&gt; trials. During the exploration phase, a lever is randomly selected (with uniform probability); during the exploitation phase, the best lever is always selected.
* '''Epsilon-decreasing strategy'''{{Citation needed|date=March 2015}}: Similar to the epsilon-greedy strategy, except that the value of &lt;math&gt;\epsilon&lt;/math&gt; decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.
* '''Adaptive epsilon-greedy strategy based on value differences (VDBE)''': Similar to the epsilon-decreasing strategy, except that  epsilon is reduced on basis of the learning progress instead of manual tuning (Tokic, 2010).&lt;ref name=&quot;Tokic2010&quot;/&gt; High fluctuations in the value estimates lead to a high epsilon (high exploration, low exploitation); low fluctuations to a low epsilon (low exploration, high exploitation). Further improvements can be achieved by a [[softmax function|softmax]]-weighted action selection in case of exploratory actions (Tokic &amp; Palm, 2011).&lt;ref name=&quot;TokicPalm2011&quot;/&gt;
* '''Contextual-Epsilon-greedy strategy''': Similar to the epsilon-greedy strategy, except that the value of &lt;math&gt;\epsilon&lt;/math&gt; is computed regarding the situation in experiment processes, which let the algorithm be Context-Aware. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which situation is most relevant for exploration or exploitation, resulting in highly explorative behavior when the situation is not critical and highly exploitative behavior at critical situation.&lt;ref name=&quot;Bouneffouf2012&quot;/&gt;

====Probability matching strategies====
Probability matching strategies reflect the idea that the number of pulls for a given lever should ''match'' its actual probability of being the optimal lever.  Probability matching strategies are also known as [[Thompson sampling]] or Bayesian Bandits,&lt;ref name=&quot;Scott2010&quot;/&gt; and are surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative.

Probability matching strategies also admit solutions to so-called contextual bandit problems.

====Pricing strategies====
Pricing strategies establish a ''price'' for each lever. For example, as illustrated with the POKER algorithm,&lt;ref name=&quot;Vermorel2005&quot;/&gt; the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge. The lever of highest price is always pulled.

====Strategies with ethical constraints====
These strategies minimize the assignment of any patient to an inferior arm ([[Medical ethics|&quot;physician's duty&quot;]]).  In a typical case, they minimize expected successes lost (ESL), that is, the expected number of favorable outcomes that were missed because of assignment to an arm later proved to be inferior.  Another version minimizes resources wasted on any inferior, more expensive, treatment.&lt;ref name=&quot;WHP&quot; /&gt;

==Contextual bandit==
A particularly useful version of the multi-armed bandit is the contextual multi-armed bandit problem. In this problem, in each iteration an agent has to choose between arms. Before making the choice, the agent sees a d-dimensional feature vector (context vector),
associated with the current iteration. The learner uses these context vectors along with the rewards of the arms played in the past to make the choice of the arm to play in
the current iteration. Over time, the learner's aim is to collect enough information about how the context vectors and rewards relate to each other, so that it can predict the next best arm to play by looking at the feature vectors.&lt;ref name=&quot;Langford2008&quot; /&gt;

===Approximate solutions for contextual bandit===
Many strategies exist which provide an approximate solution to the contextual bandit problem, and can be put into two broad categories detailed below.

====Online linear classifier====
* '''LinUCB ''(Upper Confidence Bound)'' algorithm''': the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.

====Online non-linear classifier====
* '''UCBogram algorithm''': The nonlinear reward functions are estimated using piecewise constant over a functions using a piecewise constant estimator called ''regressogram'' in [[Nonparametric regression]]. Then, UCB is employed on each constant piece. Successive refinements of the partition of the context space are scheduled or chosen adaptively.&lt;ref name=&quot;RigZee10&quot;/&gt;&lt;ref name=&quot;slivkins11&quot;/&gt;&lt;ref name=&quot;PerRig13&quot;/&gt;
* '''NeuralBandit algorithm''':  In this algorithm several neural networks are trained to modelize the value of rewards knowing the context, and it uses a multi-experts approach to choose online the parameters of multi-layer perceptrons.&lt;ref name=&quot;Robin2014&quot;/&gt;
* '''KernelUCB algorithm''': a kernelized non-linear version of linearUCB, with efficient implementation and finite-time analysis.&lt;ref name=&quot;Valko2014&quot;/&gt;
* '''Bandit Forest algorithm''': a random forest is built and analyzed w.r.t the random forest built knowing the joint distribution of contexts and rewards.&lt;ref&gt;{{Cite journal|last=Féraud|first=Raphaël|last2=Allesiardo|first2=Robin|last3=Urvoy|first3=Tanguy|last4=Clérot|first4=Fabrice|date=2016|title=Random Forest for the Contextual Bandit Problem|url=http://jmlr.org/proceedings/papers/v51/feraud16.html|journal=AISTATS|doi=|pmid=|access-date=}}&lt;/ref&gt;

===Constrained contextual bandit===
In practice, there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials. Constrained contextual bandit (CCB) is such a model that considers both the time and budget constraints in a multi-armed bandit setting.
A. Badanidiyuru et al.&lt;ref name=&quot;Badanidiyuru2014COLT&quot;/&gt; first studied contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a &lt;math&gt;O(\sqrt{T})&lt;/math&gt; regret is achievable. However, their work focuses on a finite set of policies, and the algorithm is computationally inefficient.

[[File:Framework of UCB-ALP for Constrained Contextual Bandits.jpg|thumbnail|Framework of UCB-ALP for constrained contextual bandits]]
A simple algorithm with logarithmic regret is proposed in:&lt;ref name=&quot;Wu2015UCBALP&quot;/&gt;
* '''UCB-ALP algorithm''': The framework of UCB-ALP is shown in the right figure. UCB-ALP is a simple algorithm that combines the UCB method with an Adaptive Linear Programming (ALP) algorithm, and can be easily deployed in practical systems. It is the first work that show how to achieve logarithmic regret in constrained contextual bandits. Although&lt;ref name=&quot;Wu2015UCBALP&quot;/&gt; is devoted to a special case with single budget constraint and fixed cost, the results shed light on the design and analysis of algorithms for more general CCB problems.

==Adversarial bandit==
Another variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem&lt;ref&gt;Burtini (2015)&lt;/ref&gt; as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems.

==With known trend==
Multi-armed bandit problem with known trend is a variant of the multi-armed bandit model, where the gambler knows the shape of the reward function of each arm but not its distribution. This new problem is motivated by different on-line problems like active learning, music and interface recommendation applications, where when an arm is sampled by the model the received reward change according to a known trend. By adapting the standard multi-armed bandit algorithm UCB1 to take advantage of this setting, the authors in&lt;ref name=&quot;AUCBDB&quot;/&gt; propose the new algorithm named Adjusted Upper Confidence Bound (A-UCB) that assumes a stochastic model and provide upper bounds of the regret which compare favorably with the ones of UCB1.

==Infinite-armed bandit==
In the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable &lt;math&gt;K&lt;/math&gt;. In the infinite armed case, introduced by Agarwal (1995), the &quot;arms&quot; are a continuous variable in &lt;math&gt;K&lt;/math&gt; dimensions.

==Dueling bandit==
The dueling bandit variant was introduced by Yue et al. (2012)&lt;ref name=&quot;YueEtAll2012&quot;/&gt; to model the exploration-versus-exploitation tradeoff for relative feedback.
In this variant the gambler is allowed to pull two levers at the same time, but they only get a binary feedback telling which lever provided the best reward. The difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of their actions.
The earliest algorithms for this problem are InterleaveFiltering,&lt;ref name=&quot;YueEtAll2012&quot;/&gt; Beat-The-Mean.&lt;ref name=&quot;Yue2011ICML:BTM&quot;/&gt;
The relative feedback of dueling bandits can also lead to [[voting paradoxes]]. A solution is to take the [[Condorcet winner]] as a reference.&lt;ref name = &quot;Urvoy2013ICML:SAVAGE&quot;/&gt;

More recently, researchers have generalized algorithms from traditional MAB to dueling bandits: Relative Upper Confidence Bounds (RUCB),&lt;ref name=&quot;Zoghi2014ICML:RUCB&quot;/&gt; Relative EXponential weighing (REX3),&lt;ref name=&quot;Gajane2015ICML:REX3&quot;/&gt;
Copeland Confidence Bounds (CCB),&lt;ref name=&quot;Zoghi2015NIPS:CDB&quot;/&gt; Relative Minimum Empirical Divergence (RMED),&lt;ref name=&quot;Komiyama2015COLT:DB&quot;/&gt; and Double Thompson Sampling (DTS).&lt;ref name=&quot;Wu2016DTS&quot;/&gt;

== Non-stationary bandit ==
Garivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB&lt;ref&gt;Discounted UCB, Levente Kocsis, Csaba Szepesvári, 2006&lt;/ref&gt; and Sliding-Window UCB.&lt;ref&gt;On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems, Garivier and Moulines, 2008 &lt;http://arxiv.org/abs/0805.3415&gt;&lt;/ref&gt;

Another work by Burtini et al. introduces a weighted least squares Thompson sampling approach, which proves beneficial in both the known and unknown non-stationary cases.&lt;ref&gt;Improving Online Marketing Experiments with Drifting Multi-armed Bandits, Giuseppe Burtini, Jason Loeppky, Ramon Lawrence, 2015 &lt;http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=Dx2xXEB0PJE=&amp;t=1&gt;&lt;/ref&gt;

==Clustering bandit==
The clustering of bandits (i.e., CLUB) was introduced by Gentile and Li and Zappela (ICML 2014),&lt;ref name=&quot;GLZ2014CLUB&quot;/&gt; with a novel algorithmic approach to content recommender systems based on adaptive clustering of exploration-exploitation (&quot;bandit&quot;) strategies. They provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Their experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.

==Distributed bandit==
The distributed clustering of bandits (i.e., DCCB) was introduced by Korda and Szorenyi and Li (ICML 2016),&lt;ref name=&quot;KSL2016DCCB&quot;/&gt; they provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, they assume that all the peers are solving the same linear bandit problem, and prove that their algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, they assume that there are clusters of peers solving the same bandit problem within each cluster as in,&lt;ref name=&quot;GLZ2014CLUB&quot;/&gt; and they prove that their algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, they demonstrate the performance of proposed algorithms compared to the state-of-the-art.

==Collaborative bandit==
The collaborative filtering bandits (i.e., COFIBA) was introduced by Li and Karatzoglou and Gentile (SIGIR 2016),&lt;ref name=&quot;LKG2016COFIBA&quot;/&gt; where the classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, they investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings.&lt;ref name=&quot;GLZ2014CLUB&quot;/&gt; Their algorithm (COFIBA, pronounced as &quot;Coffee Bar&quot;) takes into account the collaborative effects&lt;ref name=&quot;LKG2016COFIBA&quot;/&gt; that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. They provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. They also provide a regret analysis within a standard linear stochastic noise setting.

==Spatially correlated bandit==
The spatially correlated multi-armed bandit (SCMAB) was introduced by Wu, Schulz, Speekenbrink, Nelson, and Meder (2017),&lt;ref name=&quot;wu2017mapping&quot;/&gt; to study how humans generalize from observed to unobserved outcomes in limited horizon search.  An underlying reward function was used to map the spatial location of each playable arm of the bandit to a mean reward, where the correlation between rewards decreased as an exponential function of the distance between two arms (generated using a [[radial basis function kernel]]). So far, SCMABs have been used to study human behavior, with the finding that human choices are best predicted by combining [[Gaussian process regression]] as a function learning mechanism with Upper confidence bound sampling.&lt;ref name=&quot;wu2017mapping&quot;/&gt;

==Combinatorial bandit==
The Combinatorial Multiarmed Bandit (CMAB) problem&lt;ref name=&quot;gai2010learning&quot;/&gt;&lt;ref name=&quot;chen2013combinatorial&quot;/&gt;&lt;ref name=&quot;ontanon2017combinatorial&quot;/&gt; arises when instead of a single discrete variable to choose from, an agent needs to choose values for a set of variables. Assuming each variable is discrete, the number of possible choices per iteration is exponential in the number of variables. Several CMAB settings have been studied in the literature, from settings where the variables are binary&lt;ref name=&quot;chen2013combinatorial&quot;/&gt; to more general setting where each variable can take an arbitrary set of values.&lt;ref name=&quot;ontanon2017combinatorial&quot;/&gt;

==See also==
* [[Gittins index]]&amp;nbsp;– a powerful, general strategy for analyzing bandit problems.
* [[Greedy algorithm]]
* [[Optimal stopping]]
* [[Search theory]]

==References==
&lt;references&gt;

&lt;ref name=&quot;slivkins11&quot;&gt;
{{citation
 | last   = Slivkins
 | first  =  Aleksandrs
 | series = Conference on Learning Theory, COLT 2011
 | title  = Contextual bandits with similarity information.
 | year   = 2011
}}
&lt;/ref&gt;

&lt;ref name=&quot;RigZee10&quot;&gt;
{{citation
 | last1  = Rigollet
 | first1 = Philippe
 | last2  = Zeevi
 | first2 = Assaf
 | series = Conference on Learning Theory, COLT 2010
 | title  = Nonparametric Bandits with Covariates
 | year   = 2010
}}
&lt;/ref&gt;

&lt;ref name=&quot;PerRig13&quot;&gt;
{{citation
 | last1   = Perchet
 | first1  =  Vianney
 | last2   = Rigollet
 | first2  = Philippe
 | journal = [[Annals of Statistics]]
 | title  = The multi-armed bandit problem with covariates
 | volume = 41
 | issue  = 2
 | year   = 2013
 | doi=10.1214/13-aos1101
 | pages=693–721
}}
&lt;/ref&gt;

&lt;ref name=&quot;Valko2014&quot;&gt;
{{citation
 | author1 = Michal Valko
 | author2 = Nathan Korda
 | author3 = Rémi Munos
 | author4 = Ilias Flaounas
 | author5 = Nello Cristianini
 | series  = 29th Conference on Uncertainty in Artificial Intelligence (UAI 2013) and (JFPDA 2013).
 | title   = Finite-Time Analysis of Kernelised Contextual Bandits
 | arxiv = 1309.6869| year    = 2013
}}
&lt;/ref&gt;

&lt;ref name=&quot;Gittins89&quot;&gt;
{{citation
 | last        = Gittins
 | first       = J. C.
 | author-link = John C. Gittins
 | isbn        = 0-471-92059-2
 | location    = Chichester
 | publisher   = John Wiley &amp; Sons, Ltd.
 | series      = Wiley-Interscience Series in Systems and Optimization.
 | title       = Multi-armed bandit allocation indices
 | year        = 1989
}}
&lt;/ref&gt;

&lt;ref name=&quot;BF&quot;&gt;
{{citation
 | last1        = Berry
 | first1       = Donald A.
 | author1-link = Don Berry (statistician)
 | last2        = Fristedt
 | first2       = Bert
 | isbn         = 0-412-24810-7
 | location     = London
 | publisher    = Chapman &amp; Hall
 | series       = Monographs on Statistics and Applied Probability
 | title        = Bandit problems: Sequential allocation of experiments
 | year         = 1985
}}
&lt;/ref&gt;

&lt;ref name=&quot;Whittle79&quot;&gt;
{{citation
 | last        = Whittle
 | first       = Peter
 | author-link = Peter Whittle (mathematician)
 | journal     = [[Journal of the Royal Statistical Society]]
 | series      = Series B
 | page        = 165
 | title       = Discussion of Dr Gittins' paper
 | volume      = 41
 | issue       = 2
 | year        = 1979
 | jstor       = 2985029
}}
&lt;/ref&gt;

&lt;ref name=&quot;Whittle81&quot;&gt;
{{citation
 | last        = Whittle
 | first       = Peter
 | author-link = Peter Whittle (mathematician)
 | doi         = 10.1214/aop/1176994469
 | journal     = Annals of Probability
 | pages       = 284–292
 | title       = Arm-acquiring bandits
 | volume      = 9
 | year        = 1981
 | issue       = 2
}}
&lt;/ref&gt;

&lt;ref name=&quot;Whittle88&quot;&gt;
{{citation
 | last        = Whittle
 | first       = Peter
 | author-link = Peter Whittle (mathematician)
 | mr          = 974588
 | journal     = Journal of Applied Probability
 | pages       = 287–298
 | title       = Restless bandits: Activity allocation in a changing world
 | volume      = 25A
 | year        = 1988
 | doi=10.2307/3214163
}}
&lt;/ref&gt;

&lt;ref name=&quot;WHP&quot;&gt;
{{Citation
 | first      = William H.
 | last       = Press
 | year       = 2009
 | url        = http://www.pnas.org/content/106/52/22387
 | title      = Bandit solutions provide unified ethical models for randomized clinical trials and comparative effectiveness research
 | journal    = Proceedings of the National Academy of Sciences
 | volume     = 106
 | pages      = 22387–22392
 | pmid       = 20018711
 | doi        = 10.1073/pnas.0912378106
 | issue      = 52
 | pmc        = 2793317
 | postscript = .
}}
&lt;/ref&gt;

&lt;ref name=&quot;Scott2010&quot;&gt;
{{citation
 | last    = Scott
 | first   = S.L.
 | doi     = 10.1002/asmb.874
 | journal = Applied Stochastic Models in Business and Industry
 | pages   = 639–658
 | title   = A modern Bayesian look at the multi-armed bandit
 | volume  = 26
 | year    = 2010
 | issue   = 2
}}
&lt;/ref&gt;

&lt;ref name=&quot;Vermorel2005&quot;&gt;
{{citation
 | url       = http://bandit.sourceforge.net/Vermorel2005poker.pdf
 | last1     = Vermorel
 | first1    = Joannes
 | last2     = Mohri
 | first2    = Mehryar
 | publisher = Springer
 | series    = In European Conference on Machine Learning
 | pages     = 437–448
 | title     = Multi-armed bandit algorithms and empirical evaluation
 | year      = 2005
}}
&lt;/ref&gt;

&lt;ref name=&quot;Robin2014&quot;&gt;
{{citation
 | last1        = Allesiardo
 | first1       = Robin
 | last2        = Féraud
 | first2       = Raphaël
 | last3        = Djallel
 | first3       = Bouneffouf
 | contribution = A Neural Networks Committee for the Contextual Bandit Problem
 | pages        = 374–381
 | publisher    = Springer
 | series       = [[Lecture Notes in Computer Science]]
 | title        = Neural Information Processing - 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings
 | volume       = 8834
 | year         = 2014
 | isbn         = 978-3-319-12636-4
 | doi=10.1007/978-3-319-12637-1_47
}}
&lt;/ref&gt;

&lt;ref name=&quot;Bouneffouf2012&quot;&gt;
{{Cite book | last1 = Bouneffouf | first1 = D. | last2 = Bouzeghoub | first2 = A. | last3 = Gançarski | first3 = A. L. | doi = 10.1007/978-3-642-34487-9_40 | chapter = A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System | title = Neural Information Processing | series = Lecture Notes in Computer Science | volume = 7665 | pages = 324 | year = 2012 | isbn = 978-3-642-34486-2 | pmid =  | pmc = }}
&lt;/ref&gt;

&lt;ref name=&quot;Tokic2010&quot;&gt;
{{citation
 | last1     = Tokic
 | first1    = Michel
 | chapter   = Adaptive ε-greedy exploration in reinforcement learning based on value differences
 | doi       = 10.1007/978-3-642-16111-7_23
 | pages     = 203–210
 | publisher = Springer-Verlag
 | series    = Lecture Notes in Computer Science
 | title     = KI 2010: Advances in Artificial Intelligence
 | volume    = 6359
 | year      = 2010
 | url       = http://www.tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf
 | isbn      = 978-3-642-16110-0}}.
&lt;/ref&gt;

&lt;ref name=&quot;TokicPalm2011&quot;&gt;
{{citation
 | last1     = Tokic
 | first1    = Michel
 | last2     = Palm
 | first2    = Günther
 | chapter   = Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax
 | pages     = 335–346
 | publisher = Springer-Verlag
 | series    = Lecture Notes in Computer Science
 | title     = KI 2011: Advances in Artificial Intelligence
 | volume    = 7006
 | year      = 2011
 | url       = http://www.tokic.com/www/tokicm/publikationen/papers/KI2011.pdf
 | isbn      = 978-3-642-24455-1}}.
&lt;/ref&gt;

&lt;ref name=&quot;BrochuHoffmandeFreitas&quot;&gt;
{{citation
 | last1  = Brochu
 | first1 = Eric
 | last2  = Hoffman
 | first2 = Matthew W.
 | last3  = de Freitas
 | first3 = Nando
 | arxiv    = 1009.5419| date   = September 2010
 | title  = Portfolio Allocation for Bayesian Optimization}}
&lt;/ref&gt;

&lt;ref name=&quot;ShenWangJiangZha&quot;&gt;
{{citation
 | last1  = Shen
 | first1 = Weiwei
 | last2  = Wang
 | first2 = Jun
 | last3  = Jiang
 | first3 = Yu-Gang
 | last4  = Zha
 | first4 = Hongyuan
 | journal = Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI2015)
 | url    = http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPDFInterstitial/10972/10798
 | date   = 2015
 | title  = Portfolio Choices with Orthogonal Bandit Learning}}
&lt;/ref&gt;
&lt;ref name=&quot;Langford2008&quot;&gt;
{{citation
 | url       = http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information
 | last1     = Langford
 | first1    = John
 | last2     = Zhang
 | first2    = Tong
 | publisher = Curran Associates, Inc.
 | pages     = 817–824
 | title     = Advances in Neural Information Processing Systems 20
 | chapter   = The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits
 | year      = 2008
}}
&lt;/ref&gt;

&lt;ref name=&quot;Badanidiyuru2014COLT&quot;&gt;
{{citation
 | last1     = Badanidiyuru
 | first1    = A.
 | last2     = Langford
 | first2    = J.
 | last3     = Slivkins
 | first3    = A.
 | title     = Proceeding of Conference on Learning Theory (COLT)
 | chapter   = Resourceful contextual bandits
 | year      = 2014
}}
&lt;/ref&gt;

&lt;ref name=&quot;Wu2015UCBALP&quot;&gt;
{{citation
 | url       = https://papers.nips.cc/paper/6008-algorithms-with-logarithmic-or-sublinear-regret-for-constrained-contextual-bandits
 | last1     = Wu
 | first1    = Huasen
 | last2     = Srikant
 | first2    = R.
 | last3     = Liu
 | first3    = Xin
 | last4     = Jiang
 | first4    = Chong
 | title     = Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits
 | journal   = The 29th Annual Conference on Neural Information Processing Systems (NIPS)
 | year      = 2015
}}
&lt;/ref&gt;

&lt;ref name=&quot;YueEtAll2012&quot;&gt;
{{citation
 | url     = http://www.sciencedirect.com/science/article/pii/S0022000012000281
 | last1   = Yue
 | first1  = Yisong
 | last2   = Broder
 | first2  = Josef
 | last3   = Kleinberg
 | first3  = Robert
 | last4   = Joachims
 | first4  = Thorsten
 | volume  = 78
 | issue   = 5
 | pages   = 1538–1556
 | title   = Journal of Computer and System Sciences
 | chapter = The K-armed Dueling Bandits Problem
 | year    = 2012
 | doi=10.1016/j.jcss.2011.12.028
}}
&lt;/ref&gt;

&lt;ref name=&quot;Yue2011ICML:BTM&quot;&gt;
{{citation
 | last1   = Yue
 | first1  = Yisong
 | last2   = Joachims
 | first2  = Thorsten
 | title   = Proceedings of ICML'11
 | chapter = Beat the Mean Bandit
 | year    = 2011
}}
&lt;/ref&gt;

&lt;ref name=&quot;Urvoy2013ICML:SAVAGE&quot;&gt;
{{citation
 | url     = http://www.jmlr.org/proceedings/papers/v28/urvoy13.pdf
 | last1   = Urvoy
 | first1  = Tanguy
 | last2   =  Clérot
 | first2  = Fabrice
 | last3   = Féraud
 | first3  = Raphaël
 | last4   = Naamane
 | first4  = Sami
 | title   = Proceedings of the 30th International Conference on Machine Learning (ICML-13)
 | chapter = Generic Exploration and K-armed Voting Bandits
 | year    = 2013
}}
&lt;/ref&gt;

&lt;ref name=&quot;Zoghi2014ICML:RUCB&quot;&gt;
{{citation
 | url     = http://www.jmlr.org/proceedings/papers/v32/zoghi14.pdf
 | last1   = Zoghi
 | first1  = Masrour
 | last2   =  Whiteson
 | first2  = Shimon
 | last3   = Munos
 | first3  = Remi
 | last4   = Rijke
 | first4  = Maarten D
 | title   = Proceedings of the 31st International Conference on Machine Learning (ICML-14)
 | chapter = Relative Upper Confidence Bound for the $K$-Armed Dueling Bandit Problem
 | year    = 2014
}}
&lt;/ref&gt;

&lt;ref name=&quot;Gajane2015ICML:REX3&quot;&gt;
{{citation
 | url     = http://jmlr.org/proceedings/papers/v37/gajane15.pdf
 | last1   = Gajane
 | first1  = Pratik
 | last2  = Urvoy
 | first2  = Tanguy
 | last3   =  Clérot
 | first3  = Fabrice
 | title   = Proceedings of the 32nd International Conference on Machine Learning (ICML-15)
 | chapter = A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits
 | year    = 2015
}}
&lt;/ref&gt;

&lt;ref name=&quot;Zoghi2015NIPS:CDB&quot;&gt;
{{citation
 | arxiv     = 1506.00312| last1   = Zoghi
 | first1  = Masrour
 | last2   =  Karnin
 | first2  =  Zohar S
 | last3   = Whiteson
 | first3  =  Shimon
 | last4   =  Rijke
 | first4  = Maarten D
 | title   = Advances in Neural Information Processing Systems, NIPS'15
 | chapter = Copeland Dueling Bandits
 | year    = 2015
}}
&lt;/ref&gt;

&lt;ref name=&quot;Komiyama2015COLT:DB&quot;&gt;
{{citation
 | url     = http://jmlr.org/proceedings/papers/v40/Komiyama15.pdf
 | last1   = Komiyama
 | first1  = Junpei
 | last2   =  Honda
 | first2  =  Junya
 | last3   =  Kashima
 | first3  = Hisashi
 | last4   =  Nakagawa
 | first4  =  Hiroshi
 | title   = Proceedings of the 28th Conference on Learning Theory
 | chapter = Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem
 | year    = 2015
}}
&lt;/ref&gt;

&lt;ref name=&quot;Wu2016DTS&quot;&gt;
{{citation
 | arxiv       = 1604.07101| last1     = Wu
 | first1    = Huasen
 | last2     = Liu
 | first2    = Xin
 | title     = Double Thompson Sampling for Dueling Bandits
 | journal   = The 30th Annual Conference on Neural Information Processing Systems (NIPS)
 | year      = 2016
}}
&lt;/ref&gt;

&lt;ref name=&quot;AUCBDB&quot;&gt;
{{citation
 | last1   = Bouneffouf
 | first1  = Djallel
 | last2   = Feraud
 | first2  = Raphael
 | title   = Neurocomputing
 | chapter = Multi-armed bandit problem with known trend
 | year    = 2016
}}
&lt;/ref&gt;

&lt;ref name=&quot;GLZ2014CLUB&quot;&gt;
{{citation
 | arxiv     = 1401.8257| last1   = Gentile
 | first1  = Claudio
 | last2   =  Li
 | first2  =  Shuai
 | last3   =  Zappella
 | first3  = Giovanni
 | title   = The 31st International Conference on Machine Learning, Journal of Machine Learning Research (ICML 2014)
 | chapter = Online Clustering of Bandits
 | year    = 2014
}}
&lt;/ref&gt;

&lt;ref name=&quot;KSL2016DCCB&quot;&gt;
{{citation
 | arxiv     = 1604.07706| last1   = Korda
 | first1  = Nathan Korda
 | last2   =  Szorenyi
 | first2  =  Balazs
 | last3   =  Li
 | first3  = Shuai
 | title   = The 33rd International Conference on Machine Learning, Journal of Machine Learning Research (ICML 2016)
 | chapter = Distributed Clustering of Linear Bandits in Peer to Peer Networks
 | year    = 2016
}}
&lt;/ref&gt;

&lt;ref name=&quot;LKG2016COFIBA&quot;&gt;
{{citation
 | arxiv     = 1502.03473| last1   = Li
 | first1  = Shuai
 | last2   =  Alexandros
 | first2  =  Karatzoglou
 | last3   =  Gentile
 | first3  = Claudio
 | title   = The 39th International ACM SIGIR Conference on Information Retrieval (SIGIR 2016)
 | chapter = Collaborative Filtering Bandits
 | year    = 2016
}}
&lt;/ref&gt;

&lt;ref name=&quot;farias2011irrevocable&quot;&gt;
{{citation
 | url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.380.6983&amp;rep=rep1&amp;type=pdf
 | title=The irrevocable multiarmed bandit problem
 | last1 = Farias | first1 = Vivek F | first2 = Madan | last2 = Ritesh
 | journal=[[Operations Research (journal)|Operations Research]]
 | volume=59
 | number=2
 | pages=383–399
 | year= 2011
}}
&lt;/ref&gt;

&lt;ref name=&quot;wu2017mapping&quot;&gt;
{{citation
 | url = http://biorxiv.org/content/biorxiv/early/2017/04/28/106286.full.pdf
 | chapter=Mapping the unknown: The spatially correlated multi-armed bandit
 | last1 = Wu | first1 = Charley M | first2 = Eric | last2 = Schulz | first3 = Maarten | last3 = Speekenbrink | first4 = Jonathan D | last4 = Nelson | first5 = Björn | last5 = Meder
 | title=Proceedings of the 39th Annual Cognitive Science Society
 | year= 2017
}}
&lt;/ref&gt;

&lt;ref name=&quot;gai2010learning&quot;&gt;
{{citation
|author=Gai, Y. and Krishnamachari, B. and Jain, R.
|title=Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation
|booktitle=2010 IEEE Symposium on New Frontiers in Dynamic Spectrum
|pages=1–9
|year=2010
}}
&lt;/ref&gt;

&lt;ref name=&quot;chen2013combinatorial&quot;&gt;
{{citation
|author=Chen, Wei and Wang, Yajun and Yuan, Yang
|title=Combinatorial multi-armed bandit: General framework and applications
|booktitle=Proceedings of the 30th International Conference on Machine Learning (ICML 2013)
|pages=151–159
|year=2013
}}
&lt;/ref&gt;

&lt;ref name=&quot;ontanon2017combinatorial&quot;&gt;
{{citation
|author=Santiago Ontañón
|title= Combinatorial Multi-armed Bandits for Real-Time Strategy Games
|journal= Journal of Artificial Intelligence Research
|volume=58
|year=2017
|pages=665–702
}}
&lt;/ref&gt;

&lt;/references&gt;

==Further reading==
*{{Cite journal | last1 = Guha | first1 = S. | last2 = Munagala | first2 = K. | last3 = Shi | first3 = P. | title = Approximation algorithms for restless bandit problems | doi = 10.1145/1870103.1870106 | journal = Journal of the ACM | volume = 58 | pages = 1–50 | year = 2010 | pmid =  | pmc = }}
*{{citation
 | last1 = Dayanik | first1 = S.
 | last2 = Powell | first2 = W.
 | last3 = Yamazaki | first3 = K.
 | doi = 10.1239/aap/1214950209
 | issue = 2
 | journal = Advances in Applied Probability
 | pages = 377–400
 | title = Index policies for discounted bandit problems with availability constraints
 | volume = 40
 | year = 2008}}.
*{{citation
 | last = Powell | first = Warren B.
 | contribution = Chapter 10
 | isbn = 0-470-17155-3
 | location = New York
 | publisher = John Wiley and Sons
 | title = Approximate Dynamic Programming: Solving the Curses of Dimensionality
 | year = 2007}}.
*{{citation
 | last = Robbins | first = H. | author-link = Herbert Robbins
 | doi = 10.1090/S0002-9904-1952-09620-8
 | journal = [[Bulletin of the American Mathematical Society]]
 | pages = 527–535
 | title = Some aspects of the sequential design of experiments
 | volume = 58  | year = 1952  | issue = 5}}.
*{{citation
 | last1 = Sutton | first1 = Richard
 | last2 = Barto | first2 = Andrew
 | isbn = 0-262-19398-1
 | publisher = MIT Press
 | title = Reinforcement Learning
 | url = http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html
 | year = 1998}}.

*{{citation
 | last = Allesiardo  | first = Robin
 | contribution = A Neural Networks Committee for the Contextual Bandit Problem
 | pages = 374–381
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Neural Information Processing - 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings
 | volume = 8834
 | year = 2014
 | isbn = 978-3-319-12636-4
 | doi=10.1007/978-3-319-12637-1_47}}.

*{{citation
 | last = Bouneffouf  | first = Djallel
 | contribution = A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System
 | pages = 324–331
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Neural Information Processing - 19th International Conference, ICONIP 2012, Doha, Qatar, November 12-15,2012, Proceedings, Part III
 | volume = 7665
 | year = 2012
 | url = https://link.springer.com/chapter/10.1007%2F978-3-642-34487-9_40
 | isbn = 978-3-642-34486-2
 | doi=10.1007/978-3-642-34487-9_40}}.
* {{citation
 | last = Weber | first = Richard
 | issue = 4
 | journal = [[Annals of Applied Probability]]
 | pages = 1024–1033
 | title = On the Gittins index for multiarmed bandits
 | volume = 2
 | year = 1992
 | jstor=2959678
 | doi = 10.1214/aoap/1177005588}}.&lt;!-- &quot;The proof from God&quot; according to Whittle's survey of applied probability --&gt;
* {{Citation
|author=[[Michael N. Katehakis|Katehakis, M.]] and C. Derman
|title=Computing Optimal Sequential Allocation Rules in Clinical Trials
|journal=IMS Lecture Notes-Monograph Series
|volume=8
|year=1986
|pages=29–39
|jstor= 4355518
|postscript=.
|doi=10.1214/lnms/1215540286
}}
* {{Citation
|author=[[Michael N. Katehakis|Katehakis, M.]] and  A. F. Veinott, Jr.
|title=The multi-armed bandit problem: decomposition and computation
|journal=Mathematics of Operations Research
|volume=12
|year=1987
|pages=262–268
|jstor= 3689689
|issue=2
|doi=10.1287/moor.12.2.262
|postscript=.
}}

==External links==
*[http://mloss.org/software/view/415/ PyMaBandits], [[Open-Source]] implementation of bandit strategies in Python and Matlab
*[http://bandit.sourceforge.net bandit.sourceforge.net Bandit project ], Open-Source implementation of bandit strategies
*[https://github.com/jkomiyama/banditlib Banditlib], [[Open-Source]] implementation of bandit strategies in C++
* [http://www.cs.washington.edu/research/jair/volume4/kaelbling96a-html/node6.html Leslie Pack Kaelbling and Michael L. Littman (1996). Exploitation versus Exploration: The Single-State Case]
* Tutorial: Introduction to Bandits: Algorithms and Theory. [http://techtalks.tv/talks/54451/ Part1]. [http://techtalks.tv/talks/54455/ Part2].
* [http://www.feynmanlectures.info/exercises/Feynmans_restaurant_problem.html Feynman's restaurant problem], a classic example (with known answer) of the exploitation vs. exploration tradeoff.
* [http://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html Bandit algorithms vs. A-B testing].
* [http://homes.di.unimi.it/~cesabian/Pubblicazioni/banditSurvey.pdf S. Bubeck and N. Cesa-Bianchi A Survey on Bandits]
* [https://arxiv.org/abs/1508.03326 A Survey on Contextual Multi-armed Bandits], a survey/tutorial for Contextual Bandits.
* [https://mpatacchiola.github.io/blog/2017/08/14/dissecting-reinforcement-learning-6.html Blog post on multi-armed bandit strategies, with Python code]

{{DEFAULTSORT:Multi-Armed Bandit}}



</text>
      <sha1>as5z4ideya8c2lne6rcvu5pi1xmzguc</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Inductive logic programming</title>
    <ns>14</ns>
    <id>29003796</id>
    <revision>
      <id>387913459</id>
      <timestamp>2010-09-30T13:39:01Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with ' '</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="60">
</text>
      <sha1>flszufvp92ultg6ftkvt3usal9dq1ay</sha1>
    </revision>
  </page>
  <page>
    <title>Dimensionality reduction</title>
    <ns>0</ns>
    <id>579867</id>
    <revision>
      <id>810019729</id>
      <parentid>810000990</parentid>
      <timestamp>2017-11-12T22:59:27Z</timestamp>
      <contributor>
        <username>Mwtoews</username>
        <id>711150</id>
      </contributor>
      <minor/>
      <comment>Undid revision 810000990 by [[Special:Contributions/SciRe|SciRe]] ([[User talk:SciRe|talk]]) what is the context of this reference?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13214">{{for|dimensional reduction in physics|Dimensional reduction}}
{{Machine learning bar}}
{{Refimprove|date=November 2010}}
In [[machine learning]] and [[statistics]], '''dimensionality reduction''' or '''dimension reduction''' is the process of reducing the number of random variables under consideration&lt;ref&gt;{{Cite journal | last1 = Roweis | first1 = S. T. | last2 = Saul | first2 = L. K. | title = Nonlinear Dimensionality Reduction by Locally Linear Embedding | doi = 10.1126/science.290.5500.2323 | journal = Science | volume = 290 | issue = 5500 | pages = 2323–2326 | year = 2000 | pmid =  11125150| pmc = }}&lt;/ref&gt; by obtaining a set of principal variables. It can be divided into [[feature selection]] and [[feature extraction]].&lt;ref&gt;{{Cite book | last1 = Pudil | first1 = P.| last2 = Novovičová | first2 = J.| editor1-first = Huan | editor1-last = Liu| editor2-first = Hiroshi | editor2-last = Motoda| doi = 10.1007/978-1-4615-5725-8_7 | chapter = Novel Methods for Feature Subset Selection with Respect to Problem Knowledge | title = Feature Extraction, Construction and Selection | pages = 101 | year = 1998 | isbn = 978-1-4613-7622-4 | pmid =  | pmc = }}&lt;/ref&gt;

==Feature selection==
{{main article|Feature selection}}

[[Feature selection]] approaches try to find a subset of the original variables (also called features or attributes). There are three strategies: the ''filter'' strategy (e.g. [[Information gain in decision trees|information gain]]), the ''wrapper'' strategy (e.g. search guided by accuracy), and the ''embedded'' strategy (features are selected to add or be removed while building the model based on the prediction errors). See also [[combinatorial optimization]] problems.

In some cases, [[data analysis]] such as [[Regression analysis|regression]] or [[Statistical classification|classification]] can be done in the reduced space more accurately than in the original space.{{citation needed|date=June 2017}}

==Feature extraction==
{{main article|Feature extraction}}

[[Feature extraction]]  transforms the data in the [[high-dimensional space]] to a space of fewer dimensions. The data transformation may be linear, as in [[principal component analysis]] (PCA), but many [[nonlinear dimensionality reduction]] techniques also exist.&lt;ref&gt;Samet, H. (2006) ''Foundations of Multidimensional and Metric Data Structures''. Morgan Kaufmann. {{ISBN|0-12-369446-9}}&lt;/ref&gt;&lt;ref&gt;C. Ding, X. He, H. Zha, H.D. Simon, Adaptive Dimension Reduction for Clustering High Dimensional Data, Proceedings of International Conference on Data Mining, 2002&lt;/ref&gt; For multidimensional data, [[tensor]] representation can be used in dimensionality reduction through [[multilinear subspace learning]].&lt;ref name=&quot;MSLsurvey&quot;&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt;

===Principal component analysis (PCA)===
{{main article|Principal component analysis}}

The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the [[covariance]] (and sometimes the [[Correlation and dependence|correlation]]) [[matrix (mathematics)|matrix]] of the data is constructed and the [[Eigenvalue, eigenvector and eigenspace|eigen vectors]] on this matrix are computed. The eigen vectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigen vectors can often be interpreted in terms of the large-scale physical behavior of the system {{Citation needed|date=September 2017}} {{Why|date=September 2017}}. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.

===Kernel PCA===
{{main article|Kernel PCA}}
Principal component analysis can be employed in a nonlinear way by means of the [[kernel trick]]. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled [[kernel PCA]].

===Graph-based kernel PCA===
Other prominent nonlinear techniques include [[manifold learning]] techniques such as [[Isomap]], [[locally linear embedding]] (LLE), Hessian LLE, Laplacian eigenmaps, and [[local tangent space alignment]] (LTSA). These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.

More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using [[semidefinite programming]]. The most prominent example of such a technique is [[maximum variance unfolding]] (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.

An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical [[multidimensional scaling]], which is identical to PCA; [[Isomap]], which uses geodesic distances in the data space; [[diffusion map]]s, which use diffusion distances in the data space; [[t-distributed stochastic neighbor embedding]] (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.

A different approach to nonlinear dimensionality reduction is through the use of [[autoencoder]]s, a special kind of feed-forward [[neural network]]s with a bottle-neck [[hidden layer]].&lt;ref&gt;Hongbing Hu, Stephen A. Zahorian, (2010) [http://bingweb.binghamton.edu/~hhu1/paper/Hu2010Dimensionality.pdf &quot;Dimensionality Reduction Methods for HMM Phonetic Recognition,&quot;] ICASSP 2010, Dallas, TX&lt;/ref&gt; The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of [[restricted Boltzmann machine]]s) that is followed by a finetuning stage based on [[backpropagation]].

===Linear discriminant analysis (LDA)===
{{main article|Linear discriminant analysis}}
Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.

===Generalized discriminant analysis (GDA)===
GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space.&lt;ref name=&quot;gda&quot;&gt;G. Baudat, F. Anouar (2000). [https://dx.doi.org/10.1162/089976600300014980 Generalized discriminant analysis using a kernel approach] Neural computation, 12(10), 2385-2404.&lt;/ref&gt;&lt;ref name=&quot;cloudid&quot;&gt;M. Haghighat, S. Zonouz, &amp;  M. Abdel-Mottaleb (2015). [https://dx.doi.org/10.1016/j.eswa.2015.06.025 CloudID: Trustworthy Cloud-based and Cross-Enterprise Biometric Identification]. Expert Systems with Applications, 42(21), 7905–7916.&lt;/ref&gt; Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.

==Dimension reduction==
For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a [[K-nearest neighbors algorithm]] (k-NN) in order to avoid the effects of the [[curse of dimensionality]].
&lt;ref&gt;Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422 &quot;When is “nearest neighbor” meaningful?&quot;]. ''Database Theory—ICDT99'',  217-235&lt;/ref&gt;

[[Feature extraction]] and  dimension reduction can be combined in one step using [[principal component analysis]] (PCA),  [[linear discriminant analysis]] (LDA), or [[canonical correlation analysis]] (CCA) techniques as a pre-processing step followed by clustering by K-NN on [[Feature (machine learning)|feature vectors]] in reduced-dimension space. In [[machine learning]] this process is also called low-dimensional [[embedding]].&lt;ref&gt;{{Cite book | last1 = Shaw | first1 = B. | last2 = Jebara | first2 = T. | doi = 10.1145/1553374.1553494 | chapter = Structure preserving embedding | title = Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09 | pages = 1 | year = 2009 | isbn = 9781605585161 | url = http://www.cs.columbia.edu/~jebara/papers/spe-icml09.pdf| pmid =  | pmc = }}&lt;/ref&gt;

For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional [[time series]]) running a fast '''approximate''' K-NN search using [[locality sensitive hashing]], [[random projection]],&lt;ref&gt;{{Cite book | last1 = Bingham | first1 = E. | last2 = Mannila | first2 = H. | doi = 10.1145/502512.502546 | chapter = Random projection in dimensionality reduction | title = Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '01 | pages = 245 | year = 2001 | isbn = 158113391X | pmid =  | pmc = }}&lt;/ref&gt; &quot;sketches&quot; &lt;ref&gt;Shasha, D High (2004) ''Performance Discovery in Time Series'' Berlin: Springer. {{ISBN|0-387-00857-8}}&lt;/ref&gt; or other high-dimensional similarity search  techniques from the [[VLDB]]  toolbox might be the only feasible option.

== Advantages of dimensionality reduction ==
{{Unreferenced section|date=June 2017}}
# It reduces the time and storage space required.
# Removal of multi-collinearity improves the performance of the machine learning model.
# It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.

== Applications ==
A dimensionality reduction technique that is sometimes used in [[neuroscience]] is [[maximally informative dimensions]],{{citation needed|date=June 2017}} which finds a lower-dimensional representation of a dataset such that as much [[mutual information|information]] as possible about the original data is preserved.

==See also==
{{Recommender systems}}
{{div col start}}
* [[Nearest neighbor search]]
* [[MinHash]]
* [[Information gain in decision trees]]
* [[Semidefinite embedding]]
* [[Multifactor dimensionality reduction]]
* [[Multilinear subspace learning]]
* [[Multilinear PCA]]
* [[Random projection]]
* [[Singular value decomposition]]
* [[Latent semantic analysis]]
* [[Semantic mapping (statistics)|Semantic mapping]]
* [[Topological data analysis]]
* [[Locality sensitive hashing]]
* [[Sufficient dimension reduction]]
* [[Data transformation (statistics)]]
* [[Weighted correlation network analysis]]
* [[Hyperparameter optimization]]
* [[CUR matrix approximation]]
* [[Envelope model]]
* [[Nonlinear dimensionality reduction]]
* [[Sammon mapping]]
* [[Johnson–Lindenstrauss lemma]]
{{div col end}}

==Notes==
{{Reflist}}

==References==
* Fodor, I. (2002) [http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.8.5098 &quot;A survey of dimension reduction techniques&quot;].  Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report UCRL-ID-148494
* Cunningham, P. (2007) [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.98.1478 &quot;Dimension Reduction&quot;] University College Dublin, Technical Report UCD-CSI-2007-7
*{{Cite book | last1 =Zahorian| first1 = Stephen A. | last2 = Hu | first2 = Hongbing| doi = 10.5772/16863 | chapter = Nonlinear Dimensionality Reduction Methods for Use with Automatic Speech Recognition | title = Speech Technologies | year = 2011 | isbn = 978-953-307-996-7 | pmid =  | pmc = }}
* {{cite journal|last1=Lakshmi Padmaja |first1=Dhyaram |last2= Vishnuvardhan |first2=B |date= 18 August 2016 |title= Comparative Study of Feature Subset Selection Methods for Dimensionality Reduction on Scientific Data |pages=31–34 |doi= 10.1109/IACC.2016.16 |url= http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7544805&amp;isnumber=7544788 |accessdate= 7 October 2016 }}

==External links==
* [http://jmlr.csail.mit.edu/papers/special/feature03.html JMLR Special Issue on Variable and Feature Selection]
* [http://bioinfo-out.curie.fr/projects/elmap/ ELastic MAPs]
* [http://www.cs.toronto.edu/~roweis/lle Locally Linear Embedding]
* [https://web.archive.org/web/20040411051530/http://isomap.stanford.edu/ A Global Geometric Framework for Nonlinear Dimensionality Reduction]

{{DEFAULTSORT:Dimension Reduction}}
[[Category:Dimension reduction| ]]
</text>
      <sha1>8hq07udqqx76o25tjscebtclcjnpexh</sha1>
    </revision>
  </page>
  <page>
    <title>Sequence labeling</title>
    <ns>0</ns>
    <id>29288159</id>
    <revision>
      <id>760731398</id>
      <parentid>750842341</parentid>
      <timestamp>2017-01-18T19:30:28Z</timestamp>
      <contributor>
        <ip>2001:4898:80E8:0:0:0:0:486</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3588">{{Refimprove|date=November 2016}}
In [[machine learning]], '''sequence labeling''' is a type of [[pattern recognition]] task that involves the algorithmic assignment of a [[categorical data|categorical]] label to each member of a sequence of observed values.  A common example of a sequence labeling task is [[part of speech tagging]], which seeks to assign a [[part of speech]] to each word in an input sentence or document.  Sequence labeling can be treated as a set of independent [[classification (machine learning)|classification]] tasks, one per member of the sequence.  However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the ''globally'' best set of labels for the entire sequence at once.

As an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described.  Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right.  For example, the word &quot;sets&quot; can be either a noun or verb.  In a phrase like &quot;he sets the books down&quot;, the word &quot;he&quot; is unambiguously a pronoun, and &quot;the&quot; unambiguously a [[determiner (linguistics)|determiner]], and using either of these labels, &quot;sets&quot; can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are.  But in other cases, only one of the adjacent words is similarly helpful.  In &quot;he sets and then knocks over the table&quot;, only the word &quot;he&quot; to the left is helpful (cf. &quot;...picks up the sets and then knocks over...&quot;).  Conversely, in &quot;... and also sets the table&quot; only the word &quot;the&quot; to the right is helpful (cf. &quot;... and also sets of books were ...&quot;).  An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left.

Most sequence labeling algorithms are [[probability theory|probabilistic]] in nature, relying on [[statistical inference]] to find the best sequence.  The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a [[Markov chain]].  This leads naturally to the [[hidden Markov model]] (HMM), one of the most common statistical models used for sequence labeling.  Other common models in use are the [[maximum entropy Markov model]] and [[conditional random field]].

== Evaluation ==
{{Empty section|date=October 2010}}

==Application domains==
{{Empty section|date=October 2010}}

== See also ==
* [[Artificial intelligence]]
* [[Bayesian network]]s (of which HMMs are an example)
* [[Classification (machine learning)]]
* [[Linear dynamical system]], which applies to tasks where the &quot;label&quot; is actually a real number
* [[Machine learning]]
* [[Pattern recognition]]
* [[Sequence mining]]

==References==
{{Reflist}}

==Further reading==

* Erdogan H., [http://www.erdogan.org/publications/erdogan_icmla2010_tutorial_new.pdf]. &quot;Sequence labeling: generative and discriminative approaches, hidden Markov models, conditional random fields and structured SVMs,&quot; ICMLA 2010 tutorial, Bethesda, MD (2010)

{{DEFAULTSORT:Sequence Labeling}}
</text>
      <sha1>hy828xj6vs5eocf62soxhmtphqa9wxj</sha1>
    </revision>
  </page>
  <page>
    <title>Mixture model</title>
    <ns>0</ns>
    <id>871681</id>
    <revision>
      <id>813817830</id>
      <parentid>813817747</parentid>
      <timestamp>2017-12-05T11:26:51Z</timestamp>
      <contributor>
        <ip>178.38.132.48</ip>
      </contributor>
      <comment>/* General mixture model */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="53940">{{distinguish|mixed model}}
{{See also|Mixture distribution}}
In [[statistics]], a '''mixture model''' is a [[probabilistic model]] for representing the presence of [[subpopulation]]s within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the [[mixture distribution]] that represents the [[probability distribution]] of observations in the overall population. However, while problems associated with &quot;mixture distributions&quot; relate to deriving the properties of the overall population from those of the sub-populations, &quot;mixture models&quot; are used to make [[statistical inference]]s about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.

Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of [[unsupervised learning]] or [[cluster analysis|clustering]] procedures. However, not all inference procedures involve such steps.

Mixture models should not be confused with models for [[compositional data]], i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the [[measure (mathematics)|total size]] reading population has been normalized to 1.

==Structure of a mixture model==

===General mixture model===
A typical finite-dimensional mixture model is a [[hierarchical Bayes model|hierarchical model]] consisting of the following components:

*''N'' random variables that are observed, each distributed according to a mixture of ''K'' components, with the components belonging to the same [[parametric family]] of distributions (e.g., all [[normal distribution|normal]], all [[Zipf's law|Zipfian]], etc.) but with different parameters
*''N'' random [[latent variable]]s specifying the identity of the mixture component of each observation, each distributed according to a ''K''-dimensional [[categorical distribution]]
*A set of ''K'' mixture weights, which are probabilities that sum to 1.
*A set of ''K'' parameters, each specifying the parameter of the corresponding mixture component.  In many cases, each &quot;parameter&quot; is actually a set of parameters.  For example, if the mixture components are [[Gaussian distribution]]s, there will be a [[mean]] and [[variance]] for each component. If the mixture components are [[categorical distribution]]s (e.g., when each observation is a token from a finite alphabet of size ''V''), there will be a vector of ''V'' probabilities summing to 1.

In addition, in a [[Bayesian inference|Bayesian setting]], the mixture weights and parameters will themselves be random variables, and [[prior distribution]]s will be placed over the variables.  In such a case, the weights are typically viewed as a ''K''-dimensional random vector drawn from a [[Dirichlet distribution]] (the [[conjugate prior]] of the categorical distribution), and the parameters will be distributed according to their respective conjugate priors.

Mathematically, a basic parametric mixture model can be described as follows:

:&lt;math&gt;
\begin{array}{lcl}
K &amp;=&amp; \text{number of mixture components} \\
N &amp;=&amp; \text{number of observations} \\
\theta_{i=1 \dots K} &amp;=&amp; \text{parameter of distribution of observation associated with component } i \\
\phi_{i=1 \dots K} &amp;=&amp; \text{mixture weight, i.e., prior probability of a particular component } i \\
\boldsymbol\phi &amp;=&amp; K\text{-dimensional vector composed of all the individual } \phi_{1 \dots K} \text{; must sum to 1} \\
z_{i=1 \dots N} &amp;=&amp; \text{component of observation } i \\
x_{i=1 \dots N} &amp;=&amp; \text{observation } i \\
F(x|\theta) &amp;=&amp; \text{probability distribution of an observation, parametrized on } \theta \\
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N}|z_{i=1 \dots N} &amp;\sim&amp; F(\theta_{z_i})
\end{array}
&lt;/math&gt;

In a Bayesian setting, all parameters are associated with random variables, as follows:

:&lt;math&gt;
\begin{array}{lcl}
K,N &amp;=&amp; \text{as above} \\
\theta_{i=1 \dots K}, \phi_{i=1 \dots K}, \boldsymbol\phi &amp;=&amp; \text{as above} \\
z_{i=1 \dots N}, x_{i=1 \dots N}, F(x|\theta) &amp;=&amp; \text{as above} \\
\alpha &amp;=&amp; \text{shared hyperparameter for component parameters} \\
\beta &amp;=&amp; \text{shared hyperparameter for mixture weights} \\
H(\theta|\alpha) &amp;=&amp; \text{prior probability distribution of component parameters, parametrized on } \alpha \\
\theta_{i=1 \dots K} &amp;\sim&amp; H(\theta|\alpha) \\
\boldsymbol\phi &amp;\sim&amp; \operatorname{Symmetric-Dirichlet}_K(\beta) \\
z_{i=1 \dots N}|\boldsymbol\phi &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N}|z_{i=1 \dots N},\theta_{i=1 \dots K} &amp;\sim&amp; F(\theta_{z_i})
\end{array}
&lt;/math&gt;

This characterization uses ''F'' and ''H'' to describe arbitrary distributions over observations and parameters, respectively.  Typically ''H'' will be the [[conjugate prior]] of ''F''.  The two most common choices of ''F'' are [[Gaussian distribution|Gaussian]] aka &quot;[[normal distribution|normal]]&quot; (for real-valued observations) and [[categorical distribution|categorical]] (for discrete observations).  Other common possibilities for the distribution of the mixture components are:
*[[Binomial distribution]], for the number of &quot;positive occurrences&quot; (e.g., successes, yes votes, etc.) given a fixed number of total occurrences
*[[Multinomial distribution]], similar to the binomial distribution, but for counts of multi-way occurrences (e.g., yes/no/maybe in a survey)
*[[Negative binomial distribution]], for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs
*[[Poisson distribution]], for the number of occurrences of an event in a given period of time, for an event that is characterized by a fixed rate of occurrence
*[[Exponential distribution]], for the time before the next event occurs, for an event that is characterized by a fixed rate of occurrence
*[[Log-normal distribution]], for positive real numbers that are assumed to grow exponentially, such as incomes or prices
*[[Multivariate normal distribution]] (aka [[multivariate Gaussian distribution]]), for vectors of correlated outcomes that are individually Gaussian-distributed
*[[Multivariate Student's-t distribution]] (aka [[multivariate t-distribution]]), for vectors of heavy-tailed correlated outcomes &lt;ref&gt;Sotirios P. Chatzis, Dimitrios I. Kosmopoulos, Theodora A. Varvarigou, &quot;Signal Modeling and Classification Using a Robust Latent Space Model Based on t Distributions,&quot; IEEE Transactions on Signal Processing, vol. 56, no. 3, pp. 949-963, March 2008. [http://ieeexplore.ieee.org/document/4451278/]&lt;/ref&gt;
*A vector of [[Bernoulli distribution|Bernoulli]]-distributed values, corresponding, e.g., to a black-and-white image, with each value representing a pixel; see the handwriting-recognition example below

===Specific examples===

====Gaussian mixture model====
[[File:nonbayesian-gaussian-mixture.svg|right|250px|thumb|Non-Bayesian Gaussian mixture model using [[plate notation]].  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size ''K''.]]

A typical non-Bayesian [[Gaussian distribution|Gaussian]] mixture model looks like this:

:&lt;math&gt;
\begin{array}{lcl}
K,N &amp;=&amp; \text{as above} \\
\phi_{i=1 \dots K}, \boldsymbol\phi &amp;=&amp; \text{as above} \\
z_{i=1 \dots N}, x_{i=1 \dots N} &amp;=&amp; \text{as above} \\
\theta_{i=1 \dots K} &amp;=&amp; \{ \mu_{i=1 \dots K}, \sigma^2_{i=1 \dots K} \}  \\
\mu_{i=1 \dots K} &amp;=&amp; \text{mean of component } i \\
\sigma^2_{i=1 \dots K} &amp;=&amp; \text{variance of component } i \\
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N} &amp;\sim&amp; \mathcal{N}(\mu_{z_i}, \sigma^2_{z_i})
\end{array}
&lt;/math&gt;

{{clear}}
[[File:bayesian-gaussian-mixture.svg|right|300px|thumb|Bayesian Gaussian mixture model using [[plate notation]].  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size ''K''.]]

A Bayesian version of a [[Gaussian distribution|Gaussian]] mixture model is as follows:

:&lt;math&gt;
\begin{array}{lcl}
K,N &amp;=&amp; \text{as above} \\
\phi_{i=1 \dots K}, \boldsymbol\phi &amp;=&amp; \text{as above} \\
z_{i=1 \dots N}, x_{i=1 \dots N} &amp;=&amp; \text{as above} \\
\theta_{i=1 \dots K} &amp;=&amp; \{ \mu_{i=1 \dots K}, \sigma^2_{i=1 \dots K} \}  \\
\mu_{i=1 \dots K} &amp;=&amp; \text{mean of component } i \\
\sigma^2_{i=1 \dots K} &amp;=&amp; \text{variance of component } i \\
\mu_0, \lambda, \nu, \sigma_0^2 &amp;=&amp; \text{shared hyperparameters} \\
\mu_{i=1 \dots K} &amp;\sim&amp; \mathcal{N}(\mu_0, \lambda\sigma_i^2) \\
\sigma_{i=1 \dots K}^2 &amp;\sim&amp; \operatorname{Inverse-Gamma}(\nu, \sigma_0^2) \\
\boldsymbol\phi &amp;\sim&amp; \operatorname{Symmetric-Dirichlet}_K(\beta) \\
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N} &amp;\sim&amp; \mathcal{N}(\mu_{z_i}, \sigma^2_{z_i})
\end{array}
&lt;/math&gt;&lt;math&gt;&lt;/math&gt;
[[File:Parameter estimation process infinite Gaussian mixture model.webm|thumb|end=49|Animation of the clustering process for one-dimensional data using a Bayesian Gaussian mixture model where normal distributions are drawn from a [[Dirichlet process]]. The histograms of the clusters are shown in different colours. During the parameter estimation process, new clusters are created and grow on the data. The legend shows the cluster colours and the number of datapoints assigned to each cluster.]]

====Multivariate Gaussian mixture model====
A Bayesian Gaussian mixture model is commonly extended to fit a vector of unknown parameters (denoted in bold), or multivariate normal distributions.  In a multivariate distribution (i.e. one modelling a vector &lt;math&gt;\boldsymbol{x}&lt;/math&gt;  with ''N'' random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a Gaussian mixture model prior distribution on the vector of estimates given by
:&lt;math&gt;
p(\boldsymbol{\theta}) = \sum_{i=1}^K\phi_i \mathcal{N}(\boldsymbol{\mu_i,\Sigma_i})
&lt;/math&gt;
where the ''i&lt;sup&gt;th&lt;/sup&gt;'' vector component is characterized by normal distributions with weights &lt;math&gt;\phi_i&lt;/math&gt;, means &lt;math&gt;\boldsymbol{\mu_i}&lt;/math&gt; and covariance matrices &lt;math&gt;\boldsymbol{\Sigma_i}&lt;/math&gt;.  To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution &lt;math&gt;p(\boldsymbol{x | \theta})&lt;/math&gt; of the data &lt;math&gt;\boldsymbol{x}&lt;/math&gt; conditioned on the parameters &lt;math&gt;\boldsymbol{\theta}&lt;/math&gt; to be estimated.  With this formulation, the [[Posterior probability|posterior distribution]] &lt;math&gt;p(\boldsymbol{\theta | x})&lt;/math&gt; is ''also'' a Gaussian mixture model of the form
:&lt;math&gt;
p(\boldsymbol{\theta | x}) = \sum_{i=1}^K\tilde{\phi_i} \mathcal{N}(\boldsymbol{\tilde{\mu_i},\tilde{\Sigma_i}})
&lt;/math&gt;
with new parameters &lt;math&gt;\tilde{\phi_i}, \boldsymbol{\tilde{\mu_i}}&lt;/math&gt; and &lt;math&gt;\boldsymbol{\tilde{\Sigma_i}}&lt;/math&gt; that are updated using the [[Expectation-maximization algorithm|EM algorithm]].
&lt;ref&gt;
{{cite journal
|last=Yu |first=Guoshen
|title=Solving Inverse Problems with Piecewise Linear Estimators: From Gaussian Mixture Models to Structured Sparsity
|journal=IEEE Transactions on Image Processing
|volume=21 | date=2012|pages=2481–2499 |issue=5 |doi=10.1109/tip.2011.2176743
|pmid=22180506
|bibcode = 2012ITIP...21.2481G }}
&lt;/ref&gt;  Although EM-based parameter updates are well-established, providing the initial estimates for these parameters is currently an area of active research.  Note that this formulation yields a closed-form solution to the complete posterior distribution.  Estimations of the random variable &lt;math&gt;\boldsymbol{\theta}&lt;/math&gt; may be obtained via one of several estimators, such as the mean or maximum of the posterior distribution.

Such distributions are useful for assuming patch-wise shapes of images and clusters, for example.  In the case of image representation, each Gaussian may be tilted, expanded, and warped according to the covariance matrices &lt;math&gt;\boldsymbol{\Sigma_i}&lt;/math&gt;.  One Gaussian distribution of the set is fit to each patch (usually of size 8x8 pixels) in the image.  Notably, any distribution of points around a cluster (see [[K-means clustering|''k''-means]]) may be accurately given enough Gaussian components, but scarcely over ''K''=20 components are needed to accurately model a given image distribution or cluster of data.

====Categorical mixture model====
[[File:nonbayesian-categorical-mixture.svg|right|250px|thumb|Non-Bayesian categorical mixture model using [[plate notation]].  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size ''K''; likewise for [V].]]

A typical non-Bayesian mixture model with [[categorical distribution|categorical]] observations looks like this:

*&lt;math&gt;K,N:&lt;/math&gt; as above
*&lt;math&gt;\phi_{i=1 \dots K}, \boldsymbol\phi:&lt;/math&gt; as above
*&lt;math&gt;z_{i=1 \dots N}, x_{i=1 \dots N}:&lt;/math&gt; as above
*&lt;math&gt;V:&lt;/math&gt; dimension of categorical observations, e.g., size of word vocabulary
*&lt;math&gt;\theta_{i=1 \dots K, j=1 \dots V}:&lt;/math&gt; probability for component &lt;math&gt;i&lt;/math&gt; of observing item &lt;math&gt;j&lt;/math&gt;
*&lt;math&gt;\boldsymbol\theta_{i=1 \dots K}:&lt;/math&gt; vector of dimension &lt;math&gt;V,&lt;/math&gt; composed of &lt;math&gt;\theta_{i,1 \dots V};&lt;/math&gt; must sum to 1

The random variables:
:&lt;math&gt;
\begin{array}{lcl}
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N} &amp;\sim&amp; \text{Categorical}(\boldsymbol\theta_{z_i})
\end{array}
&lt;/math&gt;

&lt;!--
The original version, all in LaTeX.
:&lt;math&gt;
\begin{array}{lcl}
K,N &amp;=&amp; \text{as above} \\
\phi_{i=1 \dots K}, \boldsymbol\phi &amp;=&amp; \text{as above} \\
z_{i=1 \dots N}, x_{i=1 \dots N} &amp;=&amp; \text{as above} \\
V &amp;=&amp; \text{dimension of categorical observations, e.g., size of word vocabulary} \\
\theta_{i=1 \dots K, j=1 \dots V} &amp;=&amp; \text{probability for component } i \text{ of observing the } j\text{th item} \\
\boldsymbol\theta_{i=1 \dots K} &amp;=&amp; V\text{-dimensional vector, composed of }\theta_{i,1 \dots V} \text{; must sum to 1} \\
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N} &amp;\sim&amp; \text{Categorical}(\boldsymbol\theta_{z_i})
\end{array}
&lt;/math&gt;
--&gt;

{{clear}}
[[File:bayesian-categorical-mixture.svg|right|300px|thumb|Bayesian categorical mixture model using [[plate notation]].  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size ''K''; likewise for [V].]]

A typical Bayesian mixture model with [[categorical distribution|categorical]] observations looks like this:

*&lt;math&gt;K,N:&lt;/math&gt; as above
*&lt;math&gt;\phi_{i=1 \dots K}, \boldsymbol\phi:&lt;/math&gt; as above
*&lt;math&gt;z_{i=1 \dots N}, x_{i=1 \dots N}:&lt;/math&gt; as above
*&lt;math&gt;V:&lt;/math&gt; dimension of categorical observations, e.g., size of word vocabulary
*&lt;math&gt;\theta_{i=1 \dots K, j=1 \dots V}:&lt;/math&gt; probability for component &lt;math&gt;i&lt;/math&gt; of observing item &lt;math&gt;j&lt;/math&gt;
*&lt;math&gt;\boldsymbol\theta_{i=1 \dots K}:&lt;/math&gt; vector of dimension &lt;math&gt;V,&lt;/math&gt; composed of &lt;math&gt;\theta_{i,1 \dots V};&lt;/math&gt; must sum to 1
*&lt;math&gt;\alpha:&lt;/math&gt; shared concentration hyperparameter of &lt;math&gt;\boldsymbol\theta&lt;/math&gt; for each component
*&lt;math&gt;\beta:&lt;/math&gt; concentration hyperparameter of &lt;math&gt;\boldsymbol\phi&lt;/math&gt;

The random variables:
:&lt;math&gt;
\begin{array}{lcl}
\boldsymbol\phi &amp;\sim&amp; \operatorname{Symmetric-Dirichlet}_K(\beta) \\
\boldsymbol\theta_{i=1 \dots K} &amp;\sim&amp; \text{Symmetric-Dirichlet}_V(\alpha) \\
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N} &amp;\sim&amp; \text{Categorical}(\boldsymbol\theta_{z_i})
\end{array}
&lt;/math&gt;

&lt;!--
The (beginning of) equivalent of below, using no LaTeX.

*''K'',''N'' = as above
*&amp;phi;&lt;sub&gt;1,...,''K''&lt;/sub&gt;, '''&amp;phi;''' as above
*''z''&lt;sub&gt;''i''=1...''N''&lt;/sub&gt;, ''x''&lt;sub&gt;''i''=1...''N''&lt;/sub&gt; = as above
* ''V'' = dimension of categorical observations, e.g., size of word vocabulary
--&gt;
&lt;!--
The equivalent using full LaTeX.

:&lt;math&gt;
\begin{array}{lcl}
K,N &amp;=&amp; \mbox{as above} \\
\phi_{i=1 \dots K}, \boldsymbol\phi &amp;=&amp; \text{as above} \\
z_{i=1 \dots N}, x_{i=1 \dots N} &amp;=&amp; \text{as above} \\
V &amp;=&amp; \text{dimension of categorical observations, e.g., size of word vocabulary} \\
\theta_{i=1 \dots K, j=1 \dots V} &amp;=&amp; \text{probability for component } i \text{ of observing the } j\text{th item} \\
\boldsymbol\theta_{i=1 \dots K} &amp;=&amp; V\text{-dimensional vector, composed of }\theta_{i,1 \dots V} \text{; must sum to 1} \\
\alpha &amp;=&amp; \text{shared concentration hyperparameter of } \boldsymbol\theta \text{ for each component} \\
\beta &amp;=&amp; \text{concentration hyperparameter of } \boldsymbol\phi \\
\boldsymbol\phi &amp;\sim&amp; \operatorname{Symmetric-Dirichlet}_K(\beta) \\
\boldsymbol\theta_{i=1 \dots K} &amp;\sim&amp; \text{Symmetric-Dirichlet}_V(\alpha) \\
z_{i=1 \dots N} &amp;\sim&amp; \operatorname{Categorical}(\boldsymbol\phi) \\
x_{i=1 \dots N} &amp;\sim&amp; \text{Categorical}(\boldsymbol\theta_{z_i})
\end{array}
&lt;/math&gt;
--&gt;

==Examples==

===A financial model===
[[File:Normal distribution pdf.png|thumb|right|250px|The [[normal distribution]] is plotted using different means and variances]]

Financial returns often behave differently in normal situations and during crisis times. A mixture model &lt;ref&gt;Dinov, ID. &quot;[http://repositories.cdlib.org/socr/EM_MM/ Expectation Maximization and Mixture Modeling Tutorial]&quot;. ''[http://repositories.cdlib.org/escholarship California Digital Library]'', Statistics Online Computational Resource, Paper EM_MM, http://repositories.cdlib.org/socr/EM_MM, December 9, 2008&lt;/ref&gt; for return data seems reasonable. Sometimes the model used is a [[jump-diffusion model]], or as a mixture of two normal distributions. See [[Financial economics#Challenges and criticism]] for further context.

===House prices===
Assume that we observe the prices of ''N'' different houses.  Different types of houses in different neighborhoods will have vastly different prices, but the price of a particular type of house in a particular neighborhood (e.g., three-bedroom house in moderately upscale neighborhood) will tend to cluster fairly closely around the mean.  One possible model of such prices would be to assume that the prices are accurately described by a mixture model with ''K'' different components, each distributed as a [[normal distribution]] with unknown mean and variance, with each component specifying a particular combination of house type/neighborhood.  Fitting this model to observed prices, e.g., using the [[expectation-maximization algorithm]], would tend to cluster the prices according to house type/neighborhood and reveal the spread of prices in each type/neighborhood. (Note that for values such as prices or incomes that are guaranteed to be positive and which tend to grow [[exponential growth|exponentially]], a [[log-normal distribution]] might actually be a better model than a normal distribution.)

===Topics in a document===
Assume that a document is composed of ''N'' different words from a total vocabulary of size ''V'', where each word corresponds to one of ''K'' possible topics.  The distribution of such words could be modelled as a mixture of ''K'' different ''V''-dimensional [[categorical distribution]]s.  A model of this sort is commonly termed a [[topic model]].  Note that [[expectation maximization]] applied to such a model will typically fail to produce realistic results, due (among other things) to the [[overfitting|excessive number of parameters]].  Some sorts of additional assumptions are typically necessary to get good results.  Typically two sorts of additional components are added to the model:
#A [[prior distribution]] is placed over the parameters describing the topic distributions, using a [[Dirichlet distribution]] with a [[concentration parameter]] that is set significantly below 1, so as to encourage sparse distributions (where only a small number of words have significantly non-zero probabilities).
#Some sort of additional constraint is placed over the topic identities of words, to take advantage of natural clustering.
:*For example, a [[Markov chain]] could be placed on the topic identities (i.e., the latent variables specifying the mixture component of each observation), corresponding to the fact that nearby words belong to similar topics. (This results in a [[hidden Markov model]], specifically one where a [[prior distribution]] is placed over state transitions that favors transitions that stay in the same state.)
:*Another possibility is the [[latent Dirichlet allocation]] model, which divides up the words into ''D'' different documents and assumes that in each document only a small number of topics occur with any frequency.

===Handwriting recognition===
The following example is based on an example in [[Christopher M. Bishop]], ''Pattern Recognition and Machine Learning''.&lt;ref&gt;{{cite book | last = Bishop | first = Christopher | title = Pattern recognition and machine learning | publisher = Springer | location = New York | year = 2006 | isbn = 978-0-387-31073-2 }}&lt;/ref&gt;

Imagine that we are given an ''N''×''N'' black-and-white image that is known to be a scan of a hand-written digit between 0 and 9, but we don't know which digit is written.  We can create a mixture model with &lt;math&gt;K=10&lt;/math&gt; different components, where each component is a vector of size &lt;math&gt;N^2&lt;/math&gt; of [[Bernoulli distribution]]s (one per pixel).  Such a model can be trained with the [[expectation-maximization algorithm]] on an unlabeled set of hand-written digits, and will effectively cluster the images according to the digit being written.  The same model could then be used to recognize the digit of another image simply by holding the parameters constant, computing the probability of the new image for each possible digit (a trivial calculation), and returning the digit that generated the highest probability.

===Assessing projectile accuracy (a.k.a. [[circular error probable]], CEP)===
Mixture models apply in the problem of directing multiple projectiles at a target (as in air, land, or sea defense applications), where the physical and/or statistical characteristics of the projectiles differ within the multiple projectiles. An example might be shots from multiple munitions types or shots from multiple locations directed at one target. The combination of projectile types may be characterized as a Gaussian mixture model.&lt;ref&gt;Spall, J. C. and Maryak, J. L. (1992). &quot;A feasible Bayesian estimator of quantiles for projectile accuracy from non-i.i.d. data.&quot; ''Journal of the American Statistical Association'', vol. 87 (419), pp. 676–681. URL https://www.jstor.org/stable/2290205&lt;/ref&gt; Further, a well-known measure of accuracy for a group of projectiles is the [[circular error probable]] (CEP), which is the number ''R'' such that, on average, half of the group of projectiles falls within the circle of radius ''R'' about the target point. The mixture model can be used to determine (or estimate) the value ''R''. The mixture model properly captures the different types of projectiles.

===Direct and indirect applications===
The financial example above is one direct application of the mixture model, a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories. This underlying mechanism may or may not, however, be observable. In this form of mixture, each of the sources is described by a component probability density function, and its mixture weight is the probability that an observation comes from this component.

In an indirect application of the mixture model we do not assume such a mechanism. The mixture model is simply used for its mathematical flexibilities. For example, a mixture of two [[normal distribution]]s with different means may result in a density with two [[Mode (statistics)|modes]], which is not modeled by standard parametric distributions. Another example is given by the possibility of mixture distributions to model fatter tails than the basic Gaussian ones, so as to be a candidate for modeling more extreme events. When combined with [[dynamical consistency]], this approach has been applied to [[financial derivatives]] valuation in presence of the [[volatility smile]] in the context of [[local volatility]] models. This defines our application.

===Fuzzy image segmentation===
In image processing and computer vision, traditional [[image segmentation]] models often assign to one [[pixel]] only one exclusive pattern. In fuzzy or soft segmentation, any pattern can have certain &quot;ownership&quot; over any single pixel. If the patterns are Gaussian, fuzzy segmentation naturally results in Gaussian mixtures. Combined with other analytic or geometric tools (e.g., phase transitions over diffusive boundaries), such spatially regularized mixture models could lead to more realistic and computationally efficient segmentation methods.&lt;ref&gt;
{{cite journal
| last = Shen | first = Jianhong (Jackie)
| title = A stochastic-variational model for soft Mumford-Shah segmentation
| date = 2006
| volume=2006
| pages=2–16
| journal=International Journal of Biomedical Imaging
| doi=10.1155/IJBI/2006/92329
| url=http://www.hindawi.com/journals/ijbi/2006/092329/abs/
}}&lt;/ref&gt;

== Identifiability ==

Identifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedures may not be well-defined and asymptotic theory may not hold if a model is not identifiable.

=== Example ===
Let ''J'' be the class of all binomial distributions with {{nowrap|''n'' {{=}} 2}}. Then a mixture of two members of ''J'' would have

:&lt;math&gt;p_0=\pi(1-\theta_1)^2+(1-\pi)(1-\theta_2)^2&lt;/math&gt;
:&lt;math&gt;p_1=2\pi\theta_1(1-\theta_1)+2(1-\pi)\theta_2(1-\theta_2)&lt;/math&gt;

and {{nowrap|''p''&lt;sub&gt;2&lt;/sub&gt; {{=}} 1 − ''p''&lt;sub&gt;0&lt;/sub&gt; − ''p''&lt;sub&gt;1&lt;/sub&gt;}}. Clearly, given ''p''&lt;sub&gt;0&lt;/sub&gt; and ''p''&lt;sub&gt;1&lt;/sub&gt;, it is not possible to determine the above mixture model uniquely, as there are three parameters (''π'', ''θ''&lt;sub&gt;1&lt;/sub&gt;, ''θ''&lt;sub&gt;2&lt;/sub&gt;) to be determined.

=== Definition ===
Consider a mixture of parametric distributions of the same class. Let

:&lt;math&gt;J=\{f(\cdot ; \theta):\theta\in\Omega\}&lt;/math&gt;

be the class of all component distributions. Then the [[convex hull]] ''K'' of ''J'' defines the class of all finite mixture of distributions in ''J'':

:&lt;math&gt;K=\left\{p(\cdot):p(\cdot)=\sum_{i=1}^n a_i f_i(\cdot ; \theta_i), a_i&gt;0, \sum_{i=1}^n a_i=1, f_i(\cdot ; \theta_i)\in J\ \forall i,n\right\}&lt;/math&gt;

''K'' is said to be identifiable if all its members are unique, that is, given two members ''p'' and {{nowrap|''p′''}} in ''K'', being mixtures of ''k'' distributions and {{nowrap|''k′''}} distributions respectively in ''J'', we have {{nowrap|''p {{=}} p′''}} if and only if, first of all, {{nowrap|''k {{=}} k′''}} and secondly we can reorder the summations such that {{nowrap|''a&lt;sub&gt;i&lt;/sub&gt; {{=}} a&lt;sub&gt;i&lt;/sub&gt;''′}} and {{nowrap|''ƒ&lt;sub&gt;i&lt;/sub&gt; {{=}} ƒ&lt;sub&gt;i&lt;/sub&gt;''′}} for all ''i''.

== Parameter estimation and system identification ==

Parametric mixture models are often used when we know the distribution ''Y'' and we can sample from ''X'', but we would like to determine the ''a&lt;sub&gt;i&lt;/sub&gt;'' and ''θ&lt;sub&gt;i&lt;/sub&gt;'' values.  Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.

It is common to think of probability mixture modeling as a missing data problem.  One way to understand this is to assume that the data points under consideration have &quot;membership&quot; in one of the distributions we are using to model the data.  When we start, this membership is unknown, or missing.  The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.

A variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as [[expectation maximization]] (EM) or maximum ''a posteriori'' estimation (MAP).  Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values.  Some notable departures are the graphical methods as outlined in Tarter and Lock &lt;ref name=tart&gt;{{citation
|title=Model Free Curve Estimation
|first=Michael E. |last=Tarter
|date=1993
|publisher=Chapman and Hall
}}&lt;/ref&gt; and more recently [[minimum message length]] (MML) techniques such as Figueiredo and Jain &lt;ref name=Jain&gt;{{cite journal |first1=M.A.T. |last1=Figueiredo |first2=A.K. |last2=Jain |title=Unsupervised Learning of Finite Mixture Models |journal=IEEE Transactions on Pattern Analysis and Machine Intelligence |volume=24 |issue=3 |pages=381–396 |date=March 2002 |doi=10.1109/34.990138 |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=990138
}}&lt;/ref&gt; and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).&lt;ref name=mcwilli&gt;
{{citation
|title=Incorporating Multidimensional Tail-Dependencies in the Valuation of Credit Derivatives (Working Paper)
| first1 = N. | last1 = McWilliam | first2 = K. | last2 = Loh
|date = 2008
}} [http://www.misys.com/cds-portlets/digitalAssets/4/2797_CDsAndTailDep_forPublication_final1.pdf]&lt;/ref&gt;

=== Expectation maximization (EM) === &lt;!-- Linked from [[Expectation-maximization algorithm]] --&gt;

[[Expectation-maximization algorithm|Expectation maximization]] (EM) is seemingly the most popular technique used to determine the parameters of a mixture with an ''a priori'' given number of components. This is a particular way of implementing [[maximum likelihood]] estimation for this problem. EM is of particular appeal for finite normal mixtures where closed-form expressions are possible such as in the following iterative algorithm by Dempster ''et al.'' (1977)&lt;ref name=dempster1977&gt;{{cite journal |first1=A.P. |last1=Dempster |first2=N.M. |last2=Laird |first3=D.B. |last3=Rubin |title=Maximum Likelihood from Incomplete Data via the EM Algorithm |journal=Journal of the Royal Statistical Society, Series B |volume=39 |issue=1 |pages=1–38 | date = 1977 |jstor=2984875 |citeseerx = 10.1.1.163.7580 }}
&lt;/ref&gt;

:&lt;math&gt; w_s^{(j+1)} = \frac{1}{N} \sum_{t =1}^N h_s^{(j)}(t) &lt;/math&gt;
:&lt;math&gt; \mu_s^{(j+1)}  =  \frac{\sum_{t =1}^N h_s^{(j)}(t) x^{(t)}}{\sum_{t =1}^N h_s^{(j)}(t)} &lt;/math&gt;
:&lt;math&gt; \Sigma_s^{(j+1)}  =  \frac{\sum_{t =1}^N h_s^{(j)}(t) [x^{(t)}-\mu_s^{(j+1)}][x^{(t)}-\mu_s^{(j+1)}]^{\top}}{\sum_{t =1}^N h_s^{(j)}(t)} &lt;/math&gt;
with the posterior probabilities
:&lt;math&gt; h_s^{(j)}(t) = \frac{w_s^{(j)} p_s(x^{(t)}; \mu_s^{(j)},\Sigma_s^{(j)}) }{ \sum_{i = 1}^n w_i^{(j)} p_i(x^{(t)}; \mu_i^{(j)}, \Sigma_i^{(j)})}. &lt;/math&gt;

Thus on the basis of the current estimate for the parameters, the conditional probability for a given observation ''x''&lt;sup&gt;(''t'')&lt;/sup&gt; being generated from state ''s'' is determined for each {{nowrap|''t'' {{=}} 1, …, ''N''}} ; ''N'' being the sample size.  The parameters are then updated such that the new component weights correspond to the average conditional probability and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample.

Dempster&lt;ref name=&quot;dempster1977&quot;/&gt; also showed that each successive EM iteration will not decrease the likelihood, a property not shared by other gradient based maximization techniques.  Moreover, EM naturally embeds within it constraints on the probability vector, and for sufficiently large sample sizes positive definiteness of the covariance iterates.  This is a key advantage since explicitly constrained methods incur extra computational costs to check and maintain appropriate values. Theoretically EM is a first-order algorithm and as such converges slowly to a fixed-point solution. Redner and Walker (1984){{full citation needed|date=November 2012}} make this point arguing in favour of superlinear and second order Newton and quasi-Newton methods and reporting slow convergence in EM on the basis of their empirical tests.  They do concede that convergence in likelihood was rapid even if convergence in the parameter values themselves was not.  The relative merits of EM and other algorithms vis-à-vis convergence have been discussed in other literature.&lt;ref name=XuJordam&gt;{{cite journal |first1=L. |last1=Xu |first2=M.I. |last2=Jordan |title=On Convergence Properties of the EM Algorithm for Gaussian Mixtures |journal=Neural Computation |volume=8 |issue=1 |pages=129–151 |date=January 1996 |doi=10.1162/neco.1996.8.1.129 |url=http://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.1.129}}&lt;/ref&gt;

Other common objections to the use of EM are that it has a propensity to spuriously identify local maxima, as well as displaying sensitivity to initial values.&lt;ref name=&quot;McLachlan_2&quot;/&gt;&lt;ref name=&quot;botev2004global&quot;&gt;{{Cite journal |author1=Botev, Z.I. |author2=Kroese, D.P. |title=Global likelihood optimization via the cross-entropy method with an application to mixture models
|journal=[[Proceedings of the 2004 Winter Simulation Conference]] |volume= 1 |pages=517 |year=2004 |doi=10.1109/WSC.2004.1371358|isbn=0-7803-8786-4 }}&lt;/ref&gt; One may address these problems by evaluating EM at several initial points in the parameter space but this is computationally costly and other approaches, such as the annealing EM method of Udea and Nakano (1998) (in which the initial components are essentially forced to overlap, providing a less heterogeneous basis for initial guesses), may be preferable.

Figueiredo and Jain &lt;ref name=&quot;Jain&quot; /&gt; note that convergence to 'meaningless' parameter values obtained at the boundary (where regularity conditions breakdown, e.g., Ghosh and Sen (1985)) is frequently observed when the number of model components exceeds the optimal/true one.  On this basis they suggest a unified approach to estimation and identification in which the initial ''n'' is chosen to greatly exceed the expected optimal value.  Their optimization routine is constructed via a minimum message length (MML) criterion that effectively eliminates a candidate component if there is insufficient information to support it. In this way it is possible to systematize reductions in ''n'' and consider estimation and identification jointly.

The [[Expectation-maximization algorithm]] can be used to compute the parameters of a parametric mixture model distribution (the ''a&lt;sub&gt;i&lt;/sub&gt;'' and ''θ&lt;sub&gt;i&lt;/sub&gt;'').  It is an [[iterative algorithm]] with two steps: an ''expectation step'' and a ''maximization step''. [http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture Practical examples of EM and Mixture Modeling] are included in the [[SOCR]] demonstrations.

==== The expectation step ====
With initial guesses for the parameters of our mixture model, &quot;partial membership&quot; of each data point in each constituent distribution is computed by calculating [[expectation value]]s for the membership variables of each data point.  That is, for each data point ''x&lt;sub&gt;j&lt;/sub&gt;'' and distribution ''Y&lt;sub&gt;i&lt;/sub&gt;'', the membership value ''y''&lt;sub&gt;''i'', ''j''&lt;/sub&gt; is:

:&lt;math&gt; y_{i,j} = \frac{a_i f_Y(x_j;\theta_i)}{f_{X}(x_j)}.&lt;/math&gt;

==== The maximization step ====
With expectation values in hand for group membership, [[plug-in estimates]] are recomputed for the distribution parameters.

The mixing coefficients ''a&lt;sub&gt;i&lt;/sub&gt;'' are the [[arithmetic mean|mean]]s of the membership values over the ''N'' data points.

:&lt;math&gt; a_i = \frac{1}{N}\sum_{j=1}^N y_{i,j}&lt;/math&gt;

The component model parameters ''θ&lt;sub&gt;i&lt;/sub&gt;'' are also calculated by expectation maximization using data points ''x&lt;sub&gt;j&lt;/sub&gt;'' that have been weighted using the membership values.  For example, if ''θ'' is a mean ''μ''

:&lt;math&gt; \mu_{i} = \frac{\sum_{j} y_{i,j}x_{j}}{\sum_{j} y_{i,j}}.&lt;/math&gt;

With new estimates for ''a&lt;sub&gt;i&lt;/sub&gt;'' and the ''θ&lt;sub&gt;i&lt;/sub&gt;'''s, the expectation step is repeated to recompute new membership values.  The entire procedure is repeated until model parameters converge.

=== Markov chain Monte Carlo ===
As an alternative to the EM algorithm, the mixture model parameters can be deduced using [[posterior sampling]] as indicated by [[Bayes' theorem]].  This is still regarded as an incomplete data problem whereby membership of data points is the missing data.  A two-step iterative procedure known as [[Gibbs sampling]] can be used.

The previous example of a mixture of two [[Gaussian distribution]]s can demonstrate how the method works.  As before, initial guesses of the parameters for the mixture model are made.  Instead of computing partial memberships for each elemental distribution, a membership value for each data point is drawn from a [[Bernoulli distribution]] (that is, it will be assigned to either the first or the second Gaussian).  The Bernoulli parameter ''θ'' is determined for each data point on the basis of one of the constituent distributions.{{Vague|What does this mean?|date=March 2008}}  Draws from the distribution generate membership associations for each data point.  Plug-in estimators can then be used as in the M step of EM to generate a new set of mixture model parameters, and the binomial draw step repeated.

=== Moment matching ===
The [[Method of moments (statistics)|method of moment matching]] is one of the oldest techniques for determining the mixture parameters dating back to Karl Pearson’s seminal work of 1894.
In this approach the parameters of the mixture are determined such that the composite distribution has moments matching some given value.  In many instances extraction of solutions to the moment equations may present non-trivial algebraic or computational problems.  Moreover, numerical analysis by Day &lt;ref name=day&gt;{{Cite journal | last1 = Day | first1 = N. E. | title = Estimating the Components of a Mixture of Normal Distributions | journal = Biometrika | volume = 56 | issue = 3 | pages = 463–474 | doi = 10.2307/2334652 | jstor = 2334652| year = 1969 | pmid =  | pmc = }}&lt;/ref&gt; has indicated that such methods may be inefficient compared to EM. Nonetheless there has been renewed interest in this method, e.g., Craigmile and Titterington (1998) and Wang.&lt;ref name=wang&gt;{{citation
|title=Generating daily changes in market variables using a multivariate mixture of normal distributions
|first = J. | last = Wang
|date = 2001
|journal = Proceedings of the 33rd winter conference on simulation | publisher = IEEE Computer Society
|pages   =283–289
}}&lt;/ref&gt;

McWilliam and Loh (2009) consider the characterisation of a hyper-cuboid normal mixture [[copula (statistics)|copula]] in large dimensional systems for which EM would be computationally prohibitive.  Here a pattern analysis routine is used to generate multivariate tail-dependencies consistent with a set of univariate and (in some sense) bivariate moments.  The performance of this method is then evaluated using equity log-return data with [[Kolmogorov–Smirnov]] test statistics suggesting a good descriptive fit.

===Spectral method===
Some problems in mixture model estimation can be solved using [[spectral method]]s.
In particular it becomes useful if data points ''x&lt;sub&gt;i&lt;/sub&gt;'' are points in high-dimensional [[real coordinate space|real space]], and the hidden distributions are known to be [[Logarithmically concave function|log-concave]] (such as [[Gaussian distribution]] or [[Exponential distribution]]).

Spectral methods of learning mixture models are based on the use of [[Singular Value Decomposition]] of a matrix which contains data points.
The idea is to consider the top ''k'' singular vectors, where ''k'' is the number of distributions to be learned. The projection
of each data point to a [[linear subspace]] spanned by those vectors groups points originating from the same distribution
very close together, while points from different distributions stay far apart.

One distinctive feature of the spectral method is that it allows us to [[Mathematical proof|prove]] that if
distributions satisfy certain separation condition (e.g., not too close), then the estimated mixture will be very close to the true one with high probability.

=== Graphical Methods ===

Tarter and Lock &lt;ref name=&quot;tart&quot; /&gt; describe a graphical approach to mixture identification in which a kernel function is applied to an empirical frequency plot so to reduce intra-component variance.  In this way one may more readily identify components having differing means.  While this ''λ''-method does not require prior knowledge of the number or functional form of the components its success does rely on the choice of the kernel parameters which to some extent implicitly embeds assumptions about the component structure.

=== Other methods ===

Some of them can even probably learn mixtures of [[heavy-tailed distribution]]s including those with
infinite [[variance]] (see [[#Recent Papers|links to papers]] below).
In this setting, EM based methods would not work, since the Expectation step would diverge due to presence of
[[outlier]]s.

=== A simulation ===
To simulate a sample of size ''N'' that is from a mixture of distributions ''F''&lt;sub&gt;''i''&lt;/sub&gt;, ''i''=1 to ''n'', with probabilities ''p''&lt;sub&gt;''i''&lt;/sub&gt; (sum=&amp;nbsp;''p''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;1):
# Generate ''N'' random numbers from a [[categorical distribution]] of size ''n'' and probabilities ''p''&lt;sub&gt;''i''&lt;/sub&gt; for ''i''=&amp;nbsp;1=&amp;nbsp;to&amp;nbsp;''n''.  These tell you which of the ''F''&lt;sub&gt;''i''&lt;/sub&gt; each of the ''N'' values will come from.  Denote by ''m&lt;sub&gt;i&lt;/sub&gt;'' the quantity of random numbers assigned to the ''i''&lt;sup&gt;th&lt;/sup&gt; category.
# For each ''i'', generate ''m&lt;sub&gt;i&lt;/sub&gt;'' random numbers from the ''F''&lt;sub&gt;''i''&lt;/sub&gt; distribution.

== Extensions ==
In a [[Bayesian inference|Bayesian setting]], additional levels can be added to the [[graphical model]] defining the mixture model.  For example, in the common [[latent Dirichlet allocation]] [[topic model]], the observations are sets of words drawn from ''D'' different documents and the ''K'' mixture components represent topics that are shared across documents.  Each document has a different set of mixture weights, which specify the topics prevalent in that document.  All sets of mixture weights share common [[hyperparameter]]s.

A very common extension is to connect the [[latent variable]]s defining the mixture component identities into a [[Markov chain]], instead of assuming that they are [[independent identically distributed]] random variables.  The resulting model is termed a [[hidden Markov model]] and is one of the most common sequential hierarchical models.  Numerous extensions of hidden Markov models have been developed; see the resulting article for more information.

== History ==
Mixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan
,&lt;ref name=McLachlan_2&gt;{{citation
|title=Finite Mixture Models
|first=G.J. |last=McLachlan
|publisher=Wiley
|date=2000
}}&lt;/ref&gt; 2000) although common reference is made to the work of [[Karl Pearson]] (1894)&lt;ref name=Amendola2015&gt;{{Cite journal |last=Améndola |first=Carlos |display-authors=etal |arxiv=1510.04654 |year=2015 |title=Moment varieties of Gaussian mixtures|class=math.AG |doi=10.18409/jas.v7i1.42 |volume=7 |journal=Journal of Algebraic Statistics}}&lt;/ref&gt; as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations.  The motivation for this work was provided by the zoologist [[Walter Frank Raphael Weldon]] who had speculated in 1893 (in Tarter and Lock&lt;ref name=&quot;tart&quot; /&gt;) that asymmetry in the histogram of these ratios could signal evolutionary divergence. Pearson’s approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model.

While his work was successful in identifying two potentially distinct sub-populations and in demonstrating the flexibility of mixtures as a moment matching tool, the formulation required the solution of a 9th degree (nonic) polynomial which at the time posed a significant computational challenge.

Subsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of [[Maximum Likelihood]] (MLE) parameterisation techniques that research really took off.&lt;ref name=McLachlan_1&gt;{{citation
|title=Mixture Models: inference and applications to clustering
|journal=Statistics: Textbooks and Monographs |first=G.J. |last=McLachlan
|publisher=Dekker
|date=1988
|bibcode=1988mmia.book.....M }}&lt;/ref&gt;  Since that time there has been a vast body of research on the subject spanning areas such as Fisheries research, Agriculture, Botany, Economics, Medicine, Genetics, Psychology, Palaeontology, Electrophoresis, Finance, Sedimentology/Geology and Zoology.&lt;ref name=titter_1&gt;{{harvnb|Titterington|Smith|Makov|1985}}&lt;/ref&gt;

== See also ==

=== Mixture ===
* [[Mixture density]]
* [[Mixture (probability)]]
* [[Flexible Mixture Model (FMM)]]

=== Hierarchical models ===
* [[Graphical model]]
* [[Hierarchical Bayes model]]

=== Outlier detection ===
* [[RANSAC]]

{{More footnotes|date=November 2010}}

== References ==
{{Reflist}}

== Further reading ==

=== Books on mixture models ===
*{{cite book |last1=Everitt |first1=B.S. |last2=Hand |first2=D.J. |title=Finite mixture distributions |publisher=Chapman &amp; Hall |date=1981 |isbn=0-412-22420-8 }}
*{{cite book | last = Lindsay | first = B. G. | authorlink=Bruce G. Lindsay |date = 1995 | title = Mixture Models: Theory, Geometry, and Applications | series = NSF-CBMS Regional Conference Series in Probability and Statistics | volume = 5 | publisher = Institute of Mathematical Statistics | location = Hayward }}
*{{cite book |last1=Marin |first1=J.M. |last2=Mengersen |first2=K. |last3=Robert |first3=C.P. |chapter=Bayesian modelling and inference on mixtures of distributions |chapterurl=http://www.ceremade.dauphine.fr/%7Exian/mixo.pdf |editor1-first=D. |editor1-last=Dey |editor2-first=C.R. |editor2-last=Rao |title=Essential Bayesian models |publisher=Elsevier |year=2011 |isbn=9780444537324 |pages= |url= |series=Handbook of statistics: Bayesian thinking - modeling and computation |volume=25}}
*{{cite book |last1=McLachlan |first1=G.J. |last2=Peel |first2=D. |title=Finite Mixture Models |publisher=Wiley |year=2000 |isbn=0-471-00626-2 }}
*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 16.1. Gaussian Mixture Models and k-Means Clustering | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=842}}
*{{cite book |last1=Titterington |first1=D. |first2=A. |last2=Smith |first3=U. |last3=Makov |title=Statistical Analysis of Finite Mixture Distributions |publisher=Wiley |year=1985 |isbn=0-471-90763-4 |ref=harv}}

===Application of Gaussian mixture models===
#{{cite journal |first1=D.A. |last1=Reynolds |first2=R.C. |last2=Rose |title=Robust text-independent speaker identification using Gaussian mixture speaker models |journal=IEEE Transactions on Speech and Audio Processing |date=January 1995 | url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=365379 |volume=3 |issue=1 |pages=72–83 |doi=10.1109/89.365379 }}
#{{cite conference |first1=H. |last1=Permuter |first2=J. |last2=Francos |first3=I.H. |last3=Jermyn |title=Gaussian mixture models of texture and colour for image database retrieval| conference=  IEEE [[International Conference on Acoustics, Speech, and Signal Processing]], 2003. Proceedings (ICASSP '03)| year=2003 | url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1199538}}[http://www.sciencedirect.com/science/article/pii/S0031320305004334  The journal version]
#{{cite book|first=Wolfgang|last=Lemke|date=2005|title=Term Structure Modeling and Estimation in a State Space Framework|publisher=Springer Verlag|isbn=978-3-540-28342-3}}
#{{cite conference | first1=Damiano|last1=Brigo|authorlink1=Damiano Brigo|first2=Fabio|last2=Mercurio|authorlink2=Fabio Mercurio|title=Displaced and Mixture Diffusions for Analytically-Tractable Smile Models| conference=  Mathematical Finance — Bachelier Congress 2000. Proceedings | date = 2001|publisher=Springer Verlag }}
#{{cite journal |first1=Damiano |last1=Brigo |first2=Fabio |last2=Mercurio |title=Lognormal-mixture dynamics and calibration to market volatility smiles |journal=International Journal of Theoretical and Applied Finance |volume=5 |issue=4 |page=427 |date=June 2002 |doi=10.1142/S0219024902001511 |url=http://www.worldscientific.com/doi/abs/10.1142/S0219024902001511}}
#{{cite journal |first1=J. C. |last1=Spall |first2=J. L. |last2=Maryak |title=A feasible Bayesian estimator of quantiles for projectile accuracy from non-i.i.d. data|journal=Journal of the American Statistical Association |volume=87 |issue=419 |pages=676–681 |date=1992 |jstor=2290205 |doi=10.1080/01621459.1992.10475269}}
#{{cite journal |first1=Carol |last1=Alexander |title=Normal mixture diffusion with uncertain volatility: Modelling short- and long-term smile effects |journal=Journal of Banking &amp; Finance |volume=28 |issue=12 |pages=2957–80 |date=December 2004 |doi=10.1016/j.jbankfin.2003.10.017 |url=http://www.carolalexander.org/publish/download/JournalArticles/PDFs/JBF2004.pdf |format=PDF}}
#{{cite conference | first1 = Yannis | last1 = Stylianou | first2 = Yannis | last2 = Pantazis | first3 = Felipe | last3 = Calderero | first4 = Pedro | last4 = Larroy | first5 = Francois | last5 = Severin | first6 = Sascha | last6 = Schimke | first7 = Rolando | last7 = Bonal | first8 = Federico | last8 = Matta | first9 = Athanasios | last9 = Valsamakis |title=GMM-Based Multimodal Biometric Verification| date = 2005 |url=http://www.enterface.net/enterface05/docs/results/reports/project5.pdf}}
#{{cite conference |first1=J. |last1=Chen |first2=0.E. |last2=Adebomi |first3=O.S. |last3=Olusayo |first4=W. |last4=Kulesza |title=The Evaluation of the Gaussian Mixture Probability Hypothesis Density approach for multi-target tracking| conference=  IEEE [[International Conference on Imaging Systems and Techniques]], 2010.| year=2010 | url=http://ieeexplore.ieee.org/document/5548541/]}}

== External links ==
*{{cite journal |first1=Frank |last1=Nielsen |title=''k''-MLE: A fast algorithm for learning statistical mixture models |date=23 March 2012 |arxiv=1203.5181|class=cs.LG |doi=10.1109/ICASSP.2012.6288022 |journal=2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}}
* The [http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture SOCR demonstrations of EM and Mixture Modeling]
*[http://www.csse.monash.edu.au/~dld/mixturemodel.html Mixture modelling page] (and the [http://www.csse.monash.edu.au/~dld/Snob.html Snob] program for [[Minimum Message Length]] ([[Minimum Message Length|MML]]) applied to finite mixture models), maintained by D.L. Dowe.
*[http://www.pymix.org PyMix] — Python Mixture Package, algorithms and data structures for a broad variety of mixture model based data mining applications in Python
*[http://scikit-learn.org/stable/modules/mixture.html sklearn.mixture] — A Python package for learning Gaussian Mixture Models (and sampling from them), previously packaged with [[SciPy]] and now packaged as a [https://scikits.appspot.com/ SciKit]
*[http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=18785&amp;objectType=FILE GMM.m] Matlab code for GMM Implementation
*[http://stat.duke.edu/gpustatsci/software.html GPUmix] C++ implementation of Bayesian Mixture Models using EM and MCMC with 100x speed acceleration using GPGPU.
*[http://www.cs.ru.nl/~ali/index_files/EM.m] Matlab code for GMM Implementation using EM algorithm
*[https://vincentfpgarcia.github.com/jMEF/] jMEF: A Java open source library for learning and processing mixtures of exponential families (using duality with Bregman divergences). Includes a Matlab wrapper.
* Very Fast and clean C implementation of the [https://github.com/juandavm/em4gmm Expectation Maximization] (EM) algorithm for estimating [https://github.com/juandavm/em4gmm Gaussian Mixture Models] (GMMs).
* [https://cran.r-project.org/web/packages/mclust/index.html mclust] is an R package for mixture modeling.
* [https://github.com/thaines/helit/tree/master/dpgmm dpgmm] Pure Python Dirichlet process Gaussian mixture model implementation (variational).

{{DEFAULTSORT:Mixture Model}}



</text>
      <sha1>tet9a9otf2d1zg1fxhv3k6xbneqo0e3</sha1>
    </revision>
  </page>
  <page>
    <title>Statistical classification</title>
    <ns>0</ns>
    <id>1579244</id>
    <revision>
      <id>809517577</id>
      <parentid>809319000</parentid>
      <timestamp>2017-11-09T16:58:52Z</timestamp>
      <contributor>
        <username>Allforrous</username>
        <id>12120664</id>
      </contributor>
      <comment>/* External links */ Commonscat template.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15231">
{{for|the [[unsupervised learning]] approach|Cluster analysis}}
{{Machine learning bar}}
In [[machine learning]] and [[statistics]], '''classification''' is the problem of identifying to which of a set of [[categorical data|categories]] (sub-populations) a new [[observation]] belongs, on the basis of a [[training set]] of data containing observations (or instances) whose category membership is known.  An example would be assigning a given email into [[Spam filtering|&quot;spam&quot; or &quot;non-spam&quot;]] classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of [[pattern recognition]].

In the terminology of machine learning,&lt;ref&gt;{{cite book|last=Alpaydin|first=Ethem|title=Introduction to Machine Learning|date=2010|publisher=MIT Press|isbn=978-0-262-01243-0|page=9|url=https://books.google.com/books?id=7f5bBAAAQBAJ&amp;printsec=frontcover#v=onepage&amp;q=classification&amp;f=false}}&lt;/ref&gt; classification is considered an instance of [[supervised learning]], i.e. learning where a training set of correctly identified observations is available.  The corresponding [[unsupervised learning|unsupervised]] procedure is known as [[cluster analysis|clustering]], and involves grouping data into categories based on some measure of inherent similarity or [[distance]].

Often, the individual observations are analyzed into a set of quantifiable properties, known variously as [[explanatory variables]] or ''features''.  These properties may variously be [[categorical data|categorical]] (e.g. &quot;A&quot;, &quot;B&quot;, &quot;AB&quot; or &quot;O&quot;, for [[blood type]]), [[ordinal data|ordinal]] (e.g. &quot;large&quot;, &quot;medium&quot; or &quot;small&quot;), [[integer|integer-valued]] (e.g. the number of occurrences of a particular word in an [[email]]) or [[real number|real-valued]] (e.g. a measurement of [[blood pressure]]). Other classifiers work by comparing observations to previous observations by means of a [[similarity function|similarity]] or [[metric (mathematics)|distance]] function.

An [[algorithm]] that implements classification, especially in a concrete implementation, is known as a '''classifier'''.  The term &quot;classifier&quot; sometimes also refers to the mathematical [[function (mathematics)|function]], implemented by a classification algorithm, that maps input data to a category.

Terminology across fields is quite varied. In [[statistics]], where classification is often done with [[logistic regression]] or a similar procedure, the properties of observations are termed [[explanatory variable]]s (or [[independent variable]]s, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the [[dependent variable]].  In machine learning, the observations are often known as ''instances'', the explanatory variables are termed ''features'' (grouped into a [[feature vector]]), and the possible categories to be predicted are ''classes''.  Other fields may use different terminology: e.g. in [[community ecology]], the term &quot;classification&quot; normally refers to [[cluster analysis]], i.e. a type of [[unsupervised learning]], rather than the supervised learning described in this article.

==Relation to other problems==
Classification and clustering are examples of the more general problem of [[pattern recognition]], which is the assignment of some sort of output value to a given input value.  Other examples are [[regression analysis|regression]], which assigns a real-valued output to each input; [[sequence labeling]], which assigns a class to each member of a sequence of values (for example, [[part of speech tagging]], which assigns a [[part of speech]] to each word in an input sentence); [[parsing]], which assigns a [[parse tree]] to an input sentence, describing the [[syntactic structure]] of the sentence; etc.

A common subclass of classification is [[probabilistic classification]].  Algorithms of this nature use [[statistical inference]] to find the best class for a given instance.  Unlike other algorithms, which simply output a &quot;best&quot; class, probabilistic algorithms output a [[probability]] of the instance being a member of each of the possible classes.  The best class is normally then selected as the one with the highest probability.  However, such an algorithm has numerous advantages over non-probabilistic classifiers:
*It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a ''confidence-weighted classifier'').
*Correspondingly, it can ''abstain'' when its confidence of choosing any particular output is too low.
*Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of ''error propagation''.

==Frequentist procedures==

Early work on statistical classification was undertaken by [[Ronald Fisher|Fisher]],&lt;ref&gt;[[R. A. Fisher|Fisher R.A.]] (1936) &quot; The use of multiple measurements in taxonomic problems&quot;, ''Annals of Eugenics'', 7, 179&amp;ndash;188&lt;/ref&gt;&lt;ref&gt;Fisher R.A. (1938) &quot; The statistical utilization of multiple measurements&quot;, ''Annals of Eugenics'', 8, 376&amp;ndash;386&lt;/ref&gt; in the context of two-group problems, leading to [[Fisher's linear discriminant]] function as the rule for assigning a group to a new observation.&lt;ref name=G1977&gt;Gnanadesikan, R. (1977) ''Methods for Statistical Data Analysis of Multivariate Observations'', Wiley. {{ISBN|0-471-30845-5}} (p. 83&amp;ndash;86)&lt;/ref&gt; This early work assumed that data-values within each of the two groups had a [[multivariate normal distribution]]. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be [[linear]].&lt;ref name=G1977/&gt;&lt;ref&gt;[[C. R. Rao|Rao, C.R.]] (1952) ''Advanced Statistical Methods in Multivariate Analysis'', Wiley. (Section 9c)&lt;/ref&gt; Later work for the multivariate normal distribution allowed the classifier to be [[nonlinear]]:&lt;ref&gt;[[T. W. Anderson|Anderson, T.W.]] (1958) ''An Introduction to Multivariate Statistical Analysis'', Wiley.&lt;/ref&gt; several classification rules can be derived based on slight different adjustments of the [[Mahalanobis distance]], with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.

==Bayesian procedures==

Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the sub-populations associated with the different groups within the overall population.&lt;ref&gt;Binder, D.A. (1978) &quot;Bayesian cluster analysis&quot;, ''[[Biometrika]]'', 65, 31&amp;ndash;38.&lt;/ref&gt; Bayesian procedures tend to be computationally expensive and, in the days before [[Markov chain Monte Carlo]] computations were developed, approximations for Bayesian clustering rules were devised.&lt;ref&gt;Binder, D.A. (1981) &quot;Approximations to Bayesian clustering rules&quot;, ''[[Biometrika]]'', 68, 275&amp;ndash;285.&lt;/ref&gt;

Some Bayesian procedures involve the calculation of  [[class membership probabilities|group membership probabilities]]: these can be viewed as providing a more informative outcome of a data analysis than a simple attribution of a single group-label to each new observation.

==Binary and multiclass classification==
Classification can be thought of as two separate problems – [[binary classification]] and [[multiclass classification]]. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.&lt;ref&gt;Har-Peled, S., Roth, D., Zimak, D. (2003) &quot;Constraint Classification for Multiclass Classification and Ranking.&quot; In: Becker, B., Thrun, S., Obermayer, K. (Eds) ''Advances in Neural Information Processing Systems 15: Proceedings of the 2002 Conference'', MIT Press. {{ISBN|0-262-02550-7}}&lt;/ref&gt; Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.

== Feature vectors ==
Most algorithms describe an individual instance whose category is to be predicted using a [[feature vector]] of individual, measurable properties of the instance.  Each property is termed a [[feature (pattern recognition)|feature]], also known in statistics as an [[explanatory variable]] (or [[independent variable]], although features may or may not be [[statistically independent]]).  Features may variously be [[binary data|binary]] (e.g. &quot;male&quot; or &quot;female&quot;); [[categorical data|categorical]] (e.g. &quot;A&quot;, &quot;B&quot;, &quot;AB&quot; or &quot;O&quot;, for [[blood type]]); [[ordinal data|ordinal]] (e.g. &quot;large&quot;, &quot;medium&quot; or &quot;small&quot;); [[integer|integer-valued]] (e.g. the number of occurrences of a particular word in an email); or [[real number|real-valued]] (e.g. a measurement of blood pressure).  If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words.  Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be ''discretized'' into groups (e.g. less than 5, between 5 and 10, or greater than 10)

== Linear classifiers ==
A large number of [[algorithm]]s for classification can be phrased in terms of a [[linear function]] that assigns a score to each possible category ''k'' by [[linear combination|combining]] the feature vector of an instance with a vector of weights, using a [[dot product]].  The predicted category is the one with the highest score.  This type of score function is known as a [[linear predictor function]] and has the following general form:

:&lt;math&gt;\operatorname{score}(\mathbf{X}_i,k) = \boldsymbol\beta_k \cdot \mathbf{X}_i,&lt;/math&gt;

where '''X'''&lt;sub&gt;''i''&lt;/sub&gt; is the feature vector for instance ''i'', '''&amp;beta;'''&lt;sub&gt;''k''&lt;/sub&gt; is the vector of weights corresponding to category ''k'', and score('''X'''&lt;sub&gt;''i''&lt;/sub&gt;, ''k'') is the score associated with assigning instance ''i'' to category ''k''.  In [[discrete choice]] theory, where instances represent people and categories represent choices, the score is considered the [[utility]] associated with person ''i'' choosing category ''k''.

Algorithms with this basic setup are known as [[linear classifier]]s.  What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.

Examples of such algorithms are
*[[Logistic regression]] and [[Multinomial logistic regression]]
*[[Probit regression]]
*The [[perceptron]] algorithm
*[[Support vector machine]]s
*[[Linear discriminant analysis]].

== Algorithms ==
{{prose|date=May 2012}}
Examples of classification algorithms include:
* [[Linear classifier]]s
** [[Fisher's linear discriminant]]
** [[Logistic regression]]
** [[Naive Bayes classifier]]
** [[Perceptron]]
*[[Support vector machine]]s
**[[Least squares support vector machine]]s
* [[Quadratic classifier]]s
* [[Variable kernel density estimation#Use for statistical classification|Kernel estimation]]
** [[k-nearest neighbor algorithm|k-nearest neighbor]]
* [[Boosting (meta-algorithm)]]
* [[Decision tree learning|Decision tree]]s
** [[Random forest]]s
* [[Artificial neural networks|Neural network]]s
* [[Learning vector quantization]]

== Evaluation ==
Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the [[No free lunch in search and optimization|no-free-lunch theorem]]). Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science.

The measures [[precision and recall]] are popular metrics used to evaluate the quality of a classification system. More recently, [[receiver operating characteristic]] (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms.

As a performance metric, the [[uncertainty coefficient]] has the advantage over simple [[accuracy]] in that it is not affected by the relative sizes of the different classes.
&lt;ref name=&quot;Mills2010&quot;&gt;
{{Cite journal
 | author = Peter Mills
 | title = Efficient statistical classification of satellite measurements
 | journal = International Journal of Remote Sensing
 | doi= 10.1080/01431161.2010.507795
 | year = 2011
}}&lt;/ref&gt;
Further, it will not penalize an algorithm for simply ''rearranging'' the classes.

==Application domains==
{{see also|Cluster analysis#Applications}}
Classification has many applications. In some of these it is employed as a [[data mining]] procedure, while in others more detailed statistical modeling is undertaken.

* [[Computer vision]]
** [[Medical imaging]] and medical image analysis
** [[Optical character recognition]]
** [[Video tracking]]
* [[Drug discovery]] and [[Drug development|development]]
** [[Toxicogenomics]]
** [[Quantitative structure-activity relationship]]
* [[Geostatistics]]
* [[Speech recognition]]
* [[Handwriting recognition]]
* [[Biometric]] identification
*[[Biological classification]]
* [[Statistical natural language processing]]
* [[Document classification]]
* Internet [[search engines]]
* [[Credit scoring]]
* [[Pattern recognition]]
*Micro-array classification

{{More footnotes|date=January 2010}}

== See also ==
{{Portal|Statistics}}
* [[Artificial intelligence]]
* [[Binary classification]]
* [[Class membership probabilities]]
* [[Classification rule]]
* [[Compound term processing]]
* [[Data mining]]
* [[Data warehouse]]
* [[Fuzzy logic]]
* [[Information retrieval]]
* [[List of datasets for machine learning research]]
* [[Machine learning]]
* [[Recommender system]]

==References==
{{Reflist}}

==External links==
{{Commonscat}}
* [http://blog.peltarion.com/2006/07/10/classifier-showdown/ Classifier showdown] A practical comparison of classification algorithms.
* [http://cmp.felk.cvut.cz/cmp/software/stprtool/ Statistical Pattern Recognition Toolbox for Matlab].
* [http://sites.google.com/site/tooldiag/ TOOLDIAG Pattern recognition toolbox].
* [http://libagf.sourceforge.net Statistical classification software] based on [[adaptive kernel density estimation]].
* [https://pal.sri.com/classification-suite/ PAL Classification Suite] written in Java.
* [http://www.math.le.ac.uk/people/ag153/homepage/KNN/KNN3.html  kNN and Potential energy] (Applet), [[University of Leicester]]
* [http://scikit-learn.org  scikit-learn] a widely used package in python
* [http://www.cs.waikato.ac.nz/ml/weka   Weka ] A java based package with an extensive variety of algorithms.

{{Statistics|analysis||state=expanded}}

{{DEFAULTSORT:Statistical Classification}}

[[Category:Classification algorithms|*]]
[[Category:Statistical classification| ]]</text>
      <sha1>iqlx1z8i5copdajuaotwqciyk5l9xk9</sha1>
    </revision>
  </page>
  <page>
    <title>Apprenticeship learning</title>
    <ns>0</ns>
    <id>19463198</id>
    <revision>
      <id>765554432</id>
      <parentid>736135770</parentid>
      <timestamp>2017-02-15T02:17:54Z</timestamp>
      <contributor>
        <username>CoolieCoolster</username>
        <id>28438779</id>
      </contributor>
      <minor/>
      <comment>Removed orphan tag as article isn't actually an orphan</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2893">'''Apprenticeship learning''', or '''apprenticeship via inverse reinforcement learning''' (AIRP), is a concept in the field of [[artificial intelligence]] and [[machine learning]], developed by Pieter Abbeel, Associate Professor in [[University of California, Berkeley|Berkeley]]'s [[Electrical engineering|EE]][[Computer science|CS]] department, and [[Andrew Ng]], Associate Professor in [[Stanford University]]'s Computer Science Department. It was incepted in 2004. AIRP deals with &quot;[[Markov decision process]] where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform&quot;&lt;ref&gt;[http://dl.acm.org/citation.cfm?id=1015430 Pieter Abbeel, Andrew Ng, “Apprenticeship learning via inverse reinforcement learning.” In 21st International Conference on Machine Learning (ICML). 2004.]&lt;/ref&gt;

AIRP concept is closely related to [[reinforcement learning]] (RL) that is a sub-area of [[machine learning]] concerned with how an ''agent'' ought to take ''actions'' in an ''environment'' so as to maximize some notion of long-term ''reward''.  AIRP algorithms are used when the reward function is unknown.  The algorithms use observations of the behavior of an expert to teach the ''agent'' the optimal ''actions'' in certain states of the ''environment''.

AIRP is a special case of the general area of [[learning from demonstration]] (LfD), where the goal is to learn a complex task by observing a set of expert traces (demonstrations). AIRP is the intersection of LfD and RL.

==Usage==
Apprenticeship learning has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.

One domain where apprenticeship learning has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like [[aerobatics]] for shows has been successful. These include [[aerobatic maneuver]]s like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and [[Andrew Ng]] - &quot;Autonomous Helicopter Aerobatics through Apprenticeship Learning&quot;&lt;ref&gt;[http://dl.acm.org/citation.cfm?id=1894944 Pieter Abbeel, Adam Coates, Andrew Ng, “Autonomous Helicopter Aerobatics through Apprenticeship Learning.” In Vol. 29, Issue 13 International Journal of Robotics Research. 2010.]&lt;/ref&gt;

==References==
{{reflist}}

==See also==
* [[Inverse reinforcement learning]]

{{DEFAULTSORT:Apprenticeship Learning}}
</text>
      <sha1>j0q9gdzn14v4jw1o6gwqgmh1m2bdyjn</sha1>
    </revision>
  </page>
  <page>
    <title>Subclass reachability</title>
    <ns>0</ns>
    <id>3119546</id>
    <revision>
      <id>641401945</id>
      <parentid>615987415</parentid>
      <timestamp>2015-01-07T12:10:44Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10703)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="448">{{Multiple issues|
{{unreferenced|date=January 2009}}
{{orphan|date=November 2011}}
{{context|date=February 2011}}
}}

In [[computational learning theory]] in [[mathematics]], given a [[Concept class|class of concepts]] C, a subclass D is '''reachable''' if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S).




{{Math-stub}}</text>
      <sha1>ir9sxribicolp4o7wws6l9xetw4cg4p</sha1>
    </revision>
  </page>
  <page>
    <title>Binary classification</title>
    <ns>0</ns>
    <id>205393</id>
    <revision>
      <id>805313892</id>
      <parentid>805169227</parentid>
      <timestamp>2017-10-14T15:03:58Z</timestamp>
      <contributor>
        <username>Bear-rings</username>
        <id>28216728</id>
      </contributor>
      <comment>[Medical test]]ing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10975">{{Refimprove|date=May 2011}}
'''Binary''' or '''binomial classification''' is the task of [[Statistical classification|classifying]] the elements of a given [[Set (mathematics)|set]] into two groups (predicting which group each one belongs to) on the basis of a [[classification rule]]. Contexts requiring a decision as to whether or not an item has some [[qualitative property]], some specified characteristic, or some typical binary classification include:
* [[Medical test]]ing to determine if a patient has certain disease or not – the classification property is the presence of the disease.
* A &quot;pass or fail&quot; [[test method]] or [[quality control]] in factories, i.e. deciding if a specification has or has not been met – a [[Go/no go]] classification.
* [[Information retrieval]], namely deciding whether a page or an article should be in the [[result set]] of a search or not – the classification property is the relevance of the article, or the usefulness to the user.

Binary classification is [[discretization|dichotomization]] applied to practical purposes, and in many practical binary classification problems, the two groups are not symmetric – rather than overall accuracy, the relative proportion of different [[type I and type II errors|types of errors]] is of interest. For example, in medical testing, a [[false positives and false negatives#False positive error|false positive]] (detecting a disease when it is not present) is considered differently from a [[false positives and false negatives#False negative error|false negative]] (not detecting a disease when it is present).

==Statistical binary classification==
[[Statistical classification]] is a problem studied in [[machine learning]].  It is a type of [[supervised learning]], a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories.  When there are only two categories the problem is known as statistical binary classification.

Some of the methods commonly used for binary classification are:
*[[Decision tree learning|Decision trees]]
*[[Random forests]]
*[[Bayesian network]]s
*[[Support vector machine]]s
*[[Neural network]]s
*[[Logistic regression]]
Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the [[feature vector]], the noise in the data and many other factors. For example [[random forests]] perform better than [[Support vector machine|SVM]] classifiers for 3D point clouds.&lt;ref&gt;{{Cite journal|url = |title = Automatic Identification of Window Regions on Indoor Point Clouds Using LiDAR and Cameras|last = Zhang &amp; Zakhor|first = Richard &amp; Avideh|date = 2014|journal = VIP Lab Publications|doi = |pmid = |access-date = }}&lt;/ref&gt; &lt;ref&gt;{{Cite journal|url = |title = Simplified markov random fields for efficient semantic labeling of 3D point clouds|last = Y. Lu and C. Rasmussen|date = 2012|journal = IROS|doi = |pmid = |access-date = }}&lt;/ref&gt;

==Evaluation of binary classifiers==
{{main|Evaluation of binary classifiers}}

[[Image:binary-classification-labeled.svg|thumb|220px|right|The left, and right, halves respectively contain instances that in fact have, and do not have, the condition. The oval contains instances that are classified (predicted) as positive (having the condition). Green and red respectively contain instances that are correctly (true), and wrongly (false), classified. &lt;br&gt;TP=True Positive; TN=True Negative; FP=False Positive (type I error); FN=False Negative (type II error); TPR=True Positive Rate; FPR=False Positive Rate; PPV=Positive Predictive Value; NPV=Negative Predictive Value.]]

There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine [[sensitivity and specificity]] are often used, while in information retrieval [[precision and recall]] are preferred. An important distinction is between metrics that are independent on the [[prevalence]] (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.

Given a classification of a specific data set, there are four basic data: the number of [[true positive]]s (TP), [[true negative]]s (TN), [[false positive]]s (FP), and [[false negative]]s (FN). These can be arranged into a 2×2 [[contingency table]], with columns corresponding to actual value – condition positive (CP) or condition negative (CN) – and rows corresponding to classification value – test outcome positive or test outcome negative. There are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form &quot;true positive row ratio&quot; or &quot;false negative column ratio&quot;, though there are conventional terms. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements.

The column ratios are [[True Positive Rate]] (TPR, aka '''[[Sensitivity (tests)|Sensitivity]]''' or [[Recall (information retrieval)|recall]]), with complement the [[False Negative Rate]] (FNR); and [[True Negative Rate]] (TNR, aka '''[[Specificity (tests)|Specificity]],''' SPC), with complement [[False Positive Rate]] (FPR). These are the proportion of the ''population with the condition'' (resp., without the condition) for which the test is correct (or, complementarily, for which the test is incorrect); these are independent of prevalence.

The row ratios are [[Positive Predictive Value]] (PPV, aka [[Precision (information retrieval)|precision]]), with complement the [[False Discovery Rate]] (FDR); and [[Negative Predictive Value]] (NPV), with complement the [[False Omission Rate]] (FOR). These are the proportion of the ''population with a given test result'' for which the test is correct (or, complementarily, for which the test is incorrect); these depend on prevalence.

In diagnostic testing, the main ratios used are the true column ratios – True Positive Rate and True Negative Rate – where they are known as [[sensitivity and specificity]]. In informational retrieval, the main ratios are the true positive ratios (row and column) – Positive Predictive Value and True Positive Rate – where they are known as [[precision and recall]].

One can take ratios of a complementary pair of ratios, yielding four [[Likelihood ratio (disambiguation)|likelihood ratios]] (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding [[likelihood ratios in diagnostic testing]]. Taking the ratio of one of these groups of ratios yields a final ratio, the [[diagnostic odds ratio]] (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an [[odds ratio]] – and is prevalence-independent.

There are a number of other metrics, most simply the [[Accuracy and precision#In binary classification|accuracy]] or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The [[F-score]] combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score ([[F1 score]]). Some metrics come from [[regression coefficient]]s: the [[markedness]] and the [[informedness]], and their [[geometric mean]], the [[Matthews correlation coefficient]]. Other metrics include [[Youden's J statistic]], the [[uncertainty coefficient]], the Phi coefficient, and Cohen's kappa.

==Converting continuous values to binary==
{{anchor|artificial}} &lt;!--Artificially binary value redirects here--&gt;
Tests whose results are of continuous values, such as most [[blood values]], can artificially be made binary by defining a [[cutoff (reference value)|cutoff value]], with test results being designated as [[positive or negative test|positive or negative]] depending on whether the resultant value is higher or lower than the cutoff.

However, such conversion causes a loss of information, as the resultant binary classification does not tell ''how much'' above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant [[Positive predictive value|positive]] or [[negative predictive value]] is generally higher than the [[predictive value]] given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of [[Human chorionic gonadotropin|hCG]] as a continuous value, a urine [[pregnancy test]] that measured 52 mIU/ml of hCG may show as &quot;positive&quot; with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as &quot;positive&quot; as the one of 52 mIU/ml.

==See also==
{{Portal|Statistics}}

* [[Bayesian inference#Examples|Examples of Bayesian inference]]
* [[Classification rule]]
* [[Detection theory]]
* [[Kernel methods]]
* [[Multiclass classification]]
* [[Multi-label classification]]
* [[One-class classification]]
* [[Prosecutor's fallacy]]
* [[Receiver operating characteristic]]
* [[Thresholding (image processing)]]
* [[Uncertainty coefficient]], aka Proficiency
* [[Qualitative property]]

==References==
{{reflist}}

== Bibliography ==
* [[Nello Cristianini]] and [[John Shawe-Taylor]]. ''An Introduction to Support Vector Machines and other kernel-based learning methods''. Cambridge University Press, 2000. {{ISBN|0-521-78019-5}} ''([http://www.support-vector.net] SVM Book)''
* John Shawe-Taylor and Nello Cristianini.  ''Kernel Methods for Pattern Analysis''.  Cambridge University Press, 2004.  {{ISBN|0-521-81397-2}} ''([http://www.kernel-methods.net] Kernel Methods Book)''
* Bernhard Schölkopf and A. J. Smola: ''Learning with Kernels''. MIT Press, Cambridge, MA, 2002. ''(Partly available on line: [http://www.learning-with-kernels.org].)'' {{ISBN|0-262-19475-9}}

{{Statistics|analysis||state=expanded}}


</text>
      <sha1>b6mieshmz6hvhpehi28gnryxyannakj</sha1>
    </revision>
  </page>
  <page>
    <title>Explanation-based learning</title>
    <ns>0</ns>
    <id>21638340</id>
    <revision>
      <id>792233338</id>
      <parentid>787365260</parentid>
      <timestamp>2017-07-25T09:19:34Z</timestamp>
      <contributor>
        <username>Arided</username>
        <id>10645696</id>
      </contributor>
      <comment>add see also link to One-shot learning</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7685">{{Cleanup-reorganize|date=December 2012}}
'''Explanation-based learning''' ('''EBL''') is a form of [[machine learning]] that exploits a very strong, or even perfect, [[domain theory]] in order to make generalizations or form concepts from training examples.&lt;ref&gt;{{cite journal|title=Special issue on explanation in case-based reasoning |journal=Artificial Intelligence Review|date=October 2005|first=|last=|volume=24|issue=2|pages=|id= |url=|format=|accessdate=2009-02-22 }}&lt;/ref&gt;

== Details==
An example of EBL using a perfect domain theory is a program that learns to play [[chess]] through example. A specific chess position that contains an important feature such as &quot;Forced loss of black queen in two moves&quot; includes many irrelevant features, such as the specific scattering of pawns on the board. EBL can take a single training example and determine what are the relevant features in order to form a generalization.&lt;ref&gt;Black-queen example from {{cite book | last = Mitchell | first = Tom | authorlink = | title = Machine Learning | publisher = McGraw-Hill | year = 1997 | location = | pages = 308–309 | url = | doi = | id = | isbn = 0-07-042807-7 }}&lt;/ref&gt;

A domain theory is ''perfect'' or ''complete'' if it contains, in principle, all information needed to decide any question about the domain. For example, the domain theory for chess is simply the rules of chess. Knowing the rules, in principle, it is possible to deduce the best move in any situation. However, actually making such a deduction is impossible in practice due to [[combinatoric explosion]].  EBL uses training examples to make searching for deductive consequences of a domain theory efficient in practice.

In essence, an EBL system works by finding a way to deduce each training example from the system's existing database of domain theory. Having a short [[Mathematical proof|proof]] of the training example extends the domain-theory database, enabling the EBL system to find and classify future examples that are similar to the training example very quickly.&lt;ref&gt;{{cite book | last = Mitchell | first = Tom | authorlink = | title = Machine Learning | publisher = McGraw-Hill | year = 1997 | location = | pages = 320 | url = | doi = | id = | isbn = 0-07-042807-7 | quote = In its pure form, EBL involves reformulating the domain theory to produce general rules that classify examples in a single inference step. }}&lt;/ref&gt;
The main drawback of the method---the cost of applying the learned proof macros, as these become numerous---was analyzed by Minton.&lt;ref&gt;{{cite journal | doi = 10.1016/0004-3702(90)90059-9 | last = Minton | first = Steven | journal = Artificial Intelligence | volume=42 | issue = 2-3 | pages = 363–392 | title = Quantitative Results Concerning the Utility Problem in Explanation-Based Learning | year = 1990 }}&lt;/ref&gt;

=== Basic formulation===
EBL software takes four inputs:

* a hypothesis space (the set of all possible conclusions)
* a domain theory (axioms about a domain of interest)
* training examples (specific facts that rule out some possible hypothesis)
* operationality criteria (criteria for determining which features in the domain are efficiently recognizable, e.g. which features are directly detectable using sensors)&lt;ref&gt;{{cite journal|title=Defining operationality for explanation-based learning|journal=Artificial Intelligence|year=1988|first=Richard|last=Keller|volume=35|issue=2|pages=227–241|id= |url=http://www.aaai.org/Papers/AAAI/1987/AAAI87-086.pdf|format=PDF|accessdate=2009-02-22 | quote = Current Operationality Defn.: A concept description is ''operational'' if it can be used efficiently to recognize instances of the concept it denotes|doi=10.1016/0004-3702(88)90013-6}} After stating the common definition, the paper actually argues against it in favor of more-refined criteria.&lt;/ref&gt;

== Application ==
An especially good application domain for an EBL is natural language processing (NLP). Here a rich domain theory, i.e., a natural language grammar---although neither perfect nor complete, is tuned to a particular application or particular language usage, using a treebank (training examples). Rayner pioneered this work.&lt;ref&gt;{{cite news | last = Rayner | first = Manny | title = Applying Explanation-Based Generalization to Natural Language Processing | location = Procs. International Conference on Fifth Generation Computing, Kyoto | pages = 1267–1274 | year = 1988 }}&lt;/ref&gt; The first successful industrial application was to a commercial NL interface to relational databases.&lt;ref&gt;{{cite news | last = Samuelsson | first = Christer |author2=Manny Rayner | title = Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System | location = Procs. 12th International Joint Conference on Artificial Intelligence, Sydney | pages = 609–615 | year = 1991 }}&lt;/ref&gt; The method has been successfully applied to several large-scale natural language parsing system,&lt;ref&gt;{{cite book | last = Samuelsson | first = Christer | title =  Fast Natural-Language Parsing Using Explanation-Based Learning | publisher =  Doctoral Dissertation, Royal Institute of Technology |  location = Stockholm | year = 1994 }}&lt;/ref&gt; where the utility problem was solved by omitting the original grammar (domain theory) and using specialized LR-parsing techniques, resulting in huge speed-ups, at a cost in coverage, but with a gain in disambiguation.
EBL-like techniques have also been applied to surface generation, the converse of parsing.&lt;ref&gt;{{cite news | last = Samuelsson | first = Christer | title = Example-Based Optimization of Surface-Generation Tables | location = in R. Mitkov and N. Nicolov (eds.) &quot;Recent Advances in Natural Language Processing,&quot; vol. 136 of &quot;Current Issues in Linguistic Theory&quot; | publisher = John Benjamins, Amsterdam | year = 1996}}&lt;/ref&gt;

When applying EBL to NLP, the operationality criteria can be hand-crafted,&lt;ref&gt;{{cite news | last = Rayner | first = Manny |author2=David Carter | title = Fast Parsing using Pruning and Grammar Specialization | location = Procs. ACL, Santa Cruz | year = 1996 }}&lt;/ref&gt; or can be
inferred from the treebank using either the entropy of its or-nodes&lt;ref&gt;{{cite news | last = Samuelsson | first = Christer | title = Grammar Specialization through Entropy Thresholds | year = 1994 | pages = 188–195 | location = Procs. ACL, Las Cruces }}&lt;/ref&gt;
or a target coverage/disambiguation trade-off (= recall/precision trade-off  = f-score).&lt;ref&gt;{{cite news | last = Cancedda | first = Nicola |author2=Christer Samuelsson | title = Corpus-based Grammar Specialization | year = 2000 | location = Procs 4th Computational Natural Language Learning Workshop }}&lt;/ref&gt;
EBL can also be used to compile grammar-based language models for speech recognition, from general unification grammars.&lt;ref&gt;{{cite book | last = Rayner | first = Manny |author2=Beth Ann Hockey |author3=Pierrette Bouillon| title = Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler | date = n.d. | isbn = 1-57586-526-2 }}&lt;/ref&gt;
Note how the utility problem, first exposed by Minton, was solved by discarding the original grammar/domain theory, and that the quoted articles tend to contain the phrase ''grammar specialization''---quite the opposite of the original term ''explanation-based generalization.'' Perhaps the best name for this technique would be ''data-driven search space reduction.''
Other people who worked on EBL for NLP include Guenther Neumann, Aravind Joshi, Srinivas Bangalore, and Khalil Sima'an.

== See also ==

* [[One-shot learning]]

== References ==
{{reflist}}

{{DEFAULTSORT:Explanation-Based Learning}}
</text>
      <sha1>50ajm9h6se9ef3d3gtue8d1ya6kxcd1</sha1>
    </revision>
  </page>
  <page>
    <title>Category utility</title>
    <ns>0</ns>
    <id>8964665</id>
    <revision>
      <id>795958644</id>
      <parentid>793340211</parentid>
      <timestamp>2017-08-17T15:57:24Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor/>
      <comment>Rescued 1 archive link; reformat 1 link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23290">'''Category utility''' is a measure of &quot;category goodness&quot; defined in {{Harvtxt|Gluck|Corter|1985}} and {{Harvtxt|Corter|Gluck|1992}}. It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as &quot;[[cue validity]]&quot; ({{Harvnb|Reed|1972}}; {{Harvnb|Rosch|Mervis|1975}}) and &quot;collocation index&quot; {{Harv|Jones|1983}}. It provides a normative [[information-theoretic]] measure of the ''predictive advantage'' gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does ''not'' possess knowledge of the category structure.  In this sense the motivation for the ''category utility'' measure is similar to the [[Information gain in decision trees|information gain]] metric used in [[decision tree]] learning. In certain presentations, it is also formally equivalent to the [[mutual information]], as discussed below.  A review of ''category utility'' in its probabilistic incarnation, with applications to [[machine learning]], is provided in {{Harvtxt|Witten|Frank|2005|pp=260–262}}.

==Probability-theoretic definition of Category Utility==
The [[probability-theoretic]] definition of ''category utility'' given in {{Harvtxt|Fisher|1987}} and {{Harvtxt|Witten|Frank|2005}} is as follows:

:&lt;math&gt;
CU(C,F) = \tfrac{1}{p} \sum_{c_j \in C} p(c_j) \left [\sum_{f_i \in F} \sum_{k=1}^m p(f_{ik}|c_j)^2 - \sum_{f_i \in F} \sum_{k=1}^m p(f_{ik})^2\right ]
&lt;/math&gt;

where &lt;math&gt;F = \{f_i\}, \ i=1 \ldots n&lt;/math&gt; is a size-&lt;math&gt;n\ &lt;/math&gt; set of &lt;math&gt;m\ &lt;/math&gt;-ary  features, and &lt;math&gt;C = \{c_j\} \ j=1 \ldots p&lt;/math&gt; is a set of &lt;math&gt;p\ &lt;/math&gt; categories.  The term  &lt;math&gt;p(f_{ik})\ &lt;/math&gt; designates the [[marginal probability]] that feature &lt;math&gt;f_i\ &lt;/math&gt; takes on value &lt;math&gt;k\ &lt;/math&gt;, and the term  &lt;math&gt;p(f_{ik}|c_j)\ &lt;/math&gt; designates the category-[[conditional probability]] that feature &lt;math&gt;f_i\ &lt;/math&gt; takes on value &lt;math&gt;k\ &lt;/math&gt; ''given'' that the object in question belongs to category &lt;math&gt;c_j\ &lt;/math&gt;.

The motivation and development of this expression for ''category utility'', and the role of the multiplicand &lt;math&gt;\textstyle \tfrac{1}{p}&lt;/math&gt; as a crude overfitting control, is given in the above sources.  Loosely {{Harv|Fisher|1987}}, the term &lt;math&gt;\textstyle  p(c_j) \sum_{f_i \in F} \sum_{k=1}^m p(f_{ik}|c_j)^2&lt;/math&gt; is  the expected number of attribute values that can be correctly guessed by an observer using a [[probability-matching]] strategy together with knowledge of the category labels, while &lt;math&gt;\textstyle p(c_j) \sum_{f_i \in F} \sum_{k=1}^m p(f_{ik})^2&lt;/math&gt; is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels.  Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure.

== Information-theoretic definition of the Category Utility ==
The [[information-theoretic]] definition of ''category utility''  for a set of entities with size-&lt;math&gt;n\ &lt;/math&gt; binary feature set  &lt;math&gt;F = \{f_i\}, \ i=1 \ldots n&lt;/math&gt;,  and a binary category  &lt;math&gt;C = \{c,\bar{c}\}&lt;/math&gt; is given in {{Harvtxt|Gluck|Corter|1985}} as follows:

:&lt;math&gt;
CU(C,F) = \left [p(c) \sum_{i=1}^n p(f_i|c)\log p(f_i|c) + p(\bar{c}) \sum_{i=1}^n p(f_i|\bar{c})\log p(f_i|\bar{c}) \right ] - \sum_{i=1}^n p(f_i)\log p(f_i)
&lt;/math&gt;

where &lt;math&gt;p(c)\ &lt;/math&gt; is the [[prior probability]] of an entity belonging to the positive category &lt;math&gt;c\ &lt;/math&gt; (in the absence of any feature information), &lt;math&gt;p(f_i|c)\ &lt;/math&gt; is the [[conditional probability]] of an entity having feature &lt;math&gt;f_i\ &lt;/math&gt; given that the entity belongs to category &lt;math&gt;c\ &lt;/math&gt;, &lt;math&gt;p(f_i|\bar{c})&lt;/math&gt; is likewise the conditional probability of an entity having feature &lt;math&gt;f_i\ &lt;/math&gt; given that the entity belongs to category &lt;math&gt;\bar{c}&lt;/math&gt;, and &lt;math&gt;p(f_i)\ &lt;/math&gt; is the prior probability of an entity possessing feature &lt;math&gt;f_i\ &lt;/math&gt; (in the absence of any category information).

The intuition behind the above expression is as follows: The term  &lt;math&gt;p(c)\textstyle  \sum_{i=1}^n p(f_i|c)\log p(f_i|c)&lt;/math&gt; represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category &lt;math&gt;c\ &lt;/math&gt;.  Similarly, the term  &lt;math&gt;p(\bar{c})\textstyle  \sum_{i=1}^n p(f_i|\bar{c})\log p(f_i|\bar{c})&lt;/math&gt; represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to  category &lt;math&gt;\bar{c}&lt;/math&gt;.  The sum of these two terms in the brackets is therefore the [[weighted average]] of these two costs.  The  final term, &lt;math&gt;\textstyle  \sum_{i=1}^n p(f_i)\log p(f_i)&lt;/math&gt;, represents the cost (in bits) of optimally encoding (or transmitting) feature information when no category information is available.  The value of the ''category utility'' will, in the above formulation, be negative (???).

=== Category Utility and Mutual Information ===
It is mentioned in {{Harvtxt|Gluck|Corter|1985}} and {{Harvtxt|Corter|Gluck|1992}} that the category utility is equivalent to the [[mutual information]].  Here we provide a simple demonstration of the nature of this equivalence.  Let us assume a set of entities each having the same &lt;math&gt;n&lt;/math&gt;  features, i.e.,  feature set &lt;math&gt;F = \{f_i\}, \ i=1 \ldots n&lt;/math&gt;, with each feature variable having cardinality  &lt;math&gt;m&lt;/math&gt;. That is, each feature has the capacity to adopt any of &lt;math&gt;m&lt;/math&gt; distinct values (which need ''not'' be ordered; all variables can be nominal); for the special case &lt;math&gt;m=2&lt;/math&gt; these features would be considered ''binary'', but more generally, for any &lt;math&gt;m&lt;/math&gt;, the features are simply ''m-ary''.  For our purposes, without loss of generality, we can replace feature set &lt;math&gt;F&lt;/math&gt; with a single aggregate variable &lt;math&gt;F_a&lt;/math&gt; that has cardinality &lt;math&gt;m^n&lt;/math&gt;, and adopts a unique value &lt;math&gt;v_i, \ i=1 \ldots m^n&lt;/math&gt; corresponding to each feature combination in the [[Cartesian product]] &lt;math&gt;\otimes F&lt;/math&gt;.  (Ordinality does ''not'' matter, because the mutual information is not sensitive to ordinality.)  In what follows, a term such as &lt;math&gt;p(F_a=v_i)&lt;/math&gt; or simply &lt;math&gt;p(v_i)&lt;/math&gt; refers to the [[probability]] with which &lt;math&gt;F_a&lt;/math&gt; adopts the particular value &lt;math&gt;v_i&lt;/math&gt;.  (Using the aggregate feature variable &lt;math&gt;F_a&lt;/math&gt; replaces multiple summations, and simplifies the presentation to follow.)

We assume also a single category  variable &lt;math&gt;C&lt;/math&gt;, which has cardinality &lt;math&gt;p&lt;/math&gt;. This is equivalent to a classification system in which there are &lt;math&gt;p&lt;/math&gt; non-intersecting categories.  In the special case of &lt;math&gt;p=2&lt;/math&gt; we have the two-category case discussed above.  From the definition of [[mutual information]] for discrete variables, the mutual information &lt;math&gt;I(F_a;C)&lt;/math&gt; between the aggregate feature variable &lt;math&gt;F_a&lt;/math&gt; and the category variable &lt;math&gt;C&lt;/math&gt; is given by:

:&lt;math&gt;
I(F_a;C) = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i,c_j) \log \frac{p(v_i,c_j)}{p(v_i)\,p(c_j)}
&lt;/math&gt;

where &lt;math&gt;p(v_i)&lt;/math&gt; is the [[prior probability]] of feature variable &lt;math&gt;F_a&lt;/math&gt; adopting value &lt;math&gt;v_i&lt;/math&gt;, &lt;math&gt;p(c_j)&lt;/math&gt; is the [[marginal probability]] of category variable &lt;math&gt;C&lt;/math&gt; adopting value &lt;math&gt;c_j&lt;/math&gt;, and &lt;math&gt;p(v_i,c_j)&lt;/math&gt; is the [[joint probability]] of variables &lt;math&gt;F_a&lt;/math&gt; and &lt;math&gt;C&lt;/math&gt; simultaneously adopting those respective values.  In terms of the conditional probabilities this can be re-written (or defined) as

:&lt;math&gt;
\begin{align}
I(F_a;C) &amp; = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i,c_j) \log \frac{p(v_i|c_j)}{p(v_i)} \\
   &amp; = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i|c_j)p(c_j) \left [\log p(v_i|c_j)- \log p(v_i) \right ] \\
   &amp; = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i|c_j)p(c_j) \log p(v_i|c_j)- \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i|c_j)p(c_j) \log p(v_i) \\
   &amp; = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i|c_j)p(c_j) \log p(v_i|c_j)- \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i,c_j) \log p(v_i) \\
   &amp; = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i|c_j)p(c_j) \log p(v_i|c_j)- \sum_{v_i \in F_a} \log p(v_i) \sum_{c_j \in C} p(v_i,c_j)  \\
   &amp; = \sum_{v_i \in F_a} \sum_{c_j \in C} p(v_i|c_j)p(c_j) \log p(v_i|c_j)- \sum_{v_i \in F_a} p(v_i) \log p(v_i)   \\
\end{align}
&lt;/math&gt;

If we will rewrite the original [[Category utility#Definition of the Category Utility|definition of the category utility]] from above, with &lt;math&gt;C = \{c,\bar{c}\}&lt;/math&gt;, we have

:&lt;math&gt;
CU(C,F) = \sum_{f_i \in F} \sum_{c_j \in C}   p(f_i|c_j) p(c_j) \log p(f_i|c_j) - \sum_{f_i \in F} p(f_i) \log p(f_i)
&lt;/math&gt;

This equation clearly has the same '''form''' as the (&lt;span style=color:blue;&gt;blue&lt;/span&gt;) equation expressing the mutual information between the feature set and the category variable; the difference is that the sum &lt;math&gt;\textstyle \sum_{f_i \in F}&lt;/math&gt; in the ''category utility'' equation  runs over independent binary variables &lt;math&gt;F = \{f_i\}, \ i=1 \ldots n&lt;/math&gt;, whereas the sum &lt;math&gt;\textstyle \sum_{v_i \in F_a}&lt;/math&gt; in the mutual information runs over ''values'' of the single &lt;math&gt;m^n&lt;/math&gt;-ary variable &lt;math&gt;F_a&lt;/math&gt;.  The two measures are actually equivalent then ''only'' when the features &lt;math&gt;\{f_i\}&lt;/math&gt;, are ''independent'' (and assuming that terms in the sum corresponding to &lt;math&gt;p(\bar{f_i})&lt;/math&gt; are also added).

== Insensitivity of category utility to ordinality ==
Like the [[mutual information]], the ''category utility'' is not sensitive to any ''ordering'' in the feature or category variable values.  That is, as far as the ''category utility'' is concerned, the category set &lt;code&gt;{small,medium,large,jumbo}&lt;/code&gt; is not qualitatively different from the category set &lt;code&gt;{desk,fish,tree,mop}&lt;/code&gt; since the formulation of the ''category utility'' does not account for any ordering of the class variable.  Similarly, a feature variable adopting values &lt;code&gt;{1,2,3,4,5}&lt;/code&gt; is not qualitatively different from a feature variable adopting values &lt;code&gt;{fred,joe,bob,sue,elaine}&lt;/code&gt;.  As far as the ''category utility'' or  ''mutual information'' are concerned, ''all'' category and feature variables are ''nominal variables.''  For this reason, ''category utility'' does not reflect any ''[[Gestalt psychology|gestalt]]'' aspects of &quot;category goodness&quot; that might be based on such ordering effects.  One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for [[mutual information]].

== Category &quot;goodness&quot;: Models and Philosophy==
This section provides some background on the origins of, and need for, formal measures of &quot;category goodness&quot; such as the ''category utility'', and some of the history that lead to the development of this particular metric.
===What makes a good category?===
At least since the time of [[Aristotle]] there has been a tremendous fascination in philosophy with the nature of [[concept]]s and [[universals]].  What kind of ''entity'' is a concept such as &quot;horse&quot;?  Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use.  Does the concept &quot;horse&quot; therefore have an independent existence outside of the mind?  If it does, then what is the locus of this independent existence?  The question of locus was an important  issue on which the classical schools of [[Plato]] and [[Aristotle]] famously differed.  However, they remained  in agreement that universals  ''did'' indeed have a mind-independent existence. There was, therefore, always a ''fact to the matter'' about which concepts and universals exist in the world.

In the late [[Middle Ages]] (perhaps beginning with [[William of Ockham|Occam]], although [[Porphyry (philosopher)|Porphyry]] also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue  began to erode, and it became acceptable among the so-called [[nominalists]] and [[empiricists]] to consider concepts and universals as strictly mental entities or conventions of language.  On this view of concepts—that they are purely representational constructs—a new question then comes to the fore: ''Why do we possess one set of concepts rather than another?''  What makes one set of concepts &quot;good&quot; and another set of concepts &quot;bad&quot;?  This is a question that modern philosophers, and subsequently [[machine learning]] theorists and cognitive scientists, have struggled with for many decades.

===What purpose do concepts serve?===
One approach to answering such questions is to investigate the &quot;role&quot; or &quot;purpose&quot; of concepts in cognition.  Thus, we ask: ''What are concepts good for in the first place?''  The answer provided by {{Harvtxt|Mill|1843/1936|p=425}} and many others is that classification (conception) is a precursor to ''[[Inductive reasoning|induction]]'':   By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage  ({{Harvnb|Smith|Medin|1981}};{{Harvnb|Harnad|2005}}). As [[J.S. Mill]] puts it {{Harv|Mill|1843/1936|pp=466–468}},

{{quotation|The general problem of classification... [is] to provide that things shall be thought of in such groups, and those groups in such an order, as will best conduce to the remembrance and to the ascertainment of their laws... [and] one of the uses of such a classification that by drawing attention to the properties on which it is founded, and which, if the classification be good, are marks of many others, it facilitates the discovery of those others.}}

From this base, [[J.S. Mill|Mill]] reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of ''category utility'':

{{quotation|The ends of scientific classification are best answered when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed. The properties, therefore, according to which objects are classified should, if possible, be those which are causes of many other properties; or, at any rate, which are sure marks of them.}}

One may compare this to the &quot;category utility hypothesis&quot; proposed by {{Harvtxt|Corter|Gluck|1992}}: &quot;A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category.&quot;   Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the object's class, and, simultaneously, the object class is maximally informative about the object's features.  In other words, a useful classification scheme is one in which we can use category knowledge to accurately infer object properties, and we can use property knowledge to accurately infer object classes.  One may also compare this idea to [[Aristotle]]'s criterion of ''counter-predication'' for definitional predicates, as well as to the notion of concepts described in [[formal concept analysis]].

===Attempts at formalization===
A variety of different measures have been suggested with an aim of formally capturing this notion of &quot;category goodness,&quot;  the best known of which is probably the &quot;[[cue validity]]&quot;. Cue validity of a feature &lt;math&gt;f_i\ &lt;/math&gt; with respect to category &lt;math&gt;c_j\ &lt;/math&gt; is defined as the conditional probability of the category given the feature ({{Harvnb|Reed|1972}};{{Harvnb|Rosch|Mervis|1975}};{{Harvnb|Rosch|1978}}), &lt;math&gt;p(c_j|f_i)\ &lt;/math&gt;, or as the deviation of the conditional probability from the category base rate ({{Harvnb|Edgell|1993}};{{Harvnb|Kruschke|Johansen|1999}}), &lt;math&gt;p(c_j|f_i)-p(c_j)\ &lt;/math&gt;. Clearly, these measures quantify only inference from feature to category (i.e., ''cue validity''), but not from category to feature, i.e., the ''category validity'' &lt;math&gt;p(f_i|c_j)\ &lt;/math&gt;.  Also, while the cue validity was originally intended to account for the demonstrable appearance of ''[[basic categories]]'' in human cognition—categories of a particular level of generality that are evidently preferred by human learners—a number of major flaws in the cue validity quickly emerged in this regard ({{Harvnb|Jones|1983}};{{Harvnb|Murphy|1982}};{{Harvnb|Corter|Gluck|1992}}, and others).

One attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by {{Harvtxt|Jones|1983}} in defining the &quot;collocation index&quot; as the product &lt;math&gt;p(c_j|f_i) p(f_i|c_j)\ &lt;/math&gt;, but this construction was  fairly ''ad hoc'' (see {{Harvnb|Corter|Gluck|1992}}).  The ''category utility'' was introduced as a more sophisticated  refinement of the cue validity, which attempts to more rigorously quantify the full inferential power of a class structure.  As shown above, on a certain view the category utility  is equivalent  to the [[mutual information]] between the feature variable and the category variable. It has been suggested that categories having the greatest overall  ''category utility'' are those that are not only those &quot;best&quot; in a normative sense, but also those human learners prefer to use, e.g., &quot;basic&quot; categories {{Harv|Corter|Gluck|1992}}.  Other related measures of category goodness are &quot;cohesion&quot; ({{Harvnb|Hanson|Bauer|1989}};{{Harvnb|Gennari|Langley|Fisher|1989}}) and &quot;salience&quot; {{Harv|Gennari|1989}}.

== Applications ==
* Category utilility is used as the category evaluation measure in the popular [[conceptual clustering]] algorithm called COBWEB {{Harv|Fisher|1987}}.

== See also ==
* [[Concept]], [[Concept learning]]
* [[Abstraction]]
* [[Universals]]
* [[Conceptual Clustering]]
* [[Unsupervised learning]]

== References ==
{{refbegin|2}}
* {{Citation | last2=Gluck | first2=Mark A. | last=Corter | first=James E. | title=Explaining basic categories: Feature predictability and information | journal=Psychological Bulletin | volume=111 | issue=2 | year=1992 | pages=291–303 | url=http://128.83.97.10/HomePage/Group/LoveLAB/love/classes/concepts/CorterGluck1992.pdf | archive-url=https://web.archive.org/web/20110810135319/http://128.83.97.10/HomePage/Group/LoveLAB/love/classes/concepts/CorterGluck1992.pdf | dead-url=yes | archive-date=2011-08-10 | doi=10.1037/0033-2909.111.2.291 }}
* {{Citation | last=Edgell| first=Stephen E.| year= 1993| chapter=Using configural and dimensional information | editor= N. John Castellan   | title=Individual and Group Decision Making: Current Issues | publisher=Lawrence Erlbaum| place=[[Hillsdale, New Jersey]]| pages=43–64}}
* {{Citation | last=Fisher| first=Douglas H. | title=Knowledge acquisition via incremental conceptual clustering | journal=Machine Learning |volume=2 |issue=2 | year=1987| pages=139–172 | url=http://www.springerlink.com/content/qj16212n7537n6p3/ | doi=10.1007/BF00114265}}
* {{Citation | last=Gennari| first=John H.| year= 1989| chapter=Focused concept formation | editor= Alberto Maria Segre   | title=Proceedings of the Sixth International Workshop on Machine Learning | publisher=Morgan Kaufmann| place=[[Ithaca, NY]]| pages=379–382}}
* {{Citation | last=Gennari| first=John H. | last2=Langley| first2=Pat |last3=Fisher| first3=Doug | title=Models of incremental concept formation | journal=Artificial Intelligence |volume=40 |issue=1-3 | year=1989| pages=11–61 | doi=10.1016/0004-3702(89)90046-5}}
* {{Citation |  last=Gluck| first=Mark A. | last2=Corter| first2=James E. | year= 1985 | chapter=Information, uncertainty, and the utility of categories | title=Program of the Seventh Annual Conference of the Cognitive Science Society | accessdate= |pages=283–287 }}
* {{Citation | last=Hanson| first=Stephen José | last2=Bauer| first2=Malcolm | title=Conceptual clustering, categorization, and polymorphy | journal=Machine Learning |volume=3 |issue=4 | year=1989| pages=343–372 | url=http://www.springerlink.com/content/j383105014543328/ | doi=10.1007/BF00116838}}
* {{Citation | last=Harnad| first=Stevan | year= 2005| chapter=To cognize is to categorize: Cognition is categorization | editor= Henri Cohen &amp; Claire Lefebvre | title=Handbook of Categorization in Cognitive Science | publisher=Elsevier | place=Amsterdam | url=http://eprints.ecs.soton.ac.uk/11725/| pages=19–43 }}
* {{Citation | last=Jones| first=Gregory V. | title=Identifying basic categories | journal=Psychological Bulletin |volume=94 |issue=3 | year=1983 | pages=423–428 | doi=10.1037/0033-2909.94.3.423}}
* {{Citation | last=Kruschke| first=John K. | last2=Johansen| first2=Mark K. | title=A model of probabilistic category learning | journal=Journal of Experimental Psychology: Learning, Memory, and Cognition |volume=25 |issue=5 | year=1999| pages=1083–1119 | doi=10.1037/0278-7393.25.5.1083}}
* {{Citation | last=Mill| first=John Stuart | title=A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Principles of Evidence and the Methods of Scientific Investigation | publisher=Longmans, Green and Co. | place=[[London]] | year=1843/1936| authorlink=John Stuart Mill}}.
* {{Citation | last=Murphy| first=Gregory L. | title=Cue validity and levels of categorization | journal=Psychological Bulletin |volume=91 |issue=1 | year=1982| pages=174–177 | doi=10.1037/0033-2909.91.1.174}}
* {{Citation | last=Reed| first=Stephen K. | title=Pattern recognition and categorization | journal=Cognitive Psychology |volume=3 |issue=3 | year=1972 | pages=382–407 | doi=10.1016/0010-0285(72)90014-x}}
* {{Citation | last=Rosch| first=Eleanor| year= 1978| chapter=Principles of categorization | editor=  Eleanor Rosch &amp; Barbara B. Lloyd  | title=Cognition and Categorization | publisher=Lawrence Erlbaum| place=[[Hillsdale, New Jersey]]| pages=27–48}}
* {{Citation | last=Rosch| first=Eleanor | last2=Mervis| first2=Carolyn B.| title=Family Resemblances: Studies in the Internal Structure of Categories | journal=Cognitive Psychology |volume=7 |issue=4 | year=1975 | pages=573–605 | doi=10.1016/0010-0285(75)90024-9}}
* {{Citation | last=Smith| first=Edward E. | last2=Medin| first2=Douglas L. |title=Categories and Concepts | publisher=Harvard University Press | place=[[Cambridge, MA]] | year=1981}}
* {{Citation | last=Witten| first=Ian H. | last2=Frank| first2=Eibe |title=Data Mining: Practical Machine Learning Tools and Techniques | publisher=Morgan Kaufmann | place=[[Amsterdam]] | year=2005| url=http://www.cs.waikato.ac.nz/~ml/weka/book.html}}
{{refend}}

[[Category:Machine learning|Category utility]]
[[Category:Cognitive science|Category utility]]</text>
      <sha1>trqduc9m62fouu2rpz5xuix7fyovtnr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Support vector machines</title>
    <ns>14</ns>
    <id>31176997</id>
    <revision>
      <id>418738709</id>
      <parentid>418738637</parentid>
      <timestamp>2011-03-14T06:13:17Z</timestamp>
      <contributor>
        <username>X7q</username>
        <id>9215724</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="114">{{catmain|Support vector machine}}


[[Category:Kernel methods for machine learning]]</text>
      <sha1>se85tg5oezeg6i7yvgollp30luatj65</sha1>
    </revision>
  </page>
  <page>
    <title>Nearest neighbor search</title>
    <ns>0</ns>
    <id>7309022</id>
    <revision>
      <id>787166094</id>
      <parentid>782457628</parentid>
      <timestamp>2017-06-23T20:46:41Z</timestamp>
      <contributor>
        <username>Brightgalrs</username>
        <id>7948812</id>
      </contributor>
      <minor/>
      <comment>proximity search leads back to this article Copyedit (minor)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23126">'''Nearest neighbor search''' ('''NNS'''), as a form of proximity search,  is the [[optimization problem]] of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set ''S'' of points in a space ''M'' and a query point ''q''&amp;nbsp;∈&amp;nbsp;''M'', find the closest point in ''S'' to ''q''. [[Donald Knuth]] in vol. 3 of ''[[The Art of Computer Programming]]'' (1973) called it the '''post-office problem''', referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a ''k''-NN search, where we need to find the ''k'' closest points.

Most commonly ''M'' is a  [[metric space]] and dissimilarity is expressed as a [[distance metric]], which is symmetric and satisfies the [[triangle inequality]]. Even more common, ''M'' is taken to be the ''d''-dimensional [[vector space]] where dissimilarity is measured using the [[Euclidean distance]], [[Taxicab geometry|Manhattan distance]] or other [[Statistical distance|distance metric]]. However, the dissimilarity function can be arbitrary. One example are asymmetric [[Bregman divergence]]s, for which the triangle inequality does not hold.&lt;ref name=Cayton2008&gt;{{Cite journal
 | last1 = Cayton | first1 = Lawerence
 | year = 2008
 | title =  Fast nearest neighbor retrieval for bregman divergences.
 | journal = Proceedings of the 25th international conference on Machine learning
 | pages = 112–119
}}&lt;/ref&gt;

==Applications==
The nearest neighbor search problem arises in numerous fields of application, including:
* [[Pattern recognition]] – in particular for [[optical character recognition]]
* [[Statistical classification]] – see [[k-nearest neighbor algorithm]]
* [[Computer vision]]
* [[Computational geometry]] – see [[Closest pair of points problem]]
* [[Database]]s – e.g. [[content-based image retrieval]]
* [[Coding theory]] – see [[Decoding methods|maximum likelihood decoding]]
* [[Data compression]] – see [[MPEG-2]] standard
* [[Robotic]] sensing&lt;ref name=panSearch&gt;{{cite conference|last1=Bewley|first1=A.|last2=Upcroft|first2=B.|date=2013|title=Advantages of Exploiting Projection Structure for Segmenting Dense 3D Point Clouds|conference=Australian Conference on Robotics and Automation |url=http://www.araa.asn.au/acra/acra2013/papers/pap148s1-file1.pdf}}&lt;/ref&gt;
* [[Recommender system|Recommendation systems]], e.g. see [[Collaborative filtering]]
* [[Internet marketing]] – see [[contextual advertising]] and [[behavioral targeting]]
* [[DNA sequencing]]
* [[Spell checking]] – suggesting correct spelling
* [[Plagiarism detection]]
* [[Contact searching algorithms in FEA]]
* [[Similarity score]]s for predicting career paths of professional athletes.
* [[Cluster analysis]] – assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on [[Euclidean distance]]
* [[Chemical similarity]]
* [[Motion planning#Sampling-based algorithms|Sampling-based motion planning]]

==Methods==

Various solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the [[curse of dimensionality]] states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.

=== Exact methods ===

====Linear search====
The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the &quot;best so far&quot;.  This algorithm, sometimes referred to as the naive approach, has a [[running time]] of ''O''(''dN'') where ''N'' is the [[cardinality]] of ''S'' and ''d'' is the dimensionality of ''M''.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.&lt;ref&gt;{{cite journal|title=A quantitative analysis and performance study for similarity search methods in high dimensional spaces|last1=Weber|first1=Roger|last2=Schek|first2=Hans-J.|last3=Blott|first3=Stephen | url=http://www.vldb.org/conf/1998/p194.pdf}}&lt;/ref&gt;

====Space partitioning====
Since the 1970s, [[branch and bound]] methodology has been applied to the problem. In the case of Euclidean space this approach is known as [[spatial index]] or spatial access methods. Several [[Space partitioning|space-partitioning]] methods have been developed for solving the NNS problem.  Perhaps the simplest is the [[k-d tree]], which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is ''O''(log&amp;nbsp;''N'') &lt;ref&gt;{{cite web|title=An introductory tutorial on KD trees|author=Andrew Moore | url=http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&amp;language=en}}&lt;/ref&gt; in the case of randomly distributed points, worst case complexity is ''O''(''kN''^(1-1/''k''))&lt;ref name=Lee1977&gt;{{Cite journal
 | last1 = Lee | first1 = D. T. | author1-link = Der-Tsai Lee
 | last2 = Wong | first2 = C. K.
 | year = 1977
 | title = Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees
 | journal = Acta Informatica
 | volume = 9
 | issue = 1
 | pages = 23–29
 | doi = 10.1007/BF00263763
 | postscript = .
}}&lt;/ref&gt;
Alternatively the [[R-tree]] data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the [[R* tree]].&lt;ref&gt;{{Cite conference | doi = 10.1145/223784.223794| chapter = Nearest neighbor queries| title = Proceedings of the 1995 ACM SIGMOD international conference on Management of data  – SIGMOD '95| pages = 71| year = 1995| last1 = Roussopoulos | first1 = N. | last2 = Kelley | first2 = S. | last3 = Vincent | first3 = F. D. R. | isbn = 0897917316}}&lt;/ref&gt; R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.

In case of general metric space branch and bound approach is known under the name of [[metric trees]]. Particular examples include [[vp-tree]] and [[BK-tree]].

Using a set of points taken from a 3-dimensional space and put into a [[Binary space partitioning|BSP tree]], and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm.  (Strictly speaking, no such point may exist, because it may not be unique.  But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.)  The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point.  This may not be the case, but it is a good heuristic.  After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane.  This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched.  If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space.  If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result.  The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.

=== Approximation methods ===
An approximation algorithm is allowed to return a point, whose distance from the query is at most &lt;math&gt;c&lt;/math&gt; times the distance from the query to its nearest points. The appeal of this approach is that, in many cases, an approximate nearest neighbor is almost as good as the exact one. In particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter. &lt;ref&gt;{{Cite journal|last=Andoni|first=A.|last2=Indyk|first2=P.|date=2006-10-01|title=Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions|url=http://ieeexplore.ieee.org/document/4031381/|journal=2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)|pages=459–468|doi=10.1109/FOCS.2006.49}}&lt;/ref&gt;

====Locality sensitive hashing====

[[Locality sensitive hashing]] (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.&lt;ref&gt;{{cite web|author1=A. Rajaraman  |author2=J. Ullman |lastauthoramp=yes | url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3. |year=2010}}&lt;/ref&gt;

====Nearest neighbor search in spaces with small intrinsic dimension====

The [[cover tree]] has a theoretical bound that is based on the dataset's [[doubling constant]]. The bound on search time is ''O''(''c''&lt;sup&gt;12&lt;/sup&gt;&amp;nbsp;log&amp;nbsp;''n'') where ''c''  is the [[Expansivity constant|expansion constant]] of the dataset.

====Projected radial search====

In the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem.
This approach requires that the 3D data is organized by a projection to a two dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries.
These assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general.
In practice this technique has an average search time of ''O''(''1'')  or ''O''(''K'')  for the ''k''-nearest neighbor problem when applied to real world stereo vision data.
&lt;ref name=panSearch/&gt;

====Vector approximation files====

In high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.&lt;ref&gt;{{cite journal|title=An Approximation-Based Data Structure for Similarity Search|last1=Weber|first1=Roger|last2=Blott|first2=Stephen|url=https://pdfs.semanticscholar.org/83e4/e3281411ffef40654a4b5d29dae48130aefb.pdf}}&lt;/ref&gt;

====Compression/clustering based search====
The VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is [[Vector Quantization]] (VQ), implemented through clustering. The database is clustered and the most &quot;promising&quot; clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.&lt;ref&gt;{{cite journal|title=Adaptive cluster-distance bounding for similarity search in image databases|last1=Ramaswamy|first1=Sharadh|last2=Rose|first2=Kenneth|journal=ICIP|date=2007}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|title=Adaptive cluster-distance bounding for high-dimensional indexing|last1=Ramaswamy|first1=Sharadh|last2=Rose|first2=Kenneth|journal=TKDE|date=2010}}&lt;/ref&gt; Also note the parallels between clustering and LSH.

====Greedy walk search in small-world graphs====
One possible way to solve NNS is to construct a graph &lt;math&gt;G(V,E)&lt;/math&gt;, where every point &lt;math&gt;x_i \in S &lt;/math&gt; is uniquely associated with vertex &lt;math&gt;v_i \in V &lt;/math&gt;. The search of the point in the set ''S'' closest to the query ''q'' takes the form of the search of vertex in the graph &lt;math&gt;G(V,E)&lt;/math&gt;.
One of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex &lt;math&gt;v_i \in V &lt;/math&gt;. The algorithm computes a distance value from the query q to each vertex from the neighborhood &lt;math&gt;\{v_j:(v_i,v_j) \in E\}&lt;/math&gt; of  the current vertex &lt;math&gt;v_i&lt;/math&gt;, and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.
This idea was exploited in the VoroNet system for the plane,&lt;ref name=voroNet&gt;{{Cite journal
 | last1 = Olivier | first1 = Beaumont
 | last2 = Kermarrec | first2 = Anne-Marie
 | last3 = Marchal | first3 = Loris
 | last4 = Rivière | first4 = Etienne
 | year = 2006
 | title = VoroNet: A scalable object network based on Voronoi tessellations
 | journal = INRIA
 | volume = RR-5833
 | issue = 1
 | pages = 23–29
 | doi = 10.1007/BF00263763
 | postscript = .
}}&lt;/ref&gt; in the RayNet system for the &lt;math&gt;\mathbb{E}^n&lt;/math&gt;,&lt;ref name=rayNet&gt;{{Cite journal
 | last1 = Olivier | first1 = Beaumont
 | last2 = Kermarrec | first2 = Anne-Marie
 | last3 = Rivière | first3 = Etienne
 | year = 2007
 | title = Peer to Peer Multidimensional Overlays: Approximating Complex Structures
 | journal = Principles of Distributed Systems
 | volume =  4878
 | issue = .
 | pages = 315–328
 | doi = 10.1007/978-3-540-77096-1_23
 | isbn = 978-3-540-77095-4
 | postscript = .
}}&lt;/ref&gt; and for the general metric space in the Metrized Small World algorithm.&lt;ref name=msw2014&gt;{{Cite journal
 | last1 = Malkov | first1 = Yury
 | last2 = Ponomarenko | first2 = Alexander
 | last3 = Krylov | first3 = Vladimir
 | last4 = Logvinov | first4 = Andrey
 | year = 2014
 | title = Approximate nearest neighbor algorithm based on navigable small world graphs
 | journal = Information Systems
 | volume = 45
 | pages = 61–68
 | doi = 10.1016/j.is.2013.10.006
 | postscript = .
}}&lt;/ref&gt;

==Variants==

There are numerous variants of the NNS problem and the two most well-known are the [[K-nearest neighbor algorithm|''k''-nearest neighbor search]] and the [[ε-approximate nearest neighbor search]].

===&lt;span id=&quot;K-nearest neighbor&quot;&gt; ''k''-nearest neighbor &lt;/span&gt;===

[[K-nearest neighbor algorithm|''k''-nearest neighbor search]] identifies the top ''k'' nearest neighbors to the query.  This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. ''k''-nearest neighbor graphs are graphs in which every point is connected to its ''k'' nearest neighbors.

===Approximate nearest neighbor===
In some applications it may be acceptable to retrieve a &quot;good guess&quot; of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.

Algorithms that support the approximate nearest neighbor search include [[Locality-sensitive hashing#LSH algorithm for nearest neighbor search|locality-sensitive hashing]], [[best bin first]] and [[balanced box-decomposition tree]] based search.&lt;ref&gt;{{cite journal|first1=S.|last1=Arya|author2-link=David Mount|first2=D. M.|last2=Mount|author3-link=Nathan Netanyahu|first3=N. S.|last3=Netanyahu|first4=R.|last4=Silverman|first5=A.|last5=Wu|title=An optimal algorithm for approximate nearest neighbor searching|journal= Journal of the ACM|volume=45|number=6|pages=891–923|date=1998|url=http://www.cse.ust.hk/faculty/arya/pub/JACM.pdf}}&lt;/ref&gt;

===Nearest neighbor distance ratio===

[[Nearest neighbor distance ratio]] do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in [[Content-based image retrieval|CBIR]] to retrieve pictures through a &quot;query by example&quot; using the similarity between local features. More generally it is involved in several [[Pattern matching|matching]] problems.

===Fixed-radius near neighbors===

[[Fixed-radius near neighbors]] is the problem where one wants to efficiently find all points given in [[Euclidean space]] within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.

===All nearest neighbors===

For some applications (e.g. [[entropy estimation]]), we may have ''N'' data-points and wish to know which is the nearest neighbor ''for every one of those N points''. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these ''N'' queries to produce a more efficient search. As a simple example: when we find the distance from point ''X'' to point ''Y'', that also tells us the distance from point ''Y'' to point ''X'', so the same calculation can be reused in two different queries.

Given a fixed dimension, a semi-definite positive norm (thereby including every  [[lp space|L&lt;sup&gt;p&lt;/sup&gt; norm]]), and ''n'' points in this space, the nearest neighbour of every point can be found in ''O''(''n''&amp;nbsp;log&amp;nbsp;''n'') time and the ''m'' nearest neighbours of every point can be found in ''O''(''mn''&amp;nbsp;log&amp;nbsp;''n'') time.&lt;ref&gt;{{citation
 | last = Clarkson | first = Kenneth L. | author-link = Kenneth L. Clarkson
 | contribution = Fast algorithms for the all nearest neighbors problem
 | doi = 10.1109/SFCS.1983.16
 | pages = 226–232
 | title = 24th IEEE Symp. Foundations of Computer Science, (FOCS '83)
 | year = 1983| isbn = 0-8186-0508-1 }}.&lt;/ref&gt;&lt;ref name=Vaidya&gt;{{Cite journal
 | doi = 10.1007/BF02187718
 | last1 = Vaidya | first1 = P. M.
 | year = 1989
 | title = An ''O''(''n''&amp;nbsp;log&amp;nbsp;''n'') Algorithm for the All-Nearest-Neighbors Problem
 | journal = [[Discrete and Computational Geometry]]
 | volume = 4
 | issue = 1
 | pages = 101–115
 | url = http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&amp;pi=8
 | postscript = .
}}&lt;/ref&gt;

==See also==
{{div col|colwidth=20em}}
* [[Ball tree]]
* [[Closest pair of points problem]]
* [[Cluster analysis]]
* [[Content-based image retrieval]]
* [[Curse of dimensionality]]
* [[Digital signal processing]]
* [[Dimension reduction]]
* [[Fixed-radius near neighbors]]
* [[Fourier analysis]]
* [[Instance-based learning]]
* [[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]]
* [[Linear least squares (mathematics)|Linear least squares]]
* [[Locality sensitive hashing]]
* [[MinHash]]
* [[Multidimensional analysis]]
* [[Nearest-neighbor interpolation]]
* [[Neighbor joining]]
* [[Principal component analysis]]
* [[Range search]]
* [[Set cover problem]]
* [[Similarity learning]]
* [[Singular value decomposition]]
* [[Sparse distributed memory]]
* [[Statistical distance]]
* [[Time series]]
* [[Voronoi diagram]]
* [[Wavelet]]

{{div col end}}

==Notes==
&lt;references /&gt;

==References==
* {{cite journal |last= Andrews |first=L. |title= A template for the nearest neighbor problem |journal= C/C++ Users Journal |volume=19 |number=11 |date= November 2001 |pages=40–49 |issn=1075-2838 |url= http://www.ddj.com/architect/184401449}}
* {{cite journal |last1=Arya|first1=S. |first2=D.M. |last2=Mount |first3=N. S. |last3=Netanyahu |first4=R. |last4=Silverman |first5=A. Y. |last5=Wu |title= An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions |journal= Journal of the ACM|volume= 45 |number=6 |pages= 891–923}}
* {{cite journal |last1=Beyer |first1=K. |last2= Goldstein |first2=J. |last3= Ramakrishnan |first3=R. |last4=Shaft |first4=U. |date=1999|title=When is nearest neighbor meaningful? |journal=Proceedings of the 7th ICDT |location=Jerusalem, Israel}}
* {{cite journal |first1=Chung-Min |last1= Chen |first2=Yibei|last2=Ling|title=A Sampling-Based Estimator for Top-k Query |journal=ICDE |date=2002 |pages= 617–627}}
* {{cite book |last=Samet |first=H. |date= 2006 |title= Foundations of Multidimensional and Metric Data Structures |publisher= Morgan Kaufmann |isbn=0-12-369446-9}}
* {{cite book |last1=Zezula |first1=P. |last2= Amato |first2=G. |last3=Dohnal |first3=V. |last4=Batko |first4=M. |title= Similarity Search – The Metric Space Approach|publisher=Springer|date=2006|isbn=0-387-29146-6}}

==Further reading==
* {{cite book | last = Shasha | first = Dennis | title = High Performance Discovery in Time Series | publisher = Springer | location = Berlin | year = 2004 | isbn = 0-387-00857-8 }}

==External links==
{{commons category|Nearest neighbours search}}
* [http://simsearch.yury.name/tutorial.html Nearest Neighbors and Similarity Search] – a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits
* [http://sswiki.tierra-aoi.net Similarity Search Wiki] – a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours
* [http://www.cgal.org/Pkg/SpatialSearchingD dD Spatial Searching] in [[CGAL]] – the Computational Geometry Algorithms Library
* [https://github.com/searchivarius/NonMetricSpaceLib Non-Metric Space Library] – An open source similarity search library containing realisations of various Nearest neighbor search methods.

{{DEFAULTSORT:Nearest Neighbor Search}}







</text>
      <sha1>edys2jue959gabb93513gmi92ooog0h</sha1>
    </revision>
  </page>
  <page>
    <title>Supervised learning</title>
    <ns>0</ns>
    <id>20926</id>
    <revision>
      <id>809538176</id>
      <parentid>808421925</parentid>
      <timestamp>2017-11-09T19:34:25Z</timestamp>
      <contributor>
        <ip>157.27.139.34</ip>
      </contributor>
      <comment>/* Other factors to consider (important) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20935">{{see also|Unsupervised learning}}
{{Machine learning bar}}
{{more footnotes|date=January 2013}}

'''Supervised learning''' is the [[machine learning]] task of inferring a function from ''{{vanchor|labeled training data|LABELLED_DATA}}''.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The MIT Press {{ISBN|9780262018258}}.&lt;/ref&gt; The [[training set|training data]] consist of a set of ''training examples''.  In supervised learning, each example is a ''pair'' consisting of an input object (typically a vector) and a desired output value (also called the ''supervisory signal'').  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a &quot;reasonable&quot; way (see [[inductive bias]]).

The parallel task in human and animal psychology is often referred to as [[concept learning]].

== Overview ==

In order to solve a given problem of supervised learning, one has to perform the following steps:

# Determine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting.
# Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.
# Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the [[curse of dimensionality]]; but should contain enough information to accurately predict the output.
# Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use [[support vector machine]]s or [[Decision tree learning|decision tree]]s.
# Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a ''validation'' set) of the training set, or via [[cross-validation (statistics)|cross-validation]].
# Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.

A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the [[No free lunch in search and optimization|No free lunch theorem]]).

There are four major issues to consider in supervised learning:

=== Bias-variance tradeoff ===
{{Main article|Bias-variance dilemma}}

A first issue is the tradeoff between ''bias'' and ''variance''.&lt;ref&gt;S. Geman, E. Bienenstock, and R. Doursat (1992). Neural networks and the bias/variance dilemma. Neural Computation 4, 1–58.&lt;/ref&gt;  Imagine that we have available several different, but equally good, training data sets.  A learning algorithm is biased for a particular input &lt;math&gt;x&lt;/math&gt; if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for &lt;math&gt;x&lt;/math&gt;.  A learning algorithm has high variance for a particular input &lt;math&gt;x&lt;/math&gt; if it predicts different output values when trained on different training sets.  The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.&lt;ref&gt;G. James (2003) Variance and Bias for General Loss Functions,  Machine Learning 51, 115-135. (http://www-bcf.usc.edu/~gareth/research/bv.pdf)&lt;/ref&gt;  Generally, there is a tradeoff between bias and variance.  A learning algorithm with low bias must be &quot;flexible&quot; so that it can fit the data well.  But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance.  A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).

===Function complexity and amount of training data===

The second issue is the amount of training data available relative to the complexity of the &quot;true&quot; function (classifier or regression function).  If the true function is simple, then an &quot;inflexible&quot; learning algorithm with high bias and low variance will be able to learn it from a small amount of data.  But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be learnable from a very large amount of training data and using a &quot;flexible&quot; learning algorithm with low bias and high variance.

===Dimensionality of the input space===

A third issue is the dimensionality of the input space.  If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features.  This is because the many &quot;extra&quot; dimensions can confuse the learning algorithm and cause it to have high variance.  Hence, high input dimensionality typically requires tuning the classifier to have low variance and high bias.  In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function.  In addition, there are many algorithms for [[feature selection]] that seek to identify the relevant features and discard the irrelevant ones.  This is an instance of the more general strategy of [[dimensionality reduction]], which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.

===Noise in the output values===

A fourth issue is the degree of noise in the desired output values (the supervisory [[target variable]]s).  If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples.  Attempting to fit the data too carefully leads to [[overfitting]].  You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation that part of the target function that cannot be modeled &quot;corrupts&quot; your training data - this phenomenon has been called [[deterministic noise]]. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.

In practice, there are several approaches to alleviate noise in the output values such as [[early stopping]] to prevent [[overfitting]] as well as [[anomaly detection|detecting]] and removing the noisy training examples prior to training the supervised learning algorithm.  There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased [[generalization error]] with [[statistical significance]].&lt;ref&gt;C.E. Brodely and M.A. Friedl (1999). Identifying and Eliminating Mislabeled Training Instances,  Journal of Artificial Intelligence Research 11, 131-167. (http://jair.org/media/606/live-606-1803-jair.pdf)&lt;/ref&gt;&lt;ref&gt;{{cite conference |author=M.R. Smith and T. Martinez |title=Improving Classification Accuracy by Identifying and Removing Instances that Should Be Misclassified |booktitle=Proceedings of International Joint Conference on Neural Networks (IJCNN 2011) |pages=2690–2697 |year=2011 |location= |url=https://dx.doi.org/10.1109/IJCNN.2011.6033571 }}&lt;/ref&gt;

===Other factors to consider (important)===

Other factors to consider when choosing and applying a learning algorithm include the following:

# Heterogeneity of the data.  If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others.  Many algorithms, including [[Support Vector Machines]], [[linear regression]], [[logistic regression]], [[Artificial neural network|neural networks]], and [[k-nearest neighbor algorithm|nearest neighbor methods]], require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval).  Methods that employ a distance function, such as [[k-nearest neighbor algorithm|nearest neighbor methods]] and [[Support Vector Machines|support vector machines with Gaussian kernels]], are particularly sensitive to this. An advantage of [[Decision tree learning|decision trees]] is that they easily handle heterogeneous data.
# Redundancy in the data.  If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., [[linear regression]], [[logistic regression]], and [[k-nearest neighbor algorithm|distance based methods]]) will perform poorly because of numerical instabilities.  These problems can often be solved by imposing some form of [[Regularization (mathematics)|regularization]].
# Presence of interactions and non-linearities.  If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., [[linear regression]], [[logistic regression]], [[Support Vector Machines]], [[Naive Bayes classifier|naive Bayes]]) and distance functions (e.g., [[k-nearest neighbor algorithm|nearest neighbor methods]], [[Support Vector Machines|support vector machines with Gaussian kernels]]) generally perform well.  However, if there are complex interactions among features, then algorithms such as [[Decision tree learning|decision trees]] and [[Artificial neural network|neural networks]] work better, because they are specifically designed to discover these interactions.  Linear methods can also be applied, but the engineer must manually specify the interactions when using them.

When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see [[Cross-validation (statistics)|cross validation]]).  Tuning the performance of a learning algorithm can be very time-consuming.  Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.

The most widely used learning algorithms are [[Support Vector Machines]], [[linear regression]], [[logistic regression]], [[Naive Bayes classifier|naive Bayes]], [[linear discriminant analysis]], [[Decision tree learning|decision trees]], [[k-nearest neighbor algorithm]] and [[Artificial neural network|Neural Networks]] ([[Multilayer perceptron]]).

==How supervised learning algorithms work==

Given a set of &lt;math&gt;N&lt;/math&gt; training examples of the form &lt;math&gt;\{(x_1, y_1), ..., (x_N,\; y_N)\}&lt;/math&gt; such that &lt;math&gt;x_i&lt;/math&gt; is the [[feature vector]] of the i-th example and &lt;math&gt;y_i&lt;/math&gt; is its label (i.e., class), a learning algorithm seeks a function &lt;math&gt;g: X \to Y&lt;/math&gt;, where &lt;math&gt;X&lt;/math&gt; is the input space and
&lt;math&gt;Y&lt;/math&gt; is the output space.  The function &lt;math&gt;g&lt;/math&gt; is an element of some space of possible functions &lt;math&gt;G&lt;/math&gt;, usually called the ''hypothesis space''.  It is sometimes convenient to
represent &lt;math&gt;g&lt;/math&gt; using a scoring function &lt;math&gt;f: X \times Y \to \Bbb{R}&lt;/math&gt; such that &lt;math&gt;g&lt;/math&gt; is defined as returning the &lt;math&gt;y&lt;/math&gt; value that gives the highest score: &lt;math&gt;g(x) = \underset{y}{\arg\max} \; f(x,y)&lt;/math&gt;.  Let &lt;math&gt;F&lt;/math&gt; denote the space of scoring functions.

Although &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;F&lt;/math&gt; can be any space of functions, many learning algorithms are probabilistic models where &lt;math&gt;g&lt;/math&gt; takes the form of a conditional probability model &lt;math&gt;g(x) =
P(y|x)&lt;/math&gt;, or &lt;math&gt;f&lt;/math&gt; takes the form of a joint probability model &lt;math&gt;f(x,y) = P(x,y)&lt;/math&gt;.  For example, [[Naive Bayes classifier|naive Bayes]] and [[linear discriminant analysis]] are joint probability models, whereas [[logistic regression]] is a conditional probability model.

There are two basic approaches to choosing &lt;math&gt;f&lt;/math&gt; or &lt;math&gt;g&lt;/math&gt;: [[empirical risk minimization]] and [[structural risk minimization]].&lt;ref&gt;Vapnik, V. N. The Nature of Statistical Learning Theory (2nd Ed.), Springer Verlag, 2000.&lt;/ref&gt;  Empirical risk minimization seeks the function that best fits the training data.  Structural risk minimize includes a ''penalty function'' that controls the bias/variance tradeoff.

In both cases, it is assumed that the training set consists of a sample of [[independent and identically-distributed random variables|independent and identically distributed pairs]], &lt;math&gt;(x_i, \;y_i)&lt;/math&gt;.  In order to measure how well a function fits the training data, a [[loss function]] &lt;math&gt;L: Y \times Y \to
\Bbb{R}^{\ge 0}&lt;/math&gt; is defined.  For training example &lt;math&gt;(x_i,\;y_i)&lt;/math&gt;, the loss of predicting the value &lt;math&gt;\hat{y}&lt;/math&gt; is &lt;math&gt;L(y_i,\hat{y})&lt;/math&gt;.

The ''risk'' &lt;math&gt;R(g)&lt;/math&gt; of function &lt;math&gt;g&lt;/math&gt; is defined as the expected loss of &lt;math&gt;g&lt;/math&gt;.  This can be estimated from the training data as

:&lt;math&gt;R_{emp}(g) = \frac{1}{N} \sum_i L(y_i, g(x_i))&lt;/math&gt;.

===Empirical risk minimization===
{{main article|Empirical risk minimization}}

In empirical risk minimization, the supervised learning algorithm seeks the function &lt;math&gt;g&lt;/math&gt; that minimizes &lt;math&gt;R(g)&lt;/math&gt;.  Hence, a supervised learning algorithm can be constructed by applying an [[Optimization (mathematics)|optimization algorithm]] to find &lt;math&gt;g&lt;/math&gt;.

When &lt;math&gt;g&lt;/math&gt; is a conditional probability distribution &lt;math&gt;P(y|x)&lt;/math&gt; and the loss function is the negative log likelihood: &lt;math&gt;L(y, \hat{y}) = -\log P(y | x)&lt;/math&gt;, then empirical risk minimization is equivalent to [[Maximum likelihood|maximum likelihood estimation]].

When &lt;math&gt;G&lt;/math&gt; contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization.  The learning algorithm is able
to memorize the training examples without generalizing well.  This is called [[overfitting]].

===Structural risk minimization===

[[Structural risk minimization]] seeks to prevent overfitting by incorporating a [[Regularization (mathematics)|regularization penalty]] into the optimization.  The regularization penalty can be viewed as implementing a form of [[Occam's razor]] that prefers simpler functions over more complex ones.

A wide variety of penalties have been employed that correspond to different definitions of complexity.  For example, consider the case where the function &lt;math&gt;g&lt;/math&gt; is a linear function of the form

:&lt;math&gt; g(x) = \sum_{j=1}^d \beta_j x_j&lt;/math&gt;.

A popular regularization penalty is &lt;math&gt;\sum_j \beta_j^2&lt;/math&gt;, which is the squared [[Euclidean norm]] of the weights, also known as the &lt;math&gt;L_2&lt;/math&gt; norm.  Other norms include the &lt;math&gt;L_1&lt;/math&gt; norm, &lt;math&gt;\sum_j |\beta_j|&lt;/math&gt;, and the &lt;math&gt;L_0&lt;/math&gt; norm, which is the number of non-zero  &lt;math&gt;\beta_j&lt;/math&gt;s.  The penalty will be denoted by &lt;math&gt;C(g)&lt;/math&gt;.

The supervised learning optimization problem is to find the function &lt;math&gt;g&lt;/math&gt; that minimizes

:&lt;math&gt; J(g) = R_{emp}(g) + \lambda C(g).&lt;/math&gt;

The parameter &lt;math&gt;\lambda&lt;/math&gt; controls the bias-variance tradeoff.  When &lt;math&gt;\lambda = 0&lt;/math&gt;, this gives empirical risk minimization with low bias and high variance.  When &lt;math&gt;\lambda&lt;/math&gt; is large, the learning algorithm will have high bias and low variance.  The value of &lt;math&gt;\lambda&lt;/math&gt; can be chosen empirically via [[cross-validation (statistics)|cross validation]].

The complexity penalty has a Bayesian interpretation as the negative log prior probability of &lt;math&gt;g&lt;/math&gt;, &lt;math&gt;-\log P(g)&lt;/math&gt;, in which case &lt;math&gt;J(g)&lt;/math&gt; is the [[Posterior probability|posterior probabability]] of &lt;math&gt;g&lt;/math&gt;.

==Generative training==

The training methods described above are ''discriminative training'' methods, because they seek to find a function &lt;math&gt;g&lt;/math&gt; that discriminates well between the different output values (see [[discriminative model]]).  For the special case where &lt;math&gt;f(x,y) = P(x,y)&lt;/math&gt; is a joint probability distribution and the loss function is the negative log likelihood &lt;math&gt;- \sum_i \log P(x_i, y_i),&lt;/math&gt; a risk minimization algorithm is said to perform ''generative training'', because &lt;math&gt;f&lt;/math&gt; can be regarded as a [[generative model]] that explains how the data were generated.  Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms.  In some cases, the solution can be computed in closed form as in [[Naive Bayes classifier|naive Bayes]] and [[linear discriminant analysis]].

==Generalizations==

There are several ways in which the standard supervised learning problem can be generalized:
# [[Semi-supervised learning]]: In this setting, the desired output values are provided only for a subset of the training data.  The remaining data is unlabeled.
# [[Active learning (machine learning)|Active learning]]: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user.  Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.
# [[Structured prediction]]: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.
# [[Learning to rank]]: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.

== Approaches and algorithms ==

* [[Analytical learning]]
* [[Artificial neural network]]
* [[Backpropagation]]
* [[Boosting (meta-algorithm)]]
* [[Bayesian statistics]]
* [[Case-based reasoning]]
* [[Decision tree learning]]
* [[Inductive logic programming]]
* [[Gaussian process regression]]
* [[Group method of data handling]]
* [[Variable kernel density estimation#Use for statistical classification|Kernel estimators]]
* [[Learning Automata]]
* [[Learning classifier system|Learning Classifier Systems]]
* [[Minimum message length]] ([[decision tree]]s, decision graphs, etc.)
* [[Multilinear subspace learning]]
* [[Naive bayes classifier]]
* [[Maximum entropy classifier]]
* [[Conditional random field]]
* [[Nearest neighbor (pattern recognition)|Nearest Neighbor Algorithm]]
* [[Probably approximately correct learning]] (PAC) learning
* [[Ripple down rules]], a knowledge acquisition methodology
* [[Symbolic machine learning]] algorithms
* [[Subsymbolic machine learning]] algorithms
* [[Support vector machine]]s
* Minimum Complexity Machines (MCM)
* [[Random forest|Random Forests]]
* [[Ensembles of Classifiers]]
* [[Ordinal classification]]
* [[Data Pre-processing]]
* [[Handling imbalanced datasets]]
* [[Statistical relational learning]]
* [[Proaftn]], a multicriteria classification algorithm

== Applications ==
* [[Bioinformatics]]
* [[Cheminformatics]]
**[[Quantitative structure–activity relationship]]
* [[Database marketing]]
* [[Handwriting recognition]]
* [[Information retrieval]]
** [[Learning to rank]]
* [[Information extraction]]
* Object recognition in [[computer vision]]
* [[Optical character recognition]]
* [[Spamming|Spam detection]]
* [[Pattern recognition]]
* [[Speech recognition]]
* Supervised learning is a special case of [[Downward causation]] in biological systems

== General issues ==

* [[Computational learning theory]]
* [[Inductive bias]]
* [[Overfitting (machine learning)]]
* (Uncalibrated) [[Class membership probabilities]]
* [[Unsupervised learning]]
* [[Version space]]s

== See also ==
* [[List of datasets for machine learning research]]

==References==
{{reflist}}

==External links==
* [http://www.mloss.org/ Machine Learning Open Source Software (MLOSS)]

{{DEFAULTSORT:Supervised Learning}}
</text>
      <sha1>imp3v2f3vrqs6ybkw681xse22an50ua</sha1>
    </revision>
  </page>
  <page>
    <title>Multivariate adaptive regression splines</title>
    <ns>0</ns>
    <id>18475546</id>
    <revision>
      <id>799049241</id>
      <parentid>796886209</parentid>
      <timestamp>2017-09-05T09:44:01Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22250">In [[statistics]], '''multivariate adaptive regression splines (MARS)''' is a form of [[regression analysis]] introduced by [[Jerome H. Friedman]] in 1991.&lt;ref&gt;{{Cite journal | last1 = Friedman | first1 = J. H. | doi = 10.1214/aos/1176347963 | title = Multivariate Adaptive Regression Splines | journal = The Annals of Statistics | volume = 19 | pages = 1 | year = 1991 | pmid =  | pmc = |mr=1091842 | zbl = 0765.62064 | jstor = 2241837}}&lt;/ref&gt; It is a [[non-parametric regression]] technique
and can be seen as an extension of [[linear model]]s that automatically models nonlinearities and interactions between variables.

The term &quot;MARS&quot; is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open source implementations of MARS are called &quot;Earth&quot;.&lt;ref&gt;[https://cran.r-project.org/web/packages/earth/index.html CRAN Package earth]&lt;/ref&gt;&lt;ref&gt;[http://orange.biolab.si/blog/2011/12/20/earth-multivariate-adaptive-regression-splines/ Earth - Multivariate adaptive regression splines in Orange (Python machine learning library)]&lt;/ref&gt;

== The basics ==

This section introduces MARS using a few examples.  We start with a set of data: a matrix of input variables ''x'', and a vector of the observed responses ''y'', with a response for each row in ''x''. For example, the data could be:

{|
! ''x''    !!  ''y''
|-
| 10.5 ||  16.4
|-
| 10.7 ||  18.8
|-
| 10.8 ||  19.7
|-
| ...  ||  ...
|-
| 20.6 ||  77.0
|}

Here there is only one [[Dependent and independent variables|independent variable]], so the ''x'' matrix is just a single column. Given these measurements, we would like to build a model which predicts the expected ''y'' for a given ''x''.

[[File:Friedmans mars linear model.png|frame|right|A linear model]]
A [[linear model]] for the above data is

: &lt;math&gt;
\hat{y} = -37 + 5.1 x
&lt;/math&gt;
The hat on the &lt;math&gt;\hat{y}&lt;/math&gt; indicates that &lt;math&gt;\hat{y}&lt;/math&gt; is estimated from the data.  The figure on the right shows a plot of this function:
a line giving the predicted &lt;math&gt;\hat{y}&lt;/math&gt; versus ''x'', with the original values of ''y'' shown as red dots.

The data at the extremes of ''x'' indicates that  the relationship between ''y'' and ''x'' may be non-linear (look at the red dots relative to the regression line at low and high values of ''x'').  We thus turn to MARS to automatically build a model taking into account non-linearities.  MARS software constructs a model from the given ''x'' and ''y'' as follows

: &lt;math&gt;
\begin{align}
\hat{y} = &amp;\ 25 \\
&amp; + 6.1 \max(0, x  - 13) \\
&amp; - 3.1 \max(0, 13 - x) \\
\end{align}
&lt;/math&gt;

[[File:Friedmans mars simple model.png|frame|right|A simple MARS model of the same data]]

The figure on the right shows a plot of this function: the predicted &lt;math&gt;\hat{y}&lt;/math&gt; versus ''x'', with the original values of y once again shown as red dots.  The predicted response is now a better fit to the original ''y'' values.

MARS has automatically produced a kink
in the predicted ''y'' to take into account non-linearity.
The kink is produced by ''hinge functions''.
The hinge functions are the expressions starting with &lt;math&gt;\max&lt;/math&gt;
(where &lt;math&gt;\max(a,b)&lt;/math&gt;
is &lt;math&gt;a&lt;/math&gt; if &lt;math&gt;a &gt; b&lt;/math&gt;, else &lt;math&gt;b&lt;/math&gt;).
Hinge functions are described in more detail below.

In this simple example, we can easily see from the plot that
''y'' has a non-linear relationship with ''x''
(and might perhaps guess that y varies with the square of ''x'').
However, in general there will be multiple
[[Dependent and independent variables|independent variables]],
and the relationship between ''y'' and these variables will be unclear
and not easily visible by plotting.
We can use MARS to discover that non-linear relationship.

An example MARS expression with multiple variables is

: &lt;math&gt;
\begin{align}
  \mathrm{ozone} = &amp;\ 5.2 \\
&amp;      +    0.93 \max(0, \mathrm{temp} - 58)  \\
&amp;      -   0.64 \max(0, \mathrm{temp} - 68)  \\
&amp;      -   0.046 \max(0,  234 - \mathrm{ibt})  \\
&amp;      -   0.016 \max(0, \mathrm{wind} - 7) \max(0, 200 - \mathrm{vis})\\
\end{align}
&lt;/math&gt;
[[File:Friedmans mars ozone model.png|frame|right|Variable interaction in a MARS model]]

This expression models air pollution (the ozone level)
as a function of the temperature and a few other variables.
Note that the last term in the formula (on the last line)
incorporates an interaction between &lt;math&gt;\mathrm{wind}&lt;/math&gt;
and &lt;math&gt;\mathrm{vis}&lt;/math&gt;.

The figure on the right plots the predicted
&lt;math&gt;\mathrm{ozone}&lt;/math&gt; as &lt;math&gt;\mathrm{wind}&lt;/math&gt; and
&lt;math&gt;\mathrm{vis}&lt;/math&gt; vary,
with the other variables fixed at their median values.
The figure shows that wind does not affect the ozone
level unless visibility is low.
We see that MARS can build quite flexible regression surfaces
by combining hinge functions.

To obtain the above expression, the MARS model building procedure
automatically selects which variables to use (some variables are
important, others not), the positions of the kinks in the hinge
functions, and how the hinge functions are combined.

== The MARS model ==

MARS builds models of the form

: &lt;math&gt;\hat{f}(x) = \sum_{i=1}^{k} c_i B_i(x) &lt;/math&gt;.

The model is a weighted sum of basis functions
&lt;math&gt;B_i(x)&lt;/math&gt;.
Each &lt;math&gt;c_i&lt;/math&gt; is a constant coefficient.
For example, each line in the formula for ozone above is one basis function
multiplied by its coefficient.

Each [[basis function]]
&lt;math&gt;B_i(x)&lt;/math&gt;
takes one of the following three forms:

1) a constant 1. There is just one such term, the intercept.
In the ozone formula above, the intercept term is 5.2.

2) a ''hinge'' function.
A hinge function has the form
&lt;math&gt; \max(0, x - const) &lt;/math&gt;
or
&lt;math&gt; \max(0, const - x) &lt;/math&gt;.
MARS automatically selects variables
and values of those variables for knots of the hinge functions.
Examples of such basis functions can be seen
in the middle three lines of the ozone formula.

3) a product of two or more hinge functions.
These basis functions can model interaction between two or more variables.
An example is the last line of the ozone formula.

== Hinge functions ==

[[File:Friedmans mars hinge functions.png|frame|right|A mirrored pair of hinge functions with a knot at x=3.1]]

Hinge functions are a key part of MARS models. A hinge function takes the form
: &lt;math&gt;\max(0,x-c)&lt;/math&gt;
or
: &lt;math&gt;\max(0,c-x)&lt;/math&gt;
where &lt;math&gt;c&lt;/math&gt; is a constant, called the ''knot''.
The figure on the right shows a mirrored pair of hinge functions with a knot at 3.1.

A hinge function is zero for part of its range, so  can be used to partition the data into disjoint regions, each of which can be treated independently. Thus for example a mirrored pair of hinge functions in the expression
: &lt;math&gt;
6.1 \max(0, x  - 13)
- 3.1 \max(0, 13 - x)
&lt;/math&gt;
creates the [[piecewise]] linear graph shown for the simple MARS model in the previous section.

One might assume that only piecewise linear functions can be formed from hinge functions, but hinge functions can be multiplied together to form non-linear functions.

Hinge functions are also called [[ramp function|ramp]], [[Ice hockey stick|hockey stick]], or [[Rectifier (neural networks)|rectifier]] functions. Instead of the &lt;math&gt;\max&lt;/math&gt; notation used in this article, hinge functions are often represented by &lt;math&gt;[\pm(x_i - c)]_+&lt;/math&gt; where &lt;math&gt;[\cdot]_+&lt;/math&gt; means take the positive part.

== The model building process ==
{{see also|Stepwise regression}}

MARS builds a model in two phases:
the forward and the backward pass.
This two-stage approach is the same as that used by
[[recursive partitioning]] trees.

=== The forward pass ===

MARS starts with a model which consists of just the intercept term
(which is the mean of the response values).

MARS then repeatedly adds basis function in pairs to the model.
At each step it finds the pair of basis functions that
gives the maximum reduction in sum-of-squares
[[Errors and residuals in statistics|residual]] error
(it is a [[greedy algorithm]]).
The two basis functions in the pair
are identical except that a different
side of a mirrored hinge function is used for each function.
Each new basis function consists of
a term already in the model
(which could perhaps be the intercept term)
multiplied by a new hinge function.
A hinge function is defined by a variable and a knot,
so to add a new basis function, MARS must search over
all combinations of the following:

1) existing terms (called ''parent terms'' in this context)

2) all variables (to select one for the new basis function)

3) all values of each variable (for the knot of the new hinge function).

To calculate the coefficient of each term
MARS applies a linear regression over the terms.

This process of adding terms continues until
the change in residual error is too small to continue
or until the maximum number of terms is reached.
The maximum number of terms
is specified by the user before model building starts.

The search at each step is done in a [[Brute-force search|brute force]] fashion,
but a key aspect of MARS is that
because of the nature of hinge functions
the search can be done relatively
quickly using a fast least-squares update technique.
Actually, the search is not quite brute force.
The search can be sped up with  a [[Heuristics|heuristic]]
that reduces the number
of parent terms to consider at each step
(&quot;Fast MARS&quot;
&lt;ref&gt;[[Friedman, J. H.]] (1993)
''Fast MARS'',
Stanford University Department of Statistics, Technical Report 110
&lt;/ref&gt;).

=== The backward pass ===

The forward pass usually builds an [[overfit]] model.
(An overfit model has a good fit to the data used to build
the model but will not generalize well to new data.)
To build a model with better generalization ability,
the backward pass prunes the model.
It removes terms one by one,
deleting the least effective term at each step
until it finds the best submodel.
Model subsets are compared using the GCV criterion described below.

The backward pass has an advantage over the forward pass:
at any step it can choose any term to delete,
whereas the forward pass
at each step can only see the next pair of terms.

The forward pass adds terms in pairs,
but the backward pass typically discards one side of the pair
and so terms are often not seen in pairs in the final model.
A paired hinge can be seen in
the equation for &lt;math&gt;\hat{y}&lt;/math&gt; in the
first MARS example above;
there are no complete pairs retained in the ozone example.

==== Generalized cross validation ====
{{further|Cross-validation (statistics)|Model selection}}

The backward pass uses generalized cross validation (GCV) to compare the performance of model subsets in order to choose the best subset: lower values of GCV are better.
The GCV is a form of
[[Regularization (machine learning)|regularization]]:
it trades off goodness-of-fit against model complexity.

(We want to estimate how well a model performs on ''new'' data, not on the training data.  Such new data is usually not available at the time of model building, so instead we use GCV to estimate what performance would be on new data.  The raw [[Residual sum of squares|residual sum-of-squares]] (RSS) on the training data is inadequate for comparing models, because the RSS always increases as MARS terms are dropped.  In other words, if the RSS were used to compare models, the backward pass would always choose the largest model—but the largest model typically does not have the best generalization performance.)

The formula for the GCV is
:'''GCV = RSS / (N * (1 - EffectiveNumberOfParameters / N)^2)'''
where '''RSS''' is the residual sum-of-squares
measured on the training data and '''N''' is the
number of observations (the number of rows in the '''x''' matrix).

The ''EffectiveNumberOfParameters'' is defined in
the MARS context as
:'''EffectiveNumberOfParameters = NumberOfMarsTerms + Penalty * (NumberOfMarsTerms - 1 ) / 2'''
where '''Penalty''' is about 2 or 3 (the
MARS software allows the user to preset Penalty).

Note that
:'''(NumberOfMarsTerms - 1 ) / 2'''
is the number of hinge-function knots,
so the formula penalizes the addition of knots.
Thus the GCV formula adjusts (i.e. increases) the training RSS to take into
account the flexibility of the model.
We penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data.

Generalized Cross Validation is so named because
it uses a formula to approximate the error
that would be determined by leave-one-out validation.
It is just an approximation but works well in practice.
GCVs were introduced by Craven and
[[Grace Wahba|Wahba]] and extended by Friedman for MARS.

=== Constraints ===

One constraint has already been mentioned: the user
can specify the maximum number of terms in the forward pass.

A further constraint can be placed on the forward pass
by specifying a maximum allowable degree of interaction.
Typically only one or two degrees of interaction are allowed,
but higher degrees can be used when the data warrants it.
The maximum degree of interaction in the first MARS example
above is one (i.e. no interactions or an ''additive model'');
in the ozone example it is two.

Other constraints on the forward pass are possible.
For example, the user can specify that interactions are allowed
only for certain input variables.
Such constraints could make sense because of knowledge
of the process that generated the data.

== Pros and cons ==
{{original research|date=October 2016}}

No regression modeling technique is best for all situations.
The guidelines below are intended to give an idea of the pros and cons of MARS,
but there will be exceptions to the guidelines.
It is useful to compare MARS to [[recursive partitioning]] and this is done below.
(Recursive partitioning is also commonly called ''regression trees'',
''decision trees'', or [[Predictive analytics#Classification and regression trees|CART]];
see the [[Decision tree learning|recursive partitioning]] article for details).

*MARS models are more flexible than [[linear regression]] models.
*MARS models are simple to understand and interpret.  Compare the equation for ozone concentration above to, say, the innards of a trained [[Artificial neural network|neural network]] or a [[random forest]].
*MARS can handle both continuous and categorical data.&lt;ref&gt;[[Friedman, J. H.]] (1993) ''Estimating Functions of Mixed Ordinal and Categorical Variables Using Adaptive Splines'', New Directions in Statistical Data Analysis and Robustness (Morgenthaler, Ronchetti, Stahel, eds.), Birkhauser&lt;/ref&gt; MARS tends to be better than recursive partitioning for numeric data because hinges are more appropriate for numeric variables than the piecewise constant segmentation used by recursive partitioning.
*Building MARS models often requires little or no data preparation. The hinge functions automatically partition the input data, so the effect of outliers is contained.  In this respect MARS is similar to [[recursive partitioning]] which also partitions the data into disjoint regions, although using a different method.  (Nevertheless, as with most statistical modeling techniques, known outliers should be considered for removal before training a MARS model.)
*MARS (like recursive partitioning) does ''automatic variable selection'' (meaning it includes important variables in the model and excludes unimportant ones). However, bear in mind that variable selection is not a clean problem and there is usually some arbitrariness in the selection, especially in the presence of [[Multicollinearity|collinearity]] and 'concurvity'.
*MARS models tend to have a good bias-variance trade-off.  The models are flexible enough to model non-linearity and variable interactions (thus MARS models have fairly low bias), yet the constrained form of MARS basis functions prevents too much flexibility (thus MARS models have fairly low variance).
*MARS is suitable for handling fairly large datasets.  It is a routine matter to build a MARS model from an input matrix with, say, 100 predictors and 10&lt;sup&gt;5&lt;/sup&gt; observations.  Such a model can be built in about a minute on a 1&amp;nbsp;GHz machine, assuming the maximum degree of interaction of MARS terms is limited to one (i.e. additive terms only).  A degree two model with the same data on the same 1&amp;nbsp;GHz machine takes longer—about 12 minutes.  Be aware that these times are highly data dependent. Recursive partitioning is much faster than MARS.
*With MARS models, as with any non-parametric regression, parameter confidence intervals and other checks on the model cannot be calculated directly (unlike [[linear regression]] models).  [[Cross-validation (statistics)|Cross-validation]] and related techniques must be used for validating the model instead.
*MARS models do not give as good fits as [[Boosting (meta-algorithm)|boosted]] trees, but can be built much more quickly and are more interpretable. (An 'interpretable' model is in a form that makes it clear what the effect of each predictor is.)
*The &lt;code&gt;earth&lt;/code&gt;, &lt;code&gt;mda&lt;/code&gt;, and &lt;code&gt;polspline&lt;/code&gt; implementations do not allow missing values in predictors, but free implementations of regression trees (such as &lt;code&gt;rpart&lt;/code&gt; and &lt;code&gt;party&lt;/code&gt;) do allow missing values using a technique called surrogate splits.
*MARS models can make predictions quickly.  The prediction function simply has to evaluate the MARS model formula.  Compare that to making a prediction with say a [[Support Vector Machine]], where every variable has to be multiplied by the corresponding element of every support vector.  That can be a slow process if there are many variables and many support vectors.

== Extensions and related concepts ==
* [[Generalized linear model]]s (GLMs) can be incorporated into MARS models by applying a link function after the MARS model is built. Thus, for example, MARS models can incorporate [[logistic regression]] to predict probabilities.
* [[Nonlinear regression|Non-linear regression]] is used when the underlying form of the function is known and regression is used only to estimate the parameters of that function. MARS, on the other hand, estimates the functions themselves, albeit with severe constraints on the nature of the functions. (These constraints are necessary because discovering a model from the data is an [[inverse problem]] that is not [[Well-posed problem|well-posed]] without constraints on the model.)
* [[Recursive partitioning]] (commonly called CART). MARS can be seen as a generalization of recursive partitioning that allows the model to better handle numerical (i.e. non-categorical) data.
* [[Generalized additive model]]s. From the user's perspective GAMs are similar to MARS but (a) fit smooth [[Local regression|loess]] or polynomial [[Spline (mathematics)|splines]] instead of MARS basis functions, and (b) do not automatically model variable interactions. The fitting method used internally by GAMs is very different from that of MARS.  For models that do not require automatic discovery of variable interactions GAMs often compete favorably with MARS.
* [[TSMARS]]. Time Series Mars is the term used when MARS models are applied in a time series context. Typically in this set up the predictors are the lagged time series values resulting in autoregressive spline models. These models and extensions to include moving average spline models are described in &quot;Univariate Time Series Modelling and Forecasting using TSMARS: A study of threshold time series autoregressive, seasonal and moving average models using TSMARS&quot;.

== See also ==
* [[Linear regression]]
* [[Rational function modeling]]
* [[Segmented regression]]
* [[Spline interpolation]]
* [[Spline regression]]

== References ==
{{reflist}}

== Further reading ==
* Hastie T., Tibshirani R., and Friedman J.H. (2009) [http://www-stat.stanford.edu/~tibs/ElemStatLearn ''The Elements of Statistical Learning''], 2nd edition. Springer, {{ISBN|978-0-387-84857-0}} (has a section on MARS)
* Faraway J. (2005) [http://www.maths.bath.ac.uk/~jjf23 ''Extending the Linear Model with R''], CRC, {{ISBN|978-1-58488-424-8}} (has an example using MARS with R)
* Heping Zhang and Burton H. Singer (2010) [https://www.amazon.com/Recursive-Partitioning-Applications-Springer-Statistics/dp/1441968237 ''Recursive Partitioning and Applications''], 2nd edition. Springer, {{ISBN|978-1-4419-6823-4}} (has a chapter on MARS and discusses some tweaks to the algorithm)
* Denison D.G.T., Holmes C.C., Mallick B.K., and Smith A.F.M. (2004) [http://www.stat.tamu.edu/~bmallick/wileybook/book_code.html ''Bayesian Methods for Nonlinear Classification and Regression''], Wiley, {{ISBN|978-0-471-49036-4}}
* Berk R.A. (2008) ''Statistical learning from a regression persepective'', Springer, {{ISBN|978-0-387-77500-5}}

== External links ==
{{external cleanup|date=October 2016}}
Several free and commercial software packages are available for fitting MARS-type models.

; Free software:
* [[R (programming language)|R]] packages:
** &lt;code&gt;earth&lt;/code&gt; function in the &lt;code&gt;[https://cran.r-project.org/web/packages/earth/index.html earth]&lt;/code&gt; package
** &lt;code&gt;mars&lt;/code&gt; function in the &lt;code&gt;[https://cran.r-project.org/web/packages/mda/index.html mda]&lt;/code&gt; package
** &lt;code&gt;polymars&lt;/code&gt; function in the &lt;code&gt;[https://cran.r-project.org/web/packages/polspline/index.html polspline]&lt;/code&gt; package.  Not Friedman's MARS.
* Matlab code:
** [http://www.cs.rtu.lv/jekabsons/regression.html ARESLab: Adaptive Regression Splines toolbox for Matlab]
* Python
** [http://orange.biolab.si/blog/2011/12/20/earth-multivariate-adaptive-regression-splines/ Earth - Multivariate adaptive regression splines]
** [https://github.com/jcrudy/py-earth/ py-earth]

; Commercial software:
* [http://www.salford-systems.com/mars.php MARS] from Salford Systems. Based on Friedman's implementation.
* [http://statsoft.com/products/data-mining-solutions/ STATISTICA Data Miner] from StatSoft
* [http://support.sas.com/documentation/cdl/en/statug/65328/HTML/default/viewer.htm#statug_adaptivereg_overview.htm ADAPTIVEREG] from SAS.


</text>
      <sha1>pa9qdfe6ildxubt25r8b1b1yyinkug8</sha1>
    </revision>
  </page>
  <page>
    <title>Statistical relational learning</title>
    <ns>0</ns>
    <id>19667111</id>
    <revision>
      <id>792208096</id>
      <parentid>792050302</parentid>
      <timestamp>2017-07-25T04:17:54Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5968">'''Statistical relational learning''' ('''SRL''') is a subdiscipline of [[artificial intelligence]] and [[machine learning]] that is concerned with [[domain model]]s that exhibit both [[uncertainty]] (which can be dealt with using statistical methods) and complex, [[relation (mathematics)|relational]] structure.&lt;ref name=Getoor2007&gt;Lise Getoor and Ben Taskar: Introduction to statistical relational learning, MIT Press, 2007&lt;/ref&gt;&lt;ref&gt;Ryan A. Rossi, Luke K. McDowell, David W. Aha, and Jennifer Neville, [dx.doi.org/10.1613/jair.3659  &quot;Transforming Graph Data for Statistical Relational Learning.]&quot;  ''Journal of Artificial Intelligence Research (JAIR)'', '''Volume 45''' (2012), pp. 363-441.&lt;/ref&gt; Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the [[knowledge representation]] formalisms developed in SRL use (a subset of) [[first-order logic]] to describe relational properties of a domain in a general manner ([[universal quantification]]) and draw upon [[probabilistic graphical model]]s (such as [[Bayesian network]]s or [[Markov network]]s) to model the uncertainty; some also build upon the methods of [[inductive logic programming]]. Significant contributions to the field have been made since the late 1990s.&lt;ref&gt;{{citation_needed|date=December 2016}}&lt;/ref&gt;

As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with [[reasoning]] (specifically [[statistical inference|probabilistic inference]]) and [[knowledge representation]]. Therefore, alternative terms that reflect the main foci of the field include ''statistical relational learning and reasoning'' (emphasizing the importance of reasoning) and ''first-order probabilistic languages'' (emphasizing the key properties of the languages with which models are represented).

== Canonical tasks ==
A number of canonical tasks are associated with statistical relational learning, the most common ones being&lt;ref&gt;Matthew Richardson and Pedro Domingos, [http://www.cs.washington.edu/homes/pedrod/papers/mlj05.pdf  &quot;Markov Logic Networks.]&quot;  ''Machine Learning'', '''62''' (2006), pp. 107–136.&lt;/ref&gt;

* [[collective classification]], i.e. the (simultaneous) [[classification (machine learning)|prediction of the class]] of several objects given objects' attributes and their relations
* [[link prediction]], i.e. predicting whether or not two or more objects are related
* [[link-based clustering]], i.e. the [[cluster analysis|grouping]] of similar objects, where similarity is determined according to the links of an object, and the related task of [[collaborative filtering]], i.e. the filtering for information that is relevant to an entity (where a piece of information is considered relevant to an entity if it is known to be relevant to a similar entity).
* [[social network]] modelling
* [[record linkage|object identification/entity resolution/record linkage]], i.e. the identification of equivalent entries in two or more separate databases/datasets

== Representation formalisms ==
{{More footnotes|date=June 2011}}

One of the fundamental design goals of the representation formalisms developed in SRL is to abstract away from concrete entities and to represent instead general principles that are intended to be universally applicable. Since there are countless ways in which such principles can be represented, many representation formalisms have been proposed in recent years.&lt;ref name=Getoor2007 /&gt; In the following, some of the more common ones are listed in alphabetical order:

* [[Bayesian logic program]]
* [[BLOG model]]
* Logic programs with annotated disjunctions
* [[Markov logic network]]s
* [[Multi-entity Bayesian network]]
* Probabilistic relational model – a Probabilistic Relational Model (PRM) is the counterpart of a [[Bayesian network]] in statistical relational learning.&lt;ref&gt;Friedman N, Getoor L, Koller D, Pfeffer A. (1999) [https://www.biostat.wisc.edu/~page/lprm-ijcai99.pdf &quot;Learning probabilistic relational models&quot;]. In: ''International joint conferences on artificial intelligence'', 1300–09&lt;/ref&gt;&lt;ref&gt;Teodor Sommestad, Mathias Ekstedt, Pontus Johnson (2010) &quot;A probabilistic relational model for security risk analysis&quot;, ''Computers &amp; Security'', 29 (6), 659-679 {{DOI|10.1016/j.cose.2010.02.002}}&lt;/ref&gt;
* [[Probabilistic soft logic]]
* [[Recursive random field]]
* [[Relational Bayesian network]]
* [[Relational dependency network]]
* [[Relational Markov network]]
* [[Relational Kalman filtering]]

== See also ==
* [[Association rule learning]]
* [[Formal concept analysis]]
* [[Fuzzy logic]]
* [[Grammar induction]]

== Resources ==
* [[Lise Getoor]] and [[Ben Taskar]]: ''Introduction to statistical relational learning'', MIT Press, 2007
* Brian Milch, and Stuart J. Russell: ''First-Order Probabilistic Languages: Into the Unknown'', Inductive Logic Programming, volume 4455 of [[Lecture Notes in Computer Science]], page 10–24. Springer, 2006
* Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth: ''A Survey of First-Order Probabilistic Models'', Innovations in Bayesian Networks, volume 156 of Studies in Computational Intelligence, Springer, 2008
* Hassan Khosravi and Bahareh Bina: ''A Survey on Statistical Relational Learning'', Advances in Artificial Intelligence, Lecture Notes in Computer Science, Volume 6085/2010, 256–268, Springer, 2010
* Ryan A. Rossi, Luke K. McDowell, David W. Aha, and Jennifer Neville: ''Transforming Graph Data for Statistical Relational Learning'', Journal of Artificial Intelligence Research (JAIR), Volume 45, page 363-441, 2012
*  Luc De Raedt, Kristian Kersting, Sriraam Natarajan and David Poole, &quot;Statistical Relational Artificial Intelligence: Logic, Probability, and Computation&quot;, Synthesis Lectures on Artificial Intelligence and Machine Learning&quot; March 2016 {{ISBN|9781627058414}}.

== References ==
{{reflist}}



</text>
      <sha1>3ibb0oi1ueaayzofvkkpnr42s9uu8io</sha1>
    </revision>
  </page>
  <page>
    <title>Hyperparameter (machine learning)</title>
    <ns>0</ns>
    <id>32402755</id>
    <revision>
      <id>816222000</id>
      <parentid>814778870</parentid>
      <timestamp>2017-12-20T01:26:05Z</timestamp>
      <contributor>
        <username>Acyclic</username>
        <id>29997616</id>
      </contributor>
      <comment>/* Considerations */ Make LSTM statements more relevant to this article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3295">{{About|hyperparameters in machine learning|hyperparameters in Bayesian statistics|Hyperparameter}}

In [[machine learning]], a '''hyperparameter''' is [[parameter]] whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.

Different model training algorithms require different hyperparameters, some simple algorithms (as [[ordinary least squares]] regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, [[LASSO]] is an algorithm that adds a [[Regularization (mathematics)|regularization]] hyperparameter to [[ordinary least squares|OLS]] regression, that has to be set before estimating the parameters through the training algorithm.

== Considerations ==
The time required to train and test a model can depend upon the choice of its hyperparameters.&lt;ref name=abs1502.02127&gt;{{cite article |url=https://arxiv.org/abs/1502.02127 |title=Claesen, Marc, and Bart De Moor. &quot;Hyperparameter Search in Machine Learning.&quot; arXiv preprint arXiv:1502.02127 (2015).}}&lt;/ref&gt; An inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance.&lt;ref name=abs1502.02127/&gt; A hyperparameter is usually of continuous or integer type, leading to mixed-type optimization problems.&lt;ref name=abs1502.02127/&gt; The existence of some hyperparameters is conditional upon the value of others, e.g. the size of each hidden layer in a neural network can be conditional upon the number of layers.&lt;ref name=abs1502.02127/&gt;

Most performance variation can be attributed to just a few hyperparameters.&lt;ref name=hutter14&gt;{{cite article |url=http://proceedings.mlr.press/v32/hutter14.html |title=Hutter, Frank, Holger Hoos, and Kevin Leyton-Brown. &quot;An efficient approach for assessing hyperparameter importance.&quot; International Conference on Machine Learning. 2014.}}&lt;/ref&gt;&lt;ref name=abs1502.02127/&gt;&lt;ref name=abs1710.04725&gt;{{cite article |url=https://arxiv.org/abs/1710.04725 |title=van Rijn, Jan N., and Frank Hutter. &quot;Hyperparameter Importance Across Datasets.&quot; arXiv preprint arXiv:1710.04725 (2017).}}&lt;/ref&gt; For an [[Long short-term memory|LSTM]], while the learning rate followed by the network size are its most crucial hyperparameters,&lt;ref name=pmid27411231&gt;{{cite article |url=http://ieeexplore.ieee.org/abstract/document/7508408/ |title=Greff, Klaus, et al. &quot;LSTM: A search space odyssey.&quot; IEEE transactions on neural networks and learning systems (2017).}}&lt;/ref&gt; others namely batching and momentum have no significant effect on its performance.&lt;ref name=abs1508.02774&gt;{{cite article |url=https://arxiv.org/abs/1508.02774 |title=Breuel, Thomas M. &quot;Benchmarking of LSTM networks.&quot; arXiv preprint arXiv:1508.02774 (2015).}}&lt;/ref&gt;

== Optimization ==
{{main|Hyperparameter optimization}}

Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined [[loss function]] on given test data.&lt;ref name=abs1502.02127/&gt;  The objective function takes a tuple of hyperparameters and returns the associated loss.&lt;ref name=abs1502.02127/&gt;

== See also ==
* [[Hyper-heuristic]]

== References ==
{{Reflist}}


</text>
      <sha1>6s5as1luufoorrsmp685zwal6w9s94g</sha1>
    </revision>
  </page>
  <page>
    <title>Elastic matching</title>
    <ns>0</ns>
    <id>13750669</id>
    <revision>
      <id>797022452</id>
      <parentid>497849080</parentid>
      <timestamp>2017-08-24T13:40:28Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: title, journal. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="813">{{expert-subject|Robotics|date=February 2012}}
'''Elastic matching''' is one of the [[pattern recognition]] techniques in [[computer science]]. Elastic matching (EM) is also known as '''deformable template''', '''flexible matching''', or '''nonlinear template matching'''.

Elastic matching can be defined as an [[optimization problem]] of two-dimensional warping specifying corresponding [[pixel]]s between subjected images.

==References==
*{{cite journal |last=Uchida |first=Seiichi |title=A Survey of Elastic Matching Techniques for Handwritten Character Recognition |journal=Ieice Trans. Inf. &amp; Syst. |volume=E88-D |issue= 8 |date=August 2005}}

==See also==
* [[Dynamic time warping]]

{{Comp-sci-stub}}

{{DEFAULTSORT:Elastic Matching}}

</text>
      <sha1>020i4or1bdax5l04h0oil86071v74f8</sha1>
    </revision>
  </page>
  <page>
    <title>Document classification</title>
    <ns>0</ns>
    <id>1331441</id>
    <revision>
      <id>810315401</id>
      <parentid>809519672</parentid>
      <timestamp>2017-11-14T14:39:05Z</timestamp>
      <contributor>
        <username>IlPesso</username>
        <id>8233454</id>
      </contributor>
      <minor/>
      <comment>/* Automatic document classification (ADC) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11951">'''Document classification''' or '''document categorization''' is a problem in [[library science]], [[information science]] and [[computer science]]. The task is to assign a [[document]] to one or more [[Class (philosophy)|classes]] or [[Categorization|categories]]. This may be done &quot;manually&quot; (or &quot;intellectually&quot;) or [[algorithmically]]. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.

The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.

Documents may be classified according to their [[Subject (documents)|subjects]] or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.

==&quot;Content-based&quot; versus &quot;request-based&quot; classification==
'''Content-based classification''' is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned.&lt;ref&gt;Library of Congress (2008). The subject headings manual. Washington, DC.: Library of Congress, Policy and Standards Division. (Sheet H 180: &quot;Assign headings only for topics that comprise at least 20% of the work.&quot;)&lt;/ref&gt; In automatic classification it could be the number of times given words appears in a document.

'''Request-oriented classification''' (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p.&amp;nbsp;230&lt;ref&gt;Soergel, Dagobert (1985). Organizing information: Principles of data base and retrieval systems. Orlando, FL: Academic Press.&lt;/ref&gt;).

Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as ''policy-based classification'': The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.

==Classification versus indexing==
Sometimes a distinction is made between assigning documents to classes (&quot;classification&quot;) versus assigning [[Subject (documents)|subjects]] to documents (&quot;[[subject indexing]]&quot;) but as [[Frederick Wilfrid Lancaster]] has argued, this distinction is not fruitful. &quot;These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p.&amp;nbsp;21&lt;ref&gt;Lancaster, F. W. (2003). Indexing and abstracting in theory and practice. Library Association, London.&lt;/ref&gt;). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a [[thesaurus]] and vice versa (cf., Aitchison, 1986,&lt;ref&gt;Aitchison, J. (1986). &quot;A classification as a source for thesaurus: The Bibliographic Classification of H. E. Bliss as a source of thesaurus terms and structure.&quot; Journal of Documentation, Vol. 42 No. 3, pp. 160-181.&lt;/ref&gt; 2004;&lt;ref&gt;Aitchison, J. (2004). &quot;Thesauri from BC2: Problems and possibilities revealed in an experimental thesaurus derived from the Bliss Music schedule.&quot; Bliss Classification Bulletin, Vol. 46, pp. 20-26.&lt;/ref&gt; Broughton, 2008;&lt;ref&gt;Broughton, V. (2008). &quot;A faceted classification as the basis of a faceted terminology: Conversion of a classified structure to thesaurus format in the Bliss Bibliographic Classification (2nd Ed.).&quot; Axiomathes, Vol. 18 No.2, pp. 193-210.&lt;/ref&gt; Riesthuis &amp; Bliedung, 1991&lt;ref&gt;Riesthuis, G. J. A., &amp; Bliedung, St. (1991). &quot;Thesaurification of the UDC.&quot; Tools for knowledge organization and the human interface, Vol. 2, pp. 109-117. Index Verlag, Frankfurt.&lt;/ref&gt;). Therefore, is the act of labeling a document (say by assigning a term from a [[controlled vocabulary]] to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).

==Automatic document classification (ADC)==
Automatic document classification tasks can be divided into three sorts: '''supervised document classification''' where some external mechanism (such as human feedback) provides information on the correct classification for documents, '''unsupervised document classification''' (also known as [[document clustering]]), where the classification must be done entirely without reference to external information, and '''semi-supervised document classification''',&lt;ref&gt;
Rossi, R. G., Lopes, A. d. A., and Rezende, S. O. (2016). Optimization and label propagation in bipartite heterogeneous networks to improve transductive classification of texts.
Information Processing &amp; Management, 52(2):217–257.
&lt;/ref&gt; where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.&lt;ref&gt;[https://pdfs.semanticscholar.org/bea4/a204239556a29228decc9e029c326e4900b7.pdf An Interactive Automatic Document Classification Prototype]&lt;/ref&gt;&lt;ref&gt;[https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An Interactive Automatic Document Classification Prototype] {{webarchive |url=https://web.archive.org/web/20150424122349/https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An |date=April 24, 2015 }}&lt;/ref&gt;&lt;ref&gt;[https://archive.is/20141208063727/http://www.artsyltech.com/da_classification.htmlAutomatic Document Classification - Artsyl]&lt;/ref&gt;&lt;ref&gt;[http://www.abbyy.com/ocr_sdk_windows/what_is_new/classification/ ABBYY FineReader Engine 11 for Windows]&lt;/ref&gt;&lt;ref&gt;[http://www.antidot.net/classifier/ Classifier - Antidot]&lt;/ref&gt;

== Techniques ==
Automatic document classification techniques include:
* [[Expectation maximization]] (EM)
* [[Naive Bayes classifier]]
* [[tf–idf]]
* [[Instantaneously trained neural networks]]
* [[Latent semantic indexing]]
* [[Support vector machines]] (SVM)
* [[Artificial neural network]]
* [[k-nearest neighbor algorithm|K-nearest neighbour algorithms]]
* [[Decision tree learning|Decision trees]] such as [[ID3 algorithm|ID3]] or [[C4.5 algorithm|C4.5]]
* [[Concept Mining]]
* [[Rough set]]-based classifier
* [[Soft set]]-based classifier
* [[Multiple-instance learning]]
* [[Natural language processing]] approaches

== Applications ==
Classification techniques have been applied to
* [[spam filter]]ing, a process which tries to discern [[E-mail spam]] messages from legitimate emails
* email [[routing]], sending an email sent to a general address to a specific address or mailbox depending on topic&lt;ref&gt;Stephan Busemann, Sven Schmeier and Roman G. Arens (2000). Message classification in the call center. In Sergei Nirenburg, Douglas Appelt, Fabio Ciravegna and Robert Dale, eds., Proc. 6th Applied Natural Language Processing Conf. (ANLP'00), pp. 158-165, ACL.&lt;/ref&gt;
* [[language identification]], automatically determining the language of a text
* genre classification, automatically determining the genre of a text&lt;ref&gt;{{Citation|last = Santini| first = Marina | last2 = Rosso| first2 = Mark| title = Testing a Genre-Enabled Application: A Preliminary Assessment| url = http://www.bcs.org/upload/pdf/ewic_fd08_paper7.pdf| series = BCS IRSG Symposium: Future Directions in Information Access| place = London, UK | pages= 54–63| year = 2008 }}&lt;/ref&gt;
* [[Readability|readability assessment]], automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger [[text simplification]] system
* [[sentiment analysis]], determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.
* health-related classification using social media in public health surveillance &lt;ref&gt;X. Dai, M. Bikdash and B. Meyer, &quot;From social media to public health surveillance: Word embedding based clustering method for twitter classification,&quot; SoutheastCon 2017, Charlotte, NC, 2017, pp. 1-7.
doi: 10.1109/SECON.2017.7925400,
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7925400&amp;isnumber=7925258&lt;/ref&gt;
* article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.&lt;ref name=&quot;:0&quot;&gt;{{Cite journal
 | pmid = 18834495
| year = 2008
| author1 = Krallinger
| first1 = M
| title = Overview of the protein-protein interaction annotation extraction task of Bio ''Creative'' II
| journal = Genome Biology
| volume = 9 Suppl 2
| pages = S4
| last2 = Leitner
| first2 = F
| last3 = Rodriguez-Penagos
| first3 = C
| last4 = Valencia
| first4 = A
| doi = 10.1186/gb-2008-9-s2-s4
| pmc = 2559988
}}&lt;/ref&gt;

== See also ==
{{colbegin}}
* [[Categorization]]
* [[Classification (disambiguation)]]
* [[Compound term processing]]
* [[Concept-based image indexing]]
* [[Content-based image retrieval]]
* [[Document]]
* [[Supervised learning]], [[unsupervised learning]]
* [[Document retrieval]]
* [[Document clustering]]
* [[Information retrieval]]
* [[Knowledge organization]]
* [[Knowledge Organization System]]
* [[Library classification]]
* [[Machine learning]]
* [[Native Language Identification]]
* [[String metrics]]
* [[Subject (documents)]]
* [[Subject indexing]]
* [[Text mining]], [[web mining]], [[concept mining]]
{{colend}}

== Further reading ==
* Fabrizio Sebastiani. [http://arxiv.org/pdf/cs.ir/0110053 Machine learning in automated text categorization]. ACM Computing Surveys, 34(1):1–47, 2002.
* Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, 2010.

==References==
{{Reflist}}

== External links ==
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node11.html Introduction to document classification]
* [http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html Bibliography on Automated Text Categorization]
* [http://liinwww.ira.uka.de/bibliography/Ai/query-classification.html Bibliography on Query Classification]
* [http://www.gabormelli.com/RKB/Text_Classification_Task Text Classification] analysis page
* [http://www.nltk.org/book/ch06.html Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python] (available online)
* [http://techtc.cs.technion.ac.il TechTC - Technion Repository of Text Categorization Datasets]
* [http://www.daviddlewis.com/resources/testcollections/ David D. Lewis's Datasets]
* [http://www.biocreative.org/tasks/biocreative-iii/ppi/ BioCreative III ACT (article classification task) dataset]





</text>
      <sha1>jme90otn5n30iycfdhs6r3mrqmih1i4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data mining and machine learning software</title>
    <ns>14</ns>
    <id>33542714</id>
    <revision>
      <id>814530060</id>
      <parentid>809515029</parentid>
      <timestamp>2017-12-09T10:19:19Z</timestamp>
      <contributor>
        <username>PRehse</username>
        <id>410898</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="299">{{See also|Comparison of deep learning software}}

Some products in [[:Category:Data analysis software]] and [[:Category:Statistical software]] also include [[data mining]] and [[machine learning]] facilities.



</text>
      <sha1>8mzrb68p7miibagnjp84onhmic1drha</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Machine learning algorithms</title>
    <ns>14</ns>
    <id>33547228</id>
    <revision>
      <id>675167466</id>
      <parentid>457661373</parentid>
      <timestamp>2015-08-08T18:42:07Z</timestamp>
      <contributor>
        <username>AvicBot</username>
        <id>11952314</id>
      </contributor>
      <minor/>
      <comment>Bot: Adding {{Commons category|Machine learning algorithms}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="173">{{Cat see also|Data mining|Classification algorithms|Decision trees}}
{{Commons category|Machine learning algorithms}}


</text>
      <sha1>nly021sanerwp9uh3riqgz15jjaanvf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Applied machine learning</title>
    <ns>14</ns>
    <id>33547387</id>
    <revision>
      <id>457675041</id>
      <parentid>457664515</parentid>
      <timestamp>2011-10-27T16:04:55Z</timestamp>
      <contributor>
        <username>Chire</username>
        <id>12275218</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="79">
</text>
      <sha1>bt946p2iaewoe777ap9sahiik6cmeta</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Markov models</title>
    <ns>14</ns>
    <id>24059390</id>
    <revision>
      <id>757550836</id>
      <parentid>545709647</parentid>
      <timestamp>2016-12-31T08:56:21Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>moved  one level lower</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="116">{{Cat main|Markov model}}


</text>
      <sha1>jujnxez57twlnd8atsdit7djhobrr0n</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Dimension reduction</title>
    <ns>14</ns>
    <id>29549713</id>
    <revision>
      <id>546037650</id>
      <parentid>545723290</parentid>
      <timestamp>2013-03-21T17:48:27Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6960426]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="152">{{catmain|Dimensionality reduction}}




</text>
      <sha1>q0vqfgbbdtjiby9lywvd5i3viykm0qf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Latent variable models</title>
    <ns>14</ns>
    <id>20924581</id>
    <revision>
      <id>545529298</id>
      <parentid>536757088</parentid>
      <timestamp>2013-03-19T22:14:56Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7483206]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="91">

</text>
      <sha1>n1ve8pmonc96twpw7a1czs4iigng2bx</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Cluster analysis</title>
    <ns>14</ns>
    <id>22532673</id>
    <revision>
      <id>751632290</id>
      <parentid>700661551</parentid>
      <timestamp>2016-11-26T23:02:20Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed ; added  using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="192">{{catmain|Cluster analysis}}
{{Commons cat|Cluster analysis}}




</text>
      <sha1>qfxdi6u9pwodgq9e7oqcqhdh06iybx2</sha1>
    </revision>
  </page>
  <page>
    <title>Multilinear principal component analysis</title>
    <ns>0</ns>
    <id>30928751</id>
    <revision>
      <id>811620264</id>
      <parentid>810885996</parentid>
      <timestamp>2017-11-22T20:41:54Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Rep [[typographic ligature]]s like &quot;ﬁ&quot; with plain text; possible ref cleanup;  on, replaced: ﬁ → fi using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9956">{{context|date=June 2012}}
'''Multilinear principal component analysis''' (MPCA)  is a [[multilinear]] extension of [[principal component analysis]] (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a &quot;data tensor&quot;.  N-way arrays may be decomposed, analyzed, or modeled by
* linear tensor models such as CANDECOMP/Parafac, or
* multilinear tensor models, such multilinear principal component analysis (MPCA), or multilinear independent component analysis (MICA), etc.
The origin of MPCA can be traced back to the [[Tucker decomposition]]&lt;ref&gt;{{Cite journal|last1=Tucker| first1=Ledyard R
 | authorlink1 = Ledyard R Tucker
 | title = Some mathematical notes on three-mode factor analysis
 | journal = [[Psychometrika]]
 | volume = 31 | issue = 3 | pages = 279–311
 |date=September 1966
 | doi = 10.1007/BF02289464
}}&lt;/ref&gt; and Peter Kroonenberg's &quot;M-mode PCA/3-mode PCA&quot; work.&lt;ref name=&quot;Kroonenberg1980&quot;&gt;P. M. Kroonenberg and J. de Leeuw, [http://www.springerlink.com/content/c8551t1p31236776/ Principal component analysis of three-mode data by means of alternating least squares algorithms], Psychometrika, 45 (1980), pp. 69–97.&lt;/ref&gt; In 2000, De Lathauwer et al. restated Tucker and Kroonenberg's work in clear and concise numerical computational terms in their SIAM paper entitled &quot;[[Multilinear Singular Value Decomposition]]&quot;,&lt;ref name=&quot;DeLathauwer2000a&quot;&gt;L.D.  Lathauwer,  B.D.  Moor,  J.  Vandewalle (2000) [http://portal.acm.org/citation.cfm?id=354398 &quot;A multilinear singular value decomposition&quot;], ''SIAM Journal of Matrix Analysis and Applications'', 21 (4), 1253–1278&lt;/ref&gt; (HOSVD) and in their paper &quot;On the Best Rank-1 and Rank-(R&lt;sub&gt;1&lt;/sub&gt;, R&lt;sub&gt;2&lt;/sub&gt;, ..., R&lt;sub&gt;N&lt;/sub&gt; ) Approximation of Higher-order Tensors&quot;.&lt;ref name=DeLathauwer2000b&gt;L.  D.  Lathauwer,  B.  D.  Moor,  J.  Vandewalle (2000)   [http://portal.acm.org/citation.cfm?id=354405 &quot;On the best rank-1  and rank-(R1, R2, ..., RN ) approximation of higher-order tensors&quot;], ''SIAM Journal of Matrix Analysis and Applications'' 21 (4), 1324–1342.&lt;/ref&gt;

Circa 2001, Vasilescu reframed the data analysis, recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the compositional consequence of several causal factors of data formation, and are well suited for multi-modal data tensor analysis.  The power of the tensor framework was showcased by analyzing human motion joint angles, facial images or textures in terms of their causal factors of data formation in the following works: Human Motion Signatures
&lt;ref name=&quot;Vasilescu2002b&quot;&gt;M. A. O. Vasilescu (2002) [http://www.media.mit.edu/~maov/motionsignatures/hms_icpr02_corrected.pdf  &quot;Human Motion Signatures: Analysis, Synthesis, Recognition,&quot; Proceedings of International Conference on Pattern Recognition (ICPR 2002), Vol. 3, Quebec City, Canada, Aug, 2002, 456-460.]&lt;/ref&gt;
(CVPR 2001, ICPR 2002), face recognition - [[TensorFaces]],
&lt;ref name=&quot;Vasilescu2002a&quot;/&gt;
&lt;ref name=&quot;Vasilescu2003&quot;/&gt;
(ECCV 2002, CVPR 2003, etc.) and computer graphics -- [[TensorTextures]]&lt;ref name=&quot;Vasilescu2004&quot;/&gt;(Siggraph 2004).

Historically, MPCA has been referred to as &quot;M-mode PCA&quot;, a terminology which was coined by Peter Kroonenberg in 1980.&lt;ref name=&quot;Kroonenberg1980&quot;/&gt; In 2005, [[M. Alex O. Vasilescu|Vasilescu]] and [[Demetri Terzopoulos|Terzopoulos]] introduced the Multilinear PCA&lt;ref name=&quot;MPCA-MICA2005&quot;&gt;M. A. O. Vasilescu, D. Terzopoulos (2005) [http://www.media.mit.edu/~maov/mica/mica05.pdf &quot;Multilinear Independent Component Analysis&quot;], &quot;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’05), San Diego, CA, June 2005, vol.1, 547-553.&quot;&lt;/ref&gt; terminology as a way to better differentiate between linear and multilinear tensor decomposition, as well as, to better differentiate between the work&lt;ref name=&quot;Vasilescu2002b&quot;/&gt;&lt;ref name=&quot;Vasilescu2002a&quot;/&gt;&lt;ref name=&quot;Vasilescu2003&quot;/&gt;&lt;ref name=&quot;Vasilescu2004&quot;/&gt; that computed 2nd order statistics associated with each data tensor mode(axis), and subsequent work on Multilinear Independent Component Analysis&lt;ref name=&quot;MPCA-MICA2005&quot;/&gt; that computed higher order statistics associated with each tensor mode/axis.

Multilinear PCA may be applied to compute the causal factors of data formation, or as signal processing tool on data tensors whose individual observation have either been vectorized &lt;ref name=&quot;Vasilescu2002b&quot;/&gt;
&lt;ref name=&quot;Vasilescu2002a&quot;&gt;M.A.O. Vasilescu, D. Terzopoulos (2002) [http://www.media.mit.edu/~maov/tensorfaces/eccv02_corrected.pdf &quot;Multilinear Analysis of Image Ensembles: TensorFaces,&quot; Proc. 7th European Conference on Computer Vision (ECCV'02), Copenhagen, Denmark, May, 2002, in Computer Vision -- ECCV 2002, Lecture Notes in Computer Science, Vol. 2350, A. Heyden et al. (Eds.), Springer-Verlag, Berlin, 2002, 447-460. ]&lt;/ref&gt;
&lt;ref name=&quot;Vasilescu2003&quot;&gt;M.A.O. Vasilescu, D. Terzopoulos (2003) [http://www.media.mit.edu/~maov/tensorfaces/cvpr03.pdf &quot;Multilinear Subspace Analysis for Image Ensembles,'' M. A. O. Vasilescu, D. Terzopoulos, Proc. Computer Vision and Pattern Recognition Conf. (CVPR '03), Vol.2, Madison, WI, June, 2003, 93-99.]&lt;/ref&gt;
,&lt;ref name=&quot;Vasilescu2004&quot;&gt;M.A.O. Vasilescu, D. Terzopoulos (2004) [http://www.media.mit.edu/~maov/tensortextures/Vasilescu_siggraph04.pdf &quot;TensorTextures: Multilinear Image-Based Rendering&quot;, M. A. O. Vasilescu and D. Terzopoulos, Proc. ACM SIGGRAPH 2004 Conference Los Angeles, CA, August, 2004, in Computer Graphics Proceedings, Annual Conference Series, 2004, 336-342. ]&lt;/ref&gt; or whose observations are treated as matrix &lt;ref name=&quot;MPCA2008&quot;&gt;H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, (2008) [http://www.dsp.utoronto.ca/~haiping/Publication/MPCA_TNN08_rev2010.pdf &quot;MPCA: Multilinear principal component analysis of tensor objects&quot;], ''IEEE Trans. Neural Netw.'',  19 (1), 18–39&lt;/ref&gt; and concatenated into a data tensor.

MPCA computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix SVD.  This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data associated with each data tensor mode(axis).

== The algorithm ==
The MPCA solution follows the alternating least square (ALS) approach.&lt;ref name=&quot;Kroonenberg1980&quot;/&gt; It is iterative in nature.
As in PCA, MPCA works on centered data. Centering is a little more complicated for tensors, and it is problem dependent.

== Feature selection ==
MPCA features:  Supervised MPCA feature selection is used in object recognition&lt;ref name=&quot;MPCA&quot;&gt;,  M. A. O. Vasilescu, D. Terzopoulos (2003) [http://www.cs.toronto.edu/~maov/tensorfaces/cvpr03.pdf &quot;Multilinear Subspace Analysis of Image Ensembles&quot;], &quot;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’03), Madison, WI, June, 2003&quot;&lt;/ref&gt; while unsupervised MPCA feature selection is employed in visualization task.&lt;ref&gt;H.  Lu,  H.-L. Eng, M. Thida, and K.N. Plataniotis,  &quot;[http://www.dsp.utoronto.ca/~haiping/Publication/CrowdMPCA_CIKM2010.pdf Visualization and Clustering of Crowd Video Content in MPCA Subspace],&quot; in Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010), Toronto, ON, Canada, October, 2010.&lt;/ref&gt;

== Extensions ==
Various extensions of MPCA have been developed:
&lt;ref&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt;
*Uncorrelated MPCA (UMPCA) &lt;ref name=&quot;UMPCA&quot;&gt;H.  Lu,  K.  N.  Plataniotis,  and A.  N.  Venetsanopoulos,  &quot;[http://www.dsp.utoronto.ca/~haiping/Publication/UMPCA_TNN09.pdf Uncorrelated multilinear principal component analysis for unsupervised multilinear subspace learning],&quot; IEEE Trans. Neural Netw., vol. 20, no. 11, pp. 1820–1836, Nov. 2009.&lt;/ref&gt; In contrast, the uncorrelated MPCA (UMPCA) generates uncorrelated multilinear features.&lt;ref name=&quot;UMPCA&quot;/&gt;
*[[Boosting (meta-algorithm)|Boosting]]+MPCA&lt;ref&gt;H. Lu, K. N. Plataniotis and A. N. Venetsanopoulos, &quot;[http://www.hindawi.com/journals/ivp/2009/713183.html Boosting Discriminant Learners for Gait Recognition using MPCA Features]&quot;, EURASIP Journal on Image and Video Processing, Volume 2009, Article ID 713183, 11 pages, 2009. {{doi|10.1155/2009/713183}}.&lt;/ref&gt;
*Non-negative MPCA (NMPCA) &lt;ref&gt;Y. Panagakis, C. Kotropoulos, G. R. Arce, &quot;Non-negative multilinear principal component analysis of auditory temporal modulations for music genre classification&quot;, IEEE Trans. on Audio, Speech, and Language Processing, vol. 18, no. 3, pp. 576–588, 2010.&lt;/ref&gt;
*Robust MPCA (RMPCA) &lt;ref&gt;K.  Inoue,  K.  Hara,  K.  Urahama,  &quot;Robust multilinear principal component analysis&quot;, Proc. IEEE Conference on Computer Vision, 2009, pp. 591–597.&lt;/ref&gt;
*Multi-Tensor Factorization, that also finds the number of components automatically (MTF) &lt;ref&gt;{{Cite journal|last=Khan|first=Suleiman A.|last2=Leppäaho|first2=Eemeli|last3=Kaski|first3=Samuel|date=2016-06-10|title=Bayesian multi-tensor factorization|url=https://link.springer.com/article/10.1007/s10994-016-5563-y|journal=Machine Learning|language=en|volume=105|issue=2|pages=233–253|doi=10.1007/s10994-016-5563-y|issn=0885-6125}}&lt;/ref&gt;

==References==
{{Reflist}}

== External links ==
* ''Matlab code'': [http://www.mathworks.com/matlabcentral/fileexchange/26168 MPCA].
* ''Matlab code'': [http://www.mathworks.com/matlabcentral/fileexchange/35432 UMPCA (including data)].
* ''R code:'' [http://research.cs.aalto.fi/pml/software/mtf/ MTF]


</text>
      <sha1>5cxlwv8o8mx7blqwf42tcwabpa8sg0v</sha1>
    </revision>
  </page>
  <page>
    <title>Base rate</title>
    <ns>0</ns>
    <id>9732182</id>
    <revision>
      <id>797767246</id>
      <parentid>765676614</parentid>
      <timestamp>2017-08-29T01:56:24Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3384">{{about|the statistical term|interest rates|Central bank}}
{{refimprove|date=March 2016}}
In [[probability]] and [[statistics]], '''base rate''' generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as [[prior probabilities]]. For example, if it were the case that 1% of the public were &quot;medical professionals&quot;, and 99% of the public were ''not'' &quot;medical professionals&quot;, then the base rate of medical professionals is simply 1%.

In the [[sciences]], including [[medicine]], the base rate is critical for comparison. It may at first seem impressive that 1000 people beat their winter cold while using 'Treatment X', until we look at the entire 'Treatment X' population and find that the base rate of success is only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. &quot;1000 people... out of how many?&quot;) is available. Note that controls may likewise offer further information for comparison; maybe the [[control group]]s, who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that 'Treatment X' makes things worse, despite that initial proud claim about 1000 people.

The normative method for integrating base rates ([[prior probabilities]]) and featural evidence ([[likelihood]]s) is given by [[Bayes' theorem|Bayes' rule]].

==The base rate fallacy==
{{Main|Base rate fallacy}}

A large number of psychological studies have examined a phenomenon called '''[[Base rate fallacy|base-rate neglect]]'' or ''[[base rate fallacy]]''' in which category base rates are not integrated with featural evidence in the normative manner. Mathematician [[Keith Devlin]] provides an illustration of the risks of this: He asks us to imagine that there is a type of cancer that afflicts 1% of all people. A doctor then says there is a test for that cancer which is about 80% [[reliability (statistics)|reliable]]. He also says that the test provides a positive result for 100% of people who have the cancer, but it also results in a 'false positive' for 20% of people - who do not have the cancer. Now, if we test positive, we may be tempted to think it is 80% likely that we have the cancer. Devlin explains that, in fact, our odds are less than 5%. What is missing from the jumble of statistics is the most relevant base rate information. We should ask the doctor, ''&quot;Out of the number of people who test positive (this is the base rate group that we care about), how many have the cancer?&quot;''&lt;ref&gt;http://www.edge.org/responses/what-scientific-concept-would-improve-everybodys-cognitive-toolkit&lt;/ref&gt;  In assessing the probability that a given individual is a member of a particular class, we must account for other information besides the base rate. In particular, we must account for featural evidence. For example, when we see a person wearing a white doctor's coat and [[stethoscope]], and prescribing medication, we have evidence which may allow us to conclude that the probability of this ''particular'' individual being a &quot;medical professional&quot; is considerably greater than the category base rate of 1%.

==References==
{{Reflist}}




</text>
      <sha1>iayj43uoilqjze4tsdxajbnsfhnsj5w</sha1>
    </revision>
  </page>
  <page>
    <title>Inferential theory of learning</title>
    <ns>0</ns>
    <id>33762888</id>
    <revision>
      <id>786002166</id>
      <parentid>670361031</parentid>
      <timestamp>2017-06-16T17:33:12Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1330">'''Inferential theory of learning''' ('''ITL''') is an area of [[machine learning]] which describes inferential processes performed by learning agents.  ITL has been developed by Ryszard S. Michalski in 1980s.  In ITL [[Learning|learning process]] is viewed as a search ([[inference]]) through hypotheses space guided by a specific goal.  Results of learning need to be [[Data storage device|stored]], in order to be used in the future.

==References==
{{unreferenced|date=November 2011}}
{{Reflist}}

==Further reading==
{{Refbegin}}
* Ryszard S. Michalski, Jaime G. Carbonell, Tom M. Mitchell (1983), ''Machine Learning: An Artificial Intelligence Approach'', Tioga Publishing Company, {{ISBN|0-935382-05-4}}.
** Ryszard S. Michalski, Jaime G. Carbonell, Tom M. Mitchell (1986), ''Machine Learning: An Artificial Intelligence Approach, Volume II'', Morgan Kaufmann, {{ISBN|0-934613-00-1}}.
** Yves Kodratoff, Ryszard S. Michalski (1990), ''Machine Learning: An Artificial Intelligence Approach, Volume III'', Morgan Kaufmann, {{ISBN|1-55860-119-8}}.
** Ryszard S. Michalski, George Tecuci (1994), ''Machine Learning: A Multistrategy Approach'', Volume IV, Morgan Kaufmann, {{ISBN|1-55860-251-8}}.
{{Refend}}






{{Compu-AI-stub}}
{{Systemstheory-stub}}</text>
      <sha1>2ow20v8cqffqc6wilnovtbcgxcjfkbd</sha1>
    </revision>
  </page>
  <page>
    <title>Coupled pattern learner</title>
    <ns>0</ns>
    <id>34042707</id>
    <revision>
      <id>811572379</id>
      <parentid>739337694</parentid>
      <timestamp>2017-11-22T14:13:14Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Rep [[typographic ligature]]s like &quot;ﬁ&quot; with plain text; possible ref cleanup;  on, replaced: ﬁ → fi (9) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9116">{{Orphan|date=March 2012}}

Coupled Pattern Learner (CPL) is a [[machine learning]] algorithm which couples the [[semi-supervised learning]] of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.

== Coupled Pattern Learner ==
[[Semi-supervised learning]] approaches using a small number of labeled examples with many unlabeled examples are usually unreliable as they produce an internally consistent, but incorrect set of extractions. CPL solves this problem by simultaneously learning classifiers for many different categories and relations in the presence of an [[ontology]] defining constraints that couple the training of these classifiers. It was introduced by Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr. and Tom M. Mitchell in 2009.&lt;ref name=cbl2009&gt;{{cite journal|last=Carlson|first=Andrew|author2=Justin Betteridge |author3=Estevam R. Hruschka Jr. |author4= Tom M. Mitchell |year=2009|title=Coupling semi-supervised learning of categories and relations|journal=Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing |publisher=Association for Computational Linguistics|location=Colorado, USA|pages=1–9|url=http://dl.acm.org/citation.cfm?id=1621829.1621830}}&lt;/ref&gt;&lt;ref name=cpl2010&gt;{{cite journal|last=Carlson|first=Andrew|author2=Justin Betteridge |author3=Richard C. Wang |author4=Estevam R. Hruschka Jr. |author5= Tom M. Mitchell |year=2010|title=Coupled semi-supervised learning for information extraction|journal=Proceedings of the third ACM international conference on Web search and data mining |publisher=ACM|location=NY, USA|pages=101–110|url=http://dl.acm.org/citation.cfm?doid=1718487.1718501}}&lt;/ref&gt;

== CPL Overview==
CPL is an approach to [[semi-supervised learning]] that yields more accurate results by coupling the training of many information extractors. Basic idea behind CPL is that semi-supervised training of a single type of extractor such as ‘coach’ is much more difficult than simultaneously training many extractors that cover a variety of inter-related entity and relation types. Using prior knowledge about the relationships between these different entities and relations CPL makes unlabeled data as a useful constraint during training. For e.g., ‘coach(x)’ implies ‘person(x)’ and ‘not sport(x)’.

== CPL Description ==
=== Coupling of Predicates ===
CPL primarily relies on the notion of coupling the [[learning]] of multiple functions so as to constrain the semi-supervised learning problem. CPL constrains the learned function in two ways.
# Sharing among same-arity predicates according to logical relations
# Relation argument type-checking

=== Sharing among same-arity predicates ===
Each predicate P in the ontology has a list of other same-arity predicates with which P is mutually exclusive. If A is [[mutually exclusive]] with predicate B, A’s positive instances and patterns become negative instances and negative patterns for B. For example, if ‘city’, having an instance ‘Boston’ and a pattern ‘mayor of arg1’, is mutually exclusive with ‘scientist’, then ‘Boston’ and ‘mayor of arg1’ will become a negative instance and a negative pattern respectively for ‘scientist.’  Further, Some categories are declared to be a subset of another category.  For e.g., ‘athlete’ is a subset of ‘person’.

=== Relation argument type-checking ===
This is a type checking information used to couple the learning of relations and categories. For example, the arguments of the ‘ceoOf’ relation are declared to be of the categories ‘person’ and ‘company’. CPL does not promote a pair of noun phrases as an instance of a relation unless the two noun phrases are classified as belonging to the correct argument types.

=== Algorithm Description ===

Following is a quick summary of the CPL algorithm.&lt;ref name=cpl2010 /&gt;
 Input: An ontology O, and a text corpus C
 Output: Trusted instances/patterns for each predicate
 '''for''' i=1,2,...,∞ '''do'''
  '''foreach''' predicate p in O '''do'''
   EXTRACT candidate instances/contextual patterns using recently promoted patterns/instances;
   FILTER candidates that violate coupling;
   RANK candidate instances/patterns;
   PROMOTE top candidates;
  '''end'''
 '''end'''

==== Inputs ====
A large [[Text corpus|corpus]] of Part-Of-Speech tagged sentences and an initial ontology with predefined categories, relations, mutually exclusive relationships between same-arity predicates, subset relationships between some categories, seed instances for all predicates, and seed patterns for the categories.

==== Candidate extraction ====
CPL finds new candidate instances by using newly promoted patterns to extract the noun phrases that co-occur with those patterns in the text corpus. CPL extracts,
* Category Instances
* Category Patterns
* Relation Instances
* Relation Patterns

==== Candidate Filtering ====
Candidate instances and patterns are filtered to maintain high precision, and to avoid extremely specific patterns. An instance is only considered for assessment if it co-occurs with at least two promoted patterns in the text corpus, and if its co-occurrence count with all promoted patterns is at least three times greater than its co-occurrence count with negative patterns.

==== Candidate Ranking ====
CPL ranks candidate instances using the number of promoted patterns that they co-occur with so that candidates that occur with more patterns are ranked higher. Patterns are ranked using an estimate of the precision of each pattern.

==== Candidate Promotion ====
CPL ranks the candidates according to their assessment scores and promotes at most 100 instances and 5 patterns for each predicate. Instances and patterns are only promoted if they co-occur with at least two promoted patterns or instances, respectively.

== Meta-Bootstrap Learner ==
Meta-Bootstrap Learner (MBL) was also proposed by the authors of CPL in.&lt;ref name=cpl2010 /&gt; Meta-Bootstrap learner couples the training of multiple extraction techniques with a multi-view constraint, which requires the extractors to agree. It makes addition of coupling constraints on top of existing extraction algorithms, while treating them as black boxes, feasible. MBL assumes that the errors made by different extraction techniques are independent. Following is a quick summary of MBL.

 '''Input''': An ontology O, a set of extractors ε
 '''Output''': Trusted instances for each predicate
 '''for''' i=1,2,...,∞ do
  '''foreach''' predicate p in O '''do'''
   '''foreach''' extractor e in ε '''do'''
    Extract new candidates for p using e with recently promoted instances;
   '''end'''
   FILTER candidates that violate mutual-exclusion or type-checking constraints;
   PROMOTE candidates that were extracted by all extractors;
  '''end'''
 '''end'''

Subordinate algorithms used with MBL do not promote any instance on their own, they report the evidence about each candidate to MBL and MBL is responsible for promoting instances.

== Applications ==
In their paper &lt;ref name=cbl2009 /&gt; authors have presented results showing the potential of CPL to contribute new facts to existing repository of semantic knowledge, Freebase &lt;ref&gt;{{cite journal|year=2009 |title=Freebase data dumps |publisher=Metaweb Technologies |url=http://download.freebase.com/datadumps/ |deadurl=yes |archiveurl=https://web.archive.org/web/20111206102101/http://download.freebase.com/datadumps/ |archivedate=December 6, 2011 }}&lt;/ref&gt;

== See also ==
* [[Co-training]]
* [[Never-Ending Language Learning]]

== Notes ==
{{reflist}}

==References==
* {{cite journal|last=Liu|first=Qiuhua |author2=Xuejun Liao |author3=Lawrence Carin |year=2008|title=Semi-supervised multitask learning|journal=NIPS}}
* {{cite journal|last=Shinyama|first=Yusuke|author2=Satoshi Sekine|year=2006|title=Preemptive information extraction using unrestricted relation discovery|journal=HLT-NAACL}}
* {{cite journal|last=Chang|first=Ming-Wei|author2=Lev-Arie Ratinov |author3=Dan Roth |year=2007|title=Guiding semi-supervision with constraint driven learning|journal=ACL}}
*  {{cite journal|last=Banko|first=Michele|author2=Michael J. Cafarella |author3=Stephen Soderland |author4=Matt Broadhead |author5=Oren Etzioni |author5-link=Oren Etzioni |year=2007|title=Open information extraction from the web|journal=IJCAI}}
* {{cite journal|last=Blum|first=Avrim|author2=Tom Mitchell|year=1998|title=Combining labeled and unlabeled data with co-training|journal=COLT}}
* {{cite journal|last=Riloff|first=Ellen|author2=Rosie Jones|year=1999|title=Learning dictionaries for information extraction by multi-level bootstrapping|journal=AAAI}}
* {{cite journal|last=Rosenfeld|first=Benjamin|author2=Ronen Feldman|year=2007|title=Using corpus statistics on entities to improve semi-supervised relation extraction from the web|journal=ACL}}
* {{cite journal|last=Wang|first=Richard C.|author2=William W. Cohen|year=2008|title=Iterative set expansion of named entities using the web|journal=ICDM}}

</text>
      <sha1>lv5p8y73ub2c8xvqpfz9qitz004uns6</sha1>
    </revision>
  </page>
  <page>
    <title>Feature scaling</title>
    <ns>0</ns>
    <id>34061548</id>
    <revision>
      <id>813164280</id>
      <parentid>813164014</parentid>
      <timestamp>2017-12-02T04:39:23Z</timestamp>
      <contributor>
        <username>Jusdafax</username>
        <id>5519174</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/2600:387:1:817:0:0:0:A3|2600:387:1:817:0:0:0:A3]] ([[User talk:2600:387:1:817:0:0:0:A3|talk]]): unexplained content removal ([[WP:HG|HG]]) (3.3.3)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6634">'''Feature scaling''' is a method used to standardize the range of independent variables or features of data. In [[data processing]], it is also known as data normalization and is generally performed during the data preprocessing step.

==Motivation==
Since the range of values of raw data varies widely, in some [[machine learning]] algorithms, objective functions will not work properly without [[Normalization (statistics)|normalization]]. For example, the majority of [[Statistical classification|classifiers]] calculate the distance between two points by the [[Euclidean distance]]. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.

Another reason why feature scaling is applied is that [[gradient descent]] converges much faster with feature scaling than without it.&lt;ref&gt;{{cite journal|last=Ioffe|first=Sergey|author2=Christian Szegedy|year=2015|title=Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift|url=https://arxiv.org/abs/1502.03167}}&lt;/ref&gt;
{|
&lt;!-- Deleted image removed: [[Image:Before FS.png|thumb|300px|A convergence of the Gradient Descent algorithm before applying feature scaling]] --&gt;
&lt;!-- Deleted image removed: [[Image:After FS.png|thumb|300px|A convergence of the Gradient Descent algorithm after applying feature scaling]] --&gt;
|}

==Methods==

===Rescaling===
The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula is given as:

&lt;math&gt;x' = \frac{x - \text{min}(x)}{\text{max}(x)-\text{min}(x)}&lt;/math&gt;

where &lt;math&gt;x&lt;/math&gt; is an original value, &lt;math&gt;x'&lt;/math&gt; is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights).

===Mean normalisation===

&lt;math&gt;x' = \frac{x - \text{mean}(x)}{\text{max}(x)-\text{min}(x)}&lt;/math&gt;

where &lt;math&gt;x&lt;/math&gt; is an original value, &lt;math&gt;x'&lt;/math&gt; is the normalized value.

===Standardization===
In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple [[dimensions]]. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., [[support vector machine]]s, [[logistic regression]], and [[neural network]]s)&lt;ref name=&quot;:0&quot;&gt;{{Cite book|title = Data Science from Scratch|last = Grus|first = Joel|publisher = O'Reilly|year = 2015|isbn = 978-1-491-90142-7|location = Sebastopol, CA|pages = 99, 100}}&lt;/ref&gt; {{Citation needed|date=September 2014}}. The general method of calculation is to determine the distribution [[mean]] and [[standard deviation]] for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.

&lt;math&gt;x' = \frac{x - \bar{x}}{\sigma}&lt;/math&gt;

Where &lt;math&gt;x&lt;/math&gt; is the original feature vector, &lt;math&gt;\bar{x}&lt;/math&gt; is the mean of that feature vector, and &lt;math&gt;\sigma&lt;/math&gt; is its standard deviation.

===Scaling to unit length===
Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the [[Euclidean length]] of the vector:
:&lt;math&gt;x' = \frac{x}{||x||}&lt;/math&gt;

In some applications (e.g. Histogram features) it can be more practical to use the L1 norm (i.e. Manhattan Distance, City-Block Length or [[Taxicab Geometry]]) of the feature vector. This is especially important if in the following learning steps the Scalar Metric is used as a distance measure.

== Application ==
In stochastic [[gradient descent]], feature scaling can sometimes improve the convergence speed of the algorithm&lt;ref name=&quot;:0&quot; /&gt; {{Citation needed|date=September 2014}}. In support vector machines,&lt;ref&gt;{{cite journal|last=Juszczak|first=P.|author2=D. M. J. Tax |author3=R. P. W. Dui |year=2002|title=Feature scaling in support vector data descriptions|journal=Proc. 8th Annu. Conf. Adv. School Comput. Imaging|pages=25–30|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.100.2524}}&lt;/ref&gt; it can reduce the time to find support vectors. Note that feature scaling changes the SVM result {{Citation needed|date=September 2014}}.

==See also==
* [[fMLLR]]

==References==
{{reflist}}

== General references ==
* S. Aksoy and R. Haralick, &quot;Feature normalization and likelihood-based similarity measures for image retrieval,&quot; Pattern Recognit. Lett., Special Issue on Image and Video Retrieval, 2000 http://www.cs.bilkent.edu.tr/~saksoy/papers/prletters01_likelihood.pdf
* S. Tsakalidis, V. Doumpiotis &amp; W. Byrne, &quot;Discriminative Linear Transforms for Feature Normalization and Speaker Adaptation in HMM Estimation&quot;, Proc. ICSLP'02, Denver. http://malach.umiacs.umd.edu/pubs/VD_05_Discrim_linear.pdf
* Liefeng Bo, Ling Wang, and Licheng Jiao, &quot;Feature Scaling for Kernel Fisher Discriminant Analysis Using Leave-one-out Cross Validation&quot;, Neural Computation (NECO), vol. 18(4), pp.&amp;nbsp;961–978, 2006 http://www.cs.washington.edu/homes/lfb/paper/nc06.pdf
* A. Stolcke, S. Kajarekar, and L. Ferrer, &quot;Nonparametric feature normalization for SVM-based speaker verification,&quot; in Proc. ICASSP, Las Vegas, Apr. 2008. http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4517925
* {{cite journal | last1 = Youn | first1 = E. | last2 = Jeong | first2 = M. K. | year = 2009 | title = Class dependent feature scaling method using naive Bayes classifier for text datamining | url = http://www.sciencedirect.com/science/article/pii/S0167865508003553 | journal = Pattern Recognition Letters | volume =  30| issue = | pages = 477–485| doi=10.1016/j.patrec.2008.11.013}}
* S. Theodoridis, K. Koutroumbas. (2008) “Pattern Recognition”, Academic Press, 4 edition, {{ISBN|978-1-59749-272-0}}

== External links ==
*[http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&amp;video=03.1-LinearRegressionII-FeatureScaling&amp;speed=100/ Lecture by Andrew Ng on feature scaling]


</text>
      <sha1>0565fnwqlr7c27hk0jo8cdj9y0br08i</sha1>
    </revision>
  </page>
  <page>
    <title>Preference learning</title>
    <ns>0</ns>
    <id>34072838</id>
    <revision>
      <id>800861271</id>
      <parentid>793550694</parentid>
      <timestamp>2017-09-16T03:39:25Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>clean up spacing around punctuation, replaced: ,B → , B (2), ,G → , G, ,k → , k, ,y → , y (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9016">'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.&lt;ref&gt;{{Cite Mehryar Afshin Ameet 2012}}&lt;/ref&gt; In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.

While the concept of preference learning has been emerged for some time in many fields such as [[economics]],&lt;ref name=&quot;SHOG00&quot; /&gt; it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.&lt;ref name=&quot;WEB:WORKSHOP&quot; /&gt;

==Tasks==

The main task in preference learning concerns problems in &quot;[[learning to rank]]&quot;. According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':&lt;ref name=&quot;FURN11&quot; /&gt;

===Label ranking===

In label ranking, the model has an instance space &lt;math&gt;X=\{x_i\}\,\!&lt;/math&gt; and a finite set of labels &lt;math&gt;Y=\{y_i|i=1,2,\cdots,k\}\,\!&lt;/math&gt;. The preference information is given in the form &lt;math&gt;y_i \succ_{x} y_j\,\!&lt;/math&gt; indicating instance &lt;math&gt;x\,\!&lt;/math&gt; shows preference in &lt;math&gt;y_i\,\!&lt;/math&gt; rather than &lt;math&gt;y_j\,\!&lt;/math&gt;. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.

It was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:&lt;ref name=&quot;HARP03&quot; /&gt; if a training instance &lt;math&gt;x\,\!&lt;/math&gt; is labeled as class &lt;math&gt;y_i\,\!&lt;/math&gt;, it implies that &lt;math&gt;\forall j \neq i, y_i \succ_{x} y_j\,\!&lt;/math&gt;. In the [[Multi-label classification|multi-label]] case, &lt;math&gt;x\,\!&lt;/math&gt; is associated with a set of labels &lt;math&gt;L \subseteq Y\,\!&lt;/math&gt; and thus the model can extract a set of preference information &lt;math&gt;\{y_i \succ_{x} y_j | y_i \in L, y_j \in Y\backslash L\}\,\!&lt;/math&gt;. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.

===Instance ranking===

Instance ranking also has the instance space &lt;math&gt;X\,\!&lt;/math&gt; and label set &lt;math&gt;Y\,\!&lt;/math&gt;. In this task, labels are defined to have a fixed order &lt;math&gt;y_1 \succ y_2 \succ \cdots \succ y_k\,\!&lt;/math&gt; and each instance &lt;math&gt;x_l\,\!&lt;/math&gt; is associated with a label &lt;math&gt;y_l\,\!&lt;/math&gt;. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.

===Object ranking===

Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form &lt;math&gt;x_i \succ x_j\,\!&lt;/math&gt; and the model should find out a ranking order among instances.

==Techniques==

There are two practical representations of the preference information &lt;math&gt;A \succ B\,\!&lt;/math&gt;. One is assigning &lt;math&gt;A\,\!&lt;/math&gt; and &lt;math&gt;B\,\!&lt;/math&gt; with two real numbers &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt; respectively such that &lt;math&gt;a &gt; b\,\!&lt;/math&gt;. Another one is assigning a binary value &lt;math&gt;V(A,B) \in \{0,1\}\,\!&lt;/math&gt; for all pairs &lt;math&gt;(A,B)\,\!&lt;/math&gt; denoting whether &lt;math&gt;A \succ B\,\!&lt;/math&gt; or &lt;math&gt;B \succ A\,\!&lt;/math&gt;. Corresponding to these two different representations, there are two different techniques applied to the learning process.

===Utility function===

If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function &lt;math&gt;f: X \times Y \rightarrow \mathbb{R}\,\!&lt;/math&gt; such that &lt;math&gt;y_i \succ_x y_j \Rightarrow f(x,y_i) &gt; f(x,y_j)\,\!&lt;/math&gt;. For instance ranking and object ranking, the mapping is a function &lt;math&gt;f: X \rightarrow \mathbb{R}\,\!&lt;/math&gt;.

Finding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.

===Preference relations===

The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz and Hüllermeier proposed this approach in label ranking problem.&lt;ref name=&quot;FURN03&quot; /&gt; For object ranking, there is an early approach by Cohen et al.&lt;ref name=&quot;COHE98&quot; /&gt;

Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.&lt;ref name=&quot;FURN03&quot; /&gt;

==Uses==

Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.&lt;ref name=&quot;LIU09&quot; /&gt;

Another application of preference learning is [[recommender systems]].&lt;ref name=&quot;GEMM09&quot; /&gt; Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.

==See also==
*[[Learning to rank]]

==References==

{{Reflist|
refs=

&lt;ref name=&quot;SHOG00&quot;&gt;{{
cite journal
|last       = Shogren
|first      = Jason F. |author2=List, John A. |author3=Hayes, Dermot J.
|year       = 2000
|title      = Preference Learning in Consecutive Experimental Auctions
|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm
|journal    = American Journal of Agricultural Economics
|volume     = 82
|pages      = 1016–1021
|doi=10.1111/0002-9092.00099
}}&lt;/ref&gt;

&lt;ref name=&quot;WEB:WORKSHOP&quot;&gt;{{
cite web
|title      = Preference learning workshops
|url        = http://www.preference-learning.org/#Workshops
}}&lt;/ref&gt;

&lt;ref name=&quot;FURN11&quot;&gt;{{
cite book
|last       = Fürnkranz
|first      = Johannes
 |author2=Hüllermeier, Eyke
|year       = 2011
|title      = Preference Learning
|url        = https://books.google.com/books?id=nc3XcH9XSgYC
|chapter    = Preference Learning: An Introduction
|chapterurl = https://books.google.com/books?id=nc3XcH9XSgYC&amp;pg=PA4
|publisher  = Springer-Verlag New York, Inc.
|pages      = 3–8
|isbn       = 978-3-642-14124-9
}}&lt;/ref&gt;

&lt;ref name=&quot;HARP03&quot;&gt;{{
cite journal
|last       = Har-peled
|first      = Sariel |author2=Roth, Dan |author3=Zimak, Dav
|year       = 2003
|title      = Constraint classification for multiclass classification and ranking
|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02
|pages      = 785–792
}}&lt;/ref&gt;

&lt;ref name=&quot;FURN03&quot;&gt;{{
cite journal
|last       = Fürnkranz
|first      = Johannes
 |author2=Hüllermeier, Eyke
|year       = 2003
|title      = Pairwise Preference Learning and Ranking
|journal    = Proceedings of the 14th European Conference on Machine Learning
|pages      = 145–156
}}&lt;/ref&gt;

&lt;ref name=&quot;COHE98&quot;&gt;{{
cite journal
|last       = Cohen
|first      = William W. |author2=Schapire, Robert E. |author3=Singer, Yoram
|year       = 1998
|title      = Learning to order things
|url        = http://dl.acm.org/citation.cfm?id=302528.302736
|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems
|pages      = 451–457
}}&lt;/ref&gt;

&lt;ref name=&quot;LIU09&quot;&gt;{{
cite journal
|last       = Liu
|first      = Tie-Yan
|year       = 2009
|title      = Learning to Rank for Information Retrieval
|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304
|journal    = Foundations and Trends in Information Retrieval
|volume     = 3
|issue      = 3
|pages      = 225–331
|doi        = 10.1561/1500000016
}}&lt;/ref&gt;

&lt;ref name=&quot;GEMM09&quot;&gt;{{
cite journal
|last       = Gemmis
|first      = Marco De
|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro, Giovanni
|year       = 2009
|title      = Preference Learning in Recommender Systems
|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45
|journal    = PREFERENCE LEARNING
|volume     = 41
|pages      = 387–407
|doi=10.1007/978-3-642-14125-6_18
}}&lt;/ref&gt;

}}

==External links==
*[http://www.preference-learning.org/ Preference Learning site]


</text>
      <sha1>cmhbogxte50gdalz4ui6lz218msj0br</sha1>
    </revision>
  </page>
  <page>
    <title>Proactive learning</title>
    <ns>0</ns>
    <id>21985449</id>
    <revision>
      <id>725955118</id>
      <parentid>671339111</parentid>
      <timestamp>2016-06-19T00:24:54Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor/>
      <comment>/* top */ rm spaces, punct.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2178">'''Proactive learning'''&lt;ref name=donmez08/&gt; is a generalization of [[active learning]] designed to relax unrealistic assumptions and thereby reach practical applications.

&quot;Active learning seeks to select the most informative unlabeled instances and ask an omniscient [[oracle (computer science)|oracle]] for their labels, so as to retrain a [[machine learning|learning]] algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same).&quot;&lt;ref name=donmez08&gt;Donmez, P., Carbonell, J.G.: Proactive Learning: Cost-Sensitive [[Active learning|Active Learning]] with Multiple Imperfect Oracles, in Proceedings of the 17th ACM Conference on Information and [[Knowledge management|Knowledge Management]] (CIKM '08), [[Napa County, California|Napa Valley]] 2008. http://www.cs.cmu.edu/~pinard/Papers/cikm0613-donmez.pdf&lt;/ref&gt;

&quot;In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an &quot;oracle&quot; (if we generalize the term to mean any source of expert information) may be incorrect (fallible)
with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant – it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation.
Such an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors.&quot;&lt;ref  name=donmez08/&gt;

Proactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility [[optimization problem]] subject to a [[budget constraint]].

==References==
{{Reflist}}

{{DEFAULTSORT:Proactive Learning}}

</text>
      <sha1>pmzidlffvo79n2tug691876wpcezfn0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computational learning theory</title>
    <ns>14</ns>
    <id>34310097</id>
    <revision>
      <id>550844741</id>
      <parentid>535928176</parentid>
      <timestamp>2013-04-17T17:50:18Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q11275242]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="115">{{cat main|computational learning theory}}


</text>
      <sha1>ttbudu344a7sbrn53d0c99jgnl0uxp1</sha1>
    </revision>
  </page>
  <page>
    <title>Computational learning theory</title>
    <ns>0</ns>
    <id>387537</id>
    <revision>
      <id>808979083</id>
      <parentid>804610754</parentid>
      <timestamp>2017-11-06T11:51:00Z</timestamp>
      <contributor>
        <ip>122.59.191.40</ip>
      </contributor>
      <comment>/* Overview */  Removed extra &quot;the&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7499">{{see also|Statistical learning theory}}

{{Machine learning bar}}

In [[computer science]], '''computational learning theory''' (or just '''learning theory''') is a subfield of [[Artificial Intelligence]] devoted to studying the design and analysis of [[machine learning]] algorithms.&lt;ref name=&quot;ACL&quot;&gt;http://www.learningtheory.org/&lt;/ref&gt;

==Overview==
Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning.  In supervised
learning, an algorithm is given samples that are labeled in some useful way.  For example, the samples might be descriptions of
mushrooms, and the labels could be whether or not the mushrooms are edible.  The algorithm takes these previously labeled samples and
uses them to induce a classifier.  This classifier is a function that assigns labels to samples including samples that have never been
previously seen by the algorithm.  The goal of the supervised learning algorithm is to optimize some measure of performance such as
minimizing the number of mistakes made on new samples.

In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning{{citation needed|date=October 2017}}. In
computational learning theory, a computation is considered feasible if it can be done in [[polynomial time]]{{citation needed|date=October 2017}}. There are two kinds of time
complexity results:

* Positive results{{spaced ndash}}Showing that a certain class of functions is learnable in polynomial time.
* Negative results{{spaced ndash}}Showing that certain classes cannot be learned in polynomial time.

Negative results often rely on commonly believed, but yet unproven assumptions{{citation needed|date=October 2017}}, such as:

* Computational complexity – [[P versus NP problem|P &amp;ne; NP (the P versus NP problem)]];
* [[cryptography|Cryptographic]] – [[One-way function]]s exist.

There are several different approaches to computational learning theory.  These differences are based on making assumptions about the
[[inference]] principles used to generalize from limited data{{citation needed|date=October 2017}}.  This includes different definitions of [[probability]] (see [[frequency probability]], [[Bayesian probability]]) and different assumptions on the generation of samples{{citation needed|date=October 2017}}.  The different approaches include {{citation needed|date=October 2017}}:

* [[Exact learning]], proposed by [[Dana Angluin]];
* [[Probably approximately correct learning]] (PAC learning), proposed by [[Leslie Valiant]];
* [[VC theory]], proposed by [[Vladimir Vapnik]] and [[Alexey Chervonenkis]];
* [[Bayesian inference]];
* [[Algorithmic learning theory]], from the work of [[E. Mark Gold]];
* [[Online machine learning]], from the work of Nick Littlestone.

Computational learning theory has led to several practical algorithms{{according to whom|date=October 2017}}. For example, PAC theory inspired [[Boosting (meta-algorithm)|boosting]], VC theory led to [[support vector machine]]s, and Bayesian inference led to [[belief networks]] (by [[Judea Pearl]]).

==See also==
* [[Grammar induction]]
* [[Information theory]]
* [[Stability (learning theory)]]
* [[Error Tolerance (PAC learning)]]

==References==
{{Reflist}}

===Surveys===
* Angluin, D. 1992. Computational learning theory: Survey and selected bibliography. In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing (May 1992), pages&amp;nbsp;351–369. http://portal.acm.org/citation.cfm?id=129712.129746
* D. Haussler. Probably approximately correct learning. In AAAI-90 Proceedings of the Eight National Conference on Artificial Intelligence, Boston, MA, pages 1101–1108. American Association for Artificial Intelligence, 1990. http://citeseer.ist.psu.edu/haussler90probably.html

===[[VC dimension]]===
* V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264–280, 1971.

===Feature selection===
* A. Dhagat and L. Hellerstein, &quot;PAC learning with irrelevant attributes&quot;, in 'Proceedings of the IEEE Symp. on Foundation of Computer Science', 1994. http://citeseer.ist.psu.edu/dhagat94pac.html

===Inductive inference===
* {{Cite journal | last1 = Gold | first1 = E. Mark | year = 1967 | title = Language identification in the limit | journal = Information and Control | volume = 10 | issue = 5 | pages = 447–474 | publisher =  | jstor =  | doi = 10.1016/S0019-9958(67)91165-5 | url=http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf | format =  | accessdate = }}

===Optimal O notation learning===
* [[Oded Goldreich]], [[Dana Ron]]. ''[http://www.eng.tau.ac.il/~danar/Public-pdf/ul.pdf On universal learning algorithms]''. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.2224

===Negative results===
*  M. Kearns and [[Leslie Valiant]]. 1989. Cryptographic limitations on learning boolean formulae and finite automata. In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, pages 433–444, New York. ACM. http://citeseer.ist.psu.edu/kearns89cryptographic.html

===[[Boosting (machine learning)]]===
* Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990 http://citeseer.ist.psu.edu/schapire90strength.html

===[[Occam Learning]]===
* Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. &quot;Occam's razor&quot; Inf.Proc.Lett. 24, 377–380, 1987.
* A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929–865, 1989.

===[[Probably approximately correct learning]]===
* L. Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134–1142, 1984.

===Error tolerance===
* Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4):807–837, August 1993. http://citeseer.ist.psu.edu/kearns93learning.html
* Kearns, M. (1993). Efficient noise-tolerant learning from statistical queries. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, pages 392–401. http://citeseer.ist.psu.edu/kearns93efficient.html

===Equivalence===
* D.Haussler, M.Kearns, N.Littlestone and [[Manfred K. Warmuth|M. Warmuth]], Equivalence of models for polynomial learnability, Proc. 1st ACM Workshop on Computational Learning Theory, (1988) 42-55.
* {{Cite journal | last1 = Pitt | first1 = L. | last2 = Warmuth | first2 = M. K. | year = 1990 | title = Prediction-Preserving Reducibility | journal = Journal of Computer System and Science | volume = 41 | issue =  3| pages = 430–467 | publisher =  | jstor =  | doi = 10.1016/0022-0000(90)90028-J | url = http://www.sciencedirect.com/science/article/pii/002200009090028J/pdf?md5=251ac513b584de61a8fe4aa421ec9ab7&amp;pid=1-s2.0-002200009090028J-main.pdf | format =  | accessdate = }}

A description of some of these publications is given at [[list of important publications in computer science#Machine learning|important publications in machine learning]].

===[[Distribution learning theory|Distribution Learning Theory]]===

==External links==
* [http://research.microsoft.com/adapt/MSBNx/msbnx/Basics_of_Bayesian_Inference.htm Basics of Bayesian inference]






</text>
      <sha1>b6k515pq2u1p5nczblcltd8k6q32odh</sha1>
    </revision>
  </page>
  <page>
    <title>Mountain car problem</title>
    <ns>0</ns>
    <id>33998310</id>
    <revision>
      <id>801858703</id>
      <parentid>801858560</parentid>
      <timestamp>2017-09-22T10:26:38Z</timestamp>
      <contributor>
        <username>Tony1</username>
        <id>332841</id>
      </contributor>
      <comment>[[User:Ohconfucius/script|Script]]-assisted fixes: per , , , [[User:Ohconfucius/script|Script]]-assisted fixes: per , , </comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7068">{{Use dmy dates|date=September 2017}}
{{Orphan|date=July 2012}}

&lt;!-- Please leave this line alone! --&gt;

[[Image:Mcar.png|thumb|300px| The mountain car problem]]

'''Mountain Car''', a standard testing domain in [[Reinforcement Learning]], is a problem in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill. The domain has been used as a [[test bed]] in various [[Reinforcement Learning]] papers.

==Introduction==
The mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn on two continuous variables: position and velocity. For any given state (position and velocity) of the car, the agent is given the possibility of driving left, driving right, or not using the engine at all. In the standard version of the problem, the agent receives a negative reward at every time step when the goal is not reached; the agent has no information about the goal until an initial success.

==History==
The mountain car problem appeared first in Andrew Moore's PhD Thesis (1990).&lt;ref&gt;[Moore, 1990] A. Moore, Efficient Memory-Based Learning for Robot Control, PhD thesis, University of Cambridge, November 1990.&lt;/ref&gt; It was later more strictly defined in Singh and Sutton's Reinforcement Leaning paper with [[eligibility traces]].&lt;ref&gt;[Singh and Sutton, 1996] Singh, S.P. and Sutton, R.S. (1996) Reinforcement learning with replacing eligibility traces. Machine Learning 22(1/2/3):123-158.&lt;/ref&gt; The problem became more widely studied when Sutton and Barto added it to their book Reinforcement Learning: An Introduction (1998).&lt;ref&gt;[Sutton and Barto, 1998] Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto. A Bradford Book. The MIT Press Cambridge, Massachusetts London, England, 1998&lt;/ref&gt; Throughout the years many versions of the problem have been used, such as those which modify the [[reward function]], termination condition, and/or the [[start state]].

==Techniques used to solve mountain car==
[[Q-learning]] and similar techniques for mapping discrete states to discrete actions need to be extended to be able to deal with the continuous state space of the problem. Approaches often fall into one of two categories, state space [[discretization]] or [[function approximation]].

===Discretization===

In this approach, two continuous state variables are pushed into discrete states by bucketing each continuous variable into multiple discrete states. This approach works with properly tuned parameters but a disadvantage is information gathered from one state is not used to evaluate another state. [[Tile coding]] can be used to improve discretization and involves continuous variables mapping into sets of buckets offset from one another. Each step of training has a wider impact on the value function approximation because when the offset grids are summed, the information is diffused.&lt;ref&gt;http://webdocs.cs.ualberta.ca/~sutton/book/8/node6.html#SECTION00132000000000000000&lt;/ref&gt;

===Function approximation===

Function approximation is another way to solve the mountain car. By choosing a set of basis functions beforehand, or by generating them as the car drives, the agent can approximate the value function at each state. Unlike the step-wise version of the value function created with discretization, function approximation can more cleanly estimate the true smooth function of the mountain car domain.&lt;ref&gt;http://webdocs.cs.ualberta.ca/~sutton/book/8/node9.html#SECTION00140000000000000000&lt;/ref&gt;

===Traces===

An interesting aspect of the problem involves the delay of actual reward. The agent isn't able to learn about the goal until a successful completion. Given a naive approach without traces, for each trial the car can only backup the reward of the goal slightly. This is a problem for naive discretization because each discrete state will only be backup once, taking a larger number of episodes to learn the problem. To alleviate this problem, traces will automatically backup the reward given to states before dramatically increasing the speed of learning.

==Technical details==
The mountain car problem has undergone many iterations. This section will focus on the standard well defined version from Sutton (2008).&lt;ref&gt;[Sutton, 2008] Mountain Car Software. Richard s. Sutton. http://www.cs.ualberta.ca/~sutton/MountainCar/MountainCar.html&lt;/ref&gt;

===State variables===

Two-dimensional continuous state space.

&lt;math&gt;Velocity = (-0.07,0.07)&lt;/math&gt;

&lt;math&gt;Position = (-1.2,0.6)&lt;/math&gt;

===Actions===

One-dimensional discrete action space.

&lt;math&gt;motor = (left, neutral, right)&lt;/math&gt;

===Reward===

For every time step:

&lt;math&gt;reward = -1&lt;/math&gt;

===Update function===

For every time step:

&lt;math&gt;Action = [-1,0,1]&lt;/math&gt;

&lt;math&gt;Velocity = Velocity + (Action) *0.001+\cos(3*Position)*(-0.0025)&lt;/math&gt;

&lt;math&gt;Position = Position + Velocity&lt;/math&gt;

===Starting condition===

Optionally, many implementations include randomness in both parameters to show better generalized learning.

&lt;math&gt;Position = -0.5&lt;/math&gt;

&lt;math&gt;Velocity = 0.0&lt;/math&gt;

===Termination condition===

End the simulation when:

&lt;math&gt;Position \ge 0.6&lt;/math&gt;

==Variations==
There are many versions of the mountain car which deviate in different ways from the standard model. Variables that vary include but are not limited to changing the constants (gravity and steepness) of the problem so specific tuning for specific policies become irrelevant and altering the reward function to affect the agent's ability to learn in a different manner. An example is changing the reward to be equal to the distance from the goal, or changing the reward to zero everywhere and one at the goal.  Additionally we can use a 3D mountain car with a 4D continuous state space.&lt;ref&gt;http://library.rl-community.org/wiki/Mountain_Car_3D_(CPP)&lt;/ref&gt;

== References==
{{Reflist}}

==Implementations==
* [http://incompleteideas.net/sutton/MountainCar/MountainCar.html C++ Mountain Car Software. Richard s. Sutton.]
* [http://library.rl-community.org/wiki/Mountain_Car_(Java) Java Mountain Car with support for RL Glue]
* [https://en.wikipedia.org/wiki/Inverted_pendulum#External_links Implementation in Python with detailed Blog post]

== Further reading==
* {{cite paper | citeseerx = 10.1.1.51.4764 | title = Mountain Car with Sparse Coarse Coding }}
* [http://www-all.cs.umass.edu/pubs/1995_96/singh_s_ML96.pdf Mountain Car with Replacing Eligibility Traces]
* {{cite paper | citeseerx = 10.1.1.97.9314 | title = More discussion on Continuous State Spaces }}
* [http://www.mendeley.com/research/reinforcement-learning-with-gaussian-processes/ Gaussian Processes with Mountain Car]
&lt;!--- Categories ---&gt;

[[Category:Articles created via the Article Wizard]]
</text>
      <sha1>hbesteenwn0nk0rl6igxx50xfpzl3c4</sha1>
    </revision>
  </page>
  <page>
    <title>Leave-one-out error</title>
    <ns>0</ns>
    <id>33890474</id>
    <revision>
      <id>699552819</id>
      <parentid>672768196</parentid>
      <timestamp>2016-01-13T01:07:31Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor/>
      <comment>Bulleting list, caps</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2668">{{Multiple issues|
{{confusing|date=November 2011}}
{{context|date=November 2011}}
}}
{{broader|Cross-validation (statistics)}}
* '''Leave-one-out cross-validation (CVloo) Stability''' An [[algorithm]] f has CVloo stability β with respect to the [[loss function]] V if the following holds:

&lt;math&gt;\forall i\in\{1,...,m\}, \mathbb{P}_S\{\sup_{z\in Z}|V(f_S,z_i)-V(f_{S^{|i}},z_i)|\leq\beta_{CV}\}\geq1-\delta_{CV}&lt;/math&gt;
* '''Expected-to-leave-one-out error (&lt;math&gt;Eloo_{err}&lt;/math&gt;) Stability''' An algorithm f has &lt;math&gt;Eloo_{err}&lt;/math&gt; stability if for each n there exists a&lt;math&gt;\beta_{EL}^m&lt;/math&gt; and a &lt;math&gt;\delta_{EL}^m&lt;/math&gt; such that:

&lt;math&gt;\forall i\in\{1,...,m\}, \mathbb{P}_S\{|I[f_S]-\frac{1}{m}\sum_{i=1}^m V(f_{S^{|i}},z_i)|\leq\beta_{EL}^m\}\geq1-\delta_{EL}^m&lt;/math&gt;, with &lt;math&gt;\beta_{EL}^m&lt;/math&gt;and &lt;math&gt;\delta_{EL}^m&lt;/math&gt; going to zero for &lt;math&gt;n\rightarrow\inf&lt;/math&gt;

==Preliminary notations==
X and Y ⊂ R being respectively an input and an output space, we consider a [[training set]]

&lt;math&gt;S = \{z_1 = (x_1,\ y_1)\ ,..,\ z_m = (x_m,\ y_m)\}&lt;/math&gt;
of size m in &lt;math&gt;Z = X \times Y&lt;/math&gt; drawn i.i.d. from an unknown distribution D. A [[Learning algorithms|learning algorithm]] is a function &lt;math&gt;f &lt;/math&gt; from &lt;math&gt;Z_m&lt;/math&gt; into &lt;math&gt; F \subset YX &lt;/math&gt;which maps a learning set S onto a function &lt;math&gt;f_S&lt;/math&gt; from X to Y. To avoid complex notation, we consider only [[deterministic algorithm]]s. It is also assumed that the algorithm &lt;math&gt;f&lt;/math&gt; is symmetric with respect to S, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable which does not limit the interest of the results presented here.

The loss of an hypothesis '''&lt;big&gt;f&lt;/big&gt;''' with respect to an example &lt;math&gt;z = (x,y)&lt;/math&gt; is then defined as &lt;math&gt;V(f,z) = V(f(x),y)&lt;/math&gt;.
The empirical error of '''&lt;big&gt;f&lt;/big&gt;''' is &lt;math&gt;I_S[f] = \frac{1}{n}\sum V(f,z_i)&lt;/math&gt;.

The true error of '''&lt;big&gt;f&lt;/big&gt;''' is &lt;math&gt;I[f] = \mathbb{E}_z V(f,z)&lt;/math&gt;

Given a training set S of size m, we will build, for all i = 1....,m, modified training sets as follows:
* By removing the i-th element
&lt;math&gt;S^{|i} = \{z_1 ,...,\ z_{i-1},\ z_{i+1},...,\ z_m\}&lt;/math&gt;
* By replacing the i-th element
&lt;math&gt;S^i = \{z_1 ,...,\ z_{i-1},\ z_i',\ z_{i+1},...,\ z_m\}&lt;/math&gt;

==References==
*S. Mukherjee, P. Niyogi, T. Poggio, and R. M. Rifkin. Learning theory: stability is sufficient for generaliza- tion and necessary and sufficient for consistency of empirical risk minimization. Adv. Comput. Math., 25(1-3):161–193, 2006

</text>
      <sha1>igg5hag960mjexluy3c79uxqw8cw5fd</sha1>
    </revision>
  </page>
  <page>
    <title>Representer theorem</title>
    <ns>0</ns>
    <id>35887507</id>
    <revision>
      <id>805528066</id>
      <parentid>802403763</parentid>
      <timestamp>2017-10-16T00:03:53Z</timestamp>
      <contributor>
        <ip>31.49.61.171</ip>
      </contributor>
      <comment>/* Formal statement */Changed subheading to mixed-case</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10508">{{context|date=June 2012}}

In [[Computational learning theory|statistical learning theory]], a '''representer theorem''' is any of several related results stating that a minimizer &lt;math&gt;f^{*}&lt;/math&gt; of a regularized [[Empirical risk minimization|empirical risk function]] defined over a [[reproducing kernel Hilbert space]] can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.

==Formal statement==
The following Representer Theorem and its proof are due to [[Bernhard Schölkopf|Schölkopf]], Herbrich, and Smola:

'''Theorem:''' Let &lt;math&gt;\mathcal{X}&lt;/math&gt; be a nonempty set and &lt;math&gt;k&lt;/math&gt; a positive-definite real-valued kernel on &lt;math&gt;\mathcal{X} \times \mathcal{X}&lt;/math&gt; with corresponding reproducing kernel Hilbert space &lt;math&gt;H_k&lt;/math&gt;.  Given a training sample &lt;math&gt;(x_1, y_1), \dotsc, (x_n, y_n) \in \mathcal{X} \times \R&lt;/math&gt;, a strictly monotonically increasing real-valued function &lt;math&gt;g \colon [0, \infty) \to \R&lt;/math&gt;, and an arbitrary empirical risk function &lt;math&gt;E \colon (\mathcal{X} \times \R^2)^n \to \R \cup \lbrace \infty \rbrace&lt;/math&gt;, then for any &lt;math&gt;f^{*} \in H_k&lt;/math&gt; satisfying

:&lt;math&gt;
 f^{*} = \operatorname{arg min}_{f \in H_k} \left\lbrace E\left( (x_1, y_1, f(x_1)), ..., (x_n, y_n, f(x_n)) \right) + g\left( \lVert f \rVert \right) \right \rbrace, \quad (*)
&lt;/math&gt;

&lt;math&gt;f^{*}&lt;/math&gt; admits a representation of the form:

:&lt;math&gt;
 f^{*}(\cdot) = \sum_{i = 1}^n \alpha_i k(\cdot, x_i),
&lt;/math&gt;

where &lt;math&gt;\alpha_i \in \R&lt;/math&gt; for all &lt;math&gt;1 \le i \le n&lt;/math&gt;.

'''Proof:'''
Define a mapping

:&lt;math&gt;
\begin{align}
 \varphi \colon \mathcal{X} &amp;\to \R^{\mathcal{X} }\\
\varphi(x) &amp;= k(\cdot, x)
\end{align}
&lt;/math&gt;

(so that &lt;math&gt;\varphi(x) = k(\cdot, x)&lt;/math&gt; is itself a map &lt;math&gt;\mathcal{X} \to \R&lt;/math&gt;).  Since &lt;math&gt;k&lt;/math&gt; is a reproducing kernel, then

:&lt;math&gt;
 \varphi(x)(x') = k(x', x) = \langle \varphi(x'), \varphi(x) \rangle,
&lt;/math&gt;
where &lt;math&gt;\langle \cdot, \cdot \rangle&lt;/math&gt; is the inner product on &lt;math&gt;H_k&lt;/math&gt;.

Given any &lt;math&gt;x_1, ..., x_n&lt;/math&gt;, one can use orthogonal projection to decompose any &lt;math&gt;f \in H_k&lt;/math&gt; into a sum of two functions, one lying in &lt;math&gt;\operatorname{span} \left \lbrace \varphi(x_1), ..., \varphi(x_n) \right \rbrace&lt;/math&gt;, and the other lying in the orthogonal complement:

:&lt;math&gt;
 f = \sum_{i = 1}^n \alpha_i \varphi(x_i) + v,
&lt;/math&gt;
where &lt;math&gt;\langle v, \varphi(x_i) \rangle = 0&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;.

The above orthogonal decomposition and the [[Reproducing kernel Hilbert space#The Reproducing Property|reproducing property]] together show that applying &lt;math&gt;f&lt;/math&gt; to any training point &lt;math&gt;x_j&lt;/math&gt; produces

:&lt;math&gt;
 f(x_j) = \left \langle \sum_{i = 1}^n \alpha_i \varphi(x_i) + v, \varphi(x_j) \right \rangle = \sum_{i = 1}^n \alpha_i \langle \varphi(x_i), \varphi(x_j) \rangle,
&lt;/math&gt;

which we observe is independent of &lt;math&gt;v&lt;/math&gt;.  Consequently, the value of the empirical risk &lt;math&gt;E&lt;/math&gt; in (*) is likewise independent of &lt;math&gt;v&lt;/math&gt;.  For the second term (the regularization term), since &lt;math&gt;v&lt;/math&gt; is orthogonal to &lt;math&gt;\sum_{i = 1}^n \alpha_i \varphi(x_i)&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt; is strictly monotonic, we have

:&lt;math&gt;
\begin{align}
 g\left( \lVert f \rVert \right) &amp;= g \left(  \lVert \sum_{i = 1}^n \alpha_i \varphi(x_i) + v \rVert \right) \\
&amp;= g \left( \sqrt{  \lVert \sum_{i = 1}^n \alpha_i \varphi(x_i)  \rVert^2 + \lVert v \rVert^2} \right) \\
&amp;\ge g \left(  \lVert \sum_{i = 1}^n \alpha_i \varphi(x_i) \rVert \right).
\end{align}
&lt;/math&gt;

Therefore setting &lt;math&gt;v = 0&lt;/math&gt; does not affect the first term of (*), while it strictly decreasing the second term.  Consequently, any minimizer &lt;math&gt;f^{*}&lt;/math&gt; in (*) must have &lt;math&gt;v = 0&lt;/math&gt;, i.e., it must be of the form

:&lt;math&gt;
 f^{*}(\cdot) = \sum_{i = 1}^n \alpha_i \varphi(x_i) = \sum_{i = 1}^n \alpha_i k(\cdot, x_i),
&lt;/math&gt;

which is the desired result.

==Generalizations==
The Theorem stated above is a particular example of a family of results that are collectively referred to as &quot;representer theorems&quot;; here we describe several such.

The first statement of a representer theorem was due to Kimeldorf and Wahba for the special case in which

:&lt;math&gt;
\begin{align}
E\left( (x_1, y_1, f(x_1)), ...,  (x_n, y_n, f(x_n)) \right) &amp;= \frac{1}{n} \sum_{i = 1}^n (f(x_i) - y_i)^2, \\
g(\lVert f \rVert) &amp;= \lambda \lVert f \rVert^2
\end{align}
&lt;/math&gt;

for &lt;math&gt;\lambda &gt; 0&lt;/math&gt;.  Schölkopf, Herbrich, and Smola generalized this result by relaxing the assumption of the squared-loss cost and allowing the regularizer to be any strictly monotonically increasing function &lt;math&gt;g(\cdot)&lt;/math&gt; of the Hilbert space norm.

It is possible to generalize further by augmenting the regularized empirical risk function through the addition of unpenalized offset terms.  For example, Schölkopf, Herbrich, and Smola also consider the minimization

:&lt;math&gt;
 \tilde{f}^{*} = \operatorname{arg min} \left\lbrace E\left( (x_1, y_1, \tilde{f}(x_1)),  ...,  (x_n, y_n, \tilde{f}(x_n)) \right) + g\left( \lVert f \rVert \right) \mid \tilde{f} = f  + h \in H_k \oplus  \operatorname{span} \lbrace \psi_p \mid 1 \le p \le M \rbrace  \right \rbrace, \quad (\dagger)
&lt;/math&gt;

i.e., we consider functions of the form &lt;math&gt;\tilde{f} = f + h&lt;/math&gt;, where &lt;math&gt;f \in H_k&lt;/math&gt; and &lt;math&gt;h&lt;/math&gt; is an unpenalized function lying in the span of a finite set of real-valued functions &lt;math&gt;\lbrace \psi_p \colon \mathcal{X} \to \R \mid 1 \le p \le M \rbrace&lt;/math&gt;.  Under the assumption that the &lt;math&gt;m \times M&lt;/math&gt; matrix &lt;math&gt;\left( \psi_p(x_i) \right)_{ip}&lt;/math&gt; has rank &lt;math&gt;M&lt;/math&gt;, they show that the minimizer &lt;math&gt;\tilde{f}^{*}&lt;/math&gt; in &lt;math&gt;(\dagger)&lt;/math&gt;
admits a representation of the form

:&lt;math&gt;
 \tilde{f}^{*}(\cdot) = \sum_{i = 1}^n \alpha_i k(\cdot, x_i) + \sum_{p = 1}^M \beta_p \psi_p(\cdot)
&lt;/math&gt;

where &lt;math&gt;\alpha_i, \beta_p \in \R&lt;/math&gt; and the &lt;math&gt;\beta_p&lt;/math&gt; are all uniquely determined.

The conditions under which a representer theorem exists were investigated by Argyriou, Miccheli, and Pontil, who proved the following:

'''Theorem:''' Let &lt;math&gt;\mathcal{X}&lt;/math&gt; be a nonempty set, &lt;math&gt;k&lt;/math&gt; a positive-definite real-valued kernel on &lt;math&gt;\mathcal{X} \times \mathcal{X}&lt;/math&gt; with corresponding reproducing kernel Hilbert space &lt;math&gt;H_k&lt;/math&gt;, and let &lt;math&gt;R \colon H_k \to \R&lt;/math&gt; be a differentiable regularization function.  Then given a training sample &lt;math&gt;(x_1, y_1), ..., (x_n, y_n) \in \mathcal{X} \times \R&lt;/math&gt; and an arbitrary empirical risk function &lt;math&gt;E \colon (\mathcal{X} \times \R^2)^m \to \R \cup \lbrace \infty \rbrace&lt;/math&gt;, a minimizer

:&lt;math&gt;
f^{*} =  \operatorname{arg min}_{f \in H_k} \left\lbrace E\left( (x_1, y_1, f(x_1)), ...,  (x_n, y_n, f(x_n)) \right) + R(f) \right \rbrace \quad (\ddagger)
&lt;/math&gt;

of the regularized empirical risk minimization problem admits a representation of the form

:&lt;math&gt;
 f^{*}(\cdot) = \sum_{i = 1}^n \alpha_i k(\cdot, x_i),
&lt;/math&gt;

where &lt;math&gt;\alpha_i \in \R&lt;/math&gt; for all &lt;math&gt;1 \le i \le n&lt;/math&gt;, if and only if there exists a nondecreasing function &lt;math&gt;h \colon [0, \infty) \to \R&lt;/math&gt; for which

:&lt;math&gt;
R(f) = h(\lVert f \rVert).
&lt;/math&gt;

Effectively, this result provides a necessary and sufficient condition on a differentiable regularizer &lt;math&gt;R(\cdot)&lt;/math&gt; under which the corresponding regularized empirical risk minimization &lt;math&gt;(\ddagger)&lt;/math&gt; will have a representer theorem.  In particular, this shows that a broad class of regularized risk minimizations (much broader than those originally considered by Kimeldorf and Wahba) have representer theorems.

==Applications==
Representer theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem &lt;math&gt;(\ddagger)&lt;/math&gt;.  In most interesting applications, the search domain &lt;math&gt;H_k&lt;/math&gt; for the minimization will be an infinite-dimensional subspace of &lt;math&gt;L^2(\mathcal{X})&lt;/math&gt;, and therefore the search (as written) does not admit implementation on finite-memory and finite-precision computers.  In contrast, the representation of &lt;math&gt;f^{*}(\cdot)&lt;/math&gt; afforded by a representer theorem reduces the original (infinite-dimensional) minimization problem to a search for the optimal &lt;math&gt;n&lt;/math&gt;-dimensional vector of coefficients &lt;math&gt;\alpha = (\alpha_1, ..., \alpha_n) \in \R^n&lt;/math&gt;; &lt;math&gt;\alpha&lt;/math&gt; can then be obtained by applying any standard function minimization algorithm.  Consequently, representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice.

{{no footnotes|date=June 2012}}

==See also==
* [[Mercer's theorem]]

==References==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*{{cite journal
 |first1=Andreas |last1=Argyriou
 |first2=Charles A. |last2=Micchelli
 |first3=Massimiliano |last3=Pontil
 |title=When Is There a Representer Theorem? Vector Versus Matrix Regularizers
 |journal=Journal of Machine Learning Research
 |volume=10 |issue=Dec |pages=2507&amp;ndash;2529  |year=2009
}}
*{{cite journal
 |first1=Felipe |last1=Cucker
 |first2=Steve |last2=Smale
 |title=On the Mathematical Foundations of Learning
 |journal=[[Bulletin of the American Mathematical Society]]
 |volume=39 |issue=1 |pages=1&amp;ndash;49 |year=2002
 |doi=10.1090/S0273-0979-01-00923-5
 |mr=1864085
}}
*{{cite journal
 |first1=George S. |last1=Kimeldorf
 |first2=Grace |last2=Wahba
 |title=A correspondence between Bayesian estimation on stochastic processes and smoothing by splines
 |journal=The Annals of Mathematical Statistics
 |volume=41 |issue=2 |pages=495&amp;ndash;502 |year=1970
 |doi=10.1214/aoms/1177697089
}}
*{{cite journal
 |first1=Bernhard |last1=Schölkopf
 |first2=Ralf |last2=Herbrich
 |first3=Alex J. |last3=Smola
 |title=A Generalized Representer Theorem
 |journal=Computational Learning Theory
 |volume=2111 |pages=416&amp;ndash;426 |year=2001
 |doi=10.1007/3-540-44581-1_27
 |series=Lecture Notes in Computer Science
 |isbn=978-3-540-42343-0
}}




</text>
      <sha1>g509igkn53zn2jt5ci4vc8vjt8n89le</sha1>
    </revision>
  </page>
  <page>
    <title>Parity learning</title>
    <ns>0</ns>
    <id>23864280</id>
    <revision>
      <id>766481998</id>
      <parentid>766481536</parentid>
      <timestamp>2017-02-20T12:29:14Z</timestamp>
      <contributor>
        <username>CAPTAIN RAJU</username>
        <id>25523690</id>
      </contributor>
      <minor/>
      <comment>/* Noisy version (&quot;Learning Parity with Noise&quot;) */ Dating maintenance tags: {{citation needed}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1810">'''Parity learning''' is a problem in [[machine learning]]. An algorithm that solves this problem must guess the function ''ƒ'', given some samples (''x'',&amp;nbsp;''ƒ''(''x'')) and the assurance that ''ƒ'' computes the [[Parity function|parity]] of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using [[Gaussian elimination]] provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.

== Noisy version (&quot;Learning Parity with Noise&quot;) ==
In this version, the samples may contain some error. Instead of samples (''x'',&amp;nbsp;''ƒ''(''x'')), the algorithm is provided with (''x'',&amp;nbsp;''y''), where ''y''&amp;nbsp;=&amp;nbsp;1&amp;nbsp;&amp;minus;&amp;nbsp;''ƒ''(''x'') with some small probability. The noisy version of the parity learning problem is conjectured to be hard. {{citation needed|date=February  2017}}

== See also ==
* [[Learning with errors]]

== References ==
* Avrim Blum, Adam Kalai, and Hal Wasserman, “Noise-tolerant learning, the parity problem, and the statistical query model,” J. ACM 50, no. 4 (2003): 506&amp;ndash;519.
* Adam Tauman Kalai, Yishay Mansour, and Elad Verbin, “On agnostic boosting and parity learning,” in Proceedings of the 40th annual ACM symposium on Theory of computing (Victoria, British Columbia, Canada: ACM, 2008), 629&amp;ndash;638, http://portal.acm.org/citation.cfm?id=1374466.
* Oded Regev, “On lattices, learning with errors, random linear codes, and cryptography,” in Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (Baltimore, MD, USA: ACM, 2005), 84&amp;ndash;93, http://portal.acm.org/citation.cfm?id=1060590.1060603.

{{DEFAULTSORT:Parity Learning}}


{{Mathapplied-stub}}</text>
      <sha1>mhigyicqqdllkx8eufxv5khh98dcoqd</sha1>
    </revision>
  </page>
  <page>
    <title>Feature (machine learning)</title>
    <ns>0</ns>
    <id>1299404</id>
    <revision>
      <id>804252725</id>
      <parentid>804238050</parentid>
      <timestamp>2017-10-07T19:36:36Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Citation needed}} {{Clarify}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3486">In [[machine learning]] and [[pattern recognition]], a '''feature''' is an individual measurable property or characteristic of a phenomenon being observed.&lt;ref name=&quot;ml&quot;&gt;{{cite book |author=Bishop, Christopher |title=Pattern recognition and machine learning |publisher=Springer |location=Berlin |year=2006 |pages= |isbn=0-387-31073-8 |oclc= |doi= |accessdate=}}&lt;/ref&gt; Choosing informative, discriminating and independent features is a crucial step for effective algorithms in [[pattern recognition]],  [[classification (machine learning)|classification]] and [[Regression analysis|regression]]. Features are usually numeric, but structural features such as [[string (computer science)|strings]] and [[Graph (discrete mathematics)|graphs]] are used in [[syntactic pattern recognition]].
The concept of &quot;feature&quot; is related to that of [[explanatory variable]] used in [[statistics|statistical]] techniques such as [[linear regression]].

The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of [[machine learning]] and [[pattern recognition]] consists of [[Feature selection|selecting]] a subset of features, or [[Feature extraction|constructing]] a new and reduced set of features to facilitate learning, and to improve generalization and interpretability{{Citation needed|date=October 2017}}.

[[feature extraction|Extracting]] or [[Feature selection|selecting]] features is a combination of art and science; developing systems to do so is known as [[feature engineering]]. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the [[domain expert]]. Automating this process is [[feature learning]], where a machine not only uses features for learning, but learns the features itself.

==Classification==

A set of numeric features can be conveniently described by a [[feature vector]].
An example of reaching a two-way classification{{Clarify|date=October 2017}} from a feature vector (related to the [[perceptron]]) consists of
calculating the [[Dot product|scalar product]] between the feature vector and a vector of weights,
comparing the result with a threshold, and deciding the class based on the comparison.

Algorithms for classification from a feature vector include [[k-nearest neighbor algorithm|nearest neighbor classification]], [[neural networks]], and [[statistical classification|statistical techniques]] such as [[Bayesian inference|Bayesian approaches]].

==Examples==
{{also|Feature (computer vision)}}
In [[character recognition]], features may include [[histogram]]s counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.

In [[speech recognition]], features for recognizing [[phonemes]] can include noise ratios, length of sounds, relative power, filter matches and many others.

In [[spam (electronic)|spam]] detection algorithms, features may include the presence or absence of certain email headers,
the email structure, the language, the frequency of specific terms, the grammatical correctness of the text.

In [[computer vision]], there are a large number of possible [[feature (computer vision)|features]], such as edges and objects.

==References==
{{Reflist}}
{{Refimprove|date=December 2014}}

==See also==
* [[Covariate]]
* [[Hashing trick]]


</text>
      <sha1>5r30d6u03s3e80cqkracy4jowa2q0gx</sha1>
    </revision>
  </page>
  <page>
    <title>Feature hashing</title>
    <ns>0</ns>
    <id>36126852</id>
    <revision>
      <id>802890162</id>
      <parentid>781491482</parentid>
      <timestamp>2017-09-29T02:32:58Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.5.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11362">In [[machine learning]], '''feature hashing''', also known as the '''hashing trick'''&lt;ref name=&quot;Weinberger&quot;/&gt; (by analogy to the [[kernel trick]]), is a fast and space-efficient way of vectorizing [[Feature (machine learning)|features]], i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a [[hash function]] to the features and using their hash values as indices directly, rather than looking the indices up in an [[associative array]].

==Motivating example==
In a typical [[document classification]] task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a [[bag of words]] (BOW) representation is constructed: the individual [[Type–token distinction|tokens]] are extracted and counted, and each distinct token in the training set defines a [[Feature (machine learning)|feature]] (independent variable) of each of the documents in both the training and test sets.

Machine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a [[term-document matrix]] where each row is a single document, and each column is a single feature/word; the entry {{math|''i'', ''j''}} in such a matrix captures the frequency (or weight) of the {{mvar|j}}'th term of the ''vocabulary'' in document {{mvar|i}}. (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.)
Typically, these vectors are extremely [[sparse matrix|sparse]]—according to [[Zipf's law]].

The common approach is to construct, at learning time or prior to that, a ''dictionary'' representation of the vocabulary of the training set, and use that to map words to indices. [[Hash table]]s and [[trie]]s are common candidates for dictionary implementation. E.g., the three documents

* ''John likes to watch movies. ''
* ''Mary likes movies too.''
* ''John also likes football.''

can be converted, using the dictionary

{| class=&quot;wikitable&quot;
|-
! Term !! Index
|-
| John || 1
|-
| likes || 2
|-
| to || 3
|-
| watch || 4
|-
| movies || 5
|-
| Mary || 6
|-
| too || 7
|-
| also || 8
|-
| football || 9
|}

to the term-document matrix

:&lt;math&gt;
\begin{pmatrix}
\textrm{John} &amp; \textrm{likes} &amp; \textrm{to} &amp; \textrm{watch} &amp; \textrm{movies} &amp; \textrm{Mary} &amp; \textrm{too} &amp; \textrm{also} &amp; \textrm{football} \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1
\end{pmatrix}
&lt;/math&gt;

(Punctuation was removed, as is usual in document classification and clustering.)

The problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows.&lt;ref name=&quot;mobilenlp&quot;&gt;{{cite conference |author1=K. Ganchev |author2=M. Dredze |year=2008 |url=http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf |title=Small statistical models by random feature mixing |conference=Proc. ACL08 HLT Workshop on Mobile Language Processing}}&lt;/ref&gt;  On the contrary, if the vocabulary is kept fixed and not increased with a growing training set, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter.  This difficulty is why feature hashing has been tried for [[spam filtering]] at [[Yahoo! Research]].&lt;ref&gt;{{cite journal |author1=Josh Attenberg |author2=Kilian Weinberger |author3=Alex Smola |author4=Anirban Dasgupta |author5=Martin Zinkevich |title=Collaborative spam filtering with the hashing trick |journal=Virus Bulletin |year=2009|url=https://www.virusbulletin.com/virusbulletin/2009/11/collaborative-spam-filtering-hashing-trick}}&lt;/ref&gt;

Note that the hashing trick isn't limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) numbers of features.

==Feature vectorization using hashing trick==
Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function {{mvar|h}} to the features (e.g., words), then using the hash values directly as feature indices and updating the resulting vector at those indices. Here, we assume that feature actually means feature vector.
&lt;source lang=&quot;pascal&quot;&gt;
 function hashing_vectorizer(features : array of string, N : integer):
     x := new vector[N]
     for f in features:
         h := hash(f)
         x[h mod N] += 1
     return x
&lt;/source&gt;
Thus, if our feature vector is [&quot;cat&quot;,&quot;dog&quot;,&quot;cat&quot;] and hash function is &lt;math&gt; hash(x_f)=1&lt;/math&gt; if &lt;math&gt;x_f&lt;/math&gt; is &quot;cat&quot; and &lt;math&gt;2&lt;/math&gt; if &lt;math&gt;x_f&lt;/math&gt; is &quot;dog&quot;. Let us take the output feature vector dimension (&lt;tt&gt;N&lt;/tt&gt;) to be 4. Then output &lt;tt&gt;x&lt;/tt&gt; will be [0,2,1,0].
It has been suggested that a second, single-bit output hash function {{mvar|ξ}} be used to determine the sign of the update value, to counter the effect of [[Hash table#Collision resolution|hash collision]]s.&lt;ref name=&quot;Weinberger&quot;&gt;{{cite conference |author1=Kilian Weinberger |author2=Anirban Dasgupta |author3=John Langford |author4=Alex Smola |author5=Josh Attenberg |year=2009 |url=http://alex.smola.org/papers/2009/Weinbergeretal09.pdf |title=Feature Hashing for Large Scale Multitask Learning |conference=Proc. ICML}}&lt;/ref&gt; If such a hash function is used, the algorithm becomes
&lt;source lang=&quot;pascal&quot;&gt;
 function hashing_vectorizer(features : array of string, N : integer):
     x := new vector[N]
     for f in features:
         h := hash(f)
         idx := h mod N
         if ξ(f) == 1:
             x[idx] += 1
         else:
             x[idx] -= 1
     return x
&lt;/source&gt;

The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of ({{mvar|h}},{{mvar|ξ}}) pairs and let the learning and prediction algorithms consume such streams; a [[linear model]] can then be implemented as a single hash table representing the coefficient vector.

===Properties===
{| class=&quot;wikitable&quot; style=&quot;float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;&quot;
|-
! ''ξ''(''f''₁) !! ''ξ''(''f''₂) !! Final value, ''ξ''(''f''₁) + ''ξ''(''f''₂)
|-
| -1 || -1 || -2
|-
| -1 || 1 || 0
|-
| 1 || -1 || 0
|-
| 1 || 1 || 2
|}

When a second hash function ''ξ'' is used to determine the sign of a feature's value, the [[Expected value|expected]] [[mean]] of each column in the output array becomes zero because ''ξ'' causes some collisions to cancel out.&lt;ref name=&quot;Weinberger&quot;/&gt; E.g., suppose an input contains two symbolic features ''f''₁ and ''f''₂ that collide with each other, but not with any other features in the same input; then there are four possibilities which, if we make no assumptions about ''ξ'', have equal probability, as listed in the table on the right.

In this example, there is a 50% probability that the hash collision cancels out. Multiple hash functions can be used to further reduce the risk of collisions.&lt;ref name=&quot;mahout&quot;&gt;{{cite book
|last1 = Owen
|first1 = Sean
|last2 = Anil
|first2 = Robin
|last3 = Dunning
|first3 = Ted
|last4 = Friedman
|first4 = Ellen
|title=Mahout in Action
|pages=261–265
|publisher = Manning
|year = 2012
}}&lt;/ref&gt;

Furthermore, if ''φ'' is the transformation implemented by a hashing trick with a sign hash ''ξ'' (i.e. ''φ''(''x'') is the feature vector produced for a sample ''x''), then [[inner product]]s in the hashed space are unbiased:

:&lt;math&gt; \mathbb{E}[\langle \varphi(x), \varphi(x') \rangle] = \langle x, x' \rangle&lt;/math&gt;

where the expectation is taken over the hashing function ''φ''.&lt;ref name=&quot;Weinberger&quot;/&gt; It can be verified that&lt;math&gt;\langle \varphi(x), \varphi(x') \rangle&lt;/math&gt; is a [[Positive-definite matrix|positive semi-definite]] [[Kernel trick|kernel]].&lt;ref name=&quot;Weinberger&quot;/&gt;&lt;ref&gt;{{cite conference|last=Shi|first=Q.|author2=Petterson J. |author3=Dror G. |author4=Langford J. |author5=Smola A. |author6=Strehl A. |author7=Vishwanathan V. |title=Hash Kernels|conference=AISTATS|year=2009}}&lt;/ref&gt;

===Extensions and variations===
Recent work extends the hashing trick to supervised mappings from words to indices,&lt;ref&gt;{{cite conference|last=Bai|first=B.|author2=Weston J. |author3=Grangier D. |author4=Collobert R. |author5=Sadamasa K. |author6=Qi Y. |author7=Chapelle O. |author8=Weinberger K. |title=Supervised semantic indexing|conference=CIKM|year=2009|pages=187–196|url=http://www.cse.wustl.edu/~kilian/papers/ssi-cikm.pdf}}&lt;/ref&gt;
which are explicitly learned to avoid collisions of important terms.

===Applications and practical performance===
Ganchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function.&lt;ref name=&quot;mobilenlp&quot;/&gt;
Weinberger et al. applied their variant of hashing to the problem of [[spam filter]]ing, formulating this as a [[multi-task learning]] problem where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up.&lt;ref name=&quot;Weinberger&quot;/&gt;

==Implementations==
Implementations of the hashing trick are present in:

* [[Apache Mahout]]&lt;ref name=&quot;mahout&quot; /&gt;
* [[Gensim]]&lt;ref&gt;{{cite web|url=http://radimrehurek.com/gensim/corpora/hashdictionary.html |title=gensim: corpora.hashdictionary – Construct word&lt;-&gt;id mappings |publisher=Radimrehurek.com |accessdate=2014-02-13}}&lt;/ref&gt;
* [[scikit-learn]]&lt;ref&gt;{{cite web|url=http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing |title=4.1. Feature extraction — scikit-learn 0.14 documentation |publisher=Scikit-learn.org |accessdate=2014-02-13}}&lt;/ref&gt;
* sofia-ml&lt;ref&gt;{{cite web|url=https://code.google.com/p/sofia-ml/ |title=sofia-ml - Suite of Fast Incremental Algorithms for Machine Learning. Includes methods for learning classification and ranking models, using Pegasos SVM, SGD-SVM, ROMMA, Passive-Aggressive Perceptron, Perceptron with Margins, and Logistic Regression |publisher=Code.google.com |accessdate=2014-02-13}}&lt;/ref&gt;
* [[Vowpal Wabbit]]
* [[Apache Spark]]&lt;ref&gt;{{cite web|url=https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF|title=Hashing TF|quote=Maps a sequence of terms to their term frequencies using the hashing trick.|accessdate=4 September 2015}}&lt;/ref&gt;
* [[R (programming language)|R]]&lt;ref&gt;{{cite web|url=https://cran.r-project.org/web/packages/FeatureHashing/index.html |title=FeatureHashing: Creates a Model Matrix via Feature Hashing With a Formula Interface}}&lt;/ref&gt;

==See also==
* [[Bloom filter]]
* [[Count–min sketch]]
* [[Heaps' law]]
* [[Locality-sensitive hashing]]
* [[MinHash]]

==References==
{{Reflist|30em}}

==External links==
* [http://hunch.net/~jl/projects/hash_reps/index.html Hashing Representations for Machine Learning] on John Langford's website
* [https://web.archive.org/web/20120609232923/http://metaoptimize.com/qa/questions/6943/what-is-the-hashing-trick What is the &quot;hashing trick&quot;? - MetaOptimize Q+A]


</text>
      <sha1>nwnvkysa53zrjep27dkjfdy9ds7t1on</sha1>
    </revision>
  </page>
  <page>
    <title>Large margin nearest neighbor</title>
    <ns>0</ns>
    <id>28037054</id>
    <revision>
      <id>751873020</id>
      <parentid>738483157</parentid>
      <timestamp>2016-11-28T07:16:37Z</timestamp>
      <contributor>
        <username>Kercker</username>
        <id>11483221</id>
      </contributor>
      <minor/>
      <comment>/* External links */ change dead link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8941">'''Large margin nearest neighbor''' ('''LMNN''')&lt;ref name=&quot;Weinberger05&quot;&gt;{{cite journal
 | last = Weinberger
 | first = K. Q.
 |author2=Blitzer J. C. |author3=Saul L. K.
 | title = Distance Metric Learning for Large Margin Nearest Neighbor Classification,
 | journal = Advances in Neural Information Processing Systems |volume=18
 | year=2006
 | pages=1473–1480
 | url=http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification
}}&lt;/ref&gt; '''classification''' is a statistical [[machine learning]] [[algorithm]] for [[similarity learning|metric learning]]. It learns a [[Pseudometric space|pseudometric]] designed for [[k-nearest neighbor]] classification. The algorithm is based on [[semidefinite programming]], a sub-class of [[convex optimization]].

The goal of [[supervised learning]] (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The  [[k-nearest neighbor]] rule assumes a ''training'' data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined [[metric (mathematics)|metric]]. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.

==Setup==

The main intuition behind LMNN is to learn a pseudometric under which all data instances in the training set are surrounded by at least k instances that share the same class label. If this is achieved, the [[leave-one-out error]] (a special case of [[Cross-validation (statistics)|cross validation]]) is minimized. Let the training data consist of a data set &lt;math&gt; D=\{(\vec x_1,y_1),\dots,(\vec x_n,y_n)\}\subset R^d\times C&lt;/math&gt;, where the set of possible class categories is &lt;math&gt;C=\{1,\dots,c\}&lt;/math&gt;.

The algorithm learns a pseudometric of the type
:&lt;math&gt;d(\vec x_i,\vec x_j)=(\vec x_i-\vec x_j)^\top\mathbf{M}(\vec x_i-\vec x_j)&lt;/math&gt;.
For &lt;math&gt;d(\cdot,\cdot)&lt;/math&gt; to be well defined, the matrix &lt;math&gt;\mathbf{M}&lt;/math&gt; needs to be [[positive semi-definite]]. The Euclidean metric is a special case, where  &lt;math&gt;\mathbf{M}&lt;/math&gt; is the identity matrix. This generalization is often (falsely) referred  to as [[Mahalanobis metric]].

Figure 1 illustrates the effect of the metric under varying &lt;math&gt;\mathbf{M}&lt;/math&gt;. The two circles show the set of points with equal distance to the center &lt;math&gt;\vec x_i&lt;/math&gt;. In the Euclidean case this set is a circle, whereas under the modified (Mahalanobis) metric it becomes an [[ellipsoid]].

[[File:Lmnn.png|thumb|300px|Figure 1: Schematic illustration of LMNN.]]

The algorithm distinguishes between two types of special data points: ''target neighbors'' and ''impostors''.

===Target neighbors===

Target neighbors are selected before learning. Each instance &lt;math&gt;\vec x_i&lt;/math&gt; has exactly &lt;math&gt;k&lt;/math&gt; different target neighbors within &lt;math&gt;D&lt;/math&gt;, which all share the same class label &lt;math&gt;y_i&lt;/math&gt;. The target neighbors are the data points that ''should become'' nearest neighbors ''under the learned metric''. Let us denote the set of target neighbors for a data point &lt;math&gt;\vec x_i&lt;/math&gt; as &lt;math&gt;N_i&lt;/math&gt;.

===Impostors===

An impostor of a data point &lt;math&gt;\vec x_i&lt;/math&gt; is another data point &lt;math&gt;\vec x_j&lt;/math&gt; with a different class label (i.e. &lt;math&gt;y_i\neq y_j&lt;/math&gt;) which is one of the nearest neighbors of &lt;math&gt;\vec x_i&lt;/math&gt;. During learning the algorithm tries to minimize the number of impostors for all data instances in the training set.

==Algorithm==

Large margin nearest neighbors optimizes the matrix &lt;math&gt;\mathbf{M}&lt;/math&gt; with the help of [[semidefinite programming]]. The objective is twofold: For every data point &lt;math&gt;\vec x_i&lt;/math&gt;, the ''target neighbors'' should be ''close'' and  the ''impostors'' should be ''far away''. Figure 1 shows the effect of such an optimization on an illustrative example. The learned metric causes the input vector &lt;math&gt;\vec x_i&lt;/math&gt; to be surrounded by training instances of the same class.  If it was a test point, it would be classified correctly under the &lt;math&gt;k=3&lt;/math&gt; nearest neighbor rule.

The first optimization goal is achieved by minimizing the average distance between instances and their target neighbors
:&lt;math&gt;\sum_{i,j\in N_i} d(\vec x_i,\vec x_j)&lt;/math&gt;.
The second goal is achieved by constraining impostors &lt;math&gt;\vec x_l&lt;/math&gt; to be one unit further away than target neighbors &lt;math&gt;\vec x_j&lt;/math&gt; (and therefore pushing them out of the local neighborhood of &lt;math&gt;\vec x_i&lt;/math&gt;). The resulting inequality constraint can be stated as:
:&lt;math&gt;\forall_{i,j \in N_i,l, y_l\neq y_i} d(\vec x_i,\vec x_j)+1\leq d(\vec x_i,\vec x_l)&lt;/math&gt;
The margin of exactly one unit fixes the scale of the matrix &lt;math&gt;M&lt;/math&gt;. Any alternative choice &lt;math&gt;c&gt;0&lt;/math&gt; would result in a rescaling of &lt;math&gt;M&lt;/math&gt; by a factor of &lt;math&gt;1/c&lt;/math&gt;.

The final optimization problem becomes:
:&lt;math&gt; \min_{\mathbf{M}} \sum_{i,j\in N_i} d(\vec x_i,\vec x_j) + \sum_{i,j,l} \xi_{ijl}&lt;/math&gt;
:&lt;math&gt;\forall_{i,j \in N_i,l, y_l\neq y_i} &lt;/math&gt;
:&lt;math&gt;   d(\vec x_i,\vec x_j)+1\leq d(\vec x_i,\vec x_l)+\xi_{ijl}&lt;/math&gt;
:&lt;math&gt; \xi_{ijl}\geq 0&lt;/math&gt;
:&lt;math&gt; \mathbf{M}\succeq 0&lt;/math&gt;

Here the [[slack variable]]s &lt;math&gt;\xi_{ijl}&lt;/math&gt; absorb the amount of violations of the impostor constraints. Their overall sum is minimized. The last constraint ensures that &lt;math&gt;\mathbf{M}&lt;/math&gt; is [[positive semi-definite]]. The optimization problem is an instance of [[semidefinite programming]] (SDP). Although SDPs tend to suffer from high computational complexity, this particular SDP instance can be solved very efficiently due to the underlying geometric properties of the problem. In particular, most impostor constraints are naturally satisfied and do not need to be enforced during runtime. A particularly well suited solver technique is the [[working set]] method, which keeps a small set of constraints that are actively enforced and monitors the remaining (likely satisfied) constraints only occasionally to ensure correctness.

==Extensions and efficient solvers==

LMNN was extended to multiple local metrics in the 2008 paper.&lt;ref name=&quot;Weinberger08&quot;&gt;{{cite journal
 | last = Weinberger
 | first = K. Q.
 |author2=Saul L. K.
  | title = Fast solvers and efficient implementations for distance metric learning
 | journal = [[Proceedings of International Conference on Machine Learning]]
 | year=2008
 | pages = 1160–1167
 | url=http://research.yahoo.net/files/icml2008a.pdf
}}&lt;/ref&gt;
This extension significantly improves the classification error, but involves a more expensive optimization problem. In their 2009 publication in the Journal of Machine Learning Research,&lt;ref name=&quot;Weinberger09&quot;&gt;{{cite journal
 | last = Weinberger
 | first = K. Q.
 |author2=Saul L. K.
  | title = Distance Metric Learning for Large Margin Classification
 | journal = [[Journal of Machine Learning Research]]
 | year=2009
 | volume = 10 | pages = 207–244
 | url=http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf
}}&lt;/ref&gt; Weinberger and Saul derive an efficient solver for the semi-definite program. It can learn a metric for the [http://yann.lecun.com/exdb/mnist/ MNIST handwritten digit data set] in several hours, involving billions of pairwise constraints. An [[open source]] [[Matlab]] implementation is freely available at the [http://www.cse.wustl.edu/~kilian/code/code.html authors web page].

Kumal et al.&lt;ref name=&quot;kumar07&quot;&gt;{{cite journal
 | last = Kumar
 | first= M.P.
 |author2=Torr  P.H.S. |author3=Zisserman A.
  | title =An invariant large margin nearest neighbour classifier
 | journal= IEEE 11th International Conference on Computer Vision (ICCV), 2007
 | year=2007
 | pages= 1–8
 | url=http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4409041
}}&lt;/ref&gt; extended the algorithm to incorporate local invariances to multivariate [[polynomial transformations]] and improved regularization.

==See also==
{{div col}}
* [[Similarity learning]]
* [[Linear discriminant analysis]]
* [[Learning vector quantization]]
* [[Pseudometric space]]
* [[Nearest neighbor search]]
* [[Cluster analysis]]
* [[Classification (machine learning)|Data classification]]
* [[Data mining]]
* [[Machine learning]]
* [[Pattern recognition]]
* [[Predictive analytics]]
* [[Dimension reduction]]
* [[Neighbourhood components analysis]]
{{div col end}}

==References==
{{reflist}}

==External links==
* [http://www.cse.wustl.edu/~kilian/code/code.html Matlab Implementation]
* [https://compscicenter.ru/media/slides/machine_learning_1_2012_spring/2012_05_03_machine_learning_1_2012_spring.pdf ICML 2010 Tutorial on Metric Learning]


</text>
      <sha1>epnbnisyssjymzibloftmb71uogo6hf</sha1>
    </revision>
  </page>
  <page>
    <title>Statistical learning theory</title>
    <ns>0</ns>
    <id>1053303</id>
    <revision>
      <id>813156671</id>
      <parentid>786530563</parentid>
      <timestamp>2017-12-02T03:31:07Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>adding a link using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9540">{{see also|Computational learning theory}}
{{about|statistical learning in machine learning|its use in psychology|Statistical learning in language acquisition}}
{{machine learning bar}}
'''Statistical learning theory''' is a framework for [[machine learning]]
drawing from the fields of [[statistics]] and [[functional analysis]].&lt;ref&gt;[[Trevor Hastie]], Robert Tibshirani, Jerome Friedman (2009) ''The Elements of Statistical Learning'', Springer-Verlag {{isbn|978-0-387-84857-0}}.&lt;/ref&gt;&lt;ref&gt;{{Cite Mehryar Afshin Ameet 2012}}&lt;/ref&gt; Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as [[computer vision]], [[speech recognition]], [[bioinformatics]] and [[baseball]].&lt;ref&gt;Gagan Sidhu, Brian Caffo. Exploiting pitcher decision-making using Reinforcement Learning. ''Annals of Applied Statistics''&lt;/ref&gt;

==Introduction==
The goals of learning are understanding and prediction. Learning falls into many categories, including [[supervised learning]], [[unsupervised learning]], [[Online machine learning|online learning]], and [[reinforcement learning]]. From the perspective of statistical learning theory, supervised learning is best understood.&lt;ref&gt;Tomaso Poggio, Lorenzo Rosasco, et al. ''Statistical Learning Theory and Applications'', 2012, [http://www.mit.edu/~9.520/spring12/slides/class01/class01.pdf Class 1]&lt;/ref&gt; Supervised learning involves learning from a [[training set]] of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output, such that the learned function can be used to predict output from future input.

Depending on the type of output, supervised learning problems are either problems of [[regression analysis|regression]] or problems of [[Statistical classification|classification]]. If the output takes a continuous range of values, it is a regression problem. Using [[Ohm's Law]] as an example, a regression could be performed with voltage as input and current as output. The regression would find the functional relationship between voltage and current to be {{nowrap|&lt;math&gt;\frac{1}{R}&lt;/math&gt;}}, such that
:&lt;math&gt;
I = \frac{1}{R} V
&lt;/math&gt;
Classification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In [[facial recognition system|facial recognition]], for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector whose elements represent pixels in the picture.

After learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set.

==Formal description==
Take &lt;math&gt;X&lt;/math&gt; to be the vector space of all possible inputs, and &lt;math&gt;Y&lt;/math&gt; to be
the vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space &lt;math&gt;Z = X \times Y&lt;/math&gt;, i.e. there exists some unknown &lt;math&gt;p(z) = p(\vec{x},y)&lt;/math&gt;. The training set is made up of &lt;math&gt;n&lt;/math&gt; samples from this probability distribution, and is notated
:&lt;math&gt;S = \{(\vec{x}_1,y_1), \dots ,(\vec{x}_n,y_n)\} = \{\vec{z}_1, \dots ,\vec{z}_n\}&lt;/math&gt;
Every &lt;math&gt;\vec{x}_i&lt;/math&gt; is an input vector from the training data, and &lt;math&gt;y_i&lt;/math&gt;
is the output that corresponds to it.

In this formalism, the inference problem consists of finding a function &lt;math&gt;f: X \mapsto Y&lt;/math&gt; such that &lt;math&gt;f(\vec{x}) \sim y&lt;/math&gt;. Let &lt;math&gt;\mathcal{H}&lt;/math&gt; be a space of functions &lt;math&gt;f: X \to Y&lt;/math&gt; called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let &lt;math&gt;V(f(\vec{x}),y)&lt;/math&gt; be the [[Loss function|loss function]], a metric for the difference between the predicted value &lt;math&gt;f(\vec{x})&lt;/math&gt; and the actual value &lt;math&gt;y&lt;/math&gt;. The [[expected risk]] is defined to be
:&lt;math&gt;I[f] = \displaystyle \int_{X \times Y} V(f(\vec{x}),y)\, p(\vec{x},y) \,d\vec{x} \,dy&lt;/math&gt;
The target function, the best possible function &lt;math&gt;f&lt;/math&gt; that can be
chosen, is given by the &lt;math&gt;f&lt;/math&gt; that satisfies
:&lt;math&gt;I[f] = \inf_{h \in \mathcal{H}} I[h]&lt;/math&gt;

Because the probability distribution &lt;math&gt;p(\vec{x},y)&lt;/math&gt; is unknown, a
proxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the [[empirical risk]]
:&lt;math&gt;I_S[f] = \frac{1}{n} \displaystyle \sum_{i=1}^n V( f(\vec{x}_i),y_i)&lt;/math&gt;
A learning algorithm that chooses the function &lt;math&gt;f_S&lt;/math&gt; that minimizes
the empirical risk is called [[empirical risk minimization]].

==Loss functions==
The choice of loss function is a determining factor on the function &lt;math&gt;f_S&lt;/math&gt; that will be chosen by the learning algorithm. The loss function
also affects the convergence rate for an algorithm. It is important for the loss function to be convex.&lt;ref&gt;Rosasco, L., Vito, E.D., Caponnetto, A., Fiana, M., and Verri A. 2004. ''Neural computation'' Vol 16, pp 1063-1076&lt;/ref&gt;

Different loss functions are used depending on whether the problem is
one of regression or one of classification.

===Regression===
The most common loss function for regression is the square loss function (also known as the [[L2-norm]]). This familiar loss function is used in [[ordinary least squares regression]]. The form is:
:&lt;math&gt;V(f(\vec{x}),y) = (y - f(\vec{x}))^2&lt;/math&gt;

The absolute value loss (also known as the [[L1-norm]]) is also sometimes used:
:&lt;math&gt;V(f(\vec{x}),y) = |y - f(\vec{x})|&lt;/math&gt;

===Classification===
{{main|Statistical classification}}
In some sense the 0-1 [[indicator function]] is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification with &lt;math&gt;Y = \{-1, 1\}&lt;/math&gt;, this is:
:&lt;math&gt;V(f(\vec{x}),y) = \theta(- y f(\vec{x}))&lt;/math&gt;
where &lt;math&gt;\theta&lt;/math&gt; is the [[Heaviside step function]].

==Regularization==
[[File:Overfitting on Training Set Data.pdf|thumb|This image represents an example of overfitting in machine learning. The red dots represent training set data. The green line represents the true functional relationship, while the blue line shows the learned function, which has fallen victim to overfitting.]]

In machine learning problems, a major problem that arises is that of [[overfitting]]. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. [[Empirical risk minimization]] runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well.

Overfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well.&lt;ref&gt;Vapnik, V.N. and Chervonenkis, A.Y. 1971. [http://ai2-s2-pdfs.s3.amazonaws.com/a36b/028d024bf358c4af1a5e1dc3ca0aed23b553.pdf On the uniform convergence of relative frequencies of events to their probabilities]. ''Theory of Probability and its Applications'' Vol 16, pp 264-280.&lt;/ref&gt;&lt;ref&gt;Mukherjee, S., Niyogi, P. Poggio, T., and Rifkin, R. 2006. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. ''Advances in Computational Mathematics''. Vol 25, pp 161-193.&lt;/ref&gt; [[Regularization (mathematics)|Regularization]] can solve the overfitting problem and give
the problem stability.

Regularization can be accomplished by restricting the hypothesis space &lt;math&gt;\mathcal{H}&lt;/math&gt;. A common example would be restricting &lt;math&gt;\mathcal{H}&lt;/math&gt; to linear functions: this can be seen as a reduction to the standard problem of [[linear regression]]. &lt;math&gt;\mathcal{H}&lt;/math&gt; could also be restricted to polynomial of degree &lt;math&gt;p&lt;/math&gt;, exponentials, or bounded functions on [[Lp space|L1]]. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero.

One example of regularization is [[Tikhonov regularization]]. This consists of minimizing
:&lt;math&gt;\frac{1}{n} \displaystyle \sum_{i=1}^n V(f(\vec{x}_i,y_i)) + \gamma
\|f\|_{\mathcal{H}}^2&lt;/math&gt;
where &lt;math&gt;\gamma&lt;/math&gt; is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution.&lt;ref&gt;Tomaso Poggio, Lorenzo Rosasco, et al. ''Statistical Learning Theory and Applications'', 2012, [http://www.mit.edu/~9.520/spring12/slides/class02/class02.pdf Class 2]&lt;/ref&gt;

{{clear}}

==See also==
* [[Reproducing kernel Hilbert spaces]] are a useful choice for &lt;math&gt;\mathcal{H}&lt;/math&gt;.
* [[Proximal gradient methods for learning]]

==References==
{{reflist}}


</text>
      <sha1>nlpfcfbp8czr8bgqrkdhi6hzub7s3bi</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Log-linear models</title>
    <ns>14</ns>
    <id>36407925</id>
    <revision>
      <id>750277203</id>
      <parentid>750276518</parentid>
      <timestamp>2016-11-18T20:03:15Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>header</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="149">Not to be confused with [[Logistic regression]] models.



</text>
      <sha1>6gkzzu0ml6s1151yek6e5paz9ledyne</sha1>
    </revision>
  </page>
  <page>
    <title>Pattern recognition</title>
    <ns>0</ns>
    <id>126706</id>
    <revision>
      <id>814338140</id>
      <parentid>813806482</parentid>
      <timestamp>2017-12-08T05:51:41Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>linking</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="31746">{{about|pattern recognition as a branch of machine learning|the cognitive process|Pattern recognition (psychology)|other uses}}
{{Multiple issues|
{{refimprove|date=November 2014}}
{{disputed|date=May 2014}}
}}

{{machine learning bar}}
'''Pattern recognition''' is a branch of [[machine learning]] that focuses on the recognition of patterns and regularities in [[data]], although it is in some cases considered to be nearly synonymous with machine learning.&lt;ref&gt;
{{cite book |first=Christopher M. |last=Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |quote=Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same field, and together they have undergone substantial development over the past ten years. |page=vii|url=http://cds.cern.ch/record/998831/files/9780387310732_TOC.pdf}}&lt;/ref&gt; Pattern recognition systems are in many cases trained from labeled &quot;training&quot; data ([[supervised learning]]), but when no labeled data are available other algorithms can be used to discover previously unknown patterns ([[unsupervised learning]]).

The terms pattern recognition, machine learning, [[data mining]] and [[knowledge discovery in databases]] (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods{{dubious|date=November 2014}} and originates from [[artificial intelligence]], whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in [[engineering]], and the term is popular in the context of [[computer vision]]: a leading computer vision conference is named [[Conference on Computer Vision and Pattern Recognition]]. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.

In [[machine learning]], pattern recognition is the assignment of a label to a given input value. In statistics, [[Linear discriminant analysis|discriminant analysis]] was introduced for this same purpose in 1936. An example of pattern recognition is [[classification (machine learning)|classification]], which attempts to assign each input value to one of a given set of ''classes'' (for example, determine whether a given email is &quot;spam&quot; or &quot;non-spam&quot;). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are [[regression analysis|regression]], which assigns a real-valued output to each input; [[sequence labeling]], which assigns a class to each member of a sequence of values (for example, [[part of speech tagging]], which assigns a [[part of speech]] to each word in an input sentence); and [[parsing]], which assigns a [[parse tree]] to an input sentence, describing the [[syntactic structure]] of the sentence.{{Citation needed|date=November 2014}}

Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform &quot;most likely&quot; matching of the inputs, taking into account their statistical variation. This is opposed to ''[[pattern matching]]'' algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is [[regular expression]] matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many [[text editor]]s and [[word processor]]s. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.

==Overview==
Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. ''[[Supervised learning]]'' assumes that a set of ''training data'' (the ''[[training set]]'') has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a ''model'' that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of &quot;simple&quot;, in accordance with [[Occam's Razor]], discussed below). [[Unsupervised learning]], on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.&lt;ref&gt;{{Cite journal| author= Carvalko, J.R., Preston K. | year=1972 |title= On Determining Optimum Simple Golay Marking Transforms for Binary Image Processing | journal= IEEE Transactions on Computers  | volume=21 | pages=1430–33  | doi = 10.1109/T-C.1972.223519}}.&lt;/ref&gt; A combination of the two that has recently been explored is [[semi-supervised learning]], which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled ''is'' the training data.

Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as ''[[data clustering|clustering]]'', based on the common perception of the task as involving no training data to speak of, and of grouping the input data into ''clusters'' based on some inherent [[similarity measure]] (e.g. the [[distance]] between instances, considered as vectors in a multi-dimensional [[vector space]]), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in [[community ecology]], the term &quot;classification&quot; is used to refer to what is commonly known as &quot;clustering&quot;.

The piece of input data for which an output value is generated is formally termed an ''instance''. The instance is formally described by a [[feature vector|vector]] of ''features'', which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate [[space (mathematics)|multidimensional space]], and methods for manipulating vectors in [[vector space]]s can be correspondingly applied to them, such as computing the [[dot product]] or the angle between two vectors.) Typically, features are either [[categorical data|categorical]] (also known as [[nominal data|nominal]], i.e., consisting of one of a set of unordered items, such as a gender of &quot;male&quot; or &quot;female&quot;, or a blood type of &quot;A&quot;, &quot;B&quot;, &quot;AB&quot; or &quot;O&quot;), [[ordinal data|ordinal]] (consisting of one of a set of ordered items, e.g., &quot;large&quot;, &quot;medium&quot; or &quot;small&quot;), [[integer|integer-valued]] (e.g., a count of the number of occurrences of a particular word in an email) or [[real number|real-valued]] (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be ''discretized'' into groups (e.g., less than 5, between 5 and 10, or greater than 10).

===Probabilistic classifiers===
{{main article|Probabilistic classifier}}
{{split section portions|Probabilistic classifier|date=May 2014}}
Many common pattern recognition algorithms are ''probabilistic'' in nature, in that they use [[statistical inference]] to find the best label for a given instance. Unlike other algorithms, which simply output a &quot;best&quot; label, often probabilistic algorithms also output a [[probability]] of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the ''N''-best labels with associated probabilities, for some value of ''N'', instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of [[classification (machine learning)|classification]]), ''N'' may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:
*They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in [[probability theory]]. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)
*Correspondingly, they can ''abstain'' when the confidence of choosing any particular output is too low.
*Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of ''error propagation''.

===Number of important feature variables===
[[Feature selection]] algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to [[feature selection]] which summarizes approaches and challenges, has been given.&lt;ref&gt;Isabelle Guyon Clopinet, André Elisseeff (2003). ''An Introduction to Variable and Feature Selection''. The Journal of Machine Learning Research, Vol. 3, 1157-1182. [http://www-vis.lbl.gov/~romano/mlgroup/papers/guyon03a.pdf Link]&lt;/ref&gt; The complexity of feature-selection is, because of its non-monotonous character, an [[optimization problem]] where given a total of &lt;math&gt;n&lt;/math&gt; features the [[powerset]] consisting of all &lt;math&gt;2^n-1&lt;/math&gt; subsets of features need to be explored. The [[Branch and bound|Branch-and-Bound algorithm]]&lt;ref&gt;
{{Cite journal|author1=Iman Foroutan |author2=Jack Sklansky | year=1987 |
title=Feature Selection for Automatic Classification of Non-Gaussian Data | journal=IEEE Transactions on Systems, Man and Cybernetics | volume=17 | pages=187&amp;ndash;198 | doi = 10.1109/TSMC.1987.4309029 | issue=2
}}.&lt;/ref&gt; does reduce this complexity but is intractable for medium to large values of the number of available features &lt;math&gt;n&lt;/math&gt;. For a large-scale comparison of feature-selection algorithms see
.&lt;ref&gt;
{{Cite journal|author1=Mineichi Kudo |author2=Jack Sklansky | year=2000 |
title=Comparison of algorithms that select features for pattern classifiers | journal=[[Pattern Recognition (journal)|Pattern Recognition]] | volume=33 | pages=25&amp;ndash;41 | doi = 10.1016/S0031-3203(99)00041-2 | issue=1}}.&lt;/ref&gt;

Techniques to transform the raw feature vectors ('''feature extraction''') are sometimes used prior to application of the pattern-matching algorithm. For example, [[feature extraction]] algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as [[principal components analysis]] (PCA). The distinction between '''feature selection''' and '''feature extraction''' is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.

== Problem statement (supervised version)==
Formally, the problem of [[supervised learning|supervised]] pattern recognition can be stated as follows: Given an unknown function &lt;math&gt;g:\mathcal{X}\rightarrow\mathcal{Y}&lt;/math&gt; (the ''ground truth'') that maps input instances &lt;math&gt;\boldsymbol{x} \in \mathcal{X}&lt;/math&gt; to output labels &lt;math&gt;y \in \mathcal{Y}&lt;/math&gt;, along with training data &lt;math&gt;\mathbf{D} = \{(\boldsymbol{x}_1,y_1),\dots,(\boldsymbol{x}_n, y_n)\}&lt;/math&gt; assumed to represent accurate examples of the mapping, produce a function &lt;math&gt;h:\mathcal{X}\rightarrow\mathcal{Y}&lt;/math&gt; that approximates as closely as possible the correct mapping &lt;math&gt;g&lt;/math&gt;. (For example, if the problem is filtering spam, then &lt;math&gt;\boldsymbol{x}_i&lt;/math&gt; is some representation of an email and &lt;math&gt;y&lt;/math&gt; is either &quot;spam&quot; or &quot;non-spam&quot;). In order for this to be a well-defined problem, &quot;approximates as closely as possible&quot; needs to be defined rigorously. In [[decision theory]], this is defined by specifying a [[loss function]] or cost function that assigns a specific value to &quot;loss&quot; resulting from producing an incorrect label. The goal then is to minimize the [[expected value|expected]] loss, with the expectation taken over the [[probability distribution]] of &lt;math&gt;\mathcal{X}&lt;/math&gt;. In practice, neither the distribution of &lt;math&gt;\mathcal{X}&lt;/math&gt; nor the ground truth function &lt;math&gt;g:\mathcal{X}\rightarrow\mathcal{Y}&lt;/math&gt; are known exactly, but can be computed only empirically by collecting a large number of samples of &lt;math&gt;\mathcal{X}&lt;/math&gt; and hand-labeling them using the correct value of &lt;math&gt;\mathcal{Y}&lt;/math&gt; (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of [[classification (machine learning)|classification]], the simple [[zero-one loss function]] is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the [[Bayes error rate|error rate]] on independent test data (i.e. counting up the fraction of instances that the learned function &lt;math&gt;h:\mathcal{X}\rightarrow\mathcal{Y}&lt;/math&gt; labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the [[correctness (computer science)|correctness]]) on a &quot;typical&quot; test set.

For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form
:&lt;math&gt;p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = f\left(\boldsymbol{x};\boldsymbol{\theta}\right)&lt;/math&gt;
where the [[feature vector]] input is &lt;math&gt;\boldsymbol{x}&lt;/math&gt;, and the function ''f'' is typically parameterized by some parameters &lt;math&gt;\boldsymbol{\theta}&lt;/math&gt;.&lt;ref&gt;For [[linear discriminant analysis]] the parameter vector &lt;math&gt;\boldsymbol\theta&lt;/math&gt; consists of the two mean vectors &lt;math&gt;\boldsymbol\mu_1&lt;/math&gt; and &lt;math&gt;\boldsymbol\mu_2&lt;/math&gt; and the common [[covariance matrix]] &lt;math&gt;\boldsymbol\Sigma&lt;/math&gt;.&lt;/ref&gt; In a [[discriminative model|discriminative]] approach to the problem, ''f'' is estimated directly. In a [[generative model|generative]] approach, however, the inverse probability &lt;math&gt;p({\boldsymbol{x}|\rm label})&lt;/math&gt; is instead estimated and combined with the [[prior probability]] &lt;math&gt;p({\rm label}|\boldsymbol\theta)&lt;/math&gt; using [[Bayes' rule]], as follows:
:&lt;math&gt;p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = \frac{p({\boldsymbol{x}|\rm label,\boldsymbol\theta}) p({\rm label|\boldsymbol\theta})}{\sum_{L \in \text{all labels}} p(\boldsymbol{x}|L) p(L|\boldsymbol\theta)}.&lt;/math&gt;

When the labels are [[continuous distribution|continuously distributed]] (e.g., in [[regression analysis]]), the denominator involves [[integral|integration]] rather than summation:

:&lt;math&gt;p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = \frac{p({\boldsymbol{x}|\rm label,\boldsymbol\theta}) p({\rm label|\boldsymbol\theta})}{\int_{L \in \text{all labels}} p(\boldsymbol{x}|L) p(L|\boldsymbol\theta) \operatorname{d}L}.&lt;/math&gt;

The value of &lt;math&gt;\boldsymbol\theta&lt;/math&gt; is typically learned using [[maximum a posteriori]] (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest [[Bayes error rate|error-rate]]) and to find the simplest possible model. Essentially, this combines [[maximum likelihood]] estimation with a [[regularization (mathematics)|regularization]] procedure that favors simpler models over more complex models. In a [[Bayesian inference|Bayesian]] context, the regularization procedure can be viewed as placing a [[prior probability]] &lt;math&gt;p(\boldsymbol\theta)&lt;/math&gt; on different values of &lt;math&gt;\boldsymbol\theta&lt;/math&gt;. Mathematically:

:&lt;math&gt;\boldsymbol\theta^* = \arg \max_{\boldsymbol\theta} p(\boldsymbol\theta|\mathbf{D})&lt;/math&gt;

where &lt;math&gt;\boldsymbol\theta^*&lt;/math&gt; is the value used for &lt;math&gt;\boldsymbol\theta&lt;/math&gt; in the subsequent evaluation procedure, and &lt;math&gt;p(\boldsymbol\theta|\mathbf{D})&lt;/math&gt;, the [[posterior probability]] of &lt;math&gt;\boldsymbol\theta&lt;/math&gt;, is given by

:&lt;math&gt;p(\boldsymbol\theta|\mathbf{D}) = \left[\prod_{i=1}^n p(y_i|\boldsymbol{x}_i,\boldsymbol\theta) \right] p(\boldsymbol\theta).&lt;/math&gt;

In the [[Bayesian statistics|Bayesian]] approach to this problem, instead of choosing a single parameter vector &lt;math&gt;\boldsymbol{\theta}^*&lt;/math&gt;, the probability of a given label for a new instance &lt;math&gt;\boldsymbol{x}&lt;/math&gt; is computed by integrating over all possible values of &lt;math&gt;\boldsymbol\theta&lt;/math&gt;, weighted according to the posterior probability:

:&lt;math&gt;p({\rm label}|\boldsymbol{x}) = \int p({\rm label}|\boldsymbol{x},\boldsymbol\theta)p(\boldsymbol{\theta}|\mathbf{D}) \operatorname{d}\boldsymbol{\theta}.&lt;/math&gt;

=== Frequentist or Bayesian approach to pattern recognition ===
The first pattern classifier – the linear discriminant presented by [[Fisher discriminant analysis|Fisher]] – was developed in the [[Frequentist inference|frequentist]] tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the [[covariance matrix]]. Also the probability of each class &lt;math&gt;p({\rm label}|\boldsymbol\theta)&lt;/math&gt; is estimated from the collected dataset. Note that the usage of '[[Bayes rule]]' in a pattern classifier does not make the classification approach Bayesian.

[[Bayesian inference|Bayesian statistics]] has its origin in Greek philosophy where a distinction was already made between the '[[A priori and a posteriori|a priori]]' and the '[[A priori and a posteriori|a posteriori]]' knowledge. Later [[A priori and a posteriori#Immanuel Kant|Kant]] defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities &lt;math&gt;p({\rm label}|\boldsymbol\theta)&lt;/math&gt; can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the [[Beta distribution|Beta-]] ([[Conjugate prior distribution|conjugate prior]]) and [[Dirichlet distribution|Dirichlet-distributions]]. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.

Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.

==Uses==
[[File:800px-Cool Kids of Death Off Festival p 146-face selected.jpg|thumb|200px|[[Face recognition|The face was automatically detected]] by special software.]]
Within medical science, pattern recognition is the basis for [[computer-aided diagnosis]] (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.
Other typical applications of pattern recognition techniques are automatic [[speech recognition]], [[document classification|classification of text into several categories]] (e.g., spam/non-spam email messages), the [[handwriting recognition|automatic recognition of handwritten postal codes]] on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.&lt;ref&gt;{{cite journal|last=Milewski|first=Robert|author2=Govindaraju, Venu|title=Binarization and cleanup of handwritten text from carbon copy medical form images|journal=Pattern Recognition|date=31 March 2008|volume=41|issue=4|pages=1308–1315|doi=10.1016/j.patcog.2007.08.018|url=http://dl.acm.org/citation.cfm?id=1324656}}&lt;/ref&gt; The last two examples form the subtopic [[image analysis]] of pattern recognition that deals with digital images as input to pattern recognition systems.&lt;ref name=duda2001&gt;{{cite book|author=Richard O. Duda, [[Peter E. Hart]], David G. Stork|year=2001|title=Pattern classification|edition=2nd|publisher=Wiley, New York|isbn=0-471-05669-3}}&lt;/ref&gt;&lt;ref&gt;R. Brunelli, ''Template Matching Techniques in Computer Vision: Theory and Practice'', Wiley, {{ISBN|978-0-470-51706-2}}, 2009&lt;/ref&gt;

Optical character recognition is a classic example of the application of a pattern classifier, see
[http://cmp.felk.cvut.cz/cmp/software/stprtool/examples/ocr_system.gif OCR-example].
The method of signing one's name was captured with stylus and overlay starting in 1990.{{citation needed|date=January 2011}} The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..{{citation needed|date=January 2011}}

[[Artificial neural networks]] (neural net classifiers) and [[deep learning]] have many real-world applications in image processing, a few examples:
* identification and authentication: e.g., [[license plate recognition]],&lt;ref&gt;[http://anpr-tutorial.com/ THE AUTOMATIC NUMBER PLATE RECOGNITION TUTORIAL] http://anpr-tutorial.com/&lt;/ref&gt; fingerprint analysis and [[face detection]]/verification;&lt;ref&gt;[http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/mitchell/ftp/faces.html Neural Networks for Face Recognition] Companion to Chapter 4 of the textbook Machine Learning.&lt;/ref&gt;
* medical diagnosis: e.g., screening for cervical cancer (Papnet)&lt;ref&gt;[http://health-asia.org/papnet-for-cervical-screening/ PAPNET For Cervical Screening] http://health-asia.org/papnet-for-cervical-screening/&lt;/ref&gt; or breast tumors;
* defence: various navigation and guidance systems, target recognition systems, shape recognition technology etc.

For a discussion of the aforementioned applications of neural networks in image processing, see e.g.&lt;ref&gt;{{Cite journal| author=Egmont-Petersen, M., de Ridder, D., Handels, H. | year=2002 |
title=Image processing with neural networks - a review | journal=Pattern Recognition | volume=35 | pages=2279&amp;ndash;2301 | doi = 10.1016/S0031-3203(01)00178-9 | issue=10
}}&lt;/ref&gt;

In psychology, [[pattern recognition (psychology)|pattern recognition]] (making sense of and identifying objects) is closely related to perception, which explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection.
A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. If there is a match, the stimulus is identified.
Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. For example, a capital E has three horizontal lines and one vertical line.&lt;ref&gt;{{cite web|url=http://www.s-cool.co.uk/a-level/psychology/attention/revise-it/pattern-recognition |title=A-level Psychology Attention Revision - Pattern recognition &amp;#124; S-cool, the revision website |publisher=S-cool.co.uk |accessdate=2012-09-17}}&lt;/ref&gt;

==Algorithms==
Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as [[generative model|generative]] or [[discriminative model|discriminative]].

{{cleanup list|date=May 2014}}

===[[Classification (machine learning)|Classification]] algorithms ([[supervised learning|supervised]] algorithms predicting [[categorical data|categorical]] labels)===
{{Main article|Statistical classification}}
Parametric:&lt;ref&gt;Assuming known distributional shape of feature distributions per class, such as the [[Gaussian distribution|Gaussian]] shape.&lt;/ref&gt;
*[[Linear discriminant analysis]]
*[[Quadratic classifier|Quadratic discriminant analysis]]
*[[Maximum entropy classifier]] (aka [[logistic regression]], [[multinomial logistic regression]]): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.)
Nonparametric:&lt;ref&gt;No distributional assumption regarding shape of feature distributions per class.&lt;/ref&gt;
*[[Decision tree]]s, [[decision list]]s
*[[Variable kernel density estimation#Use for statistical classification|Kernel estimation]] and [[K-nearest-neighbor]] algorithms
*[[Naive Bayes classifier]]
*[[Neural network]]s (multi-layer perceptrons)
*[[Perceptron]]s
*[[Support vector machine]]s
*[[Gene expression programming]]

===[[Cluster analysis|Clustering]] algorithms ([[unsupervised learning|unsupervised]] algorithms predicting [[categorical data|categorical]] labels)===
{{Main article|Cluster analysis}}
*Categorical [[mixture model]]s
*[[Deep learning|Deep learning methods]]{{citation needed|date=May 2014}}
*[[Hierarchical clustering]] (agglomerative or divisive)
*[[K-means clustering]]
*[[Correlation clustering]]&lt;!-- not an algorithm --&gt;
*[[Kernel principal component analysis]] (Kernel PCA)&lt;!-- but not PCA? --&gt;

===[[Ensemble learning]] algorithms (supervised [[meta-algorithm]]s for combining multiple learning algorithms together)===
{{Main article|Ensemble learning}}
*[[Boosting (meta-algorithm)]]
*[[Bootstrap aggregating]] (&quot;bagging&quot;)
*[[Ensemble averaging]]
*[[Mixture of experts]], [[hierarchical mixture of experts]]

===General algorithms for predicting arbitrarily-structured (sets of) labels===
*[[Bayesian network]]s
*[[Markov random field]]s

===[[Multilinear subspace learning]] algorithms (predicting labels of multidimensional data using [[tensor]] representations)===
Unsupervised:
*[[Multilinear principal component analysis]] (MPCA)

===Real-valued [[sequence labeling]] algorithms (predicting sequences of [[real number|real-valued]] labels)===
{{Main article|sequence labeling}}
Supervised (?):
*[[Kalman filter]]s
*[[Particle filter]]s

===[[Regression analysis|Regression]] algorithms (predicting [[real number|real-valued]] labels)===
{{Main article|Regression analysis}}
Supervised:
*[[Gaussian process regression]] (kriging)
*[[Linear regression]] and extensions
*[[Neural network]]s  and [[Deep learning|Deep learning methods]]

Unsupervised:
*[[Independent component analysis]] (ICA)
*[[Principal components analysis]] (PCA)

===[[Sequence labeling]] algorithms (predicting sequences of [[categorical data|categorical]] labels)===
Supervised:
*[[Conditional random field]]s (CRFs)
*[[Hidden Markov model]]s (HMMs)
*[[Maximum entropy Markov model]]s (MEMMs)
*[[Recurrent neural networks]]

Unsupervised:
*[[Hidden Markov model]]s (HMMs)

==See also==
{{Div col|cols=2}}
* [[Adaptive resonance theory]]
* [[Black box]]
* [[Cache language model]]
* [[Compound term processing]]
* [[Computer-aided diagnosis]]
* [[Data mining]]
* [[Deep Learning]]
* [[List of numerical analysis software]]
* [[List of numerical libraries]]
* [[Machine learning]]
* [[Multilinear subspace learning]]
* [[Neocognitron]]
* [[Perception]]
* [[Perceptual learning]]
* [[Predictive analytics]]
* [[Prior knowledge for pattern recognition]]
* [[Sequence mining]]
* [[Template matching]]
* [[Contextual image classification]]
* [[List of datasets for machine learning research]]
{{Div col end}}

==References==
{{FOLDOC}}
{{reflist}}

==Further reading==
*{{cite book|last=Fukunaga|first=Keinosuke|title=Introduction to Statistical Pattern Recognition|edition=2nd|year=1990|publisher=Academic Press|location=Boston|isbn=0-12-269851-7}}
*{{cite book|last1=Hornegger|first1=Joachim|last2=Paulus|first2=Dietrich W. R.|title=Applied Pattern Recognition: A Practical Introduction to Image and Speech Processing in C++|edition=2nd|year=1999|publisher=Morgan Kaufmann Publishers|location=San Francisco|isbn=3-528-15558-2}}
*{{cite book|last=Schuermann|first=Juergen|title=Pattern Classification: A Unified View of Statistical and Neural Approaches|year=1996|publisher=Wiley|location=New York|isbn=0-471-13534-8}}
*{{cite book|editor=Godfried T. Toussaint|title=Computational Morphology|year=1988|publisher=North-Holland Publishing Company|location=Amsterdam}}
*{{cite book|last1=Kulikowski|first1=Casimir A.|last2=Weiss|first2=Sholom M.|title=Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems|series=Machine Learning|year=1991|publisher=Morgan Kaufmann Publishers|location=San Francisco|isbn=1-55860-065-5}}
*{{cite journal|last1=Jain|first1=Anil.K.|last2=Duin|first2=Robert.P.W.|last3=Mao|first3=Jianchang|title=Statistical pattern recognition: a review|year=2000|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence | volume=22 | pages=4&amp;ndash;37 | doi = 10.1109/34.824819 | issue=1}}
*[http://www.egmont-petersen.nl/classifiers.htm An introductory tutorial to classifiers (introducing the basic terms, with numeric example)]

==External links==
* [http://www.iapr.org The International Association for Pattern Recognition]
* [http://cgm.cs.mcgill.ca/~godfried/teaching/pr-web.html List of Pattern Recognition web sites]
* [http://www.jprr.org Journal of Pattern Recognition Research]
* [http://www.docentes.unal.edu.co/morozcoa/docs/pr.php Pattern Recognition Info]
* [http://www.sciencedirect.com/science/journal/00313203 Pattern Recognition] (Journal of the Pattern Recognition Society)
* [http://www.worldscinet.com/ijprai/mkt/archive.shtml International Journal of Pattern Recognition and Artificial Intelligence]
* [http://www.inderscience.com/ijapr International Journal of Applied Pattern Recognition]
* [http://www.openpr.org.cn/ Open Pattern Recognition Project], intended to be an open source platform for sharing algorithms of pattern recognition
* [https://www.academia.edu/31957815/Improved_Pattern_Matching_Applied_to_Surface_Mounting_Devices_Components_Localization_on_Automated_Optical_Inspection Improved Fast Pattern Matching] Improved Fast Pattern Matching
{{Authority control}}




</text>
      <sha1>ch6ywtxwmq9tn4uxqhn468sbkrxrdvm</sha1>
    </revision>
  </page>
  <page>
    <title>Linear predictor function</title>
    <ns>0</ns>
    <id>35272263</id>
    <revision>
      <id>760645550</id>
      <parentid>750303630</parentid>
      <timestamp>2017-01-18T06:08:00Z</timestamp>
      <contributor>
        <username>NewYorkActuary</username>
        <id>26033934</id>
      </contributor>
      <comment>three-year old merge proposal was never discussed -- removing banner</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11966">{{Unsourced|date=April 2012}}
In [[statistics]] and in [[machine learning]], a '''linear predictor function''' is a [[linear function]] ([[linear combination]]) of a set of coefficients and explanatory variables ([[independent variable]]s), whose value is used to predict the outcome of a [[dependent variable]].  This sort of function usually comes in [[linear regression]], where the coefficients are called [[regression coefficient]]s. However, they also occur in various types of [[linear classifier]]s (e.g. [[logistic regression]], [[perceptron]]s, [[support vector machine]]s, and [[linear discriminant analysis]]), as well as in various other models, such as [[principal component analysis]] and [[factor analysis]].  In many of these models, the coefficients are referred to as &quot;weights&quot;.

==Basic form==
The basic form of a linear predictor function &lt;math&gt;f(i)&lt;/math&gt; for data point ''i'' (consisting of ''p'' explanatory variables), for ''i'' = 1, ..., ''n'', is

:&lt;math&gt; f(i) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip},&lt;/math&gt;

where &lt;math&gt;\beta_0, \ldots, \beta_p&lt;/math&gt; are the coefficients (regression coefficients, weights, etc.) indicating the relative effect of a particular explanatory variable on the outcome.

It is common to write the predictor function in a more compact form as follows:
*The coefficients ''β''&lt;sub&gt;0&lt;/sub&gt;, ''β''&lt;sub&gt;1&lt;/sub&gt;, ..., ''β''&lt;sub&gt;''p''&lt;/sub&gt; are grouped into a single vector '''''β''''' of size ''p''&amp;nbsp;+&amp;nbsp;1.
*For each data point ''i'', an additional explanatory pseudo-variable ''x''&lt;sub&gt;''i''0&lt;/sub&gt; is added, with a fixed value of 1, corresponding to the [[Y-intercept|intercept]] coefficient ''β''&lt;sub&gt;0&lt;/sub&gt;.
*The resulting explanatory variables ''x''&lt;sub&gt;''i0''&lt;/sub&gt;(= 1), ''x''&lt;sub&gt;''i''1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''ip''&lt;/sub&gt; are then grouped into a single vector '''''x&lt;sub&gt;i&lt;/sub&gt;''''' of size ''p''&amp;nbsp;+&amp;nbsp;1.

This makes it possible to write the linear predictor function as follows:

:&lt;math&gt;f(i)= \boldsymbol\beta \cdot \mathbf{x}_i&lt;/math&gt;

using the notation for a [[dot product]] between two vectors.

An equivalent form using matrix notation is as follows:

:&lt;math&gt;f(i)= \boldsymbol\beta^{\mathrm T} \mathbf{x}_i = \mathbf{x}^{\mathrm T}_i \boldsymbol\beta&lt;/math&gt;

where &lt;math&gt;\boldsymbol\beta&lt;/math&gt; and &lt;math&gt;\mathbf{x}_i&lt;/math&gt; are assumed to be a ''p''-by-1 [[column vector]]s (as is standard when representing vectors as matrices), &lt;math&gt;\boldsymbol\beta^{\mathrm T}&lt;/math&gt; indicates the [[matrix transpose]] of &lt;math&gt;\boldsymbol\beta&lt;/math&gt; (which turns it into a 1-by-''p'' [[row vector]]), and &lt;math&gt;\boldsymbol\beta^{\mathrm T} \mathbf{x}_i&lt;/math&gt; indicates [[matrix multiplication]] between the 1-by-''p'' row vector and the ''p''-by-1 column vector, producing a 1-by-1 matrix that is taken to be a [[scalar (mathematics)|scalar]].

An example of the usage of such a linear predictor function is in [[linear regression]], where each data point is associated with a [[continuous variable|continuous]] outcome ''y''&lt;sub&gt;''i''&lt;/sub&gt;, and the relationship written

:&lt;math&gt;y_i = f(i) + \varepsilon_i = \boldsymbol\beta^{\mathrm T}\mathbf{x}_i\ + \varepsilon_i,&lt;/math&gt;

where &lt;math&gt;\varepsilon_i&lt;/math&gt; is a ''disturbance term'' or ''[[error variable]]'' — an unobserved [[random variable]] that adds noise to the linear relationship between the dependent variable and predictor function.

==Stacking==

In some models (standard linear regression in particular), the equations for each of the data points ''i'' = 1, ..., ''n'' are stacked together and written in vector form as

: &lt;math&gt;
 \mathbf{y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon, \,
 &lt;/math&gt;
where
: &lt;math&gt;
 \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}, \quad
 \mathbf{X} = \begin{pmatrix} \mathbf{x}'_1 \\ \mathbf{x}'_2 \\ \vdots \\ \mathbf{x}'_n \end{pmatrix}
 = \begin{pmatrix} x_{11} &amp; \cdots &amp; x_{1p} \\
 x_{21} &amp; \cdots &amp; x_{2p} \\
 \vdots &amp; \ddots &amp; \vdots \\
 x_{n1} &amp; \cdots &amp; x_{np}
 \end{pmatrix}, \quad
 \boldsymbol\beta = \begin{pmatrix} \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}, \quad
 \boldsymbol\varepsilon = \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}.
 &lt;/math&gt;

The matrix ''X'' is known as the [[design matrix]] and encodes all known information about the [[independent variables]].  The variables &lt;math&gt;\varepsilon_i&lt;/math&gt; are [[random variable]]s, which in standard linear regression are distributed according to a [[standard normal distribution]]; they express the influence of any unknown factors on the outcome.

This makes it possible to find optimal coefficients through the [[method of least squares]] using simple matrix operations.  In particular, the optimal coefficients &lt;math&gt;\boldsymbol{\hat\beta}&lt;/math&gt; as estimated by least squares can be written as follows:

:&lt;math&gt;\boldsymbol{\hat\beta} =( X^\mathrm T X)^{-1}X^{\mathrm T}\mathbf{y}.&lt;/math&gt;

The matrix &lt;math&gt;( X^\mathrm T X)^{-1}X^{\mathrm T}&lt;/math&gt; is known as the [[Moore-Penrose pseudoinverse]] of ''X''.  Note that this formula assumes that ''X'' is of [[full rank]], i.e. there is no [[multicollinearity]] among different explanatory variables (i.e. one variable can be perfectly, or almost perfectly, predicted from another).  In such cases, the [[singular value decomposition]] can be used to compute the pseudoinverse.

==The explanatory variables==
Although the outcomes (dependent variables) to be predicted are assumed to be [[random variable]]s, the explanatory variables themselves are usually not assumed to be random.  Instead, they are assumed to be fixed values, and any random variables (e.g. the outcomes) are assumed to be [[conditional probability|conditional]] on them.  As a result, the model user is free to transform the explanatory variables in arbitrary ways, including creating multiple copies of a given explanatory variable, each transformed using a different function.  Other common techniques are to create new explanatory variables in the form of [[interaction variable]]s by taking products of two (or sometimes more) existing explanatory variables.

When a fixed set of nonlinear functions are used to transform the value(s) of a data point, these functions are known as [[basis function]]s.  An example is [[polynomial regression]], which uses a linear predictor function to fit an arbitrary degree [[polynomial]] relationship (up to a given order) between two sets of data points (i.e. a single [[real-valued]] explanatory variable and a related real-valued dependent variable), by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable.  Mathematically, the form looks like this:

:&lt;math&gt;y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_p x_i^p,&lt;/math&gt;

In this case, for each data point, a set of explanatory variables is created as follows:

:&lt;math&gt;(x_{i1} = x_i, x_{i2} = x_i^2, \ldots, x_{ip} = x_i^p)&lt;/math&gt;

and then standard [[linear regression]] is run.  The basis functions in this example would be
:&lt;math&gt;\boldsymbol\phi(x) = (\phi_1(x), \phi_2(x), \ldots, \phi_p(x)) = (x, x^2, \ldots, x^p).&lt;/math&gt;

This example shows that a linear predictor function can actually be much more powerful than it first appears: It only really needs to be linear in the ''coefficients''.  All sorts of non-linear functions of the explanatory variables can be fit by the model.

There is no particular need for the inputs to basis functions to be univariate or single-dimensional (or their outputs, for that matter, although in such a case, a ''K''-dimensional output value is likely to be treated as ''K'' separate scalar-output basis functions).  An example of this is [[radial basis function]]s (RBF's), which compute some transformed version of the distance to some fixed point:

:&lt;math&gt;\phi(\mathbf{x};\mathbf{c}) = \phi(||\mathbf{x} - \mathbf{c}||) = \phi(\sqrt{(x_1 - c_1)^2 + \ldots + (x_K - c_K)^2})&lt;/math&gt;

An example is the [[Gaussian function|Gaussian]] RBF, which has the same functional form as the [[normal distribution]]:

:&lt;math&gt;\phi(\mathbf{x};\mathbf{c}) = e^{-b||\mathbf{x} - \mathbf{c}||^2}&lt;/math&gt;

which drops off rapidly as the distance from '''''c''''' increases.

A possible usage of RBF's is to create one for every observed data point.  This means that the result of an RBF applied to a new data point will be close to 0 unless the new point is near to the point around which the RBF was applied.  That is, the application of the radial basis functions will pick out the nearest point, and its regression coefficient will dominate.  The result will be a form of [[nearest neighbor interpolation]], where predictions are made by simply using the prediction of the nearest observed data point, possibly interpolating between multiple nearby data points when they are all similar distances away.  This type of [[k-nearest neighbor algorithm|nearest neighbor method]] for prediction is often considered diametrically opposed to the type of prediction used in standard linear regression: But in fact, the transformations that can be applied to the explanatory variables in a linear predictor function are so powerful that even the nearest neighbor method can be implemented as a type of linear regression.

It is even possible to fit some functions that appear non-linear in the coefficients by transforming the coefficients into new coefficients that do appear linear.  For example, a function of the form &lt;math&gt;a + b^2x_{i1} + \sqrt{c}x_{i2}&lt;/math&gt; for coefficients &lt;math&gt;a,b,c&lt;/math&gt; could be transformed into the appropriate linear function by applying the substitutions &lt;math&gt;b' = b^2, c' = \sqrt{c},&lt;/math&gt; leading to &lt;math&gt;a + b'x_{i1} + c'x_{i2},&lt;/math&gt; which is linear.  Linear regression and similar techniques could be applied and will often still find the optimal coefficients, but their error estimates and such will be wrong.

The explanatory variables may be of any [[statistical data type|type]]: [[real-valued]], [[binary variable|binary]], [[categorical variable|categorical]], etc.  The main distinction is between [[continuous variable]]s (e.g. income, age, [[blood pressure]], etc.) and [[discrete variable]]s (e.g. sex, race, political party, etc.).  Discrete variables referring to more than two possible choices are typically coded using [[dummy variable (statistics)|dummy variable]]s (or [[indicator variable]]s), i.e. separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning &quot;variable does have the given value&quot; and a 0 meaning &quot;variable does not have the given value&quot;.  For example, a four-way discrete variable of [[blood type]] with the possible values &quot;A, B, AB, O&quot; would be converted to separate two-way dummy variables, &quot;is-A, is-B, is-AB, is-O&quot;, where only one of them has the value 1 and all the rest have the value 0.  This allows for separate regression coefficients to be matched for each possible value of the discrete variable.

Note that, for ''K'' categories, not all ''K'' dummy variables are independent of each other.  For example, in the above blood type example, only three of the four dummy variables are independent, in the sense that once the values of three of the variables are known, the fourth is automatically determined.  Thus, it's really only necessary to encode three of the four possibilities as dummy variables, and in fact if all four possibilities are encoded, the overall model becomes non-[[identifiable]].  This causes problems for a number of methods, such as the simple closed-form solution used in linear regression.  The solution is either to avoid such cases by eliminating one of the dummy variables, and/or introduce a [[regularization (mathematics)|regularization]] constraint (which necessitates a more powerful, typically iterative, method for finding the optimal coefficients).

== References ==



</text>
      <sha1>dbixihwabdzhzv91e9re0jxr6y9bn9a</sha1>
    </revision>
  </page>
  <page>
    <title>Random indexing</title>
    <ns>0</ns>
    <id>37697003</id>
    <revision>
      <id>814973497</id>
      <parentid>755494606</parentid>
      <timestamp>2017-12-12T00:02:34Z</timestamp>
      <contributor>
        <username>StyloGrl</username>
        <id>29681171</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4709">'''Random indexing''' is a [[dimension reduction]] method and computational framework for [[Distributional semantics]], based on the insight that very-high-dimensional [[Vector Space Model]] implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) is encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately.

This is the original point of the [[random projection]] approach to dimension reduction first formulated as the [[Johnson–Lindenstrauss lemma]], and [[Locality-sensitive hashing]] has some of the same starting points. Random indexing, as used in representation of language, originates from the work of [[Pentti Kanerva]]&lt;ref&gt;Kanerva, Pentti, Kristoferson, Jan and Holst, Anders (2000): Random Indexing of Text Samples for Latent Semantic Analysis, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, p.&amp;nbsp;1036. Mahwah, New Jersey: Erlbaum, 2000.&lt;/ref&gt;&lt;ref&gt;Sahlgren, Magnus (2005) An Introduction to Random Indexing, Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005, August 16, Copenhagen, Denmark&lt;/ref&gt;&lt;ref&gt;Sahlgren, Magnus, Holst, Anders and Pentti Kanerva (2008) Permutations as a Means to Encode Order in Word Space, In Proceedings of the 30th Annual Conference of the Cognitive Science Society: 1300-1305.&lt;/ref&gt;&lt;ref&gt;Kanerva, Pentti (2009) Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors, Cognitive Computation, Volume 1, Issue 2, pp. 139–159.&lt;/ref&gt;&lt;ref&gt;Joshi, Aditya, Johan Halseth, and Pentti Kanerva. &quot;Language Recognition using Random Indexing.&quot; arXiv preprint arXiv:1412.7026 (2014).&lt;/ref&gt; on [[Sparse distributed memory]], and can be described as an incremental formulation of a random projection.&lt;ref&gt;Recchia, Gabriel, et al. &quot;Encoding sequential information in vector space models of semantics: Comparing holographic reduced representation and random permutation.&quot; (2010): 865-870.&lt;/ref&gt;

It can be also verified that random indexing is a random projection technique for the construction of Euclidean spaces---i.e. L2 normed vector spaces.&lt;ref&gt;Qasemi Zadeh, Behrang &amp; Handschuh, Siegrfied. (2014) [http://aran.library.nuigalway.ie/xmlui/bitstream/handle/10379/4389/tir-rmi.pdf?sequence=1 Random Manhattan Indexing], In Proceedings of the 25th International Workshop on Database and Expert Systems Applications.&lt;/ref&gt; In Euclidean spaces, random projections are elucidated using the Johnson–Lindenstrauss lemma.&lt;ref&gt;
Johnson, W. and Lindenstrauss, J. (1984) Extensions of Lipschitz mappings into a Hilbert space, in Contemporary Mathematics. American Mathematical Society, vol. 26, pp. 189–206.
&lt;/ref&gt;

TopSig &lt;ref&gt;Geva, S. &amp; De Vries, C.M. (2011) [http://eprints.qut.edu.au/43451/ TopSig: Topology Preserving Document Signatures], In Proceedings of Conference on Information and Knowledge Management 2011, 24-28 October 2011, Glasgow, Scotland.&lt;/ref&gt; extends the Random Indexing model to produce [[bit vector]]s for comparison with the [[Hamming distance]] similarity function. It is used for improving the performance of [[information retrieval]] and [[document clustering]]. In a similar line of research, Random Manhattan Integer Indexing&lt;ref&gt;Qasemi Zadeh, Behrang. &amp; Handschuh, Siegfried. (2014) [http://emnlp2014.org/papers/pdf/EMNLP2014178.pdf Random Manhattan Integer Indexing: Incremental L1 Normed Vector Space Construction], In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1713–1723,
October 25-29, 2014, Doha, Qatar.&lt;/ref&gt; is proposed for improving the performance of the methods that employ the [[Manhattan distance]] between text units. Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. Reflexive Random Indexing &lt;ref&gt; Cohen T., [[Roger W. Schvaneveldt|Schvaneveldt]] Roger &amp; Widdows Dominic (2009) Reflective Random Indexing and indirect inference: a scalable method for discovery of implicit connections, Journal of Biomedical Informatics, 43(2):240-56.&lt;/ref&gt; generates similarity from co-occurrence and from shared occurrence with other items.

== References ==
{{Reflist}}


* Zadeh Behrang Qasemi, Handshuch Siegfried. (2015) [http://pars.ie/publications/papers/pre-prints/random-indexing-dr-explained.pdf Random indexing explained with high probability], TSD.




{{compsci-stub}}</text>
      <sha1>fept92k3umbio66agw7nr1v3p5kv22g</sha1>
    </revision>
  </page>
  <page>
    <title>Bag-of-words model</title>
    <ns>0</ns>
    <id>14003441</id>
    <revision>
      <id>815406316</id>
      <parentid>811569410</parentid>
      <timestamp>2017-12-14T17:12:48Z</timestamp>
      <contributor>
        <username>Goe Ta</username>
        <id>28731857</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8365">{{for|Bag-of-words model in computer vision|Bag-of-words model in computer vision}}

The '''bag-of-words model''' is a simplifying representation used in [[natural language processing]] and [[information retrieval]] (IR). Also known as [[vector space model]]&lt;ref&gt;
{{cite conference
| first = Michael (et al)
| last = McTear
| year = 2016
| title = The Conversational Interface - Talking to Smart Devices
| pages = 166
| url = http://www.springer.com/us/book/9783319329659}}&lt;/ref&gt;. In this model, a text (such as a sentence or a document) is represented as the [[multiset|bag (multiset)]] of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been [[Bag-of-words model in computer vision|used for computer vision]].&lt;ref name=sivic&gt;{{cite conference
  | first = Josef
  | last = Sivic
  | title = Efficient visual search of videos cast as text retrieval
  | booktitle = IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 31, NO. 4
  | pages = 591–605
  | publisher = IEEE
  | date = April 2009
  | url = http://www.di.ens.fr/~josef/publications/sivic09a.pdf}}&lt;/ref&gt;

The bag-of-words model is commonly used in methods of [[document classification]] where the (frequency of) occurrence of each is used as a [[Feature (machine learning)|feature]] for training a [[Statistical classification|classifier]].

An early reference to &quot;bag of words&quot; in a linguistic context can be found in [[Zellig Harris]]'s 1954 article on ''Distributional Structure''.&lt;ref&gt;{{cite journal
  |last= Harris
  |first= Zellig
  |authorlink= Zellig Harris
  |year= 1954
  |title= Distributional Structure
  |journal= Word
  |volume= 10
  |issue= 2/3
  |pages= 146–62
  |quote= And this stock of combinations of elements becomes a factor in the way later choices are made ... for language is not merely a bag of words but a tool with particular properties which have been fashioned in the course of its use}}&lt;/ref&gt;

== Example implementation ==

The following models a text document using bag-of-words.

Here are two simple text documents:

&lt;syntaxhighlight lang=&quot;text&quot;&gt;
(1) John likes to watch movies. Mary likes movies too.
&lt;/syntaxhighlight&gt;

&lt;syntaxhighlight lang=&quot;text&quot;&gt;
(2) John also likes to watch football games.
&lt;/syntaxhighlight&gt;

Based on these two text documents, a list is constructed as follows:
&lt;syntaxhighlight lang=&quot;javascript&quot;&gt;
[
    &quot;John&quot;,
    &quot;likes&quot;,
    &quot;to&quot;,
    &quot;watch&quot;,
    &quot;movies&quot;,
    &quot;Mary&quot;,
    &quot;too&quot;,
    &quot;also&quot;,
    &quot;football&quot;,
    &quot;games&quot;
]
&lt;/syntaxhighlight&gt;

== Application ==
In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a &quot;bag of words&quot;, we can calculate various measures to characterize the text. The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text. For the example above, we can construct the following two lists to record the term frequencies of all the distinct words:
&lt;syntaxhighlight lang=&quot;javascript&quot;&gt;
(1) [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]
(2) [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]
&lt;/syntaxhighlight&gt;

Each entry of the lists refers to count of the corresponding entry in the list (this is also the histogram representation). For example, in the first list (which represents document 1), the first two entries are &quot;1,2&quot;. The first entry corresponds to the word &quot;John&quot; which is the first word in the list, and its value is &quot;1&quot; because &quot;John&quot; appears in the first document 1 time. Similarly, the second entry corresponds to the word &quot;likes&quot; which is the second word in the list, and its value is &quot;2&quot; because &quot;likes&quot; appears in the first document 2 times. This list (or vector) representation does not preserve the order of the words in the original sentences, which is just the main feature of the Bag-of-words model. This kind of representation has several successful applications, for example [[email filtering]].&lt;ref name=sivic/&gt;

However, term frequencies are not necessarily the best representation for the text. Common words like &quot;the&quot;, &quot;a&quot;, &quot;to&quot; are almost always the terms with highest frequency in the text. Thus, having a high raw count does not necessarily mean that the corresponding word is more important. To address this problem, one of the most popular ways to &quot;normalize&quot; the term frequencies is to weight a term by the inverse of document frequency, or [[tf–idf]]. Additionally, for the specific purpose of classification, [[Supervised learning|supervised]] alternatives have been developed that take into account the class label of a document.&lt;ref&gt;{{cite conference
|title = A study of term weighting schemes using class information for text classification
|author = Youngjoong Ko
|year = 2012
|booktitle = [[Special Interest Group on Information Retrieval|SIGIR'12]]
|publisher=[[Association for Computing Machinery|ACM]]
}}&lt;/ref&gt; Lastly, binary (presence/absence or 1/0) weighting is used in place of frequencies for some problems.  (For instance, this option is implemented in the [[Weka (machine learning)|WEKA]] machine learning software system.)

== N-gram model ==
Bag-of-word model is an orderless document representation—only the counts of words mattered. For instance, in the above example &quot;John likes to watch movies. Mary likes movies too&quot;, the bag-of-words representation will not reveal the fact that a person's name is always followed by the verb &quot;likes&quot; in this text. As an alternative, the [[n-gram]] model can be used to store this spatial information within the text. Applying to the same example above, a '''bigram''' model will parse the text into following units and store the term frequency of each unit as before.

&lt;syntaxhighlight lang=&quot;javascript&quot;&gt;
[
    &quot;John likes&quot;,
    &quot;likes to&quot;,
    &quot;to watch&quot;,
    &quot;watch movies&quot;,
    &quot;Mary likes&quot;,
    &quot;likes movies&quot;,
    &quot;movies too&quot;,
]
&lt;/syntaxhighlight&gt;

Conceptually, we can view bag-of-word model as a special case of the n-gram model, with n=1. See [[language model]] for a more detailed discussion.

== Hashing trick ==

A common alternative to the use of dictionaries is the [[hashing trick]], where words are directly mapped to indices with a hashing function.&lt;ref name=&quot;Weinberger05&quot;&gt;{{cite journal
 | last = Weinberger
 | first = K. Q.
 |author2=Dasgupta A. |author3=Langford J. |author4=Smola A. |author5=Attenberg, J.
 | title = Feature hashing for large scale multitask learning,
 | journal = Proceedings of the 26th Annual International Conference on Machine Learning
 | year=2009
 | pages=1113–1120
 | arxiv=0902.2206}}&lt;/ref&gt;  By mapping words to indices directly with a hash function, no memory is required to store a dictionary. Hash collisions are typically dealt with by using freed-up memory to increase the number of hash buckets. In practice, hashing greatly simplifies the implementation of bag-of-words models and improves their scalability.

== Example usage: spam filtering ==
In [[Bayesian spam filtering]], an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing [[Spam e-mail|spam]] and one representing legitimate e-mail (&quot;ham&quot;).
Imagine that there are two literal bags full of words. One bag is filled with words found in spam messages, and the other bag is filled with words found in legitimate e-mail. While any given word is likely to be found somewhere in both bags, the &quot;spam&quot; bag will contain spam-related words such as &quot;stock&quot;, &quot;Viagra&quot;, and &quot;buy&quot; much more frequently, while the &quot;ham&quot; bag will contain more words related to the user's friends or workplace.

To classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses [[Bayesian probability]] to determine which bag it is more likely to be.

== See also ==
* [[Additive smoothing]]
* [[Bag-of-words model in computer vision]]
* [[Document classification]]
* [[Document-term matrix]]
* [[Feature extraction]]
* [[Hashing trick]]
* [[Machine learning]]
* [[MinHash]]
* [[n-gram]]
* [[Natural language processing]]
* [[Vector space model]]
* [[w-shingling]]

== References ==
&lt;references/&gt;
{{Natural Language Processing}}

{{DEFAULTSORT:Bag-of-words model}}

</text>
      <sha1>jkxaas0trlqvoombi6mle8w8b99st2l</sha1>
    </revision>
  </page>
  <page>
    <title>Similarity learning</title>
    <ns>0</ns>
    <id>38059657</id>
    <revision>
      <id>800574086</id>
      <parentid>792102728</parentid>
      <timestamp>2017-09-14T11:20:45Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor/>
      <comment>link [[Similarity measure|measure of similarity]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9472">'''Similarity learning''' is an area of supervised [[machine learning]] in [[artificial intelligence]]. It is closely related to [[regression (machine learning)|regression]] and [[classification in machine learning|classification]], but the goal is to learn from examples a similarity function that measures how similar or related two objects are. It has applications in [[ranking]], in [[recommendation systems]],
visual identity tracking, face verification, and speaker verification.
== Learning setup ==

There are four common setups for similarity and metric distance learning.

; ''[[Regression (machine learning)|Regression]] similarity learning''
: In this setup, pairs of objects are given &lt;math&gt; (x_i^1, x_i^2) &lt;/math&gt; together with a measure of their similarity &lt;math&gt; y_i \in R &lt;/math&gt;. The goal is to learn a function that approximates &lt;math&gt; f(x_i^1, x_i^2) \sim y_i &lt;/math&gt; for every new labeled triplet example &lt;math&gt;(x_i^1, x_i^2, y_i)&lt;/math&gt;. This is typically achieved by  minimizing a regularized loss &lt;math&gt; \min_W \sum_i loss(w;x_i^1, x_i^2,y_i) + reg(w)&lt;/math&gt;.
; ''[[Classification in machine learning|Classification]] similarity learning''
: Given are pairs of similar objects &lt;math&gt;(x_i, x_i^+) &lt;/math&gt; and non similar objects &lt;math&gt;(x_i, x_i^-)&lt;/math&gt;. An equivalent formulation is that every pair &lt;math&gt;(x_i^1, x_i^2)&lt;/math&gt; is given together with a binary label &lt;math&gt;y_i \in \{0,1\}&lt;/math&gt; that determines if the two objects are similar or not. The goal is again to learn a classifier that can decide if a new pair of objects is similar or not.
; ''Ranking similarity learning''
: Given are triplets of objects &lt;math&gt;(x_i, x_i^+, x_i^-)&lt;/math&gt; whose relative similarity obey a predefined order: &lt;math&gt;x_i&lt;/math&gt; is known to be more similar to &lt;math&gt;x_i^+&lt;/math&gt; than to &lt;math&gt;x_i^-&lt;/math&gt;. The goal is to learn a function &lt;math&gt;f&lt;/math&gt; such that for any new triplet of objects &lt;math&gt;(x, x^+, x^-)&lt;/math&gt;, it obeys &lt;math&gt;f(x, x^+) &gt; f(x, x^-)&lt;/math&gt;. This setup assumes a weaker form of supervision than in regression, because instead of providing an exact [[Similarity measure|measure of similarity]], one only has to provide the relative order of similarity. For this reason, ranking-based similarity learning is easier to apply in real large scale applications.&lt;ref&gt;{{cite journal| last1 = Chechik | first1 = G. | last2 = Sharma | first2 = V. | last3 = Shalit | first3 = U. | last4 = Bengio | first4 = S. | title=Large Scale Online Learning of Image Similarity Through Ranking|journal=Journal of Machine Learning research|year=2010|volume=11|pages=1109–1135|url=http://www.jmlr.org/papers/volume11/chechik10a/chechik10a.pdf}}&lt;/ref&gt;
; [[Locality sensitive hashing]] (LSH)&lt;ref&gt;Gionis, Aristides, Piotr Indyk, and Rajeev Motwani. &quot;Similarity search in high dimensions via hashing.&quot; VLDB. Vol. 99. No. 6. 1999.&lt;/ref&gt;
: [[Hash Function|hashes]] input items so that similar items map to the same &quot;buckets&quot; in memory with high probability (the number of buckets being much smaller than the universe of possible input items). It is often applied in nearest neighbor search on large scale high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases.&lt;ref&gt;{{cite web
| first1 = A.|last1=Rajaraman |first2= J.|last2=Ullman|author2-link=Jeffrey Ullman
| url = http://infolab.stanford.edu/~ullman/mmds.html
| title=Mining of Massive Datasets, Ch. 3.
| year = 2010
}}&lt;/ref&gt;

A common approach for learning similarity, is to model the similarity function as a [[bilinear form]]. For example, in the case of ranking similarity learning, one aims to learn a matrix W that parametrizes the similarity function &lt;math&gt; f_W(x, z)  = x^T W z &lt;/math&gt;.

== Metric learning ==

Similarity learning is closely related to ''distance metric learning''. Metric learning is the task of learning a distance function over objects. A [[Metric (mathematics)|metric]] or [[distance function]] has to obey four axioms: [[non-negative|non-negativity]], [[Identity of indiscernibles]], [[symmetry]] and [[subadditivity]] / triangle inequality. In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.

When the objects &lt;math&gt;x_i&lt;/math&gt; are vectors in &lt;math&gt;R^d&lt;/math&gt;, then any matrix &lt;math&gt;W&lt;/math&gt; in the symmetric positive semi-definite cone &lt;math&gt;S_+^d&lt;/math&gt; defines a distance pseudo-metric of the space of x through the form &lt;math&gt;D_W(x_1, x_2)^2 = (x_1-x_2)^{\top} W (x_1-x_2)&lt;/math&gt;. When &lt;math&gt;W&lt;/math&gt; is a symmetric positive definite matrix, &lt;math&gt;D_W&lt;/math&gt; is a metric. Moreover, as any symmetric positive semi-definite matrix &lt;math&gt;W \in S_+^d&lt;/math&gt; can be decomposed as &lt;math&gt;W = L^{\top}L&lt;/math&gt; where &lt;math&gt;L \in R^{e \times d}&lt;/math&gt; and &lt;math&gt;e \geq rank(W)&lt;/math&gt;, the distance function &lt;math&gt;D_W&lt;/math&gt; can be rewritten equivalently &lt;math&gt;D_W(x_1, x_2)^2 = (x_1-x_2)^{\top} L^{\top}L (x_1-x_2) = \| L (x_1-x_2) \|_2^2&lt;/math&gt;. The distance &lt;math&gt;D_W(x_1, x_2)^2=\| x_1' - x_2' \|_2^2&lt;/math&gt; corresponds to the Euclidean distance between the projected feature vectors &lt;math&gt;x_1'= Lx_1&lt;/math&gt; and &lt;math&gt;x_2'= Lx_2&lt;/math&gt;.
Some well-known approaches for metric learning include [[Large margin nearest neighbor]],&lt;ref name=LMNN&gt;{{cite journal| last1 = Weinberger | first1 = K. Q. | last2 = Blitzer | first2 = J. C. | last3 = Saul | first3 = L. K. | title=Distance Metric Learning for Large Margin Nearest Neighbor Classification|journal=Advances in Neural Information Processing Systems |volume=18|year=2006|pages=1473–1480|url=http://books.nips.cc/papers/files/nips18/NIPS2005_0265.pdf}}&lt;/ref&gt; Information theoretic metric learning (ITML).&lt;ref name=ITML&gt;{{cite journal | last1 = Davis | first1 = J. V. | last2 = Kulis | first2 = B. | last3 = Jain | first3 = P. | last4 = Sra | first4 = S. | last5 = Dhillon | first5 = I. S. | title=Information-theoretic metric learning | journal=International conference in machine learning (ICML) | year=2007 | pages=209–216 | url=http://www.cs.utexas.edu/users/pjain/itml/}}&lt;/ref&gt;

In [[statistics]], the [[covariance]] matrix of the data is sometimes used to define a distance metric called [[Mahalanobis distance]].

== Applications ==
Similarity learning is used in information retrieval for learning to rank, in face verification or face identification,&lt;ref name=GUILLAUMIN&gt;{{cite journal| last1 = Guillaumin | first1 = M.  | last2 = Verbeek | first2 = J. | last3 = Schmid | first3 = C. | title=Is that you? Metric learning approaches for face identification|url=http://hal.inria.fr/docs/00/58/50/36/PDF/verbeek09iccv2.pdf|journal=IEEE International Conference on Computer Vision (ICCV)|year=2009}}&lt;/ref&gt;&lt;ref name=MIGNON&gt;{{cite journal| last1 = Mignon | first1 = A. | last2 = Jurie | first2 = F. | title=PCCA: A new approach for distance learning from sparse pairwise constraints|journal=IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|year=2012|url=http://hal.archives-ouvertes.fr/docs/00/80/60/07/PDF/12_cvpr_ldca.pdf}}&lt;/ref&gt; and in [[recommendation systems]]. Also, many machine learning approaches rely on some metric. This includes unsupervised learning such as [[cluster analysis |clustering]], which groups together close or similar objects. It also includes supervised approaches like [[K-nearest neighbor algorithm]] which rely on labels of nearby objects to decide on the label of a new object. Metric learning has been proposed as a preprocessing step for many of these approaches
.&lt;ref name=XING&gt;{{cite journal| last1 = Xing | first1 = E. P. | last2 = Ng | first2 = A. Y. | last3 = Jordan | first3 = M. I. | last4 = Russell | first4 = S. | title=Distance Metric Learning, with Application to Clustering with Side-information | journal=Advances in Neural Information Processing Systems |volume=15 | year=2002| pages = 505–512 | publisher = MIT Press}}&lt;/ref&gt;

== Scalability ==

Metric and similarity learning naively scale quadratically with the dimension of the input space, as can easily see when the learned metric has a bilinear form &lt;math&gt; f_W(x, z)  = x^T W z &lt;/math&gt;. Scaling to higher dimensions can be achieved by enforcing a sparseness structure over the matrix model, as done with HDSL,&lt;ref name=Liu&gt;{{Cite journal| last1=Liu | last2=Bellet | last3=Sha| title=Similarity Learning for High-Dimensional Sparse Data|year=2015|journal=International Conference on Artificial Intelligence and Statistics (AISTATS)|url=http://jmlr.org/proceedings/papers/v38/liu15.pdf}}&lt;/ref&gt; and with COMET.&lt;ref&gt;{{Cite journal | last1=Atzmon | last2=Shalit | last3=Chechik | title=Learning Sparse Metrics, One Feature at a Time | journal=J. Mach. Learn. Research (JMLR)|year=2015|url=http://jmlr.org/proceedings/papers/v44/atzmon2015.pdf}}&lt;/ref&gt;

==See also==
*[[Latent semantic analysis]]

== Further reading ==
For further information on this topic, see the surveys on metric and similarity learning by Bellet et al.&lt;ref name=survey&gt;{{cite arXiv | last1 = Bellet | first1 = A. | last2 = Habrard | first2 = A. | last3 = Sebban | first3 = M. |eprint=1306.6709 |class=cs.LG |title=A Survey on Metric Learning for Feature Vectors and Structured Data |year=2013}}&lt;/ref&gt; and Kulis.&lt;ref name=survey2&gt;{{cite journal| last = Kulis | first = B.| title=Metric Learning: A Survey | journal=Foundations and Trends in Machine Learning | year=2012 | url=http://web.cse.ohio-state.edu/~kulis/pubs/ftml_metric_learning.pdf}}&lt;/ref&gt;

== References ==
{{reflist}}



</text>
      <sha1>hre7qep3ixzt3vejfj32qsduj1cnsx7</sha1>
    </revision>
  </page>
  <page>
    <title>Feature learning</title>
    <ns>0</ns>
    <id>38870173</id>
    <revision>
      <id>811779942</id>
      <parentid>811457056</parentid>
      <timestamp>2017-11-23T22:29:52Z</timestamp>
      <contributor>
        <username>HelpUsStopSpam</username>
        <id>24038232</id>
      </contributor>
      <comment>Undid revision 811457056 by [[Special:Contributions/71.61.59.109|71.61.59.109]] ([[User talk:71.61.59.109|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20523">&lt;!-- EDIT BELOW THIS LINE --&gt;
{{Machine learning bar}}
In [[machine learning]], '''feature learning''' or '''representation learning'''&lt;ref name=&quot;pami&quot;&gt;{{cite journal |author1=Y. Bengio |author2=A. Courville |author3=P. Vincent |title=Representation Learning: A Review and New Perspectives |journal=IEEE Trans. PAMI, special issue Learning Deep Architectures |year=2013|doi=10.1109/tpami.2013.50 |volume=35 |pages=1798–1828}}&lt;/ref&gt; is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual [[feature engineering]] and allows a machine to both learn the features  and use them to perform  a specific task.

Feature learning is motivated by the fact that machine learning tasks such as [[statistical classification|classification]] often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.

Feature learning can be either supervised or unsupervised.
*In [[Supervised learning|supervised feature learning]], features are learned using labeled input data. Examples include [[artificial neural network|supervised neural networks]], [[multilayer perceptron]] and (supervised) [[dictionary learning]].
*In [[Unsupervised learning|unsupervised feature learning]], features are learned with unlabeled input data.  Examples include dictionary learning, [[independent component analysis]], [[autoencoder|autoencoders]], [[Matrix decomposition|matrix factorization]]&lt;ref&gt;{{cite conference
|author1=Nathan Srebro |author2=Jason D. M. Rennie |author3=Tommi S. Jaakkola
|title=Maximum-Margin Matrix Factorization
|conference=[[Conference on Neural Information Processing Systems|NIPS]]
|year=2004
}}&lt;/ref&gt; and various forms of [[Cluster analysis|clustering]].&lt;ref name=&quot;coates2011&quot;/&gt;&lt;ref&gt;{{cite conference
|last1 = Csurka |first1 = Gabriella
|last2 = Dance |first2 = Christopher C.
|last3 = Fan |first3 = Lixin
|last4 = Willamowski |first4 = Jutta
|last5 = Bray |first5 = Cédric
|title = Visual categorization with bags of keypoints
|conference = ECCV Workshop on Statistical Learning in Computer Vision
|year = 2004
|url = http://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/csurka-eccv-04.pdf
}}&lt;/ref&gt;&lt;ref name=&quot;jurafsky&quot;&gt;{{cite book |title=Speech and Language Processing |author1=Daniel Jurafsky |author2=James H. Martin |publisher=Pearson Education International |year=2009 |pages=145–146}}&lt;/ref&gt;

== Supervised  ==
Supervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include:

=== Supervised dictionary learning ===
Dictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with [[Regularization (mathematics)|''L1'' regularization]] on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).

Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique&lt;ref&gt;{{cite journal|last1=Mairal|first1=Julien|last2=Bach|first2=Francis|last3=Ponce|first3=Jean|last4=Sapiro|first4=Guillermo|last5=Zisserman|first5=Andrew|title=Supervised Dictionary Learning|journal=Advances in Neural Information Processing Systems|date=2009}}&lt;/ref&gt; applied dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an ''L1'' regularization on the representing weights for each data point (to enable sparse representation of data), and an ''L2'' regularization on the parameters of the classifier.

=== Neural networks===
[[Artificial neural networks|Neural networks]] are a family of learning algorithms that use a &quot;network&quot; consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).

Multilayer [[neural network]]s can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer.

== Unsupervised  ==
Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of [[semisupervised learning]] where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.&lt;ref name=&quot;liang&quot;&gt;{{cite thesis |type=M. Eng. |author=Percy Liang |year=2005 |title=Semi-Supervised Learning for Natural Language |publisher=[[Massachusetts Institute of Technology|MIT]] |url=http://people.csail.mit.edu/pliang/papers/meng-thesis.pdf |pages=44–52}}&lt;/ref&gt;&lt;ref name=&quot;turian&quot;/&gt; Several approaches are introduced in the following.

=== ''K''-means clustering ===
[[K-means clustering|''K''-means clustering]] is an approach for vector quantization. In particular, given a set of ''n'' vectors, ''k''-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally [[NP-hard]], although suboptimal [[greedy algorithm]]s have been developed.

K-means clustering can be used to group an unlabeled set of inputs into ''k'' clusters, and then use the [[Centroid|centroids]] of these clusters to produce features. These features can be produced in several ways. The simplest is to add ''k'' binary features to each sample, where each feature ''j'' has value one [[if and only if|iff]] the ''j''th centroid learned by ''k''-means is the closest to the sample under consideration.&lt;ref name=&quot;coates2011&quot;/&gt; It is also possible to use the distances to the clusters as features, perhaps after transforming them through a [[radial basis function]] (a technique that has been used to train [[Radial basis function network|RBF network]]s&lt;ref name=&quot;schwenker&quot;&gt;{{cite journal |last1=Schwenker |first1=Friedhelm |last2=Kestler |first2=Hans A. |last3=Palm |first3=Günther |title=Three learning phases for radial-basis-function networks |journal=Neural Networks |volume=14 |pages=439–458 |year=2001 |citeseerx = 10.1.1.109.312 |doi=10.1016/s0893-6080(01)00027-2}}&lt;/ref&gt;). Coates and [[Andrew Ng|Ng]] note that certain variants of ''k''-means behave similarly to [[sparse coding]] algorithms.&lt;ref name=Coates2012&gt;{{cite encyclopedia |last1 = Coates |first1 = Adam |last2 = Ng |first2 = Andrew Y. |title=Learning feature representations with k-means |encyclopedia=Neural Networks: Tricks of the Trade |year = 2012 |publisher=Springer |editor=G. Montavon, G. B. Orr and K.-R. Müller}}&lt;/ref&gt;

In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that ''k''-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task.&lt;ref name=&quot;coates2011&quot;/&gt; ''K''-means also improves performance in the domain of [[Natural language processing|NLP]], specifically for [[named-entity recognition]];&lt;ref&gt;{{cite conference |title=Phrase clustering for discriminative learning |author1=Dekang Lin |author2=Xiaoyun Wu |conference=Proc. J. Conf. of the ACL and 4th Int'l J. Conf. on Natural Language Processing of the AFNLP |pages=1030–1038 |year=2009 |url=http://wmmks.csie.ncku.edu.tw/ACL-IJCNLP-2009/ACLIJCNLP/pdf/ACLIJCNLP116.pdf}}&lt;/ref&gt; there, it competes with [[Brown clustering]], as well as with distributed word representations (also known as neural word embeddings).&lt;ref name=&quot;turian&quot;&gt;{{cite conference |author1=Joseph Turian |author2=Lev Ratinov |author3=Yoshua Bengio |title=Word representations: a simple and general method for semi-supervised learning |conference=Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics |year=2010 |url=http://www.newdesign.aclweb.org/anthology/P/P10/P10-1040.pdf}}&lt;/ref&gt;

=== Principal component analysis ===
[[Principal component analysis]] (PCA) is often used for dimension reduction. Given an unlabeled set of ''n'' input data vectors, PCA generates ''p'' (which is much smaller than the dimension of the input data) [[Singular value decomposition|right singular vectors]] corresponding to the ''p'' largest singular values of the data matrix, where the ''k''th row of the data matrix is the ''k''th input data vector shifted by the [[Sample mean and sample covariance|sample mean]] of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the [[eigenvector]]s corresponding to the ''p'' largest eigenvalues of the [[Sample mean and sample covariance|sample covariance matrix]] of the input vectors. These ''p'' singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.

PCA is a linear feature learning approach since the ''p'' singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with ''p'' iterations. In the ''i''th iteration, the projection of the data matrix on the ''(i-1)''th eigenvector is subtracted, and the ''i''th singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.

PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order [[Moment (mathematics)|moments]] of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).

=== Local linear embedding ===
[[Nonlinear dimensionality reduction|Local linear embedding]] (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000).&lt;ref name=&quot;RowSau00&quot;&gt;{{cite journal|last1=Roweis|first1=Sam T|last2=Saul|first2=Lawrence K|title=Nonlinear Dimensionality Reduction by Locally Linear Embedding|journal=Science, New Series|date=2000|volume=290|issue=5500|pages=2323–2326|doi=10.1126/science.290.5500.2323|jstor=3081722|pmid=11125150}}&lt;/ref&gt;&lt;ref name=&quot;SauRow00&quot;&gt;{{cite journal|last1=Saul|first1=Lawrence K|last2=Roweis|first2=Sam T|title=An Introduction to Locally Linear Embedding|date=2000|url=http://www.cs.toronto.edu/~roweis/lle/publications.html}}&lt;/ref&gt; The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set.

LLE consists of two major steps. The first step is for &quot;neighbor-preserving&quot;, where each input data point ''Xi'' is reconstructed as a weighted sum of [[K-nearest neighbors algorithm|''K'' nearest neighbor]] data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for &quot;dimension reduction,&quot; by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a [[least squares]] problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition.

The reconstruction weights obtained in the first step capture the &quot;intrinsic geometric properties&quot; of a neighborhood in the input data.&lt;ref name=&quot;SauRow00&quot;/&gt; It is assumed that original data lie on a smooth lower-dimensional [[manifold]], and the &quot;intrinsic geometric properties&quot; captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure.

=== Independent component analysis ===
[[Independent component analysis]] (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components.&lt;ref&gt;{{cite journal|last1=Hyvärinen|first1=Aapo|last2=Oja|first2=Erkki|title=Independent Component Analysis: Algorithms and Applications|journal=Neural Networks|date=2000|volume=13|issue=4|pages=411–430|doi= 10.1016/s0893-6080(00)00026-5|pmid=10946390}}&lt;/ref&gt; The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow [[Normal distribution|Gaussian]] distribution.

=== Unsupervised dictionary learning ===

Unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data.&lt;ref&gt;{{cite journal|last1=Lee|first1=Honglak|last2=Battle|first2=Alexis|last3=Raina|first3=Rajat|last4=Ng|first4=Andrew Y|title=Efficient sparse coding algorithms|journal=Advances in Neural Information Processing Systems|date=2007}}&lt;/ref&gt; Aharon et al. proposed algorithm [[K-SVD]] for learning a dictionary of elements that enables sparse representation.&lt;ref&gt;{{cite journal|last1=Aharon|first1=Michal|last2=Elad|first2=Michael|last3=Bruckstein|first3=Alfred|title=K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation|journal=IEEE Trans. Signal Process.|date=2006|volume=54|issue=11|pages=4311–4322|doi=10.1109/TSP.2006.881199}}&lt;/ref&gt;

== Multilayer/deep architectures ==

The hierarchical architecture of the biological neural system inspires [[deep learning]] architectures for feature learning by stacking multiple layers of learning nodes.&lt;ref&gt;{{cite journal|last1=Bengio|first1=Yoshua|title=Learning Deep Architectures for AI|journal=Foundations and Trends in Machine Learning|date=2009|volume=2|issue=1|pages=1–127|doi=10.1561/2200000006}}&lt;/ref&gt; These architectures are often designed based on the assumption of [[distributed representation]]: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input at the bottom layer is raw data, and the output of the final layer is the final low-dimensional feature or representation.

=== Restricted Boltzmann machine ===
[[Restricted Boltzmann machine]]s (RBMs) are often used as a building block for multilayer learning architectures.&lt;ref name=&quot;coates2011&quot;&gt;{{cite conference
|last1 = Coates |first1 = Adam
|last2 = Lee |first2 = Honglak
|last3 = Ng |first3 = Andrew Y.
|title = An analysis of single-layer networks in unsupervised feature learning
|conference = Int'l Conf. on AI and Statistics (AISTATS)
|year = 2011
|url = http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf
}}&lt;/ref&gt;&lt;ref name = Hinton2006&gt;{{Cite journal | last1 = Hinton | first1 = G. E. | last2 = Salakhutdinov | first2 = R. R. | title = Reducing the Dimensionality of Data with Neural Networks | doi = 10.1126/science.1127647 | journal = Science | volume = 313 | issue = 5786 | pages = 504–507 | year = 2006 | pmid =  16873662| url  = http://www.cs.toronto.edu/~hinton/science.pdf| pmc = }}&lt;/ref&gt; An RBM can be represented by an undirected bipartite graph consisting of a group of [[Binary variable|binary]] [[Latent variable|hidden variables]], a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general [[Boltzmann machine]]s with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an [[energy function]], based on which a [[joint distribution]] of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables.{{Clarify|reason=visible hidden?|date=June 2017}} Such conditional independence facilitates computations.

An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using [[Geoffrey Hinton|Hinton]]'s [[contrastive divergence]] (CD) algorithm.&lt;ref name = Hinton2006/&gt;

In general training RBM by solving the maximization problem tends to result in non-sparse representations. Sparse RBM&lt;ref name = Lee2008&gt;{{cite journal|last1=Lee|first1=Honglak|last2=Ekanadham|first2=Chaitanya|last3=Andrew|first3=Ng|title=Sparse deep belief net model for visual area V2|journal=Advances in Neural Information Processing Systems|date=2008}}&lt;/ref&gt; was proposed to enable sparse representations. The idea is to add a [[Regularization (mathematics)|regularization]] term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant &lt;math&gt;p&lt;/math&gt;.

=== Autoencoder ===
An [[autoencoder]] consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov&lt;ref name = Hinton2006/&gt; where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a [[Greedy algorithm|greedy]] layer-by-layer manner: after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with [[stochastic gradient descent]] methods. Training can be repeated until some stopping criteria are satisfied.

==See also==
* [[Basis function]]
* [[Deep learning]]
* [[Feature detection (computer vision)]]
* [[Feature extraction]]
* [[Kernel trick]]
* [[Vector quantization]]

==References==
{{Reflist|30em}}

</text>
      <sha1>17dx9lmk6pl9vcb4365b7m9wkry8884</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Statistical natural language processing</title>
    <ns>14</ns>
    <id>11737376</id>
    <revision>
      <id>712166798</id>
      <parentid>550656947</parentid>
      <timestamp>2016-03-27T11:39:09Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Fields of application of statistics to [[:Category:Applied statistics]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 March 12]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="102">

</text>
      <sha1>qqa3rn79u8nqnsd5i5iptxd9k0uqye1</sha1>
    </revision>
  </page>
  <page>
    <title>Catastrophic interference</title>
    <ns>0</ns>
    <id>39182554</id>
    <revision>
      <id>796150734</id>
      <parentid>792550729</parentid>
      <timestamp>2017-08-18T20:50:47Z</timestamp>
      <contributor>
        <username>E-clair</username>
        <id>31745273</id>
      </contributor>
      <minor/>
      <comment>&quot;to try and model&quot; is grammatically incorrect. fixed to &quot;to try to model&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="48413">{{multiple issues|
{{cleanup|reason=Some formatting issues, and &quot;prominent researchers&quot; should only be wikilinks not external links; could probably also use more wikilinks|date=April 2013}}
{{Original research|date=April 2013}}
}}

'''Catastrophic interference''', also known as catastrophic forgetting, is the tendency of an [[artificial neural network]] to completely and abruptly forget previously learned information upon learning new information.&lt;ref name=&quot;McCloskey1989&quot;&gt;McCloskey, M. &amp; Cohen, N. (1989) Catastrophic interference in connectionist networks: The sequential learning problem. In G. H. Bower (ed.) ''The Psychology of Learning and Motivation'',''24'', 109-164&lt;/ref&gt;&lt;ref name=&quot;Ratcliff1990&quot;&gt;Ratcliff, R. (1990) Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions. ''Psychological Review'',''97'', 285-308&lt;/ref&gt; Neural networks are an important part of the [[Connectionism|network approach and connectionist approach]] to [[cognitive science]]. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989),&lt;ref name=&quot;McCloskey1989&quot; /&gt; and Ractcliff (1990).&lt;ref name=&quot;Ratcliff1990&quot; /&gt; It is a radical manifestation of the 'sensitivity-stability' dilemma&lt;ref&gt;Hebb, D.O. (1949). '`Organization of Behaviour'`. New York: Wiley&lt;/ref&gt; or the 'stability-plasticity' dilemma.&lt;ref&gt;Caroebterm G., &amp; Grossberg, S. (1987) ART 2: Self-organization of stable category recognition codes for analog input patterns. ``Applied Optics, 26``, 4919-4930&lt;/ref&gt; Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. [[Lookup table]]s and connectionist networks lie on the opposite sides of the stability plasticity spectrum.&lt;ref name =&quot;French1997&quot;&gt;French, R. M. (1997) Pseudo-recurrent connectionist networks: an approach to the 'sensitivity-stability' dilemma. ''Connection Science'', ''9''(4), 353–379.&lt;/ref&gt; The former remains completely stable in the presence of new information but lacks the ability to [[machine learning#Generalization|generalize]], i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the [[backpropagation|standard backpropagation network]] are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of [[human memory]] insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.

==Artificial Neural Networks: Standard Backpropagation Networks and Their Training==

In order to understand the topic of catastrophic interference it is important to understand the components of an artificial neural network and, more specifically, the behaviour of a backpropagation network. The following account of neural networks is summarized from ''Rethinking Innateness: A Connectionist Perspective on Development'' by Elman et al. (1996).&lt;ref&gt;Elman, J., Karmiloff-Smith, A., Bates, E., &amp; Johnson, M. (1996). ``Rethinking Innateness: A Connectionist Perspective on Development.`` Cambridge, MA: MIT Press.&lt;/ref&gt;[[File:Artificial neural network.svg|thumbnail|Figure 1: A three-layer artificial neural network]]

Artificial neural networks are inspired by [[biological neural networks]]. They use [[mathematical model]]s, namely [[algorithms]], to do things such as classifying data and [[pattern recognition|learning patterns]] in data. Information is represented in these networks through patterns of activation, known as a distributed representations.

The basic components of artificial neural networks are ''nodes/units'' and ''weights''.
''Nodes'' or ''units'' are simple processing elements, which can be considered [[artificial neurons]]. These units can act in a variety of ways. They can act like [[sensory neurons]] and collect inputs from the environment, they can act like [[motor neurons]] and sent and output, they can act like [[interneurons]] and relay information, or they may do all three functions. A backpropagation network is often a three-layer neural network that includes input nodes, hidden nodes, and output nodes (see Figure 1). The hidden nodes allow the network to be transformed into an internal representation, akin to a [[mental representation]]. These internal representations give the backpropagation network its ability to capture abstract relationships between different input patterns.

The nodes are also connected to each other, thus they can send activation to one another like neurons. These connections can be unidirectional, creating a [[feedforward neural network|feedforward network]], or they can be bidirectional, creating a [[Recurrent neural network|recurrent network]]. Each of the connections between the nodes has a '`weight'`, or strength, and it is in these weights are where the knowledge is 'stored'. The weights act to multiply the output of a node. They can be excitatory (a positive value) or inhibitory (a negative value). For example, if a node has an output of 1.0 and it is connected to another node with a weight of -0.5 then the second node will receive an input signal of 1.0 &amp;times; (-0.5) = -0.5. Since any one node can receive multiple inputs, the sum of all of these inputs must be taken to calculate the net input.

The '''''net input''''' (''net''&lt;sub&gt;''i''&lt;/sub&gt;) to a node ''j'' would be defined as:

: net&lt;sub&gt;''i''&lt;/sub&gt; = ∑''w''&lt;sub&gt;''ij''&lt;/sub&gt;''o''&lt;sub&gt;''j''&lt;/sub&gt;&lt;ref name=&quot;Ratcliff1990&quot; /&gt;
: ''w''&lt;sub&gt;''ij''&lt;/sub&gt; = the weight between node ''i'' and ''j''
: ''o''&lt;sub&gt;''j''&lt;/sub&gt; = the input vector / activation

Once the input has been sent to the hidden layer from the input layer, the hidden node may then send an output to the output layer. The output of any given node depends on the activation of that node and the [[function (mathematics)|response function]] of that node. In the case of a three-layer backpropagation network, the response function is a non-linear, [[logistic function]]. This function allows a node to behave in an [[All-or-none law|all or none]] fashion towards high or low input values and in a more graded and sensitive fashion towards mid-ranged input values. It allows the nodes the result in more substantial changes in the network when the node activation is at the more extreme values. Transforming the net input into a '''''net output''''' that can be sent onto the output layer is calculated by:

: ''o''&lt;sub&gt;''i''&lt;/sub&gt; =   1/[1 +exp(net&lt;sub&gt;''i''&lt;/sub&gt;)]&lt;ref name=&quot;Ratcliff1990&quot; /&gt;
: ''o''&lt;sub&gt;''i''&lt;/sub&gt; = the activation of node ''i''

An important feature of neural networks is that they can learn. Simply put, this means that they can change their outputs when they are given new inputs. Backpropagation, specifically refers to how this the network is trained, i.e. how the network is told to learn. The way in which a backpropagation network learns, is through comparing the actual output to the desired output of the unit. The desired output is known as a 'teacher' and it can be the same as the input, as in the case of auto-associative/auto-encoder networks, or it can be completely different from the input. Either way, learning which requires a teacher is called [[machine learning#Algorithm types|supervised learning]]. The difference between these actual and desired output constitutes an error signal. This error signal is then fedback, or backpropagated, to the nodes in order to modify the weights in the neural network. Backpropagation first modifies the weights between output layer to the hidden layer, then next modifies the weights between the hidden units and the input units.  The change in weights help to decrease the discrepancy between the actual and desired output. However, learning is typically incremental in these networks. This means that these networks will require a series of presentations of the same input before it can come up with the weight changes that will result in the desired output. The weights are usually set to random values for first learning trial and after many trials the weights become more able represent the desired output. The process of converging on an output is called settling. This kind of training is based on the error signal and '''''backpropagation learning algorithm''''' / [[delta rule]]:

{|
|
{|class=&quot;wikitable&quot;
!''Error signal at output element''
| {{math|1=''e'' =     (''t''&lt;sub&gt;''i''&lt;/sub&gt; - ''o''&lt;sub&gt;''i''&lt;/sub&gt;)''o''&lt;sub&gt;''i''&lt;/sub&gt;(1-''o''&lt;sub&gt;''i''&lt;/sub&gt;)}}
|-
!''Error signal at the hidden unit''
| {{math|1=''e'' =      ''o''&lt;sub&gt;''i''&lt;/sub&gt;(1-''o''&lt;sub&gt;''i''&lt;/sub&gt;)Σ''w&lt;sub&gt;ik&lt;/sub&gt;e''}}
|-
!''Weight change''
| {{math|1=Δ''w''&lt;sub&gt;''ij''&lt;/sub&gt; =   ''k'' ''e'' ''o''&lt;sub&gt;''j''&lt;/sub&gt;}}
|}&lt;ref name=&quot;Ratcliff1990&quot; /&gt;
|
: ''e'' = error signal
: ''t''&lt;sub&gt;''i''&lt;/sub&gt; = target output of ''j''
: Δ''w''&lt;sub&gt;''ij''&lt;/sub&gt; = the weight change between node ''i'' and ''j''
: ''w''&lt;sub&gt;''ik''&lt;/sub&gt; = new weight calculated from error signal at output element
: ''k'' = learning rate
: ''o''&lt;sub&gt;''i''&lt;/sub&gt; = the activation of node ''i'' / actual output of node ''i''
: ''o''&lt;sub&gt;''j''&lt;/sub&gt; = the input vector / activation
|}

The issue of catastrophic interference, comes about when learning is sequential. Sequential training involves the network learning an input-output pattern until the error is reduced below a specific criterion, then training the network on another set of input-output patterns. Specifically, a backpropagation network will forget information if it first learns input ''A'' and then next learns input ''B''. It is not seen when learning is concurrent or interleaved. Interleaved training means the network learns both inputs-output patterns at the same time, i.e. as ''AB''. Weights are only changed when the network is being trained and not when the network is being tested on its response.

To summarize, backpropagation networks:

*Involve three-layer neural networks with input, hidden and output units
*Use a supervised learning system
*Compare the actual output to the target output
*Backwards propagate the ''error signal'' to update weights across the layers
*Learn incrementally through weight updates and eventually settle on the correct output
*Have an issue with sequential learning

===Implications for modeling human memory===

Humans often learn information in a sequential manner. For example, a child will often learn their 1st addition facts first, later followed by the 2nd addition facts, etc. It would be impossible for a child to learn all of the addition facts at the same time. Catastrophic interference can be considered an issue when modelling human memory because, unlike backpropagation networks, humans typically do not show catastrophic forgetting during sequential learning. Rather humans tend to show gradual forgetting or interference when they learn information sequentially. For example, the classic [[interference theory#Retroactive interference|retroactive interference]] study by Barnes and Underwood (1959)&lt;ref name=&quot;Barnes1959&quot;&gt;Barnes, J. M., &amp; Underwood, B. J. (1959). Fate of first-list associations in transfer theory. ''Journal of Experimental Psychology'', ''58'', 97–105.&lt;/ref&gt; used paired associate learning to determine how much new learning interfered with old learning in humans.  Paired associates, means that a pair of stimuli is and responses are learned. Their experiment used eight lists of paired associates, A-B and A-C. The pairs had the stimuli as consonant-vowel-consonant trigrams (e.g., dax) and responses as adjectives. Subjects were initially trained on the A-B list, until they could correctly recall all A-B pairings. Next subjects were given 1, 5, 10 or 20 trials on the A-C list. After learning the A-C pairs the subjects were given a final test in which the stimulus A was presented and the subject was asked to recall the response B and C. They found that as the number of learning trials on A-C list increased, the recall of C increased. But the training on A-C interfered with the recall of B. Specifically recall of B dropped to around 80% after one learning trial of A-C and to 50% after 20 learning trials of A-C. Subsequent research on the topic of retroactive interference has found similar results, with human forgetting being gradual and typically leveling off near 50% recall.&lt;ref&gt;Postman, L., &amp; Underwood, B. J. (1973). Critical issues in interference theory. ''Memory &amp; Cognition'', ''1''(1), 19-40.&lt;/ref&gt; Thus when compared with typical human retroactive interference, catastrophic interference could be likened to [[retrograde amnesia]].&lt;ref name=&quot;McCloskey1989&quot; /&gt;

Some researchers have argued that catastrophic interference is not an issue with the backpropagation model of human memory.  For example, Mirman and Spivey (2001) found that humans show more interference when learning pattern-based information.&lt;ref name=&quot;Mirman2001&quot;&gt;Mirman, D., &amp; Spivey, M. (2001). Retroactive interference in neural networks and in humans: the effect of pattern-based learning. ''Connection Science'', ''13''(3), 257-275&lt;/ref&gt; Pattern-based learning is analogous to how a standard backpropagation network learns. Thus, they concluded that catastrophic interference is not limited to connectionist memory models but rather that it is a &quot;general product of pattern-based learning that occurs in humans as well&quot; (p.&amp;nbsp;272).&lt;ref name=&quot;Mirman2001&quot; /&gt;  However, Musca, Rousset and Ans (2004) found contrasting results where retroactive interference was more pronounced in subjects who sequentially learned unstructured lists when controlling for methodological failure that occurred in the Mirman, D., &amp; Spivey, M. (2001) study.&lt;ref&gt;Musca, S.C., Rousset, S &amp; Ans, B. (2004). Differential retroactive interference in humans following exposure to structured or unstructured learning material: a single distributed neural network account. ''Connection Science'', ''16''(2), 101-118&lt;/ref&gt;

==History of catastrophic interference==

The term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).&lt;ref name=&quot;Ratcliff1990&quot; /&gt;

=== ''The Sequential Learning Problem'': McCloskey and Cohen (1989)===

McCloskey and Cohe n(1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.

* Experiment 1: ''Learning the ones and twos addition facts''
In their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 + 1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials.  Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2 + 1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number.{{clarify|date=July 2016}} This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.

* Experiment 2: ''Replication of Barnes and Underwood (1959) study''&lt;ref name=&quot;Barnes1959&quot; /&gt;
In their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.

McCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.

Overall, McCloskey and Cohen (1989) concluded that:
*at least some interference will occur whenever new learning alters the weights involved representing
*the greater the amount of new learning, the greater the disruption in old knowledge
*interference was catastrophic in the backpropagation networks when learning was sequential but not concurrent

===''Constraints Imposed by Learning and Forgetting Functions'': Ratcliff (1990)===

Ratcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned.&lt;ref name=&quot;Ratcliff1990&quot; /&gt; After inspecting the recognition performance models he found two major problems:
* Well-learned information was catastrophically forgotten as new information was learned in both small and large backpropagation networks.
Even one learning trial with new information resulted in a significant loss of the old information, paralleling the findings of McCloskey and Cohen (1989).&lt;ref name=&quot;McCloskey1989&quot; /&gt; Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference.
* Discrimination between the studied items and previously unseen items decreased as the network learned more.
This finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding 'response nodes' that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.

==Proposed solutions==

Many researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks.&lt;ref name=&quot;French1991&quot;&gt;French, R. M. (1991). Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectioniost Networks. In:  ''Proceedings of the 13th Annual Cognitive Science Society Conference'' (pp. 173-178) New Jersey: Lawrence Erlbaum.&lt;/ref&gt;&lt;ref name=&quot;McRae1993&quot;&gt;McRae, K., &amp; Hetherington, P. (1993). Catastrophic Interference is Eliminated in Pre-Trained Networks. In: ''Proceedings of the 15th Annual Conference of the Cognitive Science Society'' (pp. 723-728). Hillsdale, NJ: Lawrence Erlbaum&lt;/ref&gt;&lt;ref name=&quot;French1999&quot;&gt;French, R. M. (1999). Catastrophic forgetting in connectionist networks. ''Trends in Cognitive Sciences'', ''3''(4), 128–135.&lt;/ref&gt; In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights, where 'knowledge is stored, are changed it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input.&lt;ref name=&quot;McRae1993&quot; /&gt; Another way to conceptualize this is through visualizing learning as movement through a weight space.&lt;ref name=&quot;Lewandowsky1991&quot;&gt;Lewandowsky S. (1991). Gradual unlearning and catastrophic interference: a comparison of distributed architectures. In: Hockley WE and Lewandowsky S (eds). ''Relating theory and data: essays on human memory in honor of Bennet B. Murdock'' (pp. 445–476). Hillsdale, NJ: Lawrence Erlbaum&lt;/ref&gt; This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen.&lt;ref name=French1999 /&gt; However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern.&lt;ref name=French1999 /&gt; To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference.&lt;ref name=&quot;McRae1993&quot; /&gt;  Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.

Many of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns [[orthogonality#Definitions|orthogonal]] to one another. Lewandowsky and Li (1995)&lt;ref name=&quot;Lewandowsky1995&quot;&gt;Lewandowsky, S., &amp; Li, S-C. (1995). Catastrophic interference in neural networks: causes, solutions, and data. In: Dempster, F.N. &amp; Brainerd, C. (eds). ''Interference and Inhibition in Cognition'' (pp. 329–361). San Diego: Academic Press&lt;/ref&gt; noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0&amp;times;0 + 0&amp;times;1 + 1&amp;times;0 + 0&amp;times;0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1).&lt;ref name=&quot;French1999&quot; /&gt; Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors.&lt;ref name=&quot;Ratcliff1990&quot; /&gt; Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference&lt;ref&gt;Yamaguchi, M. (2004). Reassessment of Catastrophic Interference. ''Computational Neuroscience'', ''15''(15), 2423 – 2426&lt;/ref&gt; and other studies finding it does not.&lt;ref name=&quot;McCloskey1989&quot; /&gt;&lt;ref name=&quot;Ratcliff1990&quot; /&gt;

Below are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:

===Node sharpening technique===
French (1991)&lt;ref name=&quot;French1991&quot; /&gt;  proposed that catastrophic interference arises in [[feedforward neural network|feedforward]] backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using [binary number|binary] representation of input [row vector|vectors], activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that [[neural network]]s which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to 'semi-distributed' representations. A 'semi-distributed' representation has fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer.  French recommended that this could be done through 'activation sharpening', a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropgation). Overall the guidelines for the process of 'activation sharpening' are as follows:

# Perform a forward activation pass by feeding an input from the input layer to the hidden layer and record the activations at the hidden layer
# '''''&quot;Sharpen&quot; the activation''''' of x number of most active nodes by a sharpening factor α:
#: ''A&lt;sub&gt;new&lt;/sub&gt;'' = ''A&lt;sub&gt;old&lt;/sub&gt;'' + ''α''(1- ''A&lt;sub&gt;old&lt;/sub&gt;'') 		For nodes to be sharpened, i.e. more activated
#: ''A&lt;sub&gt;new&lt;/sub&gt;'' = ''A&lt;sub&gt;old&lt;/sub&gt;'' – ''αA&lt;sub&gt;old&lt;/sub&gt;''	           For all other nodes
#: French suggested the number of nodes to be sharpened should be log ''n'' nodes, where n is the number of hidden layer nodes
# 	Use the difference between the old activation (A&lt;sub&gt;''old''&lt;/sub&gt;) and the sharpened activation (''A&lt;sub&gt;new&lt;/sub&gt;'') as an error, backpropagate this error to the input layer, and modify the weights of input-to-output appropriately
# 	Do a full forward pass with the input through to the output layer
# 	Backpropagate as usual from the output to the input layer
# 	Repeat

In his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting ([[Herman Ebbinghaus|Ebbinghaus]] savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.

According to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed.  Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.

===Novelty rule===
Kortge (1990)&lt;ref name=&quot;Kortge1990&quot;&gt;Kortge, C. A. (1990). Episodic memory in connectionist networks. In: ''The Twelfth Annual Conference of the Cognitive Science Society'', (pp. 764-771). Hillsdale, NJ: Lawrence Erlbaum.&lt;/ref&gt; proposed a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the [[delta rule]]). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the '''''novelty vector''''' replacing the activation value (sum of the inputs):

: Δ''w''&lt;sub&gt;''ij''&lt;/sub&gt; = ''k''δ&lt;sub&gt;''i''&lt;/sub&gt; ''d''&lt;sub&gt;''i''&lt;/sub&gt;
:
: Δw&lt;sub&gt;''ij''&lt;/sub&gt; = weight change between nodes ''i'' and ''j''
: ''k'' = learning rate
: δ&lt;sub&gt;''i''&lt;/sub&gt; = error signal
: ''d''&lt;sub&gt;''i''&lt;/sub&gt;= novely vector

When the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially.&lt;ref name=&quot;Kortge1990&quot; /&gt; However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.

===Pre-training networks===

McRae and Hetherington (1993)&lt;ref name=&quot;McRae1993&quot; /&gt; argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights.

To test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs.  All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like &quot;JEP&quot; and &quot;ZEP&quot;, was greater than for dissimilar CVCs, such as &quot;JEP&quot; and &quot;YUG&quot;. This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network.  Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference.&lt;ref name=&quot;French1999&quot; /&gt; Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.

===Pseudo-recurrent networks===

French (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2).&lt;ref name =&quot;French1997&quot; /&gt; In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995).&lt;ref name=&quot;McClelland1995&quot;&gt;McClelland, J., McNaughton, B. &amp; O'Reilly, R. (1995) Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. ''Psychological Review'', ''102'', 419-457.&lt;/ref&gt; In this research McClelland et al. (1995), suggested that the [[hippocampus]] and [[neocortex]] act as separable but complementary memory systems. Specifically, the hippocampus [[short term memory]] storage and acts gradually over time to transfer memories into the neocortex for [[long term memory]] storage. They suggest that the information that is stored can be &quot;brought back&quot; to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called ''pseudopatterns''.  These pseudopatterns are approximations of previous inputs&lt;ref name =&quot;Robins1995&quot;&gt;Robins, A. (1995). Catastrophic Forgetting, rehearsal and pseudorehearal. ''Connection Science'', ''7'', 123-146.&lt;/ref&gt; and they can be interleaved with the learning of new inputs. [[File:Pseudorecurrentnetwork.jpg|thumbnail|Figure 2: The architecture of a pseudo-recurrent network]]The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information.&lt;ref name =&quot;Robins1996&quot;&gt;Robins, A. (1996). Consolidation in Neural Networks and in the Sleeping Brain. ''Connection Science'', ''8''(2), 259-276.&lt;/ref&gt; When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:

*When a pattern is fed from the environment (a real input), the information travels both to the early processing area and the final storage area, however the teacher nodes will inhibit the output from the final storage area
*The new pattern is learned by the early processing area by the standard backpropagation algorithm
*At the same time random input is also fed into the network and causes pseudopatterns to be generated by the final storage area
*Output from the final-storage area, in the form of pseudopatterns, will be used as a teacher for the early-processing area. In this way, the pseudopatterns are interleaved with the 'real inputs' from the environment
*Once the new pattern and the pseudopattern are learned by the early processing area, its weights are copied to the corresponding weights in the final storage area.

When tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.

Not only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.

=== Neural networks with self-refreshing memory ===
Following the same basic idea contributed by Robins,&lt;ref name =&quot;Robins1995&quot; /&gt;&lt;ref name =&quot;Robins1996&quot; /&gt; Ans and Rousset (1997)&lt;ref&gt;Ans, B., &amp; Rousset, S. (1997). Avoiding catastrophic forgetting by coupling two reverberating neural networks. ''CR Academie Science Paris, Life Sciences'', ''320'', 89-997.&lt;/ref&gt; have also proposed a two-network artificial neural architecture with ''memory self-refreshing'' that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a ''reverberating'' process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network ''attractors'', is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000)&lt;ref&gt;Ans, B., &amp; Rousset, S. (2000). Neural networks with a self-refreshing memory: Knowledge transfer in sequential Learning tasks without catastrophic forgetting. ''Connection Science'', ''12'', 1-19.&lt;/ref&gt; have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009)&lt;ref&gt;Musca, S. C., Rousset, S., &amp; Ans, B. (2009). Artificial neural network whispering to the brain: Nonlinear system attractors induce familiarity with never seen items. ''Connection Science'', ''21''(4), 359-377.&lt;/ref&gt; have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004)&lt;ref&gt;Ans, B. (2004). Sequential learning in distributed neural networks without catastrophic forgetting: A single and realistic self-refreshing memory can do it. ''Neural Information Processing-Letters and Reviews'', ''4'', 27-32.&lt;/ref&gt; has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.&lt;ref&gt;Xie, X.,  &amp; Seung, H. S.  (2003). Equivalence of backpropagation and Contrastive Hebbian Learning in a layered network. ''Neural Computation'', ''15'', 441-454.&lt;/ref&gt;
&lt;br /&gt;
So far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for 'static' learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004)&lt;ref&gt;Ans, B., Rousset, S., French, R. M., &amp; Musca, S. C. (2004). Self-refreshing memory in artificial neural networks: Learning temporal sequences without catastrophic forgetting. ''Connection Science'', ''16'', 71-99.&lt;/ref&gt; who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.

===Latent learning===

Latent Learning is a technique used by Gutstein &amp; Stump (2015)&lt;ref&gt;Gutstein and Stump (2015). Reduction Of Catastrophic Forgetting With Transfer Learning And Ternary Output Codes. In: Proceedings ''2015 International Joint Conference on Neural Nets'' (pp 1-8)&lt;/ref&gt; both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.

Given a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC)&lt;ref&gt;Dietterich, T. G., &amp; Bakiri, G. (1995). Solving multiclass learning problems via error-correcting output codes. ''Journal of Artificial Intelligence Research'', (pp. 263-286)&lt;/ref&gt; (as opposed to [[one-hot|1 hot codes]]), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes ''without any exposure to the new classes'', they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of [[Latent Learning]], as introduced by Tolman in 1930.&lt;ref&gt;Tolman, E.C.; C.H. Honzik (1930). &quot;&quot;Insight&quot; in Rats&quot;. University of California Publications in Psychology.&lt;/ref&gt; In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.

===Elastic weight consolidation===

Kirkpatrick et al. (2017)&lt;ref&gt;{{cite web|last1=Kirkpatrick|first1=James|title=Elastic Weight Consolidation|url=https://arxiv.org/abs/1612.00796|website=arxiv.org|accessdate=27 June 2017}}&lt;/ref&gt; demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation.

===Anapoiesis===
Practopoietic theory&lt;ref name=Nikolic2014&gt;{{cite web|url=http://www.danko-nikolic.com/?smd_process_download=1&amp;download_id=724|title=Practopoiesis: Or how life fosters a mind. arXiv:1402.5332 [q-bio.NC].|author=Danko Nikolić|date=2014|accessdate=2014-06-06}}&lt;/ref&gt; proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of ''anapoiesis'' is applied. Anapoiesis stands for &quot;reconstruction of knowledge&quot;—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the [[Variety (cybernetics)|theorems]] of [[cybernetics]] and is concerned with the question of how cybernetic systems obtain their capabilities to control and act.

==References==
{{reflist|30em}}



</text>
      <sha1>4nb6izw50l9z3nkq1j4rqhdb6v5qzz1</sha1>
    </revision>
  </page>
  <page>
    <title>Developmental robotics</title>
    <ns>0</ns>
    <id>1422176</id>
    <revision>
      <id>812359618</id>
      <parentid>807424032</parentid>
      <timestamp>2017-11-27T12:08:59Z</timestamp>
      <contributor>
        <ip>158.255.112.194</ip>
      </contributor>
      <comment>I updated the info about the AI Lab at SoftBank Robotics Europe (formerly Aldebaran).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21288">{{Use mdy dates|date=October 2014}}
'''Developmental robotics''' ('''DevRob'''), sometimes called '''[[epigenetics|epigenetic]] robotics''', is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied [[machine]]s. As in human children, [[learning]] is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with [[social relation|social interaction]]. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as [[developmental psychology]], [[neuroscience]], [[developmental biology|developmental]] and [[evolutionary biology]], and [[linguistics]], then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence developmental robotics also provides feedback and novel hypotheses on theories of human and animal development.

Developmental robotics is related to, but differs from, [[evolutionary robotics]] (ER).  ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time.

DevRob is also related to work done in the domains of [[robotics]] and [[artificial life]].

== Background ==

Can a robot learn like a child? Can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment? How can it discover its body and its relationships with the physical and social environment? How can its cognitive capacities continuously develop without the intervention of an engineer once it is &quot;out of the factory&quot;? What can it learn through natural social interactions with humans? These are the questions at the center of developmental robotics. Alan Turing, as well as a number of other pioneers of cybernetics, already formulated those questions and the general approach in 1950,&lt;ref name=&quot;Turing50&quot;&gt;{{cite journal
| last = Turing | first = A.M. | date = 1950 | url = http://www.csee.umbc.edu/courses/471/papers/turing.pdf | title = Computing machinery and intelligence | journal = Mind | publisher = LIX | issue = 236 | pages = 433–460 }}&lt;/ref&gt;
but it is only since the end of the 20th century that they began to be investigated systematically.&lt;ref name=&quot;Weng01&quot;&gt;{{cite journal
| last1 = Weng | first1 = J. | last2 = McClelland | last3 = Pentland | first3 = A. | last4 = Sporns | first4 = O. | last5 = Stockman | first5 = I. | last6 = Sur | first6 = M. | first7 = E. | last7 = Thelen | date = 2001 | url = http://www.cse.msu.edu/dl/SciencePaper.pdf | title = Autonomous mental development by robots and animals | journal = Science | volume = 291 | pages = 599–600 | doi=10.1126/science.291.5504.599}}&lt;/ref&gt;&lt;ref name=&quot;Lungarella03&quot;&gt;{{cite journal
| last1 = Lungarella | first1 = M. | last2 = Metta | first2 = G. | last3 = Pfeifer | first3 = R. | first4 = G. | last4 = Sandini | date = 2003 | title =  Developmental robotics: a survey | citeseerx = 10.1.1.83.7615 | journal = Connection Science | volume = 15 | pages = 151–190 | doi=10.1080/09540090310001655110}}&lt;/ref&gt;&lt;ref name=&quot;Asada09&quot;&gt;{{cite journal
| last1 = Asada | first1 = M. | last2 = Hosoda | first2 = K. | last3 = Kuniyoshi | first3 = Y. | last4 = Ishiguro | first4 = H. | last5 = Inui | first5 = T. | last6 = Yoshikawa | first6 = Y. | last7 = Ogino | first7 = M. | first8 = C. | last8 = Yoshida | date = 2009 | url = http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4895715&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4563672%2F5038478%2F04895715.pdf%3Farnumber%3D4895715 | title = Cognitive developmental robotics: a survey | journal = IEEE Transactions on Autonomous Mental Development | volume = 1 | issue = 1 | pages = 12–34 | doi=10.1109/tamd.2009.2021702}}&lt;/ref&gt;&lt;ref name=&quot;Oudeyer10&quot;&gt;{{cite journal
| authorlink1 = Pierre-Yves Oudeyer | last1 = Oudeyer | first1 = P-Y. | date = 2010 | url = http://www.pyoudeyer.com/IEEETAMDOudeyer10.pdf | title = On the impact of robotics in behavioral and cognitive sciences: from insect navigation to human cognitive development | journal = IEEE Transactions on Autonomous Mental Development | volume = 2 | issue = 1 | pages = 2–16 | doi=10.1109/tamd.2009.2039057}}&lt;/ref&gt;

Because the concept of adaptive intelligent machine is central to developmental robotics, it has relationships with fields such as artificial intelligence, machine learning, [[cognitive robotics]] or [[computational neuroscience]]. Yet, while it may reuse some of the techniques elaborated in these fields, it differs from them from many perspectives. It differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems. It differs from traditional machine learning because it targets task- independent self-determined learning rather than task-specific inference over &quot;spoon fed human-edited sensori data&quot; (Weng et al., 2001). It differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves. It differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning. More generally, developmental robotics is uniquely characterized by the following three features:
# It targets task-independent architectures and learning mechanisms, i.e. the machine/robot has to be able to learn new tasks that are unknown by the engineer;
# It emphasizes open-ended development and lifelong learning, i.e. the capacity of an organism to acquire continuously novel skills. This should not be understood as a capacity for learning &quot;anything&quot; or even “everything”, but just that the set of skills that is acquired can be infinitely extended at least in some (not all) directions;
# The complexity of acquired knowledge and skills shall increase (and the increase be controlled) progressively.

Developmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self- organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines, but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development.&lt;ref name=&quot;Oudeyer10&quot; /&gt;

== Research directions ==

=== Skill domains ===
Due to the general approach and methodology, developmental robotics projects typically focus on having robots develop the same types of skills as human infants. A first category that is importantly being investigated is the acquisition of sensorimotor skills. These include the discovery of one's own body, including its structure and dynamics such as hand–eye coordination, locomotion, and interaction with objects as well as tool use, with a particular focus on the discovery and learning of affordances. A second category of skills targeted by developmental robots are social and linguistic skills: the acquisition of simple social behavioural games such as turn-taking, coordinated interaction, lexicons, syntax and grammar, and the grounding of these linguistic skills into sensorimotor skills (sometimes referred as symbol grounding). In parallel, the acquisition of associated cognitive skills are being investigated such as the emergence of the self/non-self distinction, the development of attentional capabilities, of categorization systems and higher-level representations of affordances or social constructs, of the emergence of values, empathy, or theories of mind.

=== Mechanisms and constraints ===
The sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a life-time. Thus, mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity. There are several important families of these guiding mechanisms and constraints which are studied in developmental robotics, all inspired by human development:
# Motivational systems, generating internal reward signals that drive exploration and learning, which can be of two main types:
#* extrinsic motivations push robots/organisms to maintain basic specific internal properties such as food and water level, physical integrity, or light (e.g. in phototropic systems);
#* intrinsic motivations push robot to search for novelty, challenge, compression or learning progress per se, thus generating what is sometimes called curiosity-driven learning and exploration, or alternatively active learning and exploration;
#Social guidance: as humans learn a lot by interacting with their peers, developmental robotics investigates mechanisms which can allow robots to participate to human-like social interaction. By perceiving and interpreting social cues, this may allow robots both to learn from humans (through diverse means such as imitation, emulation, stimulus enhancement, demonstration, etc. ...) and to trigger natural human pedagogy. Thus, social acceptance of developmental robots is also investigated;
# Statistical inference biases and cumulative knowledge/skill reuse: biases characterizing both representations/encodings and inference mechanisms can typically allow considerable improvement of the efficiency of learning and are thus studied. Related to this, mechanisms allowing to infer new knowledge and acquire new skills by reusing previously learnt structures is also an essential field of study;
#The properties of embodiment, including geometry, materials, or innate motor primitives/synergies often encoded as dynamical systems, can considerably simplify the acquisition of sensorimotor or social skills, and is sometimes referred as morphological computation. The interaction of these constraints with other constraints is an important axis of investigation;
#Maturational constraints: In human infants, both the body and the neural system grow progressively, rather than being full-fledged already at birth. This implies for example that new degress of freedom, as well as increases of the volume and resolution of available sensorimotor signals, may appear as learning and development unfold. Transposing these mechanisms in developmental robots, and understanding how it may hinder or on the contrary ease the acquisition of novel complex skills is a central question in developmental robotics.

=== From bio-mimetic development to functional inspiration. ===
While most developmental robotics projects strongly interact with theories of animal and human development, the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots, as well as the abstraction levels of modeling, may vary a lot. While some projects aim at modeling precisely both the function and biological implementation (neural or morphological models), such as in neurorobotics, some other projects only focus on functional modeling of the mechanisms and constraints described above, and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields.

== Open questions ==

As developmental robotics is a relatively novel research field and at the same time very ambitious, many fundamental open challenges remain to be solved.

First of all, existing techniques are far from allowing real-world high-dimensional robots to learn an open- ended repertoire of increasingly complex skills over a life-time period. High-dimensional continuous sensorimotor spaces are a major obstacle to be solved. Lifelong [[cumulative learning]] is another one. Actually, no experiments lasting more than a few days have been set up so far, which contrasts severely with the time period needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms.

Among the strategies to explore in order to progress towards this target, the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically. Indeed, they have so far mainly been studied in isolation. For example, the interaction of intrinsically motivated learning and socially guided learning, possibly constrained by maturation, is an essential issue to be investigated.

Another important challenge is to allow robots to perceive, interpret and leverage the diversity of multimodal social cues provided by non-engineer humans during human-robot interaction. These capacities are so far mostly too limited to allow efficient general purpose teaching from humans.

A fundamental scientific issue to be understood and resolved, which applied equally to human development, is how compositionality, functional hierarchies, primitives, and modularity, at all levels of sensorimotor and social structures, can be formed and leveraged during development. This is deeply linked with the problem of the emergence of symbols, sometimes referred as the &quot;symbol grounding problem&quot; when it comes to language acquisition. Actually, the very existence and need for symbols in the brain is actively questioned, and alternative concepts, still allowing for compositionality and functional hierarchies are being investigated.

During biological epigenesis, morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills. The development of morphology poses obvious practical problems with robots, but it may be a crucial mechanism that should be further explored, at least in simulation, such as in morphogenetic robotics.

Another open problem is the understanding of the relation between the key phenomena investigated by developmental robotics (e.g., hierarchical and modular sensorimotor systems, intrinsic/extrinsic/social motivations, and open-ended learning) and the underlying brain mechanisms.

Similarly, in biology, developmental mechanisms (operating at the ontogenetic time scale) strongly interact with evolutionary mechanisms (operating at the phylogenetic time scale) as shown in the flourishing &quot;[[evo-devo]]&quot; scientific literature.&lt;ref name=&quot;Muller07&quot;&gt;{{cite journal
| last1 = Müller | first1 = G. B. | date = 2007 | url = http://www.nature.com/nrg/journal/v8/n12/full/nrg2219.html | title = Evo-devo: extending the evolutionary synthesis | journal = Nature Reviews Genetics | volume = 8 | pages = 943–949 | doi=10.1038/nrg2219 | pmid=17984972}}&lt;/ref&gt;
However, the interaction of those mechanisms in artificial organisms, developmental robots in particular, is still vastly understudied. The interaction of evolutionary mechanisms, unfolding morphologies and developing sensorimotor and social skills will thus be a highly stimulating topic for the future of developmental robotics.

==Main journals==
* IEEE Transactions on Autonomous Mental Development: http://www.ieee-cis.org/pubs/tamd/
* AMD Newsletter: http://www.cse.msu.edu/amdtc/amdnl/

==Main conferences==
* International Conference on Development and Learning: http://www.cogsci.ucsd.edu/~triesch/icdl/
* Epigenetic Robotics: http://www.epigenetic-robotics.org/
* ICDL-EpiRob: http://www.icdl-epirob.org/ (the two above joined since 2011)
* Developmental Robotics: http://cs.brynmawr.edu/DevRob05/
The NSF/DARPA funded [http://www.cse.msu.edu/dl/ Workshop on Development and Learning] was held April 5–7, 2000 at Michigan State University. It was the first international meeting devoted to computational understanding of mental development by robots and animals. The term &quot;by&quot; was used since the agents are active during development.

==See also==
* [[Robot learning]]

== References ==
{{reflist}}

==External links==

=== Technical committees ===
*IEEE Technical committee on Autonomous Mental Development, http://www.icdl-epirob.org/amdtc
*IEEE Technical Committee on Robot Learning, http://www.learning-robots.de/

=== Academic institutions and researchers in the field ===
* [http://www.iub.edu/~cogdev/ Cognitive Development Lab, University of Indiana, US]
* [[Michigan State University]] – [http://www.cse.msu.edu/ei Embodied Intelligence Lab]
* [http://flowers.inria.Fr Inria and Ensta ParisTech FLOWERS team, France]: Exploration, interaction and learning in developmental robotics
* [http://www.isi.imi.i.u-tokyo.ac.jp/ University of Tokyo—Intelligent Systems and Informatics Lab]
* [http://www.idsia.ch/~juergen/cogbotlab.html Cognitive Robotics Lab] of [[Juergen Schmidhuber]] at [[IDSIA]] and [[Technical University of Munich]]
* [http://www.liralab.it LIRA-Lab], University of Genova, Italy
* [https://www.cit-ec.de/ CITEC at University of Bielefeld, Germany]
* [http://matthew.siu.edu/ Vision Lab], Psychology Department, Southern Illinois University Carbondale
* [http://fias.uni-frankfurt.de/~triesch/ FIAS (J. Triesch lab.)]
* [http://nivea.psycho.univ-paris5.fr/ LPP, CNRS (K. Oregan lab.)]
* AI Lab, SoftBank Robotics Europe, France
* [http://homepages.abdn.ac.uk/f.guerin/pages/ Departement of Computer Science, University of Aberdeen]
* [http://www.er.ams.eng.osaka-u.ac.jp/asadalab/index_en.html Asada Laboratory], Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan
* The University of Texas at Austin, [http://www.cs.utexas.edu/users/qr/robotics/bootstrap-learning.html UTCS Intelligent Robotics Lab]
* [[Bryn Mawr College]]'s [http://cs.brynmawr.edu/devrob/ Developmental Robotics Project]: research projects by faculty and students at Swarthmore and Bryn Mawr Colleges, Philadelphia, PA, USA
* [http://eksl.isi.edu/cgi-bin/page.cgi?page=project-jean.html Jean Project]: Information Sciences Institute of the University of Southern California
* [http://www.nrl.navy.mil/aic/iss/aas/CognitiveRobots.php Cognitive Robotics (including Hide and Seek) at the Naval Research Laboratory]
* [http://www-robotics.cs.umass.edu/index.php The Laboratory for Perceptual Robotics], [[University of Massachusetts Amherst]] Amherst, USA
* [http://www.tech.plym.ac.uk/SOCCE/CRNS/ Centre for Robotics and Neural Systems], [http://www.plymouth.ac.uk/ Plymouth University] Plymouth, United Kingdom
* [http://www.istc.cnr.it/group/locen Laboratory of Computational Embodied Neuroscience], [http://www.istc.cnr.it/ Institute of Cognitive Science and Technologies] [https://web.archive.org/web/20140209072327/http://www.cnr.it/sitocnr/home.html National Research Council], Rome, Italy
* [http://www-etis.ensea.fr/index.php/neuro-neurocybernetics.html Neurocybernetic team], ETIS Lab., ENSEA – University of Cergy-Pontoise – CNRS, France
* [http://mpcrlab.com Machine Perception and Cognitive Robotics Lab], Florida Atlantic University,  Boca Raton, Florida

=== Related large-scale projects ===
* [http://www.robotdoc.org RobotDoC Project] (funded by European Commission)
* [http://www.italkproject.org/ Italk Project] (funded by European Commission)
* [http://www.im-clever.eu/ IM-CLeVeR Project] (funded by European Commission)
* [http://flowers.inria.Fr ERC Grant EXPLORERS Project] (funded by European Research Council)
* [http://www.robotcub.org/ RobotCub Project] (funded by European Commission)
* [[Feelix Growing Project]] (funded by European Commission)

=== Courses ===
The first undergraduate [http://dangermouse.brynmawr.edu/cs380/ courses] in DevRob were offered at [[Bryn Mawr College]] and [[Swarthmore College]] in the Spring of 2003 by Douglas Blank and Lisa Meeden, respectively.
The [http://www.cs.iastate.edu/~alex/classes/2005_Fall_610as/ first graduate course] in DevRob was offered at [[Iowa State University]] by Alexander Stoytchev in the Fall of 2005.

===Blogs and other links===
* The Mental Development Repository: http://www.mentaldev.org
* Developing Intelligence: http://develintel.blogspot.com
* Developmental Robotics: http://developmentalrobotics.org : general information about developmental robotics

{{Robotics}}


[[Category:Robot control|Learning]]
</text>
      <sha1>05f879xc2yu7flfxx7m91zkdvna5wln</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Structured prediction</title>
    <ns>14</ns>
    <id>40149461</id>
    <revision>
      <id>567084110</id>
      <timestamp>2013-08-04T07:33:48Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '{{cat main|Structured prediction}} '</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="64">{{cat main|Structured prediction}}
</text>
      <sha1>gqjsv6psdxhbe20duug11vfb17hxj5d</sha1>
    </revision>
  </page>
  <page>
    <title>Confusion matrix</title>
    <ns>0</ns>
    <id>847558</id>
    <revision>
      <id>794367294</id>
      <parentid>794366737</parentid>
      <timestamp>2017-08-07T15:36:24Z</timestamp>
      <contributor>
        <username>Loraof</username>
        <id>22399950</id>
      </contributor>
      <comment>/* Table of confusion */ ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4609">{{Confusion matrix terms|recall=}}
In the field of [[machine learning]] and specifically the problem of [[statistical classification]], a '''confusion matrix''', also known as an error matrix,&lt;ref&gt;{{cite journal |last1=Stehman |first1= Stephen V. |year= 1997|title=Selecting and interpreting measures of thematic classification accuracy |journal=Remote Sensing of Environment |volume=62 |issue=1 |pages=77–89  |doi= 10.1016/S0034-4257(97)00083-7  }}&lt;/ref&gt; is a specific table layout that allows visualization of the performance of an algorithm, typically a [[supervised learning]] one (in [[unsupervised learning]] it is usually called a '''matching matrix''').  Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).&lt;ref name=&quot;Powers2011&quot;/&gt;  The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).

It is a special kind of [[contingency table]], with two dimensions (&quot;actual&quot; and &quot;predicted&quot;), and identical sets of &quot;classes&quot; in both dimensions (each combination of dimension and class is a variable in the contingency table).

== Example ==

If a classification system has been trained to distinguish between cats, dogs and rabbits, a confusion matrix will summarize the results of testing the algorithm for further inspection.  Assuming a sample of 27 animals &amp;mdash; 8 cats, 6 dogs, and 13 rabbits, the resulting confusion matrix could look like the table below:
{|
|-
|
{| class=&quot;wikitable&quot; style=&quot;border:none; float:left; margin-top:0;&quot;
!style=&quot;background:white; border:none;&quot; colspan=&quot;2&quot; rowspan=&quot;2&quot;|
!colspan=&quot;3&quot; style=&quot;background:none;&quot;| Actual class
|-
!Cat
!Dog
!Rabbit
|-
!rowspan=&quot;3&quot; style=&quot;height:6em;&quot;|&lt;div style=&quot;{{rotate|-90}}&quot;&gt;Predicted&lt;br&gt; class&lt;/div&gt;
!Cat
|5
|2
|0
|-
!Dog
|3
|3
|2
|-
!Rabbit
|0
|1
|11
|}

| In this confusion matrix, of the 8 actual cats, the system predicted that three were dogs, and of the six dogs, it predicted that one was a rabbit and two were cats.  We can see from the matrix that the system in question has trouble distinguishing between cats and dogs, but can make the distinction between rabbits and other types of animals pretty well.  All correct predictions are located in the diagonal of the table, so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal.
|}

== Table of confusion ==
In [[predictive analytics]], a '''table of confusion''' (sometimes also called a '''confusion matrix'''), is a table with two rows and two columns that reports the number of ''false positives'', ''false negatives'', ''true positives'', and ''true negatives''. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.

Assuming the confusion matrix above, its corresponding table of confusion, for the cat class, would be:

{| class=&quot;wikitable&quot; style=&quot;border:none; float:left; margin-top:0;&quot;
!style=&quot;background:white; border:none;&quot; colspan=&quot;2&quot; rowspan=&quot;2&quot;|
!colspan=&quot;3&quot; style=&quot;background:none;&quot;| Actual class
|-
!Cat
!Non-cat
|-
!rowspan=&quot;3&quot; style=&quot;height:6em;&quot;|&lt;div style=&quot;{{rotate|-90}}&quot;&gt;Predicted&lt;br&gt; class&lt;/div&gt;
!Cat
|5 True Positives
|2 False Positives
|-
!Non-cat
|3 False Negatives
|17 True Negatives
|-
|}


The final table of confusion would contain the average values for all classes combined.

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''confusion matrix'', as follows:

{{DiagnosticTesting_Diagram}}

==References==
{{reflist}}

== External links ==
* [http://www2.cs.uregina.ca/~dbd/cs831/notes/confusion_matrix/confusion_matrix.html Theory about the confusion matrix]
* [http://www.gabormelli.com/RKB/Confusion_Matrix GM-RKB Confusion Matrix concept page]




[[de:Beurteilung eines Klassifikators#Wahrheitsmatrix: Richtige und falsche Klassifikationen]]</text>
      <sha1>iffhrdt5tur4myukss25h8ncejkyi8t</sha1>
    </revision>
  </page>
  <page>
    <title>Active learning (machine learning)</title>
    <ns>0</ns>
    <id>28801798</id>
    <revision>
      <id>815280118</id>
      <parentid>806366883</parentid>
      <timestamp>2017-12-13T22:15:24Z</timestamp>
      <contributor>
        <ip>2A01:CB08:8169:5A00:C4F:5217:8FB5:E8EB</ip>
      </contributor>
      <comment>Remove shameless self promotion</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7835">{{about|a machine learning method|active learning in the context of education|active learning}}
'''Active learning''' is a special case of [[semi-supervised learning|semi-supervised machine learning]] in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.&lt;ref name=&quot;settles&quot;&gt;{{Citation
 | title = Active Learning Literature Survey
 | url = http://pages.cs.wisc.edu/~bsettles/pub/settles.activelearning.pdf
 | author = Settles, Burr
 | journal = Computer Sciences Technical Report 1648. University of Wisconsin–Madison
 | year = 2010
 | accessdate = 2014-11-18
}}&lt;/ref&gt; &lt;ref name=&quot;rubens2016&quot;&gt;{{cite book
|last1=Rubens |first1=Neil
|last2= [https://www.linkedin.com/in/mehdielahi Elahi]|first2=Mehdi
 |last3=Sugiyama|first3=Masashi|last4=Kaplan|first4=Dain|editor1-last=Ricci
 |editor1-first=Francesco
 |editor2-last=Rokach|editor2-first=Lior
 |editor3-last=Shapira |editor3-first=Bracha
 |title=Recommender Systems Handbook
 |date=2016
 |publisher=Springer US
 |isbn=978-1-4899-7637-6
 |edition=2
 |chapter=Active Learning in Recommender Systems
 |chapter-url= https://rd.springer.com/chapter/10.1007/978-1-4899-7637-6_24
 |url = https://rd.springer.com/book/10.1007/978-1-4899-7637-6
|doi=10.1007/978-1-4899-7637-6
}}&lt;/ref&gt;  In statistics literature it is sometimes also called [[optimal experimental design]]. &lt;ref name=&quot;olsson&quot;&gt;{{cite journal | url=http://eprints.sics.se/3600/ | title=A literature survey of active machine learning in the context of natural language processing | author=Olsson, Fredrik}}&lt;/ref&gt;

There are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm be overwhelmed by uninformative examples.
Recent developments are dedicated to multi-label active learning&lt;ref name=&quot;multi&quot;/&gt;, hybrid active learning&lt;ref name=&quot;hybrid&quot;/&gt; and active learning in a single-pass (on-line) context,&lt;ref name=&quot;single-pass&quot;/&gt; combining concepts from the field of Machine Learning (e.g., conflict and ignorance) with adaptive, [[incremental learning]] policies in the field of [[Online machine learning]].

==Definitions==

Let &lt;math&gt;T&lt;/math&gt; be the total set of all data under consideration. For example, in a protein engineering problem, &lt;math&gt;T&lt;/math&gt; would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.

During each iteration, &lt;math&gt;i&lt;/math&gt;, &lt;math&gt;T&lt;/math&gt; is broken up into three subsets
#&lt;math&gt;\mathbf{T}_{K,i}&lt;/math&gt;: Data points where the label is '''known'''.
#&lt;math&gt;\mathbf{T}_{U,i}&lt;/math&gt;: Data points where the label is '''unknown'''.
#&lt;math&gt;\mathbf{T}_{C,i}&lt;/math&gt;: A subset of &lt;math&gt;T_{U,i}&lt;/math&gt; that is '''chosen''' to be labeled.

Most of the current research in active learning involves the best method to choose the data points for &lt;math&gt;T_{C,i}&lt;/math&gt;.

==Query strategies==

Algorithms for determining which data points should be labeled can be organized into a number of different categories:&lt;ref name=&quot;settles&quot; /&gt;

*Uncertainty sampling: label those points for which the current model is least certain as to what the correct output should be
*Query by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the &quot;committee&quot; disagrees the most
*Expected model change: label those points that would most change the current model
*Expected error reduction: label those points that would most reduce the model's generalization error
*Variance reduction: label those points that would minimize output variance, which is one of the components of error
*Balance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al.&lt;ref name=&quot;Bouneffouf(2014)&quot;/&gt; propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.
*Exponentiated Gradient Exploration for Active Learning:&lt;ref name=&quot;Bouneffouf(2016)&quot;/&gt; In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.

A wide variety of algorithms have been studied that fall into these categories.&lt;ref name=&quot;settles&quot; /&gt;&lt;ref name=&quot;olsson&quot; /&gt;

==Minimum Marginal Hyperplane==

Some active learning algorithms are built upon [[Support vector machine|Support vector machines (SVMs)]] and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the [[margin (machine learning)|margin]], &lt;math&gt;W&lt;/math&gt;, of each unlabeled datum in &lt;math&gt;T_{U,i}&lt;/math&gt; and treat &lt;math&gt;W&lt;/math&gt; as an &lt;math&gt;n&lt;/math&gt;-dimensional distance from that datum to the separating hyperplane.

Minimum Marginal Hyperplane methods assume that the data with the smallest &lt;math&gt;W&lt;/math&gt; are those that the SVM is most uncertain about and therefore should be placed in &lt;math&gt;T_{C,i}&lt;/math&gt; to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest &lt;math&gt;W&lt;/math&gt;. Tradeoff methods choose a mix of the smallest and largest &lt;math&gt;W&lt;/math&gt;s.


==See also==
* [[Proactive learning]]
* [[List of datasets for machine learning research]]

==Notes==
{{reflist |refs=
&lt;ref name=&quot;hybrid&quot;&gt;E. Lughofer (2012), [http://www.sciencedirect.com/science/article/pii/S0031320311003463 Hybrid Active Learning (HAL) for Reducing the Annotation Efforts of Operators in Classification Systems.] Pattern Recognition, vol. 45 (2), pp. 884-896, 2012.&lt;/ref&gt;
&lt;ref name=&quot;Bouneffouf(2014)&quot;&gt;Bouneffouf et al. (2014), [https://hal.archives-ouvertes.fr/hal-01069802 Contextual Bandit for Active Learning: Active Thompson Sampling.]  Neural Information Processing - 21st International Conference, ICONIP 2014&lt;/ref&gt;
&lt;ref name=&quot;multi&quot;&gt;Yang B, Sun J T, Wang T J, et al.(2009), [https://www.microsoft.com/en-us/research/wp-content/uploads/2009/01/sigkdd09-yang.pdf &quot;Effective Multi-Label Active Learning for Text Classification&quot;.] Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2009: 917-926.&lt;/ref&gt;
&lt;ref name=&quot;single-pass&quot;&gt;E. Lughofer (2012), [https://link.springer.com/article/10.1007/s12530-012-9060-7 Single-Pass Active Learning with Conflict and Ignorance.] Evolving Systems, vol. 3 (4), pp. 251-271, 2012.&lt;/ref&gt;
&lt;ref name=&quot;Bouneffouf(2016)&quot;&gt;Bouneffouf et al. (2016), [http://www.mdpi.com/2073-431X/5/1/1 Exponentiated Gradient Exploration for Active Learning.]  Computers, vol. 5 (1), 2016, pp. 1-12&lt;/ref&gt;
}}

==Other references==
* [http://ActiveIntelligence.org N. Rubens], [https://www.linkedin.com/in/mehdielahi M. Elahi], M. Sugiyama, D. Kaplan. Recommender Systems Handbook: Active Learning in Recommender Systems (eds. F. Ricci, P.B. Kantor, L. Rokach, B. Shapira). Springer, 2015 [https://www.researchgate.net/publication/296481444_Active_Learning_in_Recommender_Systems], [http://activeintelligence.org/research/al-rs/].
* [http://hunch.net/~active_learning/ Active Learning Tutorial], S. Dasgupta and J. Langford.

</text>
      <sha1>3dahxc67b2qcqefcwfb25o5fef06z5n</sha1>
    </revision>
  </page>
  <page>
    <title>Grammar induction</title>
    <ns>0</ns>
    <id>4375576</id>
    <revision>
      <id>815626821</id>
      <parentid>815541805</parentid>
      <timestamp>2017-12-16T00:40:10Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v478)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14495">{{Machine learning bar}}
'''Grammar induction''' (or '''grammatical inference'''&lt;ref name=&quot;Grammatical Inference&quot;&gt;{{cite book|last=de la Higuera|first=Colin|title=Grammatical Inference: Learning Automata and Grammars|date=2010|publisher=Cambridge University Press|location=Cambridge|url=http://bootcamp.lif.univ-mrs.fr/de-la-higuera.pdf}}&lt;/ref&gt;) is the process in [[machine learning]] of learning a [[formal grammar]] (usually as a collection of ''re-write rules'' or ''[[productions (computer science)|productions]]'' or alternatively as a [[finite state machine]] or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.

==Grammar classes==

Grammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article [[Induction of regular languages]] for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.

Since the beginning of the century, these approaches have been extended to the problem of inference of [[context-free grammars]] and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.
Other classes of grammars for which grammatical inference has been studied are contextual grammars and pattern languages.

==Learning models==

The simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).
However, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin&lt;ref&gt;{{cite journal|author=Dana Angluin |title=Learning Regular Sets from Queries and Counter-Examples |journal=[[Information and Control]] |year=1987 |volume=75 |pages=87–106 |url=http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf |doi=10.1016/0890-5401(87)90052-6 |deadurl=yes |archiveurl=https://web.archive.org/web/20131202232143/http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf |archivedate=2013-12-02 |df= }}&lt;/ref&gt;.

==Methodologies==
There is a wide variety of methods for grammatical inference.  Two of the classic sources are {{Harvtxt|Fu|1977}} and {{Harvtxt|Fu|1982}}. {{Harvtxt|Duda|Hart|Stork|2001}} also devote a brief section to the problem, and cite a number of references.  The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of [[regular languages]] in particular, see ''[[Induction of regular languages]]''. A more recent textbook is de la Higuera (2010),&lt;ref name = &quot;Grammatical Inference&quot;/&gt; which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni&lt;ref&gt;D’Ulizia, A., Ferri, F., Grifoni, P. (2011) &quot;[https://www.academia.edu/download/41900378/A_survey_of_grammatical_inference_method20160202-5760-79hwcu.pdf A Survey of Grammatical Inference Methods for Natural Language Learning]&quot;, ''Artificial Intelligence Review'', Vol. 36, No. 1, pp. 1–27.&lt;/ref&gt; provide a survey that explores grammatical inference methods for natural languages.

===Grammatical inference by trial-and-error===
The method proposed in Section 8.7 of {{Harvtxt|Duda|Hart|Stork|2001}} suggests successively guessing grammar rules (productions) and testing them against positive and negative observations.  The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded.  This particular approach can be characterized as &quot;hypothesis testing&quot; and bears some similarity to Mitchel's [[version space]] algorithm. The {{Harvtxt|Duda|Hart|Stork|2001}} text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.

=== Grammatical inference by genetic algorithms ===
Grammatical induction using [[evolutionary algorithm]]s is the process of evolving a representation of the grammar of a target language through some evolutionary process. [[Formal grammar]]s can easily be represented as [[tree (data structure)|tree structures]] of production rules that can be subjected to evolutionary operators. [[Algorithm]]s of this sort stem from the [[genetic programming]] paradigm pioneered by [[John Koza]].{{Citation needed|date=August 2007}} Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the [[Extended Backus–Naur form|EBNF]] language made trees a more flexible approach.

Koza represented [[Lisp (programming language)|Lisp]] programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the [[grammatical function|functions]] of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.

In the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a [[terminal symbol]] of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a [[noun phrase]] or a [[verb phrase]]) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.

===Grammatical inference by greedy algorithms===
Like all [[greedy algorithm]]s, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.
The decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.
Because there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.

These [[context-free grammar]] generating algorithms make the decision after every read symbol:
* [[LZW|Lempel-Ziv-Welch algorithm]] creates a context-free grammar in a deterministic way such that it is necessary to store only the start rule of the generated grammar.
* [[Sequitur algorithm|Sequitur]] and its modifications.

These context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:
* [[Byte pair encoding]] and its optimizations.

===Distributional learning===
A more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning [[context-free grammars]] and [[mildly context-sensitive language]]s and have been proven to be correct and efficient for large subclasses of these grammars.&lt;ref&gt;Clark and Eyraud (2007) ''Journal of Machine Learning Research''; Ryo Yoshinaka (2011) ''Theoretical Computer Science''&lt;/ref&gt;

===Learning of [[Pattern language (formal languages)|pattern languages]]===

Angluin defines a ''pattern'' to be &quot;a string of constant symbols from Σ and '''variable symbols''' from a disjoint set&quot;.
The language of such a pattern is the set of all its nonempty ground instances  i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.&lt;ref group=note&gt;The language of a pattern with at least two occurrences of the same variable is not regular due to the [[Pumping lemma for regular languages|pumping lemma]].&lt;/ref&gt;
A pattern is called '''descriptive''' for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.

Angluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable ''x''.&lt;ref group=note&gt;''x'' may occur several times, but no other variable ''y'' may occur&lt;/ref&gt;
To this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on ''x'' being the only variable, the state count can be drastically reduced.&lt;ref&gt;{{cite journal| author=Dana Angluin| title=Finding Patterns Common to a Set of Strings| journal=Journal of Computer and System Sciences| year=1980| volume=21| pages=46–62| url=http://www.sciencedirect.com/science/article/pii/0022000080900410/pdf?md5=c3534f6c086df22fbf814b12984fab5e&amp;pid=1-s2.0-0022000080900410-main.pdf| doi=10.1016/0022-0000(80)90041-0}}&lt;/ref&gt;

Erlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.&lt;ref&gt;{{cite book|author1=T. Erlebach |author2=P. Rossmanith |author3=H. Stadtherr |author4=A. Steger |author4-link=Angelika Steger|author5=T. Zeugmann | chapter=Learning One-Variable Pattern Languages Very Efficiently on Average, in Parallel, and by Asking Queries| title=Proc. 8th International Workshop on Algorithmic Learning Theory — ALT'97| year=1997| volume=1316| pages=260–276| publisher=Springer|editor1=M. Li |editor2=A. Maruoka | series=LNAI}}&lt;/ref&gt;

Arimura et al. show that a language class  obtained from limited unions of patterns can be learned in polynomial time.&lt;ref&gt;{{cite book|author1=Hiroki Arimura |author2=Takeshi Shinohara |author3=Setsuko Otsuki | chapter=Finding Minimal Generalizations for Unions of Pattern Languages and Its Application to Inductive Inference from Positive Data| title=Proc. STACS 11| year=1994| volume=775| pages=649–660| publisher=Springer| series=LNCS|url=http://ai2-s2-pdfs.s3.amazonaws.com/6a4c/0482e0030b0e5791cf75b0edd9f55fdfc10e.pdf}}&lt;/ref&gt;

===Pattern theory===
[[Pattern theory]], formulated by [[Ulf Grenander]],&lt;ref&gt;Grenander, Ulf, and Michael I. Miller. ''[http://www.ulb.tu-darmstadt.de/tocs/185410162.pdf Pattern theory: from representation to inference]''. Vol. 1. Oxford: Oxford university press, 2007.&lt;/ref&gt; is a mathematical [[Formalism (mathematics)|formalism]] to describe knowledge of the world as patterns. It differs from other approaches to [[artificial intelligence]] in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.

In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:
* Identify the [[Latent variable|hidden variables]] of a data set using real world data rather than artificial stimuli, which was commonplace at the time.
* Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.
* Study the randomness and variability of these graphs.
* Create the basic classes of stochastic models applied by listing the deformations of the patterns.
* Synthesize (sample) from the models, not just analyze signals with it.
Broad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.

== Applications ==
The principle of grammar induction has been applied to other aspects of [[natural language processing]], and has been applied (among many other problems) to [[morpheme]] analysis, and place name derivations. Grammar induction has also been used for [[lossless data compression]] and [[statistical inference]] via [[minimum message length]] (MML) and [[minimum description length]] (MDL) principles.{{citation needed|date=August 2017}}

==See also==
* [[Artificial grammar learning#Artificial intelligence]]
* [[Inductive inference]]
* [[Inductive programming]]
* [[Kolmogorov complexity]]
* [[Language identification in the limit]]
* [[Straight-line grammar]]
* [[Syntactic pattern recognition]]

==Notes==
{{reflist|group=note}}

==References==
{{Reflist}}

==Sources==
* {{Citation
  | last=Duda | first=Richard O.| last2=Hart| first2=Peter E.
  | last3=Stork| first3=David G.
  | title=Pattern Classification | publisher=John Wiley &amp; Sons
  | place=[[New York City|New York]] | year=2001| edition=2
  | url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html}}
* {{Citation
  | last=Fu | first=King Sun
  | title=Syntactic Pattern Recognition and Applications
  | publisher=Prentice-Hall | place=[[Englewood Cliffs, NJ]]
  | year=1982}}
* {{Citation
  | last=Fu | first=King Sun
  | title=Syntactic Pattern Recognition, Applications
  | publisher=Springer-Verlag | place=[[Berlin]] | year=1977}}
* {{Citation
  | last=Horning | first=James Jay
  | title=A Study of Grammatical Inference
  | publisher=Stanford University Computer Science Department
  | place=[[Stanford]] | year=1969 | edition=Ph.D. Thesis
  | url=http://proquest.umi.com/pqdlink?Ver=1&amp;Exp=05-16-2013&amp;FMT=7&amp;DID=757518381&amp;RQT=309&amp;attempt=1&amp;cfc=1}}
* {{Citation
  | last=Gold | first=E. Mark
  | title=Language Identification in the Limit
  | url=http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html
  | year=1967| volume=10
  | pages=447–474
  | publisher=[[Information and Control]] }}
* {{Citation
  | last=Gold | first=E. Mark
  | title=Language Identification in the Limit
  | volume=10
  | pages=447–474
  | url=http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf
  | publisher=[[Information and Control]] | year=1967}}






</text>
      <sha1>s195dmerbqbale58qg8xctcdm3le4li</sha1>
    </revision>
  </page>
  <page>
    <title>Pattern language (formal languages)</title>
    <ns>0</ns>
    <id>40946774</id>
    <revision>
      <id>777376123</id>
      <parentid>777375807</parentid>
      <timestamp>2017-04-26T20:27:41Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>/* Location in the Chomsky hierarchy */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15333">In [[theoretical computer science]], a '''pattern language''' is a [[formal language]] that can be defined as the set of all particular instances of a [[string (formal languages)|string]] of constants and variables. Pattern Languages were introduced by [[Dana Angluin]] in the context of [[machine learning]].&lt;ref&gt;{{cite journal| author=Dana Angluin| title=Finding Patterns Common to a Set of Strings| journal=Journal of Computer and System Sciences| year=1980| volume=21| pages=46–62| url=http://www.sciencedirect.com/science/article/pii/0022000080900410/pdf?md5=c3534f6c086df22fbf814b12984fab5e&amp;pid=1-s2.0-0022000080900410-main.pdf| doi=10.1016/0022-0000(80)90041-0}}&lt;/ref&gt;

== Definition ==
Given a finite set Σ of '''[[Term (logic)#Formal definition|constant]]''' symbols and a countable set ''X'' of '''[[Term (logic)#Formal definition|variable]]''' symbols disjoint from Σ, a '''pattern''' is a finite [[empty string|non-empty]] [[String (computer science)|string]] of symbols from Σ∪''X''.
The '''length''' of a pattern ''p'', denoted by |''p''|, is just the number of its symbols.
The set of all patterns containing exactly ''n'' distinct variables (each of which may occur several times) is denoted by ''P''&lt;sub&gt;''n''&lt;/sub&gt;, the set of all patterns at all by ''P''&lt;sub&gt;*&lt;/sub&gt;.
A '''substitution''' is a mapping ''f'': ''P''&lt;sub&gt;*&lt;/sub&gt; → ''P''&lt;sub&gt;*&lt;/sub&gt; such that&lt;ref group=note&gt;Angluin's notion of substitution differs from the usual notion of [[string substitution]].&lt;/ref&gt;
* ''f'' is a [[monoid homomorphism|homomorphism]] with respect to [[String_(computer_science)#Concatenation_and_substrings|string concatenation]] (⋅), formally: ∀''p'',''q''∈''P''&lt;sub&gt;*&lt;/sub&gt;. ''f''(''p''⋅''q'') = ''f''(''p'')⋅''f''(''q'');
* ''f'' is non-erasing, formally: ∀''p''∈''P''&lt;sub&gt;*&lt;/sub&gt;. ''f''(''p'') ≠ ε, where ε denotes the [[empty string]]; and
* ''f'' respects constants, formally: ∀''s''∈Σ. ''f''(''s'') = ''s''.
If ''p'' = ''f''(''q'') for some patterns ''p'', ''q'' ∈ ''P''&lt;sub&gt;*&lt;/sub&gt; and some substitution ''f'', then ''p'' is said to be '''less general than''' ''q'', written ''p''≤''q'';
in that case, necessarily |''p''| ≥ |''q''| holds.
For a pattern ''p'', its '''language''' is defined as the set of all less general patterns that are built from constants only, formally: ''L''(''p'') = { ''s'' ∈ Σ&lt;sup&gt;+&lt;/sup&gt; : ''s'' ≤ ''p'' }, where [[Kleene plus|Σ&lt;sup&gt;+&lt;/sup&gt;]] denotes the set of all finite non-empty strings of symbols from Σ.

For example, using the constants Σ = { 0, 1 } and the variables ''X'' = { ''x'', ''y'', ''z'', ... }, the pattern 0''x''10''xx''1 ∈''P''&lt;sub&gt;1&lt;/sub&gt; and ''xxy'' ∈''P''&lt;sub&gt;2&lt;/sub&gt; has length 7 and 3, respectively.
An instance of the former pattern is 00''z''100''z''0''z''1 and 01''z''101''z''1''z''1, it is obtained by the substitution that maps ''x'' to 0''z'' and to 1''z'', respectively, and each other symbol to itself. Both 00''z''100''z''0''z''1 and 01''z''101''z''1''z''1 are also instances of ''xxy''. In fact, ''L''(0''x''10''xx''1) is a subset of ''L''(''xxy''). The language of the pattern ''x''0 and ''x''1 is the set of all bit strings which denote an even and odd [[binary number]], respectively. The language of ''xx'' is the set of all strings obtainable by concatenating a bit string with itself, e.g. 00, 11, 0101, 1010, 11101110 ∈ ''L''(''xx'').

== Properties ==
{| style=&quot;float:right&quot;
| [[File:NP-hardness of pattern language membership svg.svg|thumb|600px|NP-hardness of pattern language membership, by [[Reduction (complexity)|reduction]] from the [[NP-complete]] [[Boolean_satisfiability_problem#Exactly-1_3-satisfiability|1-in-3-SAT problem]]: Given a [[Conjunctive normal form|CNF]] of ''m'' clauses with ''n'' variables, a pattern of length 3''n''+4''m''+1 with 2''n'' variables and a string of length ''4''n+5''m''+1 can be constructed as shown (''m''=3 and ''n''=4 in the example). Upper-case variables in the pattern correspond to negated variables in the CNF.  The string matches the pattern if and only if an assignment exists such that in each clause exactly one literal is 1 (meaning &quot;''true''&quot; in the CNF). In the left part, e.g. &quot;0''wW''0&quot; is matched by &quot;01110&quot; just if one of ''w'',''W'' is matched by &quot;1&quot; (corresponding to &quot;''false''&quot;) and the other by &quot;11&quot; (corresponding to &quot;''true''&quot;), i.e. if ''w'' corresponds to the negation of ''W''. In the right part, e.g. &quot;0''xYZ''0&quot; is matched by &quot;011110&quot; just if exactly one of ''x'',''Y'',''Z'' is matched by &quot;11&quot; and the others by &quot;1&quot;, i.e. if exactly one literal corresponds to &quot;''true''&quot;.]]
|}
The problem of deciding whether ''s'' ∈ ''L''(''p'') for an arbitrary string ''s'' ∈ Σ&lt;sup&gt;+&lt;/sup&gt; and pattern ''p'' is [[NP-complete]] (see picture),
and so is hence the problem of deciding ''p'' ≤ ''q'' for arbitrary patterns ''p'', ''q''.&lt;ref&gt;Theorem 3.6, p.50; Corollary 3.7, p.52&lt;/ref&gt;

The class of pattern languages is '''not closed''' under ...
* union: e.g. for Σ = {0,1} as [[#Definition|above]], ''L''(01)∪''L''(10) is not a pattern language;
* complement: Σ&lt;sup&gt;+&lt;/sup&gt; \ ''L''(0) is not a pattern language;
* intersection: ''L''(''x''0''y'')∩''L''(''x''1''y'') is not a pattern language;
* [[Kleene plus]]: ''L''(0)&lt;sup&gt;+&lt;/sup&gt; is not a pattern language;
* homomorphism: ''f''(''L''(''x'')) = ''L''(0)&lt;sup&gt;+&lt;/sup&gt; is not a pattern language, assuming ''f''(0) = 0 = ''f''(1);
* [[String_operations#String_homomorphism|inverse homomorphism]]: ''f''&lt;sup&gt;−1&lt;/sup&gt;(111) = { 01, 10, 000 } is not a pattern language, assuming ''f''(0) = 1 and ''f''(1) = 11.
The class of pattern languages is '''closed''' under ...
* concatenation: ''L''(''p'')⋅''L''(''q'') = ''L''(''p''⋅''q'');
* reversal: ''L''(''p'')&lt;sup&gt;rev&lt;/sup&gt; = ''L''(''p''&lt;sup&gt;rev&lt;/sup&gt;).&lt;ref&gt;Theorem 3.10, p.53&lt;/ref&gt;

If ''p'', ''q'' ∈ ''P''&lt;sub&gt;1&lt;/sub&gt; are patterns containing exactly one variable, then ''p'' ≤ ''q'' if and only if ''L''(''p'') ⊆ ''L''(''q'');
the same equivalence holds for patterns of equal length.&lt;ref&gt;Lemma 3.9, p.52; Corollary 3.4, p.50&lt;/ref&gt;
For patterns of different length, the [[#Definition|above]] example ''p'' = 0''x''10''xx''1 and ''q'' = ''xxy'' shows that ''L''(''p'') ⊆ ''L''(''q'') may hold without implying ''p'' ≤ ''q''.
However, any two patterns ''p'' and ''q'', of arbitrary lengths, generate the same language if and only if they are equal up to consistent variable renaming.&lt;ref&gt;Theorem 3.5, p.50&lt;/ref&gt;
Each pattern ''p'' is a [[Anti-unification_(computer_science)#Anti-unification_problem.2C_generalization_set|common generalization]] of all strings in its generated language ''L''(''p''), modulo associativity of (⋅).

==Location in the Chomsky hierarchy==
In a refined [[Chomsky hierarchy]], the class of pattern languages is a proper superclass and subclass of the singleton&lt;ref group=note&gt;i.e. languages consisting of a single string; they correspond to [[straight-line grammar]]s&lt;/ref&gt; and the [[indexed language]]s, respectively, but incomparable to the language classes in between; due to the latter, the pattern language class is not explicitly shown in the table [[#References|below]].

The class of pattern languages is incomparable with the class of [[finite language]]s, with the class of [[regular language]]s, and with the class of [[context-free language]]s:
* the pattern language ''L''(''xx'') is not context-free (hence neither [[Regular language#Deciding whether a language is regular|regular]] nor [[Regular language#Formal definition|finite]]) due to the [[pumping lemma for context-free languages|pumping lemma]];
* the finite (hence also regular and context-free) language { 01, 10 } is not a pattern language.
Each singleton language is trivially a pattern language, generated by a pattern without variables.

Each pattern language can be produced by an [[indexed grammar]]:
For example, using Σ = { ''a'', ''b'', ''c'' } and ''X'' = { '''''x''''', '''''y''''' },
the pattern ''a'' '''''x''''' ''b'' '''''y''''' ''c'' '''''x''''' ''a'' '''''y''''' ''b'' is generated by a grammar with nonterminal symbols ''N'' = { ''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;, ''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;, ''S'' } ∪ ''X'', terminal symbols ''T'' = Σ, index symbols ''F'' = { ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt;, ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;, ''c''&lt;sub&gt;'''''x'''''&lt;/sub&gt;, ''a''&lt;sub&gt;'''''y'''''&lt;/sub&gt;, ''b''&lt;sub&gt;'''''y'''''&lt;/sub&gt;, ''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; }, start symbol ''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;, and the following production rules:
{|
|-
| ''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[σ]
| →   ''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ]
| | | ''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ]
| | | ''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[''c''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ]
| | | ''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[σ]
|-
| ''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[σ]
| →   ''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[''a''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ]
| | | ''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[''b''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ]
| | | ''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ]
| | | ''S''[σ]
|-
| ''S''[σ]
| colspan=4 | → ''a'' '''''x'''''[σ] ''b'' '''''y'''''[σ] ''c'' '''''x'''''[σ] ''a'' '''''y'''''[σ] ''b''
|}
{|
|-
| '''''x'''''[''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ] || → ''a'' || '''''x'''''[σ] &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
| '''''y'''''[''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ] || →       || '''''y'''''[σ] &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
|-
| '''''x'''''[''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ] || → ''b'' || '''''x'''''[σ]
| '''''y'''''[''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ] || →       || '''''y'''''[σ]
|-
| '''''x'''''[''c''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ] || → ''c'' || '''''x'''''[σ]
| '''''y'''''[''c''&lt;sub&gt;'''''x'''''&lt;/sub&gt; σ] || →       || '''''y'''''[σ]
|-
| '''''x'''''[''a''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ] || →       || '''''x'''''[σ]
| '''''y'''''[''a''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ] || → ''a'' || '''''y'''''[σ]
|-
| '''''x'''''[''b''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ] || →       || '''''x'''''[σ]
| '''''y'''''[''b''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ] || → ''b'' || '''''y'''''[σ]
|-
| '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ] || →       || '''''x'''''[σ]
| '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; σ] || → ''c'' || '''''y'''''[σ]
|-
| '''''x'''''[] || → ε ||
| '''''y'''''[] || → ε
|}

An example derivation is:

{{nowrap|''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[]}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;]}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''S''&lt;sub&gt;'''''x'''''&lt;/sub&gt;[''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;]}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;]}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''S''&lt;sub&gt;'''''y'''''&lt;/sub&gt;[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;]}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''S''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;]}}
&amp;nbsp; ⇒  &amp;nbsp; {{nowrap|''a'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒  &amp;nbsp; {{nowrap|''a'' '''''x'''''[''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒  &amp;nbsp; {{nowrap|''a'' ''a'' '''''x'''''[''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒  &amp;nbsp; {{nowrap|''a'' ''ab'' '''''x'''''[] ''b'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒  &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒ ... ⇒ &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' ''c'' '''''y'''''[] ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' ''c'' ''c'' '''''x'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒ ... ⇒ &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' ''c'' ''c'' ''ab'' '''''x'''''[] ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' ''c'' ''c'' ''ab'' ''a'' '''''y'''''[''c''&lt;sub&gt;'''''y'''''&lt;/sub&gt; ''a''&lt;sub&gt;'''''x'''''&lt;/sub&gt; ''b''&lt;sub&gt;'''''x'''''&lt;/sub&gt;] ''b''}}
&amp;nbsp; ⇒ ... ⇒ &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' ''c'' ''c'' ''ab'' ''a'' ''c'' '''''y'''''[] ''b''}}
&amp;nbsp; ⇒ &amp;nbsp; {{nowrap|''a'' ''ab'' ''b'' ''c'' ''c'' ''ab'' ''a'' ''c'' ''b''}}

In a similar way, an index grammar can be constructed from any pattern.

== Learning patterns ==
Given a sample set ''S'' of strings, a pattern ''p'' is called '''descriptive''' of ''S'' if ''S'' ⊆ ''L''(''p''), but not ''S'' ⊆ ''L''(''q'') ⊂ ''L''(''p'') for any other pattern ''q''.

Given any sample set ''S'', a descriptive pattern for ''S'' can be computed by
* enumerating all patterns (up to variable renaming) not longer than the shortest string in ''S'',
* selecting from them the patterns that generate a superset of ''S'',
* selecting from them the patterns of maximal length, and
* selecting from them a pattern that is minimal with respect to ≤.&lt;ref&gt;Theorem 4.1, p.53&lt;/ref&gt;
Based on this algorithm, the class of pattern languages can be [[Language identification in the limit|identified in the limit]] from positive examples.&lt;ref&gt;{{cite journal| author=Dana Angluin| title=Inductive Inference of Formal Languages from Positive Data| journal=Information and Control| year=1980| volume=45| pages=117-135| url=http://www-personal.umich.edu/~yinw/papers/Angluin80.pdf| doi=10.1016/s0019-9958(80)90285-5}}; here: Example 1, p.125&lt;/ref&gt;

==Notes==
{{reflist|group=note}}
== References ==
{{reflist}}
{{formal languages and grammars}}



</text>
      <sha1>5v3z8xzbf0ea5o3k34wu0wuq1afbmee</sha1>
    </revision>
  </page>
  <page>
    <title>Bayesian optimization</title>
    <ns>0</ns>
    <id>40973765</id>
    <revision>
      <id>814238111</id>
      <parentid>797629844</parentid>
      <timestamp>2017-12-07T16:46:46Z</timestamp>
      <contributor>
        <username>Dendisuhubdy</username>
        <id>28104646</id>
      </contributor>
      <minor/>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6894">'''Bayesian optimization''' is a sequential design strategy
for global [[optimization]] of black-box functions&lt;ref&gt;Jonas Mockus (2013). Bayesian approach to global optimization: theory and applications. Kluwer Academic.&lt;/ref&gt; that [[Derivative-free optimization|doesn't require derivatives]].

==History==
The term is generally attributed to [[Jonas Mockus]] and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.&lt;ref&gt;Jonas Mockus: On Bayesian Methods for Seeking the Extremum. Optimization Techniques 1974: 400-404&lt;/ref&gt;&lt;ref&gt;Jonas Mockus: On Bayesian Methods for Seeking the Extremum and their Application. IFIP Congress 1977: 195-200&lt;/ref&gt;&lt;ref&gt;J. Mockus, Bayesian Approach to Global Optimization. Kluwer Academic Publishers, Dordrecht, 1989&lt;/ref&gt;

==Strategy==

Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a [[Prior distribution|prior]] over it.
The prior captures our beliefs about the behaviour of the function.
After gathering the function evaluations, which are treated as data, the prior is updated
to form the [[posterior distribution]] over the objective function.
The posterior distribution, in turn, is used to construct
an acquisition function (often also referred to as infill sampling criteria) that determines what the next query point should be.

==Examples==
Examples of acquisition functions include probability of improvement,
expected improvement, Bayesian expected losses, upper confidence bounds (UCB), [[Thompson sampling]]
and mixtures of these.&lt;ref&gt;Matthew W. Hoffman, Eric Brochu, [[Nando de Freitas]]: Portfolio Allocation for Bayesian Optimization. Uncertainty in Artificial Intelligence (UAI): 327–336 (2011)&lt;/ref&gt; They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are very expensive to evaluate.

==Solution methods==
The maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer.

==Applications==
The approach has been applied to solve a wide range of problems,&lt;ref&gt;Eric Brochu, Vlad M. Cora, Nando de Freitas: A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. CoRR abs/1012.2599 (2010)&lt;/ref&gt; including learning to rank,&lt;ref&gt;Eric Brochu, Nando de Freitas, Abhijeet Ghosh: Active Preference Learning with Discrete Choice Data. NIPS 2007&lt;/ref&gt; interactive animation,&lt;ref&gt;Eric Brochu, Tyson Brochu, Nando de Freitas: A Bayesian Interactive Optimization Approach to Procedural Animation Design. Symposium on Computer Animation 2010: 103–112&lt;/ref&gt; [[robotics]],&lt;ref&gt;Daniel J. Lizotte, Tao Wang, Michael H. Bowling, Dale Schuurmans: Automatic Gait Optimization with Gaussian Process Regression. IJCAI 2007: 944–949&lt;/ref&gt;&lt;ref&gt;Ruben Martinez-Cantin, Nando de Freitas, Eric Brochu, Jose Castellanos and Arnaud Doucet. [https://link.springer.com/article/10.1007%2Fs10514-009-9130-2# A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot]. Autonomous Robots. Volume 27, Issue 2, pp 93–103 (2009)&lt;/ref&gt;&lt;ref&gt;Scott Kuindersma, Roderic Grupen, and Andrew Barto. [http://ijr.sagepub.com/content/32/7/806.abstract# Variable Risk Control via Stochastic Optimization]. International Journal of Robotics Research, volume 32, number 7, pp 806–825 (2013)&lt;/ref&gt;&lt;ref&gt;    Roberto Calandra, André Seyfarth, Jan Peters, and Marc P. Deisenroth [https://link.springer.com/article/10.1007%2Fs10472-015-9463-9 Bayesian optimization for learning gaits under uncertainty]. Ann. Math. Artif. Intell. Volume 76, Issue 1, pp 5-23 (2016) DOI:10.1007/s10472-015-9463-9&lt;/ref&gt; [[sensor networks]],&lt;ref&gt;Niranjan Srinivas, Andreas Krause, Sham M. Kakade, Matthias W. Seeger: Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting. IEEE Transactions on Information Theory 58(5):3250–3265 (2012)&lt;/ref&gt;&lt;ref&gt;Roman Garnett, Michael A. Osborne, Stephen J. Roberts: Bayesian optimization for sensor set selection. IPSN 2010: 209–219&lt;/ref&gt; automatic algorithm configuration,&lt;ref&gt;Frank Hutter, Holger Hoos, and Kevin Leyton-Brown (2011). [http://www.cs.ubc.ca/labs/beta/Projects/SMAC/papers/11-LION5-SMAC.pdf Sequential model-based optimization for general algorithm configuration], Learning and Intelligent Optimization&lt;/ref&gt; automatic [[machine learning]] toolboxes,&lt;ref&gt;J. Bergstra, D. Yamins, D. D. Cox (2013).
Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms.
Proc. SciPy 2013.&lt;/ref&gt;&lt;ref&gt;Chris Thornton, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown: Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms. KDD 2013: 847–855&lt;/ref&gt;&lt;ref&gt;Jasper Snoek, Hugo Larochelle and Ryan Prescott Adams. Practical Bayesian Optimization of Machine Learning Algorithms. Neural Information Processing Systems, 2012&lt;/ref&gt; [[reinforcement learning]], planning, visual attention, architecture configuration in [[deep learning]], static program analysis, etc.

==See also==
* [[Multi-armed bandit]]
* [[Thompson sampling]]
* [[Global optimization]]
* [[Bayesian experimental design]]

==References==
{{reflist}}

==External links==
* [http://bayesopt.com/ BayesOpt], NIPS workshop on Bayesian Optimization (BayesOpt).
* [https://sheffieldml.github.io/GPyOpt/ GPyOpt], Python open-source library for Bayesian Optimization based on [https://github.com/SheffieldML/GPy GPy].
* [https://rmcantin.bitbucket.io/html/ Bayesopt], an efficient implementation in C/C++ with support for Python, Matlab and Octave.
* [https://github.com/HIPS/Spearmint Spearmint], a Python implementation focused on parallel and cluster computing.
* [https://github.com/hyperopt/hyperopt Hyperopt], a Python implementation for hyperparamenter optimization.
* [http://www.cs.ubc.ca/labs/beta/Projects/SMAC/ SMAC], a Java implementation of random-forest-based Bayesian optimization for general algorithm configuration.
* [https://github.com/mwhoffman/pybo pybo], a Python implementation of modular Bayesian optimization.
* [https://bitbucket.org/mlcircus/bayesopt.m Bayesopt.m], a Matlab implementation of Bayesian optimization with or without constraints.
* [https://github.com/yelp/MOE MOE] MOE is a Python/C++/CUDA implementation of Bayesian Global Optimization using Gaussian Processes.
* [https://sigopt.com/ SigOpt] SigOpt offers Bayesian Global Optimization as a SaaS service.
* [https://github.com/mila-udem/metaopt Metaopt], a Python implementation of hyperparameter optimization focused on parallel and cluster computing.




</text>
      <sha1>7v4sjcon4ddss85xg8zr1co8oyf4w8o</sha1>
    </revision>
  </page>
  <page>
    <title>Early stopping</title>
    <ns>0</ns>
    <id>213214</id>
    <revision>
      <id>797650365</id>
      <parentid>748816617</parentid>
      <timestamp>2017-08-28T12:34:12Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Bots/Requests for approval/KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13338">In [[machine learning]], '''early stopping''' is a form of [[regularization (mathematics)|regularization]] used to avoid [[overfitting]] when training a learner with an iterative method, such as [[gradient descent]]. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased [[generalization error]]. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.

==Background==
This section presents some of the basic machine-learning concepts required for a description of early stopping methods.

===Overfitting===
{{Main article|Overfitting}}
[[File:Overfitting on Training Set Data.pdf|thumb|This image represents the problem of overfitting in machine learning. The red dots represent training set data. The green line represents the true functional relationship, while the blue line shows the learned function, which has fallen victim to overfitting.]]

[[Machine learning]] algorithms train a model based on a finite set of training data. During this training, the model is evaluated based on how well it predicts the observations contained in the training set. In general, however, the goal of a machine learning scheme is to produce a model that generalizes, that is, that predicts previously unseen observations. Overfitting occurs when a model fits the data in the training set well, while incurring larger [[generalization error]].

===Regularization===
{{Main article|Regularization (mathematics)}}
Regularization, in the context of machine learning, refers to the process of modifying a learning algorithm so as to prevent overfitting. This generally involves imposing some sort of smoothness constraint on the learned model.&lt;ref&gt;{{Cite journal
| doi = 10.1162/neco.1995.7.2.219
| issn = 0899-7667
| volume = 7
| issue = 2
| pages = 219–269
| last = Girosi
| first = Federico
|author2=Michael Jones |author3=Tomaso Poggio
 | title = Regularization Theory and Neural Networks Architectures
| journal = Neural Computation
| accessdate = 2013-12-14
| date = 1995-03-01
| url = https://dx.doi.org/10.1162/neco.1995.7.2.219
}}&lt;/ref&gt;
This smoothness may be enforced explicitly, by fixing the number of parameters in the model, or by augmenting the cost function as in [[Tikhonov regularization]]. Tikhonov regularization, along with [[principal component regression]] and many other regularization schemes, fall under the umbrella of [[spectral regularization]], regularization characterized by the application of a filter. Early stopping also belongs to this class of methods.

===Gradient descent methods===
{{Main article|Gradient descent}}
Gradient descent methods are first-order, iterative, optimization methods. Each iteration updates an approximate solution to the optimization problem by taking a step in the direction of the negative of the gradient of the objective function. By choosing the step-size appropriately, such a method can be made to converge to a local minimum of the objective function. Gradient descent is used in machine-learning by defining a ''loss function'' that reflects the error of the learner on the training set and then minimizing that function.

==Early stopping based on analytical results==

===Early stopping in [[statistical learning theory]]===
Early-stopping can be used to regularize [[non-parametric regression]] problems encountered in [[machine learning]]. For a given input space, &lt;math&gt;X&lt;/math&gt;, output space, &lt;math&gt;Y&lt;/math&gt;, and samples drawn from an unknown probability measure, &lt;math&gt;\rho&lt;/math&gt;, on &lt;math&gt;Z = X \times Y&lt;/math&gt;, the goal of such problems is to approximate a ''regression function'', &lt;math&gt;f_{\rho}&lt;/math&gt;, given by

:&lt;math&gt; f_{\rho}(x) = \int_{Y} y d\rho(y|x), x \in X&lt;/math&gt;,

where &lt;math&gt;\rho(y|x)&lt;/math&gt; is the conditional distribution at &lt;math&gt;x&lt;/math&gt; induced by &lt;math&gt;\rho&lt;/math&gt;.&lt;ref name=&quot;smale_learning_2007&quot;&gt;{{Cite journal
| doi = 10.1007/s00365-006-0659-y
| issn = 0176-4276
| volume = 26
| issue = 2
| pages = 153–172
| last = Smale
| first = Steve
|author2=Ding-Xuan Zhou
| title = Learning Theory Estimates via Integral Operators and Their Approximations
| journal = Constructive Approximation
| accessdate = 2013-12-15
| date = 2007-08-01
| url = https://link.springer.com/article/10.1007/s00365-006-0659-y
}}&lt;/ref&gt;
One common choice for approximating the regression function is to use functions from a [[reproducing kernel Hilbert space]].&lt;ref name=&quot;smale_learning_2007&quot;/&gt; These spaces can be infinite dimensional, in which they can supply solutions that overfit training sets of arbitrary size. Regularization is, therefore, especially important for these methods. One way to regularize non-parametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent.

The early stopping rules proposed for these problems are based on analysis of upper bounds on the generalization error as a function of the iteration number. They yield prescriptions for the number of iterations to run that can be computed prior to starting the solution process.&lt;ref name=&quot;yao_early_2007&quot;&gt;{{Cite journal
| doi = 10.1007/s00365-006-0663-2
| issn = 0176-4276
| volume = 26
| issue = 2
| pages = 289–315
| last = Yao
| first = Yuan
|author2=Lorenzo Rosasco |author3=Andrea Caponnetto
 | title = On Early Stopping in Gradient Descent Learning
| journal = Constructive Approximation
| accessdate = 2013-12-05
| date = 2007-08-01
| url = https://link.springer.com/article/10.1007/s00365-006-0663-2
}}&lt;/ref&gt;
&lt;ref name=&quot;raskutti_early_2011&quot;&gt;{{Cite conference
| doi = 10.1109/Allerton.2011.6120320
| conference = 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)
| pages = 1318–1325
| last = Raskutti
| first = G. |author2=M.J. Wainwright |author3=Bin Yu
| title = Early stopping for non-parametric regression: An optimal data-dependent stopping rule
| booktitle = 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)
| year = 2011
}}&lt;/ref&gt;

====Example: Least-squares loss====
(Adapted from Yao, Rosasco and Caponnetto, 2007&lt;ref name=&quot;yao_early_2007&quot;/&gt;)

Let &lt;math&gt;X\subseteq\mathbb{R}^{n}&lt;/math&gt; and &lt;math&gt;Y=\mathbb{R}&lt;/math&gt;. Given a set of samples

:&lt;math&gt;\mathbf{z} = \left \{(x_{i},y_{i}) \in X \times Y: i = 1, \dots, m\right\} \in Z^{m}&lt;/math&gt;,

drawn independently from &lt;math&gt;\rho&lt;/math&gt;, minimize the functional

:&lt;math&gt;
\mathcal{E}(f) = \int_{X\times Y}\left(f(x) - y\right)^2 d\rho
&lt;/math&gt;

where, &lt;math&gt;f&lt;/math&gt; is a member of the reproducing kernel Hilbert space &lt;math&gt;\mathcal{H}&lt;/math&gt;. That is, minimize the expected risk for a Least-squares loss function. Since &lt;math&gt;\mathcal{E}&lt;/math&gt; depends on the unknown probability measure &lt;math&gt;\rho&lt;/math&gt;, it cannot be used for computation. Instead, consider the following empirical risk

:&lt;math&gt;
\mathcal{E}_{\mathbf{z}}(f) = \frac{1}{m} \sum_{i=1}^{m} \left(f(x_{i}) - y_{i}\right)^{2}.
&lt;/math&gt;

Let &lt;math&gt;f_{t}&lt;/math&gt; and &lt;math&gt;f_{t}^{\mathbf{z}}&lt;/math&gt; be the ''t''-th iterates of gradient descent applied to the expected and empirical risks, respectively, where both iterations are initialized at the origin, and both use the step size &lt;math&gt;\gamma_{t}&lt;/math&gt;. The &lt;math&gt;f_{t}&lt;/math&gt; form the ''population iteration'', which converges to &lt;math&gt;f_{\rho}&lt;/math&gt;, but cannot be used  in computation, while the &lt;math&gt;f_{t}^{\mathbf{z}}&lt;/math&gt; form the ''sample iteration'' which usually converges to an overfitting solution.

We want to control the difference between the expected risk of the sample iteration and the minimum expected risk, that is, the expected risk of the regression function:

:&lt;math&gt;\mathcal{E}(f_{t}^{\mathbf{z}}) - \mathcal{E}(f_{\rho})&lt;/math&gt;

This difference can be rewritten as the sum of two terms: the difference in expected risk between the sample and population iterations and that between the population iteration and the regression function:

:&lt;math&gt;\mathcal{E}(f_{t}^{\mathbf{z}}) - \mathcal{E}(f_{\rho}) = \left[ \mathcal{E}(f_{t}^{\mathbf{z}}) - \mathcal{E}(f_{t})\right] + \left[ \mathcal{E}(f_{t}) - \mathcal{E}(f_{\rho})\right]&lt;/math&gt;

This equation presents a [[Bias-variance dilemma|bias-variance tradeoff]], which is then solved to give an optimal stopping rule that may depend on the unknown probability distribution. That rule has associated probabilistic bounds on the generalization error. For the analysis leading to the early stopping rule and bounds, the reader is referred to the original article.&lt;ref name=&quot;yao_early_2007&quot;/&gt; In practice, data-driven methods, e.g. cross-validation can be used to obtain an adaptive stopping rule.

===Early stopping in boosting===
[[Boosting (machine learning)|Boosting]] refers to a family of algorithms in which a set of '''weak learners''' (learners that are only slightly correlated with the true process) are combined to produce a '''strong learner'''. It has been shown, for several boosting algorithms (including [[AdaBoost]]), that regularization via early stopping can provide guarantees of [[consistency (statistics)|consistency]], that is, that the result of the algorithm approaches the true solution as the number of samples goes to infinity.&lt;ref&gt;{{Cite journal
| doi = 10.1214/aos/1079120128
| issn = 0090-5364
| volume = 32
| issue = 1
| pages = 13–29
| last = Wenxin Jiang
| title = Process consistency for AdaBoost
| journal = The Annals of Statistics
| accessdate = 2013-12-05
| date = February 2004
| url = http://projecteuclid.org/euclid.aos/1079120128
}}&lt;/ref&gt;
&lt;ref&gt;{{Cite journal
| issn = 0162-1459
| volume = 98
| issue = 462
| pages = 324–339
| last = Bühlmann
| first = Peter
|author2=Bin Yu
| title = Boosting with the L₂ Loss: Regression and Classification
| journal = Journal of the American Statistical Association
| date = 2003-06-01
| jstor = 30045243
| doi=10.1198/016214503000125
}}&lt;/ref&gt;
&lt;ref&gt;{{Cite journal
| issn = 0090-5364
| volume = 33
| issue = 4
| pages = 1538–1579
| last = Tong Zhang
|author2=Bin Yu
| title = Boosting with Early Stopping: Convergence and Consistency
| journal = The Annals of Statistics
| date = 2005-08-01
| jstor = 3448617
| doi=10.1214/009053605000000255
}}&lt;/ref&gt;

====L{{sub|2}}-boosting====
Boosting methods have close ties to the gradient descent methods described [[#Early stopping in non-parametric regression|above]] can be regarded as a boosting method based on the &lt;math&gt;L_{2}&lt;/math&gt; loss: ''L{{sub|2}}Boost''.&lt;ref name=&quot;yao_early_2007&quot;/&gt;

==Validation-based early stopping==
These early stopping rules work by splitting the original training set into a new training set and a [[validation set]]. The error on the validation set is used as a proxy for the [[generalization error]] in determining when overfitting has begun. These methods are most commonly employed in the training of [[artificial neural network|neural networks]]. Prechelt gives the following summary of a naive implementation of [[holdout method|holdout]]-based early stopping as follows:&lt;ref name=&quot;prechelt_early_2012&quot;&gt;{{Cite book
| publisher = Springer Berlin Heidelberg
| isbn = 978-3-642-35289-8
| pages = 53–67
| editors = Grégoire Montavon, [[Klaus-Robert Müller]] (eds.)
| last = Prechelt
| first = Lutz
|author2=Geneviève B. Orr
| title = Neural Networks: Tricks of the Trade
| chapter = Early Stopping — But When?
| series = Lecture Notes in Computer Science
| accessdate = 2013-12-15
| date = 2012-01-01
| chapterurl = https://link.springer.com/chapter/10.1007/978-3-642-35289-8_5
}}&lt;/ref&gt;

{{Quotation|1=&lt;nowiki /&gt;
# Split the training data into a training set and a validation set, e.g. in a 2-to-1 proportion.
# Train only on the training set and evaluate the per-example error on the validation set once in a while, e.g. after every fifth epoch.
# Stop training as soon as the error on the validation set is higher than it was the last time it was checked.
# Use the weights the network had in that previous step as the result of the training run.|2=Lutz Prechelt|3=''Early Stopping – But When?''}}

More sophisticated forms use [[cross-validation (statistics)|cross-validation]]&amp;nbsp;– multiple partitions of the data into training set and validation set&amp;nbsp;– instead of a single partition into a training set and validation set. Even this simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.&lt;ref name=&quot;prechelt_early_2012&quot;/&gt;

==See also==
* [[Overfitting]], early stopping is one of methods used to prevent overfitting
* [[Generalization error]]
* [[Regularization (mathematics)]]
* [[Statistical learning theory]]
* [[Boosting (machine learning)]]
* [[Cross-validation (statistics)|Cross-validation]], in particular using a &quot;validation set&quot;
* [[Artificial neural network|Neural networks]]

==References==
{{Reflist}}


</text>
      <sha1>3vcsgrgp3jpeq81qsgbe6tbb0k1xo2g</sha1>
    </revision>
  </page>
  <page>
    <title>Inductive programming</title>
    <ns>0</ns>
    <id>41644056</id>
    <revision>
      <id>815658004</id>
      <parentid>815657865</parentid>
      <timestamp>2017-12-16T06:12:02Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>adding a link using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24292">{{Programming paradigms}}
'''Inductive programming''' ('''IP''') is a special area of [[automatic programming]], covering research from [[artificial intelligence]] and [[Computer programming|programming]], which addresses [[machine learning|learning]] of typically [[declarative programming|declarative]] ([[logic programming|logic]] or [[functional programming|functional]]) and often [[recursion|recursive]] programs from incomplete specifications, such as input/output examples or constraints.

Depending on the programming language used, there are several kinds of inductive programming. '''Inductive functional programming''', which uses functional programming languages such as [[Lisp (programming language)|Lisp]] or [[Haskell (programming language)|Haskell]], and most especially [[inductive logic programming]], which uses logic programming languages such as [[Prolog]] and other logical representations  such as [[description logics]], have been more prominent, but other (programming) language paradigms have also been used, such as [[constraint programming]] or [[probabilistic programming language|probabilistic programming]].

== Definition ==

Inductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete ([[formal specification|formal]]) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, [[Tracing (software)|traces]] or action sequences which describe the process of calculating specific outputs, [[Constraint (mathematics)|constraints]] for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard [[data type]]s, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.

Output of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of [[Turing completeness|Turing-complete]] [[Knowledge representation and reasoning|representation]] language.

In many applications the output program must be correct with respect to the examples and partial specification,  and this leads to the consideration of inductive programming as a special area inside automatic programming or [[program synthesis]],&lt;ref&gt;{{cite journal|first1=A.W.|last1=Biermann|title=Automatic programming|editor1-first=S.C.|editor1-last=Shapiro|publisher=Wiley|journal=Encyclopedia of artificial intelligence|pages=18–35|year=1992}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|first1=C.|last1=Rich|first2=R.C.|last2=Waters|title=Approaches to automatic programming|editor1-first=M.C.|editor1-last=Yovits|publisher=Academic Press|journal=Advances in computers|volume=37|year=1993|url=http://www.merl.com/publications/docs/TR92-04.pdf}}&lt;/ref&gt; usually opposed to 'deductive' program synthesis,&lt;ref&gt;{{cite book|editor1-first=M.L.|editor1-last=Lowry|editor2-first=R.D.|editor2-last=McCarthy|title=Automatic software design|year=1991}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|first1=Z.|last1=Manna|first2=R.|last2=Waldinger|title=Fundamentals of deductive program synthesis|journal=IEEE Trans Softw Eng|volume=18 | issue = 8|pages=674–704|year=1992|doi=10.1109/32.153379}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|first1=P.|last1=Flener|title=Achievements and prospects of program synthesis|editor1-first=A.|editor1-last=Kakas|editor2-first=F.|editor2-last=Sadri|publisher=Springer|journal=Computational logic: logic programming and beyond; essays in honour of Robert A. Kowalski|volume=LNAI 2407|pages=310–346|year=2002|doi=10.1007/3-540-45628-7_13}}&lt;/ref&gt; where the specification is usually complete.

In other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general [[machine learning]], the more specific area of [[structure mining]] or the area of [[symbolic artificial intelligence]]. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.

The diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as [[functional logic programming]], [[constraint programming]], [[probabilistic programming language|probabilistic programming]], [[abductive logic programming]], [[modal logic]], [[action language]]s, agent languages and many types of [[imperative languages]].

== History ==

Research on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers&lt;ref&gt;{{cite journal|first1=P.D.|last1=Summers|title=A methodology for LISP program construction from examples|journal=J ACM|volume=24 | issue = 1|pages=161–175|year=1977|doi=10.1145/321992.322002}}&lt;/ref&gt; and work of Biermann.&lt;ref&gt;{{cite journal|first1=A.W.|last1=Biermann|title=The inference of regular LISP programs from examples|journal=IEEE Trans Syst Man Cybern|volume=8 | issue = 8|pages=585–600|year=1978|doi=10.1109/tsmc.1978.4310035}}&lt;/ref&gt;
These approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program.  The main results until the mid 1980s are surveyed by Smith.&lt;ref&gt;{{cite journal|first1=D.R.|last1=Smith|title=The synthesis of LISP programs from examples: a survey|editor1-first=A.W.|editor1-last=Biermann|editor2-first=G.|editor2-last=Guiho|publisher=Macmillan|journal=Automatic program construction techniques|pages=307–324|year=1984|url=https://www.researchgate.net/profile/Douglas_Smith13/publication/239059541_The_Synthesis_of_LISP_Programs_from_Examples_A_Survey/links/5696c9f208ae1c427903e3c3.pdf}}&lt;/ref&gt; Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.

The advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro&lt;ref&gt;{{cite book|first1=E.Y.|last1=Shapiro|title=Algorithmic program debugging|publisher=The MIT Press|year=1983}}&lt;/ref&gt; eventually spawning the new field of inductive logic programming (ILP).&lt;ref&gt;{{Cite journal | last1 = Muggleton | first1 = S. | title = Inductive logic programming | doi = 10.1007/BF03037089 | journal = New Generation Computing | volume = 8 | issue = 4 | pages = 295–318 | year = 1991 | pmid =  | pmc = }}&lt;/ref&gt; The early works of Plotkin,&lt;ref&gt;{{cite journal|first1=Gordon D.|last1=Plotkin|title=A Note on Inductive Generalization|editor1-first=B.|editor1-last=Meltzer|editor2-first=D.|editor2-last=Michie|publisher=Edinburgh University Press|journal=Machine Intelligence|volume=5|pages=153–163|year=1970|url=http://homepages.inf.ed.ac.uk/gdp/publications/MI5_note_ind_gen.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|first1=Gordon D.|last1=Plotkin|title=A Further Note on Inductive Generalization|editor1-first=B.|editor1-last=Meltzer|editor2-first=D.|editor2-last=Michie|publisher=Edinburgh University Press|journal=Machine Intelligence|volume=6|pages=101–124|year=1971}}&lt;/ref&gt; and his &quot;''relative least general generalization (rlgg)''&quot;, had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM.&lt;ref&gt;{{cite journal|first1=S.H.|last1=Muggleton|first2=C.|last2=Feng|title=Efficient induction of logic programs|publisher=the Japanese Society for AI|journal=Proceedings of the Workshop on Algorithmic Learning Theory|volume=6|pages=368–381|year=1990|url=https://pdfs.semanticscholar.org/6a66/9636e0ada62a0fb444e95435e24fdbdf4dbd.pdf}}&lt;/ref&gt; But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs&lt;ref&gt;{{cite journal
|first1=J.R.|last1=Quinlan|first2=R.M.|last2=Cameron-Jones
|title=Avoiding Pitfalls When Learning Recursive Theories
|publisher=
|journal=IJCAI
|pages=1050–1057
|year=1993
|url=https://pdfs.semanticscholar.org/4af3/18f1d267889faec6ecf1be6bc5fe570838dd.pdf}}
&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=J.R.|last1=Quinlan|first2=R.M.|last2=Cameron-Jones
|title=Induction of logic programs: FOIL and related systems
|publisher=Springer
|volume=13 |issue=3-4|pages=287–312
|year=1995
|url=http://dottorato.di.uniba.it/dottoratoXXVI/dm/FOILvsRelatedSystems.pdf}}
&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=P.|last1=Flener|first2=S.|last2=Yilmaz
|title=Inductive synthesis of recursive logic programs: Achievements and prospects
|journal=The Journal of Logic Programming
|volume=41 | issue = 2
|pages=141–195
|year=1999|doi=10.1016/s0743-1066(99)00028-x}}
&lt;/ref&gt; with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in [[relational data mining]] and knowledge discovery.&lt;ref&gt;{{citation|first1=Sašo|last1=Džeroski|contribution=Inductive Logic Programming and Knowledge Discovery in Databases|pages=117–152|editor1-first=U.M.|editor1-last=Fayyad|editor2-first=G.|editor2-last=Piatetsky-Shapiro|editor3-first=P.|editor3-last=Smith|editor4-first=R.|editor4-last=Uthurusamy|title=Advances in Knowledge Discovery and Data Mining|publisher=MIT Press|year=1996}}&lt;/ref&gt;

In parallel to work in ILP, Koza&lt;ref&gt;
{{cite book
|first1=J.R.|last1=Koza
|title=Genetic Programming: vol. 1, On the programming of computers by means of natural selection
|publisher=MIT Press
|year=1992
|url=https://books.google.com/books?id=Bhtxo60BV0EC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false}}
&lt;/ref&gt; proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE&lt;ref&gt;
{{cite journal
|first1=J.R.|last1=Olsson
|title=Inductive functional programming using incremental program transformation
|publisher=Elsevier
|journal=Artificial Intelligence
|volume=74 | issue = 1|pages=55–83
|year=1995|doi=10.1016/0004-3702(94)00042-y}}
&lt;/ref&gt; and the systematic-search-based system MagicHaskeller.&lt;ref&gt;
{{cite journal
|first1=Susumu|last1=Katayama
|title=Efficient exhaustive generation of functional programs using Monte-Carlo search with iterative deepening
|journal=PRICAI 2008: Trends in Artificial Intelligence
|pages=199–210
|year=2008
|url=http://nautilus.cs.miyazaki-u.ac.jp/~skata/skatayama_pricai2008.pdf}}
&lt;/ref&gt; Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.

The early work in [[grammar induction]] (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem.&lt;ref&gt;
{{cite journal
|first1=D.|last1=Angluin|first2=Smith|last2=C.H.
|title=Inductive inference: Theory and methods
|publisher=ACM
|journal=ACM Computing Surveys
|volume=15|pages=237–269
|year=1983|doi=10.1145/356914.356918}}
&lt;/ref&gt; The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold.&lt;ref&gt;
{{cite journal
 |first1=E.M.
 |last1=Gold
 |title=Language identification in the limit
 |publisher=Elsevier
 |journal=Information and Control
 |volume=10
 |issue=5
 |pages=447–474
 |url=http://www.isrl.uiuc.edu/~amag/langev/paper/gold67limit.html
 |year=1967
 |doi=10.1016/s0019-9958(67)91165-5
 |deadurl=yes
 |archiveurl=https://web.archive.org/web/20090125120159/http://www.isrl.uiuc.edu/~amag/langev/paper/gold67limit.html
 |archivedate=2009-01-25
 |df=
}}
&lt;/ref&gt; More recently, the language learning problem was addressed by the inductive programming community.&lt;ref&gt;{{cite journal|first1=Stephen|last1=Muggleton|title=Inductive Logic Programming: Issues, Results and the Challenge of Learning Language in Logic|journal=Artificial Intelligence|volume=114|pages=283–296|year=1999|doi=10.1016/s0004-3702(99)00067-3}}; here: Sect.2.1&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=J.R.|last1=Olsson|first2=D.M.W.|last2=Powers
|title=Machine learning of human language through automatic programming
|publisher=University of New South Wales
|journal=Proceedings of the International Conference on Cognitive Science
|pages=507–512
|year=2003}}
&lt;/ref&gt;

In the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).

Other ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures;&lt;ref&gt;
{{cite journal
|first1=J.W.|last1=Lloyd
|title=Knowledge Representation, Computation, and Learning in Higher-order Logic
|year=2001
|url=http://users.cecs.anu.edu.au/~jwl/logic.pdf}}
&lt;/ref&gt;&lt;ref&gt;
{{cite book
|first1=J.W.|last1=Lloyd
|title=Logic for learning: learning comprehensible theories from structured data
|publisher=Springer
|year=2003}}
&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=V.|last1=Estruch|first2=C.|last2=Ferri|first3=J.|last3=Hernandez-Orallo|first4=M.J.|last4=Ramirez-Quintana
|title=Bridging the gap between distance and generalization
|publisher=Wiley
|journal=Computational Intelligence
|year=2014
|url=https://pdfs.semanticscholar.org/48f2/2821220555f8e327c2aa9614fb28c98f9542.pdf}}
&lt;/ref&gt; abstraction has also been explored as a more powerful approach to [[cumulative learning]] and function invention.&lt;ref&gt;
{{cite journal
|first1=R.J.|last1=Henderson|first2=S.H.|last2=Muggleton
|title=Automatic invention of functional abstractions
|publisher=Imperial College Press
|journal=Advances in Inductive Logic Programming
|year=2012
|url=http://ilp11.doc.ic.ac.uk/short_papers/ilp2011_submission_62.pdf}}
&lt;/ref&gt;&lt;ref&gt;
{{cite arXiv
|first1=H.|last1=Irvin|first2=A.|last2=Stuhlmuller|first3=N.D.|last3=Goodman
|title=Inducing probabilistic programs by Bayesian program merging|arxiv=1110.5667
|year=2011}}
&lt;/ref&gt;

One powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of [[generative model]]s) is [[probabilistic programming language|probabilistic programming]] (and related paradigms, such as stochastic logic programs and Bayesian logic programming).&lt;ref&gt;
{{cite journal
|first1=S.|last1=Muggleton
|title=Learning stochastic logic programs
|journal=Electron. Trans. Artif. Intell.
|volume=4(B)
|pages=141–153
|year=2000
|url=https://ocs.aaai.org/Papers/Workshops/2000/WS-00-06/WS00-06-006.pdf}}
&lt;/ref&gt;&lt;ref&gt;
{{cite book
|first1=L.|last1=De Raedt|first2=K.|last2=Kersting
|title=Probabilistic inductive logic programming
|publisher=Springer
|year=2008}}
&lt;/ref&gt;&lt;ref&gt;
{{cite arXiv
|first1=H.|last1=Irvin|first2=A.|last2=Stuhlmuller|first3=N.D.|last3=Goodman
|title=Inducing probabilistic programs by Bayesian program merging|arxiv=1110.5667
|year=2011}}
&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=A.|last1=Stuhlmuller|first2=N.D.|last2=Goodman
|title=Reasoning about reasoning by nested conditioning: Modeling theory of mind with probabilistic programs
|publisher=Elsevier
|journal=Cognitive Systems Research
|year=2012
|url=https://pdfs.semanticscholar.org/7179/dc966bbfedeccd65fdb2f4b6f1e95f1cb073.pdf}}
&lt;/ref&gt;

== Application areas ==

The [http://www.cogsys.wiai.uni-bamberg.de/aaip05/objectives.html first workshop on Approaches and Applications of Inductive Programming (AAIP) ] held in conjunction with [[ICML]] 2005 identified all applications where &quot;learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations&quot;.

Since then, these and many other areas have shown to be successful application niches for inductive programming, such as [[End-user development|end-user programming]],&lt;ref&gt;
{{cite book
|first1=H.|last1=Lieberman|first2=F.|last2=Paternò|first3=V.|last3=Wulf
|title=End user development
|publisher=Springer
|year=2006}}
&lt;/ref&gt; the related areas of [[programming by example]]&lt;ref&gt;
{{cite book
|first1=H.|last1=Lieberman
|title=Your wish is my command: Programming by example
|publisher=Morgan Kaufmann
|year=2001
|url=https://books.google.com/books?id=wM2JYafw11gC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false}}
&lt;/ref&gt; and [[programming by demonstration]],&lt;ref&gt;
{{cite journal
|first1=E.|last1=Cypher|first2=D.C.|last2=Halbert
|title=Watch what I do: programming by demonstration |publisher=
|journal=
|volume=|pages=
|year=
|url=https://books.google.com/books?id=Ggzjo0-W1y0C&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false}}
&lt;/ref&gt; and [[intelligent tutoring system]]s.

Other areas where inductive inference has been recently applied are [[knowledge acquisition]],&lt;ref&gt;
{{cite journal
|first1=U.|last1=Schmid|first2=M.|last2=Hofmann|first3=E.|last3=Kitzelmann
|title=Analytical inductive programming as a cognitive rule acquisition devise
|journal=Proceedings of the Second Conference on Artificial General Intelligence
|pages=162–167
|year=2009
|url=http://www.cogsys.wiai.uni-bamberg.de/publications/cognigor-final.pdf}}
&lt;/ref&gt; [[artificial general intelligence]],&lt;ref&gt;
{{cite journal
|first1=N.|last1=Crossley|first2=E.|last2=Kitzelmann|first3=M.|last3=Hofmann|first4=U.|last4=Schmid
|title=Combining analytical and evolutionary inductive programming
|journal=Proceedings of the Second Conference on Artificial General Intelligence
|pages=19–24
|year=2009}}
&lt;/ref&gt; [[reinforcement learning]] and theory evaluation,&lt;ref&gt;
{{cite journal
|first1=J.|last1=Hernandez-Orallo
|title=Constructive reinforcement learning
|journal=International Journal of Intelligent Systems
|volume=15 | issue = 3|pages=241–264
|year=2000
|doi=10.1002/(sici)1098-111x(200003)15:3&lt;241::aid-int6&gt;3.0.co;2-z}}
&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=C.|last1=Kemp|first2=N.|last2=Goodman|first3=J.B.|last3=Tenenbaum
|title=Learning and using relational theories
|journal=Advances in Neural Information Processing Systems
|pages=753–760
|year=2007}}
&lt;/ref&gt; and [[cognitive science]] in general.&lt;ref&gt;
{{cite journal
|first1=U.|last1=Schmid|first2=E.|last2=Kitzelmann
|title=Inductive rule learning on the knowledge level
|journal=Cognitive Systems Research
|volume=12 | issue = 3
|pages=237–248
|year=2011|doi=10.1016/j.cogsys.2010.12.002}}
&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|first1=A.|last1=Stuhlmuller|first2=N.D.|last2=Goodman
|title=Reasoning about reasoning by nested conditioning: Modeling theory of mind with probabilistic programs
|publisher=Elsevier
|journal=Cognitive Systems Research
|year=2012}}
&lt;/ref&gt; There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.

== See also ==
* [[Automatic programming]]
* [[Declarative programming]]
* [[Evolutionary programming]]
* [[Functional programming]]
* [[Genetic programming]]
* [[Grammar induction]]
* [[Inductive reasoning]]
* [[Inductive logic programming]]
&lt;!---* [[Inductive functional programming]]---commented out since it redirects here---&gt;
* [[Logic programming]]
* [[Machine learning]]
* [[Probabilistic programming language]]
* [[Program synthesis]]
* [[Programming by example]]
* [[Programming by demonstration]]
* [[Structure mining]]
* [[Test-driven development]]&lt;!-- starting with input/output examples and manually producing a program that satisfies them --&gt;

== External links ==
* [http://www.inductive-programming.org/ Inductive Programming community page], hosted by the University of Bamberg.

== Further reading ==
{{Refbegin}}

* {{cite journal|first1=P.|last1=Flener|first2=U.|last2=Schmid|title=An introduction to inductive programming|publisher=Springer|journal=Artificial Intelligence Review|volume=29 | issue = 1|pages=45–62|year=2008|doi=10.1007/s10462-009-9108-7}}
* {{cite journal|first1=E.|last1=Kitzelmann|title=Inductive programming: A survey of program synthesis techniques|publisher=Springer|journal=Approaches and Applications of Inductive Programming|pages=50–73|year=2010|url=http://emanuel.kitzelmann.org/documents/publications/Kitzelmann2010.pdf}}
* {{cite journal|first1=D.|last1=Partridge|title=The case for inductive programming|publisher=IEEE|journal=Computer|volume=30 | issue = 1|pages=36–41|year=1997|doi=10.1109/2.562924}}
* {{cite journal|first1=P.|last1=Flener|first2=D.|last2=Partridge|title=Inductive Programming|publisher=Springer|journal=Automated Software Engineering|volume=8 | issue = 2|pages=131–137|year=2001|doi=10.1023/a:1008797606116}}
* {{cite journal|first1=M.|last1=Hofmann|first2=E.|last2=Kitzelmann|title=A unifying framework for analysis and evaluation of inductive programming systems|journal=Proceedings of the Second Conference on Artificial General Intelligence|pages=55–60|year=2009|url=http://www.atlantis-press.com/php/download_paper.php?id=1839}}
* {{Cite journal | last1 = Muggleton | first1 = S. | last2 = De Raedt | doi = 10.1016/0743-1066(94)90035-3 | first2 = L. | title = Inductive Logic Programming: Theory and methods | journal = The Journal of Logic Programming | volume = 19-20 | pages = 629–679 | year = 1994 | pmid =  | pmc = }}
* {{cite book | first1 = N. | last1 = Lavrac | first2 = S. | last2 = Dzeroski | title = Inductive Logic Programming: Techniques and Applications | publisher = Ellis Horwood | location = New York | year = 1994 | isbn = 0-13-457870-8  }} https://web.archive.org/web/20040906084947/http://www-ai.ijs.si/SasoDzeroski/ILPBook/
* {{cite journal|first1=S.|last1=Muggleton|first2=Luc.|last2=De Raedt|first3=D.|last3=Poole|first4=I.|last4=Bratko|first5=P.|last5=Flach|first6=K.|last6=Inoue|first7=A.|last7=Srinivasan|title=ILP turns 20|publisher=Springer|journal=Machine Learning|volume=86 | issue = 1|pages=3–23|year=2012|doi=10.1007/s10994-011-5259-2}}
* {{cite journal|first1=S.|last1=Galwani|first2=J.|last2=Hernandez-Orallo|first3=E.|last3=Kitzelmann|first4=S.H.|last4=Muggleton|first5=U.|last5=Schmid|first6=B.|last6=Zorn|title=Inductive Programming Meets the Real World|publisher=ACM|journal=Communications of the ACM|volume=58 | issue = 11|pages=90–99|year=2015|doi=10.1145/2736282|url=http://cacm.acm.org/magazines/2015/11/193326-inductive-programming-meets-the-real-world/abstract}}

== References ==
{{reflist}}




</text>
      <sha1>3lf449ojxbzbv1ge9gaoypj5tav7vkk</sha1>
    </revision>
  </page>
  <page>
    <title>Proximal gradient methods for learning</title>
    <ns>0</ns>
    <id>41200806</id>
    <revision>
      <id>799628678</id>
      <parentid>793711633</parentid>
      <timestamp>2017-09-08T21:40:04Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor/>
      <comment>/* See also */ alpha</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19618">'''Proximal gradient''' (forward backward splitting) '''methods for learning''' is an area of research in [[optimization]] and [[statistical learning theory]] which studies algorithms for a general class of [[Convex function#Definition|convex]] [[Regularization (mathematics)|regularization]] problems where the regularization penalty may not be [[Differentiable function|differentiable]]. One such example is &lt;math&gt;\ell_1&lt;/math&gt; regularization (also known as Lasso) of the form
:&lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+ \lambda \|w\|_1, \quad \text{ where } x_i\in \mathbb{R}^d\text{ and } y_i\in\mathbb{R}.&lt;/math&gt;

Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application.&lt;ref name=combettes&gt;{{cite journal|last=Combettes|first=Patrick L.|author2=Wajs, Valérie R. |title=Signal Recovering by Proximal Forward-Backward Splitting|journal=Multiscale Model. Simul.|year=2005|volume=4|issue=4|pages=1168–1200|url=http://epubs.siam.org/doi/abs/10.1137/050626090|doi=10.1137/050626090}}&lt;/ref&gt;&lt;ref name=structSparse&gt;{{cite journal|last=Mosci|first=S.|author2=Rosasco, L. |author3=Matteo, S. |author4=Verri, A. |author5=Villa, S. |title=Solving Structured Sparsity Regularization with Proximal Methods|journal=Machine Learning and Knowledge Discovery in Databases|year=2010|volume=6322|pages=418–433 |doi=10.1007/978-3-642-15883-4_27}}&lt;/ref&gt; Such customized penalties can help to induce certain structure in problem solutions, such as ''sparsity'' (in the case of [[Lasso (statistics)|lasso]]) or ''group structure'' (in the case of  [[Lasso (statistics)#Group LASSO|group lasso]]).

== Relevant background ==

[[Proximal gradient method]]s are applicable in a wide variety of scenarios for solving [[convex optimization]] problems of the form
:&lt;math&gt; \min_{x\in \mathcal{H}} F(x)+R(x),&lt;/math&gt;
where &lt;math&gt;F&lt;/math&gt; is [[Convex function|convex]] and differentiable with [[Lipschitz continuity|Lipschitz continuous]] [[gradient]], &lt;math&gt; R&lt;/math&gt; is a [[Convex function|convex]], [[Semicontinuous function|lower semicontinuous]] function which is possibly nondifferentiable, and &lt;math&gt;\mathcal{H}&lt;/math&gt; is some set, typically a [[Hilbert space]]. The usual criterion of &lt;math&gt; x&lt;/math&gt; minimizes &lt;math&gt; F(x)+R(x)&lt;/math&gt; if and only if &lt;math&gt; \nabla (F+R)(x) = 0&lt;/math&gt; in the convex, differentiable setting is now replaced by
:&lt;math&gt; 0\in \partial (F+R)(x), &lt;/math&gt;
where &lt;math&gt;\partial \varphi&lt;/math&gt; denotes the [[subdifferential]] of a real-valued, convex function &lt;math&gt; \varphi&lt;/math&gt;.

Given a convex function &lt;math&gt;\varphi:\mathcal{H} \to \mathbb{R}&lt;/math&gt; an important operator to consider is its '''proximity operator''' &lt;math&gt;\operatorname{prox}_{\varphi}:\mathcal{H}\to\mathcal{H} &lt;/math&gt; defined by
:&lt;math&gt; \operatorname{prox}_{\varphi}(u) = \operatorname{arg}\min_{x\in\mathcal{H}} \varphi(x)+\frac{1}{2}\|u-x\|_2^2,&lt;/math&gt;
which is well-defined because of the strict convexity of the &lt;math&gt; \ell_2&lt;/math&gt; norm. The proximity operator can be seen as a generalization of a [[Projection (linear algebra)|projection]].&lt;ref name=combettes /&gt;&lt;ref name=moreau /&gt;&lt;ref name=bauschke&gt;{{cite book|last=Bauschke|first=H.H., and Combettes, P.L.|title=Convex analysis and monotone operator theory in Hilbert spaces|year=2011|publisher=Springer}}&lt;/ref&gt;
We see that the proximity operator is important because &lt;math&gt; x^* &lt;/math&gt; is a minimizer to the problem &lt;math&gt; \min_{x\in\mathcal{H}} F(x)+R(x)&lt;/math&gt; if and only if
:&lt;math&gt;x^* = \operatorname{prox}_{\gamma R}\left(x^*-\gamma\nabla F(x^*)\right),&lt;/math&gt; where &lt;math&gt;\gamma&gt;0&lt;/math&gt; is any positive real number.&lt;ref name=combettes /&gt;

=== Moreau decomposition ===

One important technique related to proximal gradient methods is the '''Moreau decomposition,''' which decomposes the identity operator as the sum of two proximity operators.&lt;ref name=combettes /&gt; Namely, let &lt;math&gt;\varphi:\mathcal{X}\to\mathbb{R}&lt;/math&gt; be a [[Semi-continuity|lower semicontinuous]], convex function on a vector space &lt;math&gt;\mathcal{X}&lt;/math&gt;. We define its [[Convex conjugate|Fenchel conjugate]] &lt;math&gt;\varphi^*:\mathcal{X}\to\mathbb{R}&lt;/math&gt; to be the function
:&lt;math&gt;\varphi^*(u) := \sup_{x\in\mathcal{X}} \langle x,u\rangle - \varphi(x).&lt;/math&gt;
The general form of Moreau's decomposition states that for any &lt;math&gt;x\in\mathcal{X}&lt;/math&gt; and any &lt;math&gt;\gamma&gt;0&lt;/math&gt; that
:&lt;math&gt;x = \operatorname{prox}_{\gamma \varphi}(x) + \gamma\operatorname{prox}_{\varphi^*/\gamma}(x/\gamma),&lt;/math&gt;
which for &lt;math&gt;\gamma=1&lt;/math&gt; implies that &lt;math&gt;x = \operatorname{prox}_{\varphi}(x)+\operatorname{prox}_{\varphi^*}(x)&lt;/math&gt;.&lt;ref name=combettes /&gt;&lt;ref name=moreau&gt;{{cite journal|last=Moreau|first=J.-J.|title=Fonctions convexes duales et points proximaux dans un espace hilbertien|journal=C. R. Acad. Sci. Paris Ser. A Math.|year=1962|volume=255|pages=2897–2899|mr=144188|zbl=0118.10502}}&lt;/ref&gt; The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections.&lt;ref name=combettes /&gt;

In certain situations it may be easier to compute the proximity operator for the conjugate &lt;math&gt;\varphi^*&lt;/math&gt; instead of the function &lt;math&gt;\varphi&lt;/math&gt;, and therefore the Moreau decomposition can be applied. This is the case for  [[Lasso (statistics)#Group LASSO|group lasso]].

== Lasso regularization ==

Consider the [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with square loss and with the [[L1-norm|&lt;math&gt;\ell_1&lt;/math&gt; norm]] as the regularization penalty:
:&lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+ \lambda \|w\|_1, &lt;/math&gt;
where &lt;math&gt;x_i\in \mathbb{R}^d\text{ and } y_i\in\mathbb{R}.&lt;/math&gt; The &lt;math&gt;\ell_1&lt;/math&gt; regularization problem is sometimes referred to as ''lasso'' ([[Lasso (statistics)|least absolute shrinkage and selection operator]]).&lt;ref name=tibshirani /&gt; Such &lt;math&gt;\ell_1&lt;/math&gt; regularization problems are interesting because they induce '' sparse'' solutions, that is, solutions &lt;math&gt;w&lt;/math&gt; to the minimization problem have relatively few nonzero components. Lasso can be seen to be a convex relaxation of the non-convex problem
:&lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+ \lambda \|w\|_0, &lt;/math&gt;
where &lt;math&gt;\|w\|_0&lt;/math&gt; denotes the &lt;math&gt;\ell_0&lt;/math&gt; &quot;norm&quot;, which is the number of nonzero entries of the vector &lt;math&gt;w&lt;/math&gt;. Sparse solutions are of particular interest in learning theory for interpretability of results: a sparse solution can identify a small number of important factors.&lt;ref name=tibshirani&gt;{{cite journal|last=Tibshirani|first=R.|title=Regression shrinkage and selection via the lasso|journal=J. R. Stat. Soc., Ser. B|year=1996|volume=58|series=1|issue=1|pages=267–288}}&lt;/ref&gt;

=== Solving for &lt;math&gt;\ell_1&lt;/math&gt; proximity operator ===

For simplicity we restrict our attention to the problem where &lt;math&gt;\lambda=1&lt;/math&gt;. To solve the problem
:&lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+  \|w\|_1, &lt;/math&gt;
we consider our objective function in two parts: a convex, differentiable term &lt;math&gt;F(w) = \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2&lt;/math&gt; and a convex function &lt;math&gt;R(w) = \|w\|_1&lt;/math&gt;. Note that &lt;math&gt;R&lt;/math&gt; is not strictly convex.

Let us compute the proximity operator for &lt;math&gt;R(w)&lt;/math&gt;. First we find an alternative characterization of the proximity operator &lt;math&gt;\operatorname{prox}_{R}(x)&lt;/math&gt; as follows:

&lt;math&gt;
\begin{align}
u = \operatorname{prox}_R(x) \iff &amp; 0\in \partial \left(R(u)+\frac{1}{2}\|u-x\|_2^2\right)\\
\iff &amp; 0\in \partial R(u) + u-x\\
\iff &amp; x-u\in \partial R(u).
\end{align}
&lt;/math&gt;

For &lt;math&gt;R(w) = \|w\|_1&lt;/math&gt; it is easy to compute &lt;math&gt;\partial R(w)&lt;/math&gt;: the &lt;math&gt;i&lt;/math&gt;th entry of &lt;math&gt;\partial R(w)&lt;/math&gt; is precisely

:&lt;math&gt; \partial |w_i| = \begin{cases}
1,&amp;w_i&gt;0\\
-1,&amp;w_i&lt;0\\
\left[-1,1\right],&amp;w_i = 0.
\end{cases}&lt;/math&gt;

Using the recharacterization of the proximity operator given above, for the choice of &lt;math&gt;R(w) = \|w\|_1&lt;/math&gt; and &lt;math&gt;\gamma&gt;0&lt;/math&gt; we have that &lt;math&gt;\operatorname{prox}_{\gamma R}(x)&lt;/math&gt; is defined entrywise by

::&lt;math&gt;\left(\operatorname{prox}_{\gamma R}(x)\right)_i = \begin{cases}
x_i-\gamma,&amp;x_i&gt;\gamma\\
0,&amp;|x_i|\leq\gamma\\
x_i+\gamma,&amp;x_i&lt;-\gamma,
\end{cases}&lt;/math&gt;

which is known as the [[Thresholding (image processing)|soft thresholding]] operator &lt;math&gt;S_{\gamma}(x)=\operatorname{prox}_{\gamma \|\cdot\|_1}(x)&lt;/math&gt;.&lt;ref name=combettes /&gt;&lt;ref name=daubechies&gt;{{cite journal|last=Daubechies|first=I. |author2=Defrise, M. |author3=De Mol, C.|title=An iterative thresholding algorithm for linear inverse problem with a sparsity constraint|journal=Comm. Pure Appl. Math|year=2004|volume=57|issue=11|pages=1413–1457|doi=10.1002/cpa.20042}}&lt;/ref&gt;

=== Fixed point iterative schemes ===

To finally solve the lasso problem we consider the fixed point equation shown earlier:
:&lt;math&gt;x^* = \operatorname{prox}_{\gamma R}\left(x^*-\gamma\nabla F(x^*)\right).&lt;/math&gt;

Given that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial &lt;math&gt;w^0\in\mathbb{R}^d&lt;/math&gt;, and for &lt;math&gt;k=1,2,\ldots&lt;/math&gt; define
:&lt;math&gt;w^{k+1} = S_{\gamma}\left(w^k - \gamma \nabla F\left(w^k\right)\right).&lt;/math&gt;
Note here the effective trade-off between the empirical error term &lt;math&gt;F(w) &lt;/math&gt; and the regularization penalty &lt;math&gt;R(w)&lt;/math&gt;. This  fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step (&lt;math&gt; w^k - \gamma \nabla F\left(w^k\right)&lt;/math&gt;) and a soft thresholding step (via &lt;math&gt;S_\gamma&lt;/math&gt;).

Convergence of this fixed point scheme is well-studied in the literature&lt;ref name=combettes /&gt;&lt;ref name=daubechies /&gt; and is guaranteed under appropriate choice of step size &lt;math&gt;\gamma&lt;/math&gt; and loss function (such as the square loss taken here). [[Gradient descent#Extensions|Accelerated methods]] were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on &lt;math&gt;F&lt;/math&gt;.&lt;ref name=nesterov&gt;{{cite journal|last=Nesterov|first=Yurii|title=A method of solving a convex programming problem with convergence rate &lt;math&gt;O(1/k^2)&lt;/math&gt;|journal=Soviet Math. Doklady|year=1983|volume=27|issue=2|pages=372–376}}&lt;/ref&gt; Such methods have been studied extensively in previous years.&lt;ref&gt;{{cite book|last=Nesterov|first=Yurii|title=Introductory Lectures on Convex Optimization|year=2004|publisher=Kluwer Academic Publisher}}&lt;/ref&gt;
For more general learning problems where the proximity operator cannot be computed explicitly for some regularization term &lt;math&gt;R&lt;/math&gt;, such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator.&lt;ref name=bauschke /&gt;&lt;ref&gt;{{cite journal|last=Villa|first=S.|author2=Salzo, S. |author3=Baldassarre, L. |author4=Verri, A. |title=Accelerated and inexact forward-backward algorithms|journal=SIAM J. Optim.|year=2013|volume=23|issue=3|pages=1607–1633|doi=10.1137/110844805}}&lt;/ref&gt;

== Practical considerations ==

There have been numerous developments within the past decade in [[convex optimization]] techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods.&lt;ref name=structSparse /&gt;&lt;ref name=bach&gt;{{cite journal|last=Bach|first=F.|author2=Jenatton, R. |author3=Mairal, J. |author4=Obozinski, Gl. |title=Optimization with sparsity-inducing penalties|journal=Found. &amp; Trends Mach. Learn.|year=2011|volume=4|issue=1|pages=1–106|doi=10.1561/2200000015}}&lt;/ref&gt;

=== Adaptive step size ===

In the fixed point iteration scheme
:&lt;math&gt;w^{k+1} = \operatorname{prox}_{\gamma R}\left(w^k-\gamma \nabla F\left(w^k\right)\right),&lt;/math&gt;
one can allow variable step size &lt;math&gt;\gamma_k&lt;/math&gt; instead of a constant &lt;math&gt;\gamma&lt;/math&gt;. Numerous adaptive step size schemes have been proposed throughout the literature.&lt;ref name=combettes /&gt;&lt;ref name=bauschke /&gt;&lt;ref&gt;{{cite journal|last=Loris|first=I. |author2=Bertero, M. |author3=De Mol, C. |author4=Zanella, R. |author5=Zanni, L. |title=Accelerating gradient projection methods for &lt;math&gt;\ell_1&lt;/math&gt;-constrained signal recovery by steplength selection rules|journal=Applied &amp; Comp. Harmonic Analysis|volume=27|issue=2|pages=247–254|year=2009|doi=10.1016/j.acha.2009.02.003}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Wright|first=S.J.|author2=Nowak, R.D. |author3=Figueiredo, M.A.T. |title=Sparse reconstruction by separable approximation|journal=IEEE Trans. Image Process.|year=2009|volume=57|issue=7|pages=2479–2493|doi=10.1109/TSP.2009.2016892}}&lt;/ref&gt; Applications of these schemes&lt;ref name=structSparse /&gt;&lt;ref&gt;{{cite journal|last=Loris|first=Ignace|title=On the performance of algorithms for the minimization of &lt;math&gt;\ell_1&lt;/math&gt;-penalized functionals|journal=Inverse Problems|year=2009|volume=25|issue=3|doi=10.1088/0266-5611/25/3/035008|page=035008}}&lt;/ref&gt;  suggest that these can offer substantial improvement in number of iterations required for fixed point convergence.

=== Elastic net (mixed norm regularization) ===

[[Elastic net regularization]] offers an alternative to pure &lt;math&gt;\ell_1&lt;/math&gt; regularization. The problem of lasso (&lt;math&gt;\ell_1&lt;/math&gt;) regularization involves the penalty term &lt;math&gt;R(w) = \|w\|_1&lt;/math&gt;, which is not strictly convex. Hence, solutions to &lt;math&gt;\min_w F(w) + R(w),&lt;/math&gt; where &lt;math&gt;F&lt;/math&gt; is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an &lt;math&gt;\ell_2&lt;/math&gt; norm regularization penalty. For example, one can consider the problem
:&lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+ \lambda \left((1-\mu)\|w\|_1+\mu \|w\|_2^2\right), &lt;/math&gt;
where &lt;math&gt;x_i\in \mathbb{R}^d\text{ and } y_i\in\mathbb{R}.&lt;/math&gt;
For &lt;math&gt;0&lt;\mu\leq 1&lt;/math&gt; the penalty term &lt;math&gt;\lambda \left((1-\mu)\|w\|_1+\mu \|w\|_2^2\right)&lt;/math&gt; is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small &lt;math&gt;\mu &gt; 0&lt;/math&gt;, the additional penalty term &lt;math&gt;\mu \|w\|_2^2&lt;/math&gt; acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.&lt;ref name=structSparse /&gt;&lt;ref name=deMolElasticNet&gt;{{cite journal|last=De Mol|first=C. |author2=De Vito, E. |author3=Rosasco, L.|title=Elastic-net regularization in learning theory|journal=J. Complexity|year=2009|volume=25|issue=2|pages=201–230|doi=10.1016/j.jco.2009.01.002}}&lt;/ref&gt;

== Exploiting group structure ==

Proximal gradient methods provide a general framework which is applicable to a wide variety of problems in [[statistical learning theory]]. Certain problems in learning can often involve data which has additional structure that is known '' a priori''. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods.

=== Group lasso ===

Group lasso is a generalization of the [[Lasso (statistics)|lasso method]] when features are grouped into disjoint blocks.&lt;ref name=groupLasso&gt;{{cite journal|last=Yuan|first=M.|author2=Lin, Y. |title=Model selection and estimation in regression with grouped variables|journal=J. R. Stat. Soc. B|year=2006|volume=68|issue=1|pages=49–67|doi=10.1111/j.1467-9868.2005.00532.x}}&lt;/ref&gt; Suppose the features are grouped into blocks &lt;math&gt;\{w_1,\ldots,w_G\}&lt;/math&gt;. Here we take as a regularization penalty

:&lt;math&gt;R(w) =\sum_{g=1}^G \|w_g\|_2,&lt;/math&gt;

which is the sum of the &lt;math&gt;\ell_2&lt;/math&gt; norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group &lt;math&gt;w_g&lt;/math&gt; we have that proximity operator of &lt;math&gt;\lambda\gamma\left(\sum_{g=1}^G \|w_g\|_2\right) &lt;/math&gt; is given by

:&lt;math&gt;\widetilde{S}_{\lambda\gamma }(w_g) =  \begin{cases}
w_g-\lambda\gamma \frac{w_g}{\|w_g\|_2}, &amp; \|w_g\|_2&gt;\lambda\gamma \\
0, &amp; \|w_g\|_2\leq \lambda\gamma
\end{cases}&lt;/math&gt;

where &lt;math&gt;w_g&lt;/math&gt; is the &lt;math&gt;g&lt;/math&gt;th group.

In contrast to lasso, the derivation of the proximity operator for group lasso relies on the [[#Moreau decomposition|Moreau decomposition]]. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the [[Ball (mathematics)|ball]] of a [[dual norm]].&lt;ref name=structSparse /&gt;

=== Other group structures ===

In contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure.  Such generalizations of group lasso have been considered in a variety of contexts.&lt;ref&gt;{{cite journal|last=Chen|first=X.|author2=Lin, Q. |author3=Kim, S. |author4=Carbonell, J.G. |author5=Xing, E.P. |title=Smoothing proximal gradient method for general structured sparse regression|journal=Ann. Appl. Stat.|year=2012|volume=6|issue=2|pages=719–752|doi=10.1214/11-AOAS514}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Mosci|first=S.|author2=Villa, S. |author3=Verri, A. |author4=Rosasco, L. |title=A primal-dual algorithm for group sparse regularization with overlapping groups|journal=NIPS|year=2010|volume=23|pages=2604–2612}}&lt;/ref&gt;&lt;ref name=nest&gt;{{cite journal|last=Jenatton|first=R. |author2=Audibert, J.-Y. |author3=Bach, F. |title=Structured variable selection with sparsity-inducing norms|journal=J. Mach. Learn. Res.|year=2011|volume=12|pages=2777–2824}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Zhao|first=P.|author2=Rocha, G. |author3=Yu, B. |title=The composite absolute penalties family for grouped and hierarchical variable selection|journal=Ann. Stat.|year=2009|volume=37|issue=6A|pages=3468–3497|doi=10.1214/07-AOS584}}&lt;/ref&gt; For overlapping groups one common approach is known as ''latent group lasso'' which introduces latent variables to account for overlap.&lt;ref&gt;{{cite journal|last=Obozinski|first=G. |author2=Laurent, J. |author3=Vert, J.-P. |title=Group lasso with overlaps: the latent group lasso approach|journal=INRIA Technical Report|year=2011|url=http://hal.inria.fr/inria-00628498/en/}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Villa|first=S.|author2=Rosasco, L. |author3=Mosci, S. |author4=Verri, A. |title=Proximal methods for the latent group lasso penalty|journal=preprint|year=2012|arxiv=1209.0368}}&lt;/ref&gt; Nested group structures are studied in ''hierarchical structure prediction'' and with [[directed acyclic graph]]s.&lt;ref name=nest /&gt;

== See also ==
* [[Convex analysis]]
* [[Proximal gradient method]]
* [[Regularization (mathematics)#Regularization in statistics and machine learning|Regularization]]
* [[Statistical learning theory]]

== References ==

{{reflist}}

[[Category:First order methods|First order methods]]

</text>
      <sha1>c4an8k24s28yl0mgxjce5ydnd5fbiwb</sha1>
    </revision>
  </page>
  <page>
    <title>Kernel density estimation</title>
    <ns>0</ns>
    <id>2090057</id>
    <revision>
      <id>808330657</id>
      <parentid>805990581</parentid>
      <timestamp>2017-11-02T06:26:50Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v475)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29424">[[File:Kernel density.svg|thumb|right|250px|Kernel density estimation of 100 [[normal distribution|normally distributed]] [[random number generator|random numbers]] using different smoothing bandwidths.]]
In [[statistics]], '''kernel density estimation''' ('''KDE''') is a [[non-parametric statistics|non-parametric]] way to [[density estimation|estimate]] the [[probability density function]] of a [[random variable]].  Kernel density estimation is a fundamental data smoothing problem where inferences about the [[statistical population|population]] are made, based on a finite data [[statistical sample|sample]]. In some fields such as [[signal processing]] and [[econometrics]] it is also termed the ''Parzen–Rosenblatt window'' method,  after [[Emanuel Parzen]] and [[Murray Rosenblatt]], who are usually credited with independently creating it in its current form.&lt;ref name=&quot;Ros1956&quot;&gt;{{Cite journal | last1 = Rosenblatt | first1 = M. |authorlink = Murray Rosenblatt| title = Remarks on Some Nonparametric Estimates of a Density Function | doi = 10.1214/aoms/1177728190 | journal = The Annals of Mathematical Statistics | volume = 27 | issue = 3 | pages = 832 | year = 1956 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name=&quot;Par1962&quot;&gt;{{Cite journal | last1 = Parzen | first1 = E. | authorlink = Emanuel Parzen| title = On Estimation of a Probability Density Function and Mode | doi = 10.1214/aoms/1177704472 | journal = [[The Annals of Mathematical Statistics]]| volume = 33 | issue = 3 | pages = 1065 | year = 1962 | jstor = 2237880| pmid =  | pmc = }}&lt;/ref&gt;

==Definition==
Let (''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;'') be a univariate [[Independent and identically distributed random variables|independent and identically distributed]] sample drawn from some distribution with an unknown [[probability density function|density]] ''ƒ''. We are interested in estimating the shape of this function ''ƒ''. Its ''kernel density estimator'' is
: &lt;math&gt;
    \hat{f}_h(x) = \frac{1}{n}\sum_{i=1}^n K_h (x - x_i) = \frac{1}{nh} \sum_{i=1}^n K\Big(\frac{x-x_i}{h}\Big),
  &lt;/math&gt;
where ''K'' is the [[kernel (statistics)#In non-parametric statistics|kernel]] — a non-negative function that integrates to one — and {{nowrap|''h'' &gt; 0}} is a [[smoothing]] parameter called the ''bandwidth''. A kernel with subscript ''h'' is called the ''scaled kernel'' and defined as {{nowrap|''K&lt;sub&gt;h&lt;/sub&gt;''(''x'') {{=}} 1/''h K''(''x/h'')}}. Intuitively one wants to choose ''h'' as small as the data will allow; however, there is always a trade-off between the bias of the estimator and its variance. The choice of bandwidth is discussed in more detail below.

A range of [[kernel (statistics)#Kernel functions in common use|kernel functions]] are commonly used: uniform, triangular, biweight, triweight, Epanechnikov, normal, and others. The Epanechnikov kernel is optimal in a mean square error sense,&lt;ref&gt;{{cite journal |doi=10.1137/1114019 |author=Epanechnikov, V.A. |title=Non-parametric estimation of a multivariate probability density |journal=Theory of Probability and its Applications |volume=14 |pages=153–158 |year=1969}}&lt;/ref&gt; though the loss of efficiency is small for the kernels listed previously,&lt;ref name=&quot;WJ1995&quot;&gt;{{Cite book| author1=Wand, M.P |author2=Jones, M.C. |title=Kernel Smoothing |publisher=Chapman &amp; Hall/CRC |location=London |year=1995 |isbn=0-412-55270-1}}&lt;/ref&gt; and due to its convenient mathematical properties, the normal kernel is often used, which means {{nowrap|''K''(''x'') {{=}} ''ϕ''(''x'')}}, where ''ϕ'' is the [[standard normal]] density function.

The construction of a kernel density estimate finds interpretations in fields outside of density estimation.&lt;ref name=&quot;bo10&quot;&gt;{{Cite journal |author1=Botev, Z.I. |author2=Grotowski, J.F. |author3=Kroese, D.P. |title=Kernel density estimation via diffusion |journal=[[Annals of Statistics]] |volume= 38 |issue=5 |pages=2916–2957 |year=2010 |doi=10.1214/10-AOS799}}&lt;/ref&gt; For example, in [[thermodynamics]], this is equivalent to the amount of heat generated when [[heat kernel]]s (the fundamental solution to the [[heat equation]]) are placed at each data point locations ''x&lt;sub&gt;i&lt;/sub&gt;''. Similar methods are used to construct [[discrete Laplace operator]]s on point clouds for [[manifold learning]].

Kernel density estimates are closely related to [[histograms]], but can be endowed with properties such as smoothness or continuity by using a suitable kernel. To see this, we compare the construction of histogram and kernel density estimators, using these 6 data points: ''x''&lt;sub&gt;1&lt;/sub&gt; = −2.1, ''x''&lt;sub&gt;2&lt;/sub&gt; = −1.3, ''x''&lt;sub&gt;3&lt;/sub&gt; = −0.4, ''x''&lt;sub&gt;4&lt;/sub&gt; = 1.9, ''x''&lt;sub&gt;5&lt;/sub&gt; = 5.1, ''x''&lt;sub&gt;6&lt;/sub&gt; = 6.2. For the histogram, first the horizontal axis is divided into sub-intervals or bins which cover the range of the data. In this case, we have 6 bins each of width 2. Whenever a data point falls inside this interval, we place a box of height 1/12. If more than one data point falls inside the same bin, we stack the boxes on top of each other.

For the kernel density estimate, we place a normal kernel with variance 2.25 (indicated by the red dashed lines) on each of the data points ''x&lt;sub&gt;i&lt;/sub&gt;''. The kernels are summed to make the kernel density estimate (solid blue curve). The smoothness of the kernel density estimate is evident compared to the discreteness of the histogram, as kernel density estimates converge faster to the true underlying density for continuous random variables.&lt;ref&gt;{{cite journal |author=Scott, D. |title=On optimal and data-based histograms |journal=Biometrika |year=1979 |volume=66 |pages=605–610 |doi=10.1093/biomet/66.3.605 |issue=3}}&lt;/ref&gt;

[[File:Comparison of 1D histogram and KDE.png|thumb|center|500px|alt=Comparison of the histogram (left) and kernel density estimate (right) constructed using the same data. The 6 individual kernels are the red dashed curves, the kernel density estimate the blue curves. The data points are the rug plot on the horizontal axis.|Comparison of the histogram (left) and kernel density estimate (right) constructed using the same data. The 6 individual kernels are the red dashed curves, the kernel density estimate the blue curves. The data points are the rug plot on the horizontal axis.]]

==Bandwidth selection==

[[File:Comparison of 1D bandwidth selectors.png|thumb|Kernel density estimate (KDE) with different bandwidths of a random sample of 100 points from a standard normal distribution. Grey: true density (standard normal). Red: KDE with h=0.05. Black: KDE with h=0.337. Green: KDE with h=2.]]

The bandwidth of the kernel is a [[free parameter]] which exhibits a strong influence on the resulting estimate. To illustrate its effect, we take a simulated [[Random number generator|random sample]] from the standard [[normal distribution]] (plotted at the blue spikes in the [[Carpet plot|rug plot]] on the horizontal axis). The grey curve is the true density (a normal density with mean 0 and variance 1). In comparison, the red curve is ''undersmoothed'' since it contains too many spurious data artifacts arising from using a bandwidth ''h'' = 0.05, which is too small. The green curve is ''oversmoothed'' since using the bandwidth ''h'' = 2 obscures much of the underlying structure. The black curve with a bandwidth of ''h'' = 0.337 is considered to be optimally smoothed since its density estimate is close to the true density.

The most common optimality criterion used to select this parameter is the expected ''L''&lt;sub&gt;2&lt;/sub&gt; [[risk function]], also termed the [[mean integrated squared error]]:

: &lt;math&gt;\operatorname{MISE} (h) = \operatorname{E}\!\left[\, \int (\hat{f}_h(x) - f(x))^2 \, dx \right].&lt;/math&gt;

Under weak assumptions on ''ƒ'' and ''K'',&lt;ref name=&quot;Ros1956&quot;/&gt;&lt;ref name=&quot;Par1962&quot;/&gt;
MISE (''h'') = AMISE(''h'') + ''o(1/(nh) + h&lt;sup&gt;4&lt;/sup&gt;)'' where ''o'' is the [[little o notation]].
The AMISE is the Asymptotic MISE which consists of the two leading terms

:&lt;math&gt;\operatorname{AMISE}(h) = \frac{R(K)}{nh} + \frac{1}{4} m_2(K)^2 h^4 R(f'')&lt;/math&gt;

where &lt;math&gt;R(g) = \int g(x)^2 \, dx&lt;/math&gt; for a function ''g'', &lt;math&gt;m_2(K) = \int x^2 K(x) \, dx&lt;/math&gt;
and ''ƒ'''' is the second derivative of ''ƒ''. The minimum of this AMISE is the solution to this differential equation

:&lt;math&gt; \frac{\partial}{\partial h} \operatorname{AMISE}(h) = -\frac{R(K)}{nh^2} +  m_2(K)^2 h^3 R(f'') = 0 &lt;/math&gt;

or

:&lt;math&gt;h_{\operatorname{AMISE}} = \frac{ R(K)^{1/5}}{m_2(K)^{2/5}R(f'')^{1/5} n^{1/5}}.&lt;/math&gt;

Neither the AMISE nor the ''h''&lt;sub&gt;AMISE&lt;/sub&gt; formulas are able to be used directly since they involve the unknown density function ''ƒ'' or its second derivative ''ƒ'''', so a variety of automatic, data-based methods have been developed for selecting the bandwidth. Many review studies have been carried out to compare their efficacies,&lt;ref&gt;{{cite journal |author1=Park, B.U. |author2=Marron, J.S. |year=1990 |title=Comparison of data-driven bandwidth selectors |journal=Journal of the American Statistical Association |volume=85 |issue=409 |pages=66–72 |jstor=2289526 |doi=10.1080/01621459.1990.10475307}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author1=Park, B.U. |author2=Turlach, B.A. |year=1992 |title=Practical performance of several data driven bandwidth selectors (with discussion) |journal=Computational Statistics |volume=7 |pages=251–270}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author1=Cao, R. |author2=Cuevas, A. |author3=Manteiga, W. G. |year=1994 |title=A comparative study of several smoothing methods in density estimation |journal=Computational Statistics and Data Analysis |volume=17 |pages=153–176 |doi=10.1016/0167-9473(92)00066-Z|issue=2}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |doi=10.2307/2291420 |author1=Jones, M.C. |author2=Marron, J.S. |author3=Sheather, S. J. |year=1996 |title=A brief survey of bandwidth selection for density estimation| journal=Journal of the American Statistical Association |volume=91 |issue=433 |pages=401–407 |jstor=2291420}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author=Sheather, S.J. |year=1992 |title=The performance of six popular bandwidth selection methods on some real data sets (with discussion) |journal=Computational Statistics |volume=7 |pages=225–250, 271–281}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author1=Agarwal, N. |author2=Aluru,   N.R. |year=2010 |title=A data-driven stochastic collocation approach for uncertainty quantification in MEMS |journal=International Journal for Numerical Methods in Engineering |volume=83 |issue=5 |pages=575–597 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author1=Xu, X. |author2=Yan, Z. |author3=Xu, S. |year=2015 |title=Estimating wind speed probability distribution by diffusion-based kernel density method |journal=Electric Power Systems Research|volume=121 |pages=28–37 }}&lt;/ref&gt; with the general consensus that the plug-in selectors&lt;ref name=&quot;bo10&quot;/&gt;
&lt;ref name=&quot;SJ91&quot;&gt;{{cite journal |author1=Sheather, S.J. |author2=Jones, M.C. |year=1991 |title=A reliable data-based bandwidth selection method for kernel density estimation |journal=Journal of the Royal Statistical Society, Series B |volume=53 |issue=3 |pages=683–690 |jstor=2345597}}&lt;/ref&gt; and [[Cross-validation (statistics)|cross validation]] selectors&lt;ref&gt;{{cite journal |author=Rudemo, M. |year=1982 |title=Empirical choice of histograms and kernel density estimators |journal=Scandinavian Journal of Statistics |volume=9 |issue=2 |pages=65–78 |jstor=4615859}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author=Bowman, A.W. |year=1984 |title=An alternative method of cross-validation for the smoothing of density estimates |journal=Biometrika |volume=71 |pages=353–360 |doi=10.1093/biomet/71.2.353 |issue=2}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author1=Hall, P. |author2=Marron, J.S. |author3=Park, B.U. |year=1992 |title=Smoothed cross-validation |journal=Probability Theory and Related Fields |volume=92 |pages=1–20 |doi=10.1007/BF01205233}}&lt;/ref&gt; are the most useful over a wide range of data sets.

Substituting any bandwidth ''h'' which has the same asymptotic order ''n''&lt;sup&gt;−1/5&lt;/sup&gt; as ''h''&lt;sub&gt;AMISE&lt;/sub&gt; into the AMISE
gives that AMISE(''h'') = ''O''(''n''&lt;sup&gt;−4/5&lt;/sup&gt;), where ''O'' is the [[big o notation]]. It can be shown that, under weak assumptions, there cannot exist a non-parametric estimator that converges at a faster rate than the kernel estimator.&lt;ref&gt;{{Cite journal|doi=10.1214/aos/1176342997|last=Wahba|first=G.|title=Optimal convergence properties of variable knot, kernel, and orthogonal series methods for density estimation|url=http://projecteuclid.org/euclid.aos/1176342997|journal=[[Annals of Statistics]]|year=1975|volume=3|issue=1|pages=15–29}}&lt;/ref&gt; Note that the ''n''&lt;sup&gt;−4/5&lt;/sup&gt; rate is slower than the typical ''n''&lt;sup&gt;−1&lt;/sup&gt; convergence rate of parametric methods.

If the bandwidth is not held fixed, but is varied depending upon the location of either the estimate (balloon estimator) or the samples (pointwise estimator), this produces a particularly powerful method termed [[variable kernel density estimation|adaptive or variable bandwidth kernel density estimation]].

Bandwidth selection for kernel density estimation of heavy-tailed distributions is said to be relatively difficult.&lt;ref name=&quot;Buch2005&quot;&gt;{{Cite journal | last1 = Buch-Larsen | first1 = TINE | title = Kernel density estimation for heavy-tailed distributions using the Champernowne transformation | doi = 10.1080/02331880500439782 | journal = Statistics | volume = 39 | issue = 6 | pages = 503–518 | year = 2005 | pmid =  | pmc = }}&lt;/ref&gt;

===A rule-of-thumb bandwidth estimator===

If Gaussian basis functions are used to approximate [[univariate]] data, and the underlying density being estimated is Gaussian, the optimal choice for ''h'' (that is, the bandwidth that minimises the [[mean integrated squared error]]) is&lt;ref name=&quot;SI1998&quot;&gt;{{Cite book| last=Silverman |first= B.W. | authorlink = Bernard Silverman |title=Density Estimation for Statistics and Data Analysis |publisher=Chapman &amp; Hall/CRC |location=London |year=1986 |isbn=0-412-24620-1| page=48}}&lt;/ref&gt;

:&lt;math&gt;h = \left(\frac{4\hat{\sigma}^5}{3n}\right)^{\frac{1}{5}} \approx 1.06 \hat{\sigma} n^{-1/5},&lt;/math&gt;

where &lt;math&gt;\hat{\sigma}&lt;/math&gt; is the standard deviation of the samples.
This approximation is termed the ''normal distribution approximation'', Gaussian approximation, or ''[[Bernard Silverman|Silverman]]'s (1986) rule of thumb''.
While this rule of thumb is easy to compute, it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal. For example,  consider estimating the bimodal Gaussian mixture:

:&lt;math&gt;\textstyle\frac{1}{2\sqrt{2\pi}}\exp(-(x-10)^2/2)+\frac{1}{2\sqrt{2\pi}}\exp(-(x+10)^2/2)&lt;/math&gt;

from a sample of 200 points. The figure on the right below shows the true density and two kernel density estimates --- one using the rule-of-thumb bandwidth, and the other using
a solve-the-equation bandwidth.&lt;ref name=&quot;bo10&quot;/&gt;&lt;ref name=&quot;SJ91&quot;/&gt; The estimate based on the rule-of-thumb bandwidth is significantly oversmoothed.
The Matlab script for this example uses
[http://www.mathworks.com/matlabcentral/fileexchange/14034 kde.m] and is given below.
[[File:Wikipedia_kde.png|thumb|250px|alt=Comparison between rule of thumb and solve-the-equation bandwidth|Comparison between rule of thumb and solve-the-equation bandwidth.]]
&lt;source lang=&quot;matlab&quot; style=&quot;overflow:auto;&quot;&gt;
randn('seed',1) % use for reproducibility
data=[randn(100,1)-10;randn(100,1)+10]; % normal mixture with two humps
n=length(data); % number of samples
h=std(data)*(4/3/n)^(1/5); % Silverman's rule of thumb
phi=@(x)(exp(-.5*x.^2)/sqrt(2*pi)); % normal pdf
ksden=@(x)mean(phi((x-data)/h)/h); % kernel density
fplot(ksden,[-25,25],'k') % plot kernel density with rule of thumb
hold on
fplot(@(x)(phi(x-10)/2+phi(x+10)/2),[-25,25],'b') % plot the true density
ksden(data); % plot kde with solve-the-equation bandwidth
&lt;/source&gt;

==Relation to the characteristic function density estimator==
Given the sample (''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;''), it is natural to estimate the [[characteristic function (probability theory)|characteristic function]] {{nowrap|''φ''(''t'') {{=}} E[''e''&lt;sup&gt;''itX''&lt;/sup&gt;]}} as
: &lt;math&gt;
    \hat\varphi(t) = \frac{1}{n} \sum_{j=1}^n e^{itx_j}
  &lt;/math&gt;
Knowing the characteristic function, it is possible to find the corresponding probability density function through the [[Fourier transform]] formula. One difficulty with applying this inversion formula is that it leads to a diverging integral, since the estimate &lt;math style=&quot;vertical-align:-.3em&quot;&gt;\scriptstyle\hat\varphi(t)&lt;/math&gt; is unreliable for large ''t''’s. To circumvent this problem, the estimator &lt;math style=&quot;vertical-align:-.3em&quot;&gt;\scriptstyle\hat\varphi(t)&lt;/math&gt; is multiplied by a damping function {{nowrap|''ψ&lt;sub&gt;h&lt;/sub&gt;''(''t'') {{=}} ''ψ''(''ht'')}}, which is equal to 1 at the origin and then falls to 0 at infinity. The “bandwidth parameter” ''h'' controls how fast we try to dampen the function &lt;math style=&quot;vertical-align:-.3em&quot;&gt;\scriptstyle\hat\varphi(t)&lt;/math&gt;. In particular when ''h'' is small, then ''ψ&lt;sub&gt;h&lt;/sub&gt;''(''t'') will be approximately one for a large range of ''t''’s, which means that &lt;math style=&quot;vertical-align:-.3em&quot;&gt;\scriptstyle\hat\varphi(t)&lt;/math&gt; remains practically unaltered in the most important region of ''t''’s.

The most common choice for function ''ψ'' is either the uniform function {{nowrap|''ψ''(''t'') {{=}} '''1'''{−1 ≤ ''t'' ≤ 1}}}, which effectively means truncating the interval of integration in the inversion formula to {{nowrap|[−1/''h'', 1/''h'']}}, or the [[gaussian function]] {{nowrap|''ψ''(''t'') {{=}} ''e''&lt;sup&gt;''−π t''&lt;sup&gt;2&lt;/sup&gt;&lt;/sup&gt;}}. Once the function ''ψ'' has been chosen, the inversion formula may be applied, and the density estimator will be
: &lt;math&gt;\begin{align}
    \hat{f}(x) &amp;= \frac{1}{2\pi} \int_{-\infty}^{+\infty} \hat\varphi(t)\psi_h(t) e^{-itx}dt
                = \frac{1}{2\pi} \int_{-\infty}^{+\infty} \frac{1}{n} \sum_{j=1}^n e^{it(x_j-x)} \psi(ht) dt \\
               &amp;= \frac{1}{nh} \sum_{j=1}^n \frac{1}{2\pi} \int_{-\infty}^{+\infty} e^{-i(ht)\frac{x-x_j}{h}} \psi(ht) d(ht)
                = \frac{1}{nh} \sum_{j=1}^n K\Big(\frac{x-x_j}{h}\Big),
  \end{align}&lt;/math&gt;
where ''K'' is the [[Fourier transform]] of the damping function ''ψ''. Thus the kernel density estimator coincides with the characteristic function density estimator.

==Statistical implementation==

A non-exhaustive list of software implementations of kernel density estimators includes:

* In [[Analytica (software)|Analytica]] release 4.4, the ''Smoothing'' option for PDF results uses KDE, and from expressions it is available via the built-in &lt;code&gt;Pdf&lt;/code&gt; function.
* In [[C (programming language)|C]]/[[C++]], [http://www.umiacs.umd.edu/~morariu/figtree/ FIGTree] is a library that can be used to compute kernel density estimates using normal kernels. MATLAB interface available.
* In [[C++]], [http://libagf.sf.net libagf] is a library for [[variable kernel density estimation]].
* In [[CrimeStat]], kernel density estimation is implemented using five different kernel functions – normal, uniform, quartic, negative exponential, and triangular.  Both single- and dual-kernel density estimate routines are available.  Kernel density estimation is also used in interpolating a Head Bang routine, in estimating a two-dimensional Journey-to-crime density function, and in estimating a three-dimensional Bayesian Journey-to-crime estimate.
* In [[ELKI]], kernel density functions can be found in the package &lt;code&gt;de.lmu.ifi.dbs.elki.math.statistics.kernelfunctions&lt;/code&gt;
* In [[ESRI]] products, kernel density mapping is managed out of the Spatial Analyst toolbox and uses the Quartic(biweight) kernel.
* In [[Microsoft Excel|Excel]], the Royal Society of Chemistry has created an add-in to run kernel density estimation based on their [http://www.rsc.org/Membership/Networking/InterestGroups/Analytical/AMC/Software/kerneldensities.asp Analytical Methods Committee Technical Brief 4].
* In [[gnuplot]], kernel density estimation is implemented by the &lt;code&gt;smooth kdensity&lt;/code&gt; option, the datafile can contain a weight and bandwidth for each point, or the bandwidth can be set automatically&lt;ref&gt;{{cite book |last=Janert |first=Philipp K |title=Gnuplot in action : understanding data with graphs |year=2009 |publisher=Manning Publications |location=Connecticut, USA |isbn=978-1-933988-39-9 }} See section 13.2.2 entitled ''Kernel density estimates''.&lt;/ref&gt; according to &quot;Silverman's rule of thumb&quot; (see above).
* In [[Haskell (programming language)|Haskell]], kernel density is implemented in the [http://hackage.haskell.org/package/statistics statistics] package.
* In [[Java (programming language)|Java]], the [[Weka (machine learning)]] package provides [http://weka.sourceforge.net/doc.stable/weka/estimators/KernelEstimator.html weka.estimators.KernelEstimator], among others.
* In [[JavaScript]], the visualization package [[D3js|D3.js]] offers a KDE package in its science.stats package.
* In [[JMP (statistical software)|JMP]], The Distribution platform can be used to create univariate kernel density estimates, and the Fit Y by X platform can be used to create bivariate kernel density estimates.
* In [[Julia (programming language)|Julia]], kernel density estimation is implemented in the [https://github.com/JuliaStats/KernelDensity.jl KernelDensity.jl] package.
* In [[MATLAB]], kernel density estimation is implemented through the &lt;code&gt;ksdensity&lt;/code&gt; function (Statistics Toolbox). This function does not provide an automatic data-driven bandwidth but uses a [[rule of thumb]], which is optimal only when the target density is normal.  A free MATLAB software package which implements an automatic bandwidth selection method&lt;ref name=&quot;bo10&quot;/&gt; is available from the MATLAB Central File Exchange for
**[http://www.mathworks.com/matlabcentral/fileexchange/14034-kernel-density-estimator 1-dimensional data]
**[http://www.mathworks.com/matlabcentral/fileexchange/17204-kernel-density-estimation 2-dimensional data]
**[http://www.mathworks.com/matlabcentral/fileexchange/58312-kernel-density-estimator-for-high-dimensions n-dimensional data] &lt;br /&gt; A free MATLAB toolbox with implementation of kernel regression, kernel density estimation, kernel estimation of hazard function and many others is available on [http://www.math.muni.cz/english/science-and-research/developed-software/232-matlab-toolbox.html these pages] (this toolbox is a part of the book &lt;ref name=&quot;HorKolZel&quot;&gt;{{cite book|last1=Horová|first1=I.|last2=Koláček|first2=J.|last3=Zelinka|first3=J.|title=Kernel Smoothing in MATLAB: Theory and Practice of Kernel Smoothing|date=2012|publisher=World Scientific Publishing|location=Singapore|isbn=978-981-4405-48-5}}&lt;/ref&gt;).
* In [[Mathematica]], numeric kernel density estimation is implemented by the function &lt;code&gt;SmoothKernelDistribution&lt;/code&gt; [http://reference.wolfram.com/mathematica/ref/SmoothKernelDistribution.html here] and symbolic estimation is implemented using the function &lt;code&gt;KernelMixtureDistribution&lt;/code&gt; [http://reference.wolfram.com/mathematica/ref/KernelMixtureDistribution.html here] both of which provide data-driven bandwidths.
* In [[Minitab]], the Royal Society of Chemistry has created a macro to run kernel density estimation based on their [http://www.rsc.org/Membership/Networking/InterestGroups/Analytical/AMC/Software/kerneldensities.asp Analytical Methods Committee Technical Brief 4].
* In the [[NAG Numerical Library|NAG Library]], kernel density estimation is implemented via the &lt;code&gt;g10ba&lt;/code&gt; routine (available in both the Fortran&lt;ref&gt;{{cite web |last=The Numerical Algorithms Group |first=|title=NAG Library Routine Document: nagf_smooth_kerndens_gauss (g10baf) |date=|work=NAG Library Manual, Mark 23 |url=http://www.nag.co.uk/numeric/fl/nagdoc_fl23/pdf/G10/g10baf.pdf |accessdate=2012-02-16 }}&lt;/ref&gt; and the C&lt;ref&gt;{{cite web |last=The Numerical Algorithms Group |first=|title=NAG Library Routine Document: nag_kernel_density_estim (g10bac) |date=|work=NAG Library Manual, Mark 9 |url=http://www.nag.co.uk/numeric/CL/nagdoc_cl09/pdf/G10/g10bac.pdf |accessdate=2012-02-16 }}&lt;/ref&gt; versions of the Library).
* In [http://nuklei.sourceforge.net/ Nuklei], [[C++]] kernel density methods focus on data from the Special Euclidean group &lt;math&gt;SE(3)&lt;/math&gt;.
* In [[GNU Octave|Octave]], kernel density estimation is implemented by the &lt;code&gt;kernel_density&lt;/code&gt; option (econometrics package).
* In [[Origin (software)|Origin]], 2D kernel density plot can be made from its user interface, and two functions, Ksdensity for 1D and Ks2density for 2D can be used from its [http://wiki.originlab.com/~originla/ltwiki/index.php?title=Category:LabTalk_Programming LabTalk], [[Python (programming language)|Python]], or [[C (programming language)|C]] code.
* In [[Perl]], an implementation can be found in the [http://search.cpan.org/~janert/Statistics-KernelEstimation-0.05 Statistics-KernelEstimation module]
* In [[PHP]], an implementation can be found in the [https://github.com/markrogoyski/math-php MathPHP library]
* In [[Python (programming language)|Python]], many implementations exist:  [http://pythonhosted.org/PyQt-Fit/mod_kde.html pyqt_fit.kde Module] in the [https://pypi.python.org/packages/source/P/PyQt-Fit/PyQt-Fit-1.3.4.tar.gz PyQt-Fit package], SciPy (&lt;code&gt;scipy.stats.gaussian_kde&lt;/code&gt;), Statsmodels (&lt;code&gt;KDEUnivariate&lt;/code&gt; and &lt;code&gt;KDEMultivariate&lt;/code&gt;), and Scikit-learn (&lt;code&gt;KernelDensity&lt;/code&gt;) (see comparison&lt;ref&gt;{{cite web |last=Vanderplas|first=Jake|title=Kernel Density Estimation in Python|date=2013-12-01|url=https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/|accessdate=2014-03-12}}&lt;/ref&gt;).
* In [[R (programming language)|R]], it is implemented through the &lt;code&gt;density&lt;/code&gt;, the &lt;code&gt;bkde&lt;/code&gt; function in the [https://cran.r-project.org/web/packages/KernSmooth/index.html KernSmooth library] and the pareto density estimation in the &lt;code&gt;ParetoDensityEstimation&lt;/code&gt; function [https://cran.r-project.org/web/packages/AdaptGauss/index.html AdaptGauss library] (the first two included in the base distribution), the &lt;code&gt;kde&lt;/code&gt; function in the [https://cran.r-project.org/web/packages/ks/index.html ks library], the &lt;code&gt;dkden&lt;/code&gt; and &lt;code&gt;dbckden&lt;/code&gt; functions in the [https://cran.r-project.org/web/packages/evmix/index.html evmix library] (latter for boundary corrected kernel density estimation for bounded support), the &lt;code&gt;npudens&lt;/code&gt; function in the [https://cran.r-project.org/web/packages/np/index.html np library] (numeric and categorical data), the &lt;code&gt;sm.density&lt;/code&gt; function in the [https://cran.r-project.org/web/packages/sm/index.html sm library]. For an implementation of the &lt;code&gt;kde.R&lt;/code&gt; function, which does not require installing any packages or libraries, see [http://web.maths.unsw.edu.au/~zdravkobotev/kde.R kde.R]. btb package [https://cran.r-project.org/web/packages/btb/index.html] dedicated to urban analysis implements a kernel density estimator &lt;code&gt;kernel_smoothing&lt;/code&gt;.
* In [[SAS (software)|SAS]], &lt;code&gt; proc kde &lt;/code&gt; can be used to estimate univariate and bivariate kernel densities.
* In [[Stata]], it is implemented through &lt;code&gt;kdensity&lt;/code&gt;;&lt;ref&gt;https://www.stata.com/manuals13/rkdensity.pdf&lt;/ref&gt; for example &lt;code&gt;histogram x, kdensity&lt;/code&gt;. Alternatively a free Stata module KDENS is available from [https://ideas.repec.org/c/boc/bocode/s456410.html here] allowing a user to estimate 1D or 2D density functions.
* In [[Apache Spark]], you can use the &lt;code&gt;KernelDensity()&lt;/code&gt; class (see official documentation for more details [http://spark.apache.org/docs/latest/mllib-statistics.html#kernel-density-estimation])

==See also==
{{Commons category|Kernel density estimation}}
*[[Kernel (statistics)]]
*[[Kernel smoothing]]
*[[Kernel regression]]
*[[Density estimation]] (with presentation of other examples)
*[[Mean-shift]]
*[[Scale space]] The triplets {(''x'', ''h'', KDE with bandwidth ''h'' evaluated at ''x'': all ''x'', ''h'' &gt; 0} form a [[scale space]] representation of the data.
*[[Multivariate kernel density estimation]]
*[[Variable kernel density estimation]]
*[[Head/tail Breaks|Head/tail breaks]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.mvstat.net/tduong/research/seminars/seminar-2001-05 Introduction to kernel density estimation] A short tutorial which motivates kernel density estimators as an improvement over histograms.
* [http://2000.jukuin.keio.ac.jp/shimazaki/res/kernel.html Kernel Bandwidth Optimization] A free online tool that instantly generates an optimized kernel density estimate of your data.
* [http://www.wessa.net/rwasp_density.wasp Free Online Software (Calculator)] computes the Kernel Density Estimation for any data series according to the following Kernels: Gaussian, Epanechnikov, Rectangular, Triangular, Biweight, Cosine, and Optcosine.
* [http://pcarvalho.com/things/kerneldensityestimation/index.html Kernel Density Estimation Applet] An online interactive example of kernel density estimation.  Requires .NET 3.0 or later.

{{DEFAULTSORT:Kernel density estimation}}



[[Category:Articles with example MATLAB/Octave code]]</text>
      <sha1>84a50ya2rhf1be8kwwm2e0oww2sty24</sha1>
    </revision>
  </page>
  <page>
    <title>Linear separability</title>
    <ns>0</ns>
    <id>523173</id>
    <revision>
      <id>800165332</id>
      <parentid>790730973</parentid>
      <timestamp>2017-09-11T21:27:26Z</timestamp>
      <contributor>
        <ip>128.8.120.3</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6230">In [[Euclidean geometry]], '''linear separability''' is a geometric property of a pair of sets of [[point (geometry)|points]]. This is most easily visualized in two dimensions (the [[Euclidean plane]]) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are ''linearly separable'' if there exists at least one [[line (geometry)|line]] in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if line is replaced by [[hyperplane]].

The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are arises in several areas.  In [[statistics]] and [[machine learning]], classifying certain types of data is a problem for which good algorithms exist that are based on this concept.

==Mathematical definition==

Let &lt;math&gt;X_{0}&lt;/math&gt; and &lt;math&gt;X_{1}&lt;/math&gt; be two sets of points in an ''n''-dimensional Euclidean space. Then &lt;math&gt;X_{0}&lt;/math&gt; and &lt;math&gt;X_{1}&lt;/math&gt; are ''linearly separable'' if there exists ''n'' + 1 real numbers &lt;math&gt;w_{1}, w_{2},..,w_{n}, k&lt;/math&gt;, such that every point &lt;math&gt;x \in X_{0}&lt;/math&gt; satisfies &lt;math&gt;\sum^{n}_{i=1} w_{i}x_{i} &gt; k&lt;/math&gt; and every point &lt;math&gt;x \in X_{1}&lt;/math&gt; satisfies &lt;math&gt;\sum^{n}_{i=1} w_{i}x_{i} &lt; k&lt;/math&gt;, where &lt;math&gt;x_{i}&lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;-th component of &lt;math&gt;x&lt;/math&gt;.

Equivalently, two sets are linearly separable precisely when their respective [[convex hull]]s are [[disjoint sets|disjoint]] (colloquially, do not overlap).{{Citation needed|reason=It is unclear that this is equivalent|date=September 2017}}

== Examples ==

Three non-[[collinear]] points in two classes ('+' and '-') are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all '+' case is not shown, but is similar to the all '-' case):

{| align=&quot;center&quot; border=&quot;0&quot; cellpadding=&quot;4&quot; cellspacing=&quot;10&quot;
| align=&quot;center&quot; | [[File:VC1.svg]]
| align=&quot;center&quot; | [[File:VC2.svg]]
| align=&quot;center&quot; | [[File:VC3.svg]]
|}

However, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need ''two'' straight lines and thus is not linearly separable:

{| align=&quot;center&quot; border=&quot;0&quot; cellpadding=&quot;4&quot; cellspacing=&quot;0&quot;
| [[File:VC4.svg]]
|}

Notice that three points which are collinear and of the form &quot;+ ⋅⋅⋅ &amp;mdash; ⋅⋅⋅ +&quot; are also not linearly separable.

== Linear separability of Boolean functions in ''n'' variables ==

A [[Boolean function]] in ''n'' variables can be thought of as an assignment of ''0'' or ''1'' to each vertex of a Boolean [[hypercube]] in ''n'' dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be ''linearly separable'' provided these two sets of points are linearly separable.

{| class=&quot;wikitable&quot;
|+&lt;small&gt;Number of linearly separable Boolean functions in each dimension&lt;/small&gt;&lt;ref&gt;
{{cite paper
| last=Gruzling
| first=Nicolle
| title=Linear separability of the vertices of an n-dimensional hypercube. M.Sc Thesis
| publisher= University of Northern British Columbia
| year=2006
}}&lt;/ref&gt; {{OEIS|id=A000609}}
!Number of variables
!Linearly separable Boolean functions
|-
|  2 || 14
|-
|  3 || 104
|-
|  4 || 1882
|-
|  5 || 94572
|-
|  6 || 15028134
|-
|  7 || 8378070864
|-
|  8 || 17561539552946
|-
|  9 || 144130531453121108
|}

== Support vector machines==
{{main|Support vector machine}}

[[Image:Svm separating hyperplanes (SVG).svg|thumb|right|H&lt;sub&gt;1&lt;/sub&gt; does not separate the sets. H&lt;sub&gt;2&lt;/sub&gt; does, but only with a small margin.  H&lt;sub&gt;3&lt;/sub&gt; separates them with the maximum margin.]]
[[Statistical classification|Classifying data]] is a common task in [[machine learning]].
Suppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a ''new'' data point will be in. In the case of [[support vector machine]]s, a data point is viewed as a ''p''-dimensional vector (a list of ''p'' numbers), and we want to know whether we can separate such points with a (''p''&amp;nbsp;&amp;minus;&amp;nbsp;1)-dimensional [[hyperplane]]. This is called a [[linear classifier]]. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the ''[[maximum-margin hyperplane]]'' and the linear classifier it defines is known as a ''maximum [[margin classifier]]''.

More formally, given some training data &lt;math&gt;\mathcal{D}&lt;/math&gt;, a set of ''n'' points of the form

:&lt;math&gt;\mathcal{D} = \left\{ (\mathbf{x}_i, y_i)\mid\mathbf{x}_i \in \mathbb{R}^p,\, y_i \in \{-1,1\}\right\}_{i=1}^n&lt;/math&gt;

where the ''y''&lt;sub&gt;''i''&lt;/sub&gt; is either 1 or −1, indicating the set to which the point &lt;math&gt;\mathbf{x}_i &lt;/math&gt; belongs. Each &lt;math&gt; \mathbf{x}_i &lt;/math&gt; is a ''p''-dimensional [[real number|real]] vector. We want to find the maximum-margin hyperplane that divides the points having &lt;math&gt;y_i=1&lt;/math&gt; from those having &lt;math&gt;y_i=-1&lt;/math&gt;. Any hyperplane can be written as the set of points &lt;math&gt;\mathbf{x}&lt;/math&gt; satisfying

: &lt;math&gt;\mathbf{w}\cdot\mathbf{x} - b=0,&lt;/math&gt;

where &lt;math&gt;\cdot&lt;/math&gt; denotes the [[dot product]] and &lt;math&gt;{\mathbf{w}}&lt;/math&gt; the (not necessarily normalized) [[Normal (geometry)|normal vector]] to the hyperplane. The parameter &lt;math&gt;\tfrac{b}{\|\mathbf{w}\|}&lt;/math&gt; determines the offset of the hyperplane from the origin along the normal vector &lt;math&gt;{\mathbf{w}}&lt;/math&gt;.

If the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance.

== See also ==

* [[Perceptron]]
* [[Vapnik–Chervonenkis dimension]]

== References ==
{{reflist}}



</text>
      <sha1>cg9n0p273iksqt7hc03yg35wkywr2lc</sha1>
    </revision>
  </page>
  <page>
    <title>Bias–variance tradeoff</title>
    <ns>0</ns>
    <id>40678189</id>
    <revision>
      <id>814721841</id>
      <parentid>814721773</parentid>
      <timestamp>2017-12-10T14:44:37Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* K-nearest neighbors */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17836">{{Refimprove|date=August 2017}}
{{more footnotes|date=August 2017}}

{{Machine learning bar}}
{{multiple image| align=right|direction=vertical|width=200
|image1=Test function and noisy data.png|caption1=Function and noisy data.
|image2=Radial basis function fit, spread=5.png|caption2=spread=5
|image3=Radial basis function fit, spread=1.png|caption3=spread=1
|image4=Radial basis function fit, spread=0.1.png|caption4=spread=0.1
|footer=A function (red) is approximated using [[radial basis functions]] (blue). Several trials are shown in each graph. For each trial, a few noisy data points are provided as training set (top). For a wide spread (image 2) the bias is high: the RBFs cannot fully approximate the function (especially the central dip), but the variance between different trials is low. As spread decreases (image 3 and 4) the bias decreases: the blue curves more closely approximate the red. However, depending on the noise in different trials the variance between trials increases. In the lowermost image the approximated values for x=0 varies wildly depending on where the data points were located.}}

In [[statistics]] and [[machine learning]], the '''bias–variance tradeoff''' (or '''dilemma''') is the problem of simultaneously minimizing two sources of [[Errors and residuals in statistics|error]] that prevent [[supervised learning]] algorithms from generalizing beyond their [[training set]]:{{Citation needed|reason=This is very generic statement, bias-variance is investigated in neural-networks and we need to put more references from Statistics literature |date=August 2017}}

* The [[Bias of an estimator|''bias'']] is error from erroneous assumptions in the learning [[algorithm]]. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
* The ''[[variance]]'' is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random [[Noise (signal processing)|noise]] in the training data, rather than the intended outputs ([[overfitting]]).

The '''bias–variance decomposition''' is a way of analyzing a learning algorithm's [[expected value|expected]] [[generalization error]] with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the ''irreducible error'', resulting from noise in the problem itself.

This tradeoff applies to all forms of [[supervised learning]]: [[Statistical classification|classification]], [[Regression analysis|regression]] (function fitting),&lt;ref name=&quot;geman&quot;&gt;{{cite journal |last1=Geman |first1=Stuart |authorlink1=Stuart Geman |author2=E. Bienenstock |author3=R. Doursat |year=1992 |title=Neural networks and the bias/variance dilemma |journal=Neural Computation |volume=4 |pages=1–58 |doi=10.1162/neco.1992.4.1.1 |url=http://web.mit.edu/6.435/www/Geman92.pdf}}&lt;/ref&gt;&lt;ref&gt;Bias–variance decomposition, In Encyclopedia of Machine Learning. Eds. Claude Sammut, Geoffrey I. Webb. Springer 2011. pp. 100-101&lt;/ref&gt; and [[Structured prediction|structured output learning]]. It has also been invoked to explain the effectiveness of heuristics in human learning.&lt;ref name=&quot;ReferenceA&quot;&gt;{{Cite journal | last1 = Gigerenzer | first1 = Gerd| last2 = Brighton | first2 = Henry| doi = 10.1111/j.1756-8765.2008.01006.x | title = Homo Heuristicus: Why Biased Minds Make Better Inferences | journal = Topics in Cognitive Science | volume = 1 | pages = 107–143| year = 2009 | pmid =  25164802| pmc = }}&lt;/ref&gt;

==Motivation==
The bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to [[Model selection|choose a model]] that both accurately captures the regularities in its training data, but also [[Generalization|generalizes]] well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit, but may ''underfit'' their training data, failing to capture important regularities.

Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large [[Noise (signal processing)|noise]] component  in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set.

==Bias–variance decomposition of squared error==
Suppose that we have a training set consisting of a set of points &lt;math&gt;x_1, \dots, x_n&lt;/math&gt; and [[Real number|real]] values &lt;math&gt;y_i&lt;/math&gt; associated with each point &lt;math&gt;x_i&lt;/math&gt;. We assume that there is a function with noise &lt;math&gt;y = f(x) + \varepsilon&lt;/math&gt;, where the noise, &lt;math&gt;\varepsilon&lt;/math&gt;, has zero mean and variance &lt;math&gt;\sigma^2&lt;/math&gt;.

We want to find a function &lt;math&gt;\hat{f}(x)&lt;/math&gt;, that approximates the true function &lt;math&gt;f(x)&lt;/math&gt; as well as possible, by means of some learning algorithm. We make &quot;as well as possible&quot; precise by measuring the [[mean squared error]] between &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;\hat{f}(x)&lt;/math&gt;: we want &lt;math&gt;(y - \hat{f}(x))^2&lt;/math&gt; to be minimal, both for &lt;math&gt;x_1, \dots, x_n&lt;/math&gt; ''and for points outside of our sample''. Of course, we cannot hope to do so perfectly, since the &lt;math&gt;y_i&lt;/math&gt; contain noise &lt;math&gt;\varepsilon&lt;/math&gt;; this means we must be prepared to accept an ''irreducible error'' in any function we come up with.

Finding an &lt;math&gt;\hat{f}&lt;/math&gt; that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function &lt;math&gt;\hat{f}&lt;/math&gt; we select, we can decompose its [[expected value|expected]] error on an unseen sample &lt;math&gt;x&lt;/math&gt; as follows:&lt;ref name=&quot;islr&quot;&gt;{{cite book |author1=Gareth James |author2=Daniela Witten |author3=Trevor Hastie |author4=Robert Tibshirani |title=An Introduction to Statistical Learning |publisher=Springer |year=2013 |url=http://www-bcf.usc.edu/~gareth/ISL/}}&lt;/ref&gt;{{rp|34}}&lt;ref name=&quot;ESL&quot;&gt;{{cite book |first1=Trevor |last1=Hastie |first2=Robert |last2=Tibshirani |first3=Jerome |last3=Friedman |year=2009 |title=The Elements of Statistical Learning |url=http://statweb.stanford.edu/~tibs/ElemStatLearn/}}&lt;/ref&gt;{{rp|223}}

:&lt;math&gt;
\begin{align}
\operatorname{E}\Big[\big(y - \hat{f}(x)\big)^2\Big]
 &amp; = \operatorname{Bias}\big[\hat{f}(x)\big]^2 + \operatorname{Var}\big[\hat{f}(x)\big] + \sigma^2 \\
\end{align}
&lt;/math&gt;

Where:
:&lt;math&gt;
\begin{align}
 \operatorname{Bias}\big[\hat{f}(x)\big] = \operatorname{E}\big[\hat{f}(x) - f(x)\big]
\end{align}
&lt;/math&gt;

and

:&lt;math&gt;
\begin{align}
\operatorname{Var}\big[\hat{f}(x)\big] = \operatorname{E}[\hat{f}(x)^2] - \operatorname{E}[{\hat{f}}(x)]^2
\end{align}
&lt;/math&gt;

The expectation ranges over different choices of the training set &lt;math&gt;x_1, \dots, x_n, y_1, \dots, y_n&lt;/math&gt;, all sampled from the same joint distribution &lt;math&gt;P(x,y)&lt;/math&gt;. The three terms represent:
* the square of the ''bias'' of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function &lt;math&gt;f(x)&lt;/math&gt; using a learning method for [[linear model]]s, there will be error in the estimates &lt;math&gt;\hat{f}(x)&lt;/math&gt; due to this assumption;
* the ''variance'' of the learning method, or, intuitively, how much the learning method &lt;math&gt;\hat{f}(x)&lt;/math&gt; will move around its mean;
* the irreducible error &lt;math&gt;\sigma^2&lt;/math&gt;. Since all three terms are non-negative, this forms a lower bound on the expected error on unseen samples.&lt;ref name=&quot;islr&quot;/&gt;{{rp|34}}

The more complex the model &lt;math&gt;\hat{f}(x)&lt;/math&gt; is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model &quot;move&quot; more to capture the data points, and hence its variance will be larger.

===Derivation===
The derivation of the bias–variance decomposition for squared error proceeds as follows.&lt;ref&gt;{{cite web |first1=Sethu |last1=Vijayakumar |title=The Bias–Variance Tradeoff |publisher=University Edinburgh |year=2007 |accessdate=19 August 2014 |url=http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=Notes on derivation of bias-variance decomposition in linear regression |first=Greg|last=Shakhnarovich |year=2011 |accessdate=20 August 2014 |url=http://ttic.uchicago.edu/~gregory/courses/wis-ml2012/lectures/biasVarDecom.pdf|archiveurl=https://web.archive.org/web/20140821063842/http://ttic.uchicago.edu/~gregory/courses/wis-ml2012/lectures/biasVarDecom.pdf|archivedate=21 August 2014}}&lt;/ref&gt;
For notational convenience, abbreviate &lt;math&gt;f = f(x)&lt;/math&gt; and &lt;math&gt;\hat{f} = \hat{f}(x)&lt;/math&gt;. First, recall that, by definition, for any random variable &lt;math&gt;X&lt;/math&gt;, we have

:&lt;math&gt;
\begin{align}
\operatorname{Var}[X] = \operatorname{E}[X^2] - \operatorname{E}[X]^2
\end{align}
&lt;/math&gt;
Rearranging, we get:
:&lt;math&gt;
\begin{align}
\operatorname{E}[X^2] = \operatorname{Var}[X] + \operatorname{E}[X]^2
\end{align}
&lt;/math&gt;

Since &lt;math&gt;f&lt;/math&gt; is [[Deterministic algorithm|deterministic]]

:&lt;math&gt;
\begin{align}
\operatorname{E}[f] = f
\end{align}
&lt;/math&gt;.

This, given &lt;math&gt;y = f + \varepsilon&lt;/math&gt; and &lt;math&gt;\operatorname{E}[\varepsilon] = 0&lt;/math&gt;, implies &lt;math&gt;\operatorname{E}[y] = \operatorname{E}[f + \varepsilon] = \operatorname{E}[f] = f&lt;/math&gt;.

Also, since &lt;math&gt;\operatorname{Var}[\varepsilon] = \sigma^2&lt;/math&gt;

:&lt;math&gt;
\begin{align}
\operatorname{Var}[y] = \operatorname{E}[(y - \operatorname{E}[y])^2] = \operatorname{E}[(y - f)^2] = \operatorname{E}[(f + \varepsilon - f)^2] = \operatorname{E}[\varepsilon^2] = \operatorname{Var}[\varepsilon] + \operatorname{E}[\varepsilon]^2  = \sigma^2
\end{align}
&lt;/math&gt;

Thus, since &lt;math&gt;\varepsilon&lt;/math&gt; and &lt;math&gt;\hat{f}&lt;/math&gt; are independent, we can write

:&lt;math&gt;
\begin{align}
\operatorname{E}\big[(y - \hat{f})^2\big]
 &amp; = \operatorname{E}[y^2 + \hat{f}^2 - 2 y\hat{f}] \\
 &amp; = \operatorname{E}[y^2] + \operatorname{E}[\hat{f}^2] - \operatorname{E}[2y\hat{f}] \\
 &amp; = \operatorname{Var}[y] + \operatorname{E}[y]^2 + \operatorname{Var}[\hat{f}] + \operatorname{E}[\hat{f}]^2 - 2f\operatorname{E}[\hat{f}] \\
 &amp; = \operatorname{Var}[y] + \operatorname{Var}[\hat{f}] + (f^2 - 2f\operatorname{E}[\hat{f}] + \operatorname{E}[\hat{f}]^2) \\
 &amp; = \operatorname{Var}[y] + \operatorname{Var}[\hat{f}] + (f - \operatorname{E}[\hat{f}])^2 \\
 &amp; = \sigma^2 + \operatorname{Var}[\hat{f}] + \operatorname{Bias}[\hat{f}]^2
\end{align}
&lt;/math&gt;

== Application to regression ==
The bias–variance decomposition forms the conceptual basis for regression [[Regularization (mathematics)|regularization]] methods such as [[Lasso (statistics)|Lasso]] and [[ridge regression]]. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the OLS solution.  Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.

==Application to classification==
The bias–variance decomposition was originally formulated for least-squares regression. For the case of [[statistical classification|classification]] under the [[0-1 loss]] (misclassification rate), it's possible to find a similar decomposition.&lt;ref&gt;{{cite conference |last=Domingos |first=Pedro |title=A unified bias-variance decomposition |conference=ICML |year=2000 |url=http://homes.cs.washington.edu/~pedrod/bvd.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |first1=Giorgio |last1=Valentini |first2=Thomas G. |last2=Dietterich |title=Bias–variance analysis of support vector machines for the development of SVM-based ensemble methods |journal=[[Journal of Machine Learning Research|JMLR]] |volume=5 |year=2004 |pages=725–775}}&lt;/ref&gt; Alternatively, if the classification problem can be phrased as [[probabilistic classification]], then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.&lt;ref&gt;{{cite book |first1=Christopher D. |last1=Manning |first2=Prabhakar |last2=Raghavan |first3=Hinrich |last3=Schütze |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/ |pages=308–314}}&lt;/ref&gt;

==Approaches==
[[Dimensionality reduction]] and [[feature selection]] can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance, e.g.:

* ([[Generalized linear model|Generalized]]) linear models can be [[Regularization (mathematics)|regularized]] to decrease their variance at the cost of increasing their bias.&lt;ref&gt;{{cite book | last = Belsley | first = David | title = Conditioning diagnostics : collinearity and weak data in regression | publisher = Wiley | location = New York | year = 1991 | isbn = 978-0471528890 }}&lt;/ref&gt;
* In [[artificial neural network]]s, the variance increases and the bias decreases with the number of hidden units.&lt;ref name=&quot;geman&quot;/&gt; Like in GLMs, regularization is typically applied.
* In [[k-nearest neighbor]] models, a high value of {{mvar|k}} leads to high bias and low variance (see below).
* In [[Instance-based learning]], regularization can be achieved varying the mixture of [[prototype]]s and exemplars.&lt;ref&gt;{{cite journal | last1 = Gagliardi | first1 = F | year = 2011 | title = Instance-based classifiers applied to medical databases: diagnosis and knowledge extraction | url = | journal = Artificial Intelligence in Medicine | volume = 52 | issue = 3| pages = 123–139 | doi = 10.1016/j.artmed.2011.04.002 }}&lt;/ref&gt;
* In [[decision tree]]s, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.&lt;ref name=&quot;islr&quot;/&gt;{{rp|307}}

One way of resolving the trade-off is to use [[mixture models]] and [[ensemble learning]].&lt;ref&gt;Jo-Anne Ting, Sethu Vijaykumar, Stefan Schaal, Locally Weighted Regression for Control. In Encyclopedia of Machine Learning. Eds. Claude Sammut, Geoffrey I. Webb. Springer 2011. p. 615&lt;/ref&gt;&lt;ref&gt;Scott Fortmann-Roe. Understanding the Bias–Variance Tradeoff. 2012. http://scott.fortmann-roe.com/docs/BiasVariance.html&lt;/ref&gt;
For example, [[Boosting (machine learning)|boosting]] combines many &quot;weak&quot; (high bias) models in an ensemble that has lower bias than the individual models, while [[Bootstrap aggregating|bagging]] combines &quot;strong&quot; learners in a way that reduces their variance.

===''k''-nearest neighbors===
In the case of [[k-nearest neighbors algorithm|{{mvar|k}}-nearest neighbors regression]], a [[closed-form expression]] exists that relates the bias–variance decomposition to the parameter {{mvar|k}}:&lt;ref name=&quot;ESL&quot;/&gt;{{rp|37, 223}}

:&lt;math&gt;
\operatorname{E}[(y - \hat{f}(x))^2\mid X=x] = \left( f(x) - \frac{1}{k}\sum_{i=1}^k f(N_i(x)) \right)^2 + \frac{\sigma^2}{k} + \sigma^2
&lt;/math&gt;

where &lt;math&gt;N_1(x), \dots, N_k(x)&lt;/math&gt; are the {{mvar|k}} nearest neighbors of {{mvar|x}} in the training set. The bias (first term) is a monotone rising function of {{mvar|k}}, while the variance (second term) drops off as {{mvar|k}} is increased. In fact, under &quot;reasonable assumptions&quot; the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.&lt;ref name=&quot;geman&quot;/&gt;

==Application to human learning ==
While widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of [[Cognitive science|human cognition]], most notably by [[Gerd Gigerenzer]] and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.&lt;ref name=&quot;ReferenceA&quot;/&gt;

[[Stuart Geman|Geman]] et al.&lt;ref name=&quot;geman&quot;/&gt; argue that the bias-variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of “hard wiring”   that is later tuned by experience.  This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.

==See also==
{{Div col||25em}}
* [[Accuracy and precision]]
* [[Bias of an estimator]]
* [[Gauss–Markov theorem]]
* [[Hyperparameter optimization]]
* [[Minimum-variance unbiased estimator]]
* [[Model selection]]
* [[Regression model validation]]
* [[Supervised learning]]
{{Div col end}}

==References==
{{Reflist|30em}}

==External links==
* {{cite web |url= http://scott.fortmann-roe.com/docs/BiasVariance.html |title= Understanding the Bias-Variance Tradeoff |date= June 2012 |first= Scott |last= Fortmann-Roe  }}

{{DEFAULTSORT:Bias-variance dilemma}}



</text>
      <sha1>qyyn24ap9t5w8f1ym32pzc7a8308bz7</sha1>
    </revision>
  </page>
  <page>
    <title>Solomonoff's theory of inductive inference</title>
    <ns>0</ns>
    <id>405562</id>
    <revision>
      <id>813162877</id>
      <parentid>791942425</parentid>
      <timestamp>2017-12-02T04:25:18Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>[[User:JCW-CleanerBot#Logic|task]], replaced: Lecture notes in computer science → Lecture Notes in Computer Science using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18513">{{multiple issues|
{{cleanup|reason=mess|date=April 2013}}
{{clarity|reason=large swathes of this article are filled with [[WP:JARGON|jargon]] and (unsourced!) vague generalizations|date=June 2017}}
{{refimprove|date=June 2017}}
}}

[[Ray Solomonoff]]'s theory of universal '''inductive inference''' is a theory of prediction based on logical observations, such as predicting the next symbol based upon a given series of symbols. The only assumption that the theory makes is that the environment follows some unknown but [[Computable function|computable]] [[probability distribution]]. It is a mathematical formalization of [[Occam's razor]]&lt;ref name=&quot;ReferenceA&quot;&gt;JJ McCall. Induction: From Kolmogorov and Solomonoff to De Finetti and Back to Kolmogorov – Metroeconomica, 2004 – Wiley Online Library.&lt;/ref&gt;&lt;ref name=&quot;ReferenceB&quot;&gt;D Stork. Foundations of Occam's razor and parsimony in learning from ricoh.com – NIPS 2001 Workshop, 2001&lt;/ref&gt;&lt;ref name=&quot;ReferenceC&quot;&gt;A.N. Soklakov. Occam's razor as a formal basis for a physical theory
[http://arxiv.org/abs/math-ph/0009007 from arxiv.org] – Foundations of Physics Letters, 2002 – Springer&lt;/ref&gt;&lt;ref name=&quot;Hernandez.1999&quot;&gt;{{cite journal| author=Jose Hernandez-Orallo| title=Beyond the Turing Test| journal=Journal of Logic, Language and Information| year=1999| volume=9| url=http://users.dsic.upv.es/proy/anynt/Beyond.pdf}}&lt;/ref&gt;&lt;ref name=&quot;Hutter.2003&quot;&gt;M Hutter. On the existence and convergence of computable universal priors [http://arxiv.org/abs/cs/0305052 arxiv.org] – Algorithmic Learning Theory, 2003 – Springer&lt;/ref&gt; and the [[Principle of Multiple Explanations]].&lt;ref name=&quot;Paul Vitanyi p 339&quot;&gt;Ming Li and Paul Vitanyi, ''An Introduction to Kolmogorov Complexity and Its Applications.''  Springer-Verlag, N.Y., 2008p 339 ff.&lt;/ref&gt;

Prediction is done using a completely [[Bayesian probability|Bayesian]] framework. The [[universal prior]] is calculated for all computable sequences—this is the universal a priori probability distribution;
no computable hypothesis will have a [[zero probability]]. This means that Bayes rule of causation can be used in predicting the continuation of any particular computable sequence.

==Origin==

===Philosophical===
The theory is based in philosophical foundations, and was founded by [[Ray Solomonoff]] around 1960.&lt;ref&gt;Samuel Rathmanner and [[Marcus Hutter]]. A philosophical treatise of universal induction. Entropy, 13(6):1076–1136, 2011&lt;/ref&gt; It is a mathematically formalized combination of [[Occam's razor]]&lt;ref name=&quot;ReferenceA&quot;/&gt;&lt;ref name=&quot;ReferenceB&quot;/&gt;&lt;ref name=&quot;ReferenceC&quot;/&gt;&lt;ref name=&quot;Hernandez.1999&quot;/&gt;&lt;ref name=&quot;Hutter.2003&quot;/&gt; and the [[Principle of Multiple Explanations]].&lt;ref name=&quot;Paul Vitanyi p 339&quot;/&gt;
All [[computable]] theories which perfectly describe previous observations are used to calculate the probability of the next observation, with more weight put on the shorter computable theories. Marcus Hutter's [[universal artificial intelligence]] builds upon this to calculate the [[expected value]] of an action.

===Mathematical===
The proof of the &quot;razor&quot; is based on the known mathematical properties of a probability distribution over a [[Countable set|countable]] set. These properties are relevant because the infinite set of all programs is a denumerable set. The sum S of the probabilities of all programs must be exactly equal to one (as per the definition of [[probability]]) thus the probabilities must roughly decrease as we enumerate the infinite set of all programs, otherwise S will be strictly greater than one.  To be more precise, for every &lt;math&gt;\epsilon&lt;/math&gt; &gt; 0, there is some length ''l'' such that the probability of all programs longer than ''l'' is at most &lt;math&gt;\epsilon&lt;/math&gt;.  This does not, however, preclude very long programs from having very high probability.

Fundamental ingredients of the theory are the concepts of [[algorithmic probability]]  and [[Kolmogorov complexity]]. The universal [[prior probability]] of any prefix ''p'' of a computable sequence ''x'' is the sum of the probabilities of all programs (for a [[universal computer]]) that compute something starting with ''p''. Given some ''p'' and any computable but unknown probability distribution from which ''x'' is sampled, the universal prior and [[Bayes' theorem]] can be used to predict the yet unseen parts of ''x'' in optimal fashion.

==Modern applications==

===Artificial intelligence===
Though Solomonoff's inductive inference is not [[computable]], several [[AIXI]]-derived algorithms approximate it in order to make it run on a modern computer. The more computing power they are given, the closer their predictions are to the predictions of inductive inference (their mathematical [[limit (math)|limit]] is Solomonoff's inductive inference).&lt;ref&gt;J. Veness, K.S. Ng, M. Hutter, W. Uther, D. Silver. &quot;A Monte Carlo AIXI Approximation&quot; – [http://arxiv.org/abs/0909.0801 Arxiv preprint], 2009 arxiv.org&lt;/ref&gt;&lt;ref&gt;J. Veness, K.S. Ng, M. Hutter, D. Silver. &quot;Reinforcement Learning via AIXI Approximation&quot; [http://arxiv.org/abs/1007.2049 Arxiv preprint], 2010 – aaai.org&lt;/ref&gt;&lt;ref&gt;S. Pankov. A computational approximation to the AIXI model from agiri.org – Artificial general intelligence, 2008: proceedings of …, 2008 – books.google.com&lt;/ref&gt;

Another direction of inductive inference is based on [[E. Mark Gold]]'s model of [[Language identification in the limit|learning in the limit]] from 1967 and has developed since then more and more models of learning.&lt;ref&gt;{{Cite journal | last1 = Gold | first1 = E. Mark | year = 1967 | title = Language identification in the limit | journal = Information and Control | volume = 10 | issue = 5 | pages = 447–474 | publisher =  | jstor =  | doi = 10.1016/S0019-9958(67)91165-5 | url=http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf | format =  | accessdate = }}&lt;/ref&gt; The general scenario is the following: Given a class ''S'' of computable functions, is there a learner (that is, recursive functional) which for any input of the form (''f''(0),''f''(1),...,''f''(''n'')) outputs a hypothesis (an index ''e'' with respect to a previously agreed on acceptable numbering of all computable functions; the indexed function should be consistent with the given values of ''f''). A learner ''M'' learns a function ''f'' if almost all its hypotheses are the same index ''e'', which generates the function ''f''; ''M'' learns ''S'' if ''M'' learns every ''f'' in ''S''. Basic results are that all recursively enumerable classes of functions are learnable while the class REC of all computable functions is not learnable.
{{citation needed|reason=The previously cited paper (Gold 1967) is only about learning of a language, i.e. (a description of) a set of strings, from a sequence of member strings. Another reference is needed for the more general scenario about learning functions from sample values, in particular about the basic results.|date=January 2014}}
Many related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from Gold's pioneering paper in 1967 onwards. A far reaching extension of the Gold’s approach is developed by Schmidhuber's theory of generalized Kolmogorov complexities,&lt;ref name=GenKolm&gt;{{cite journal| author=J. Schmidhuber | title=Hierarchies of generalized Kolmogorov complexities and nonenumerable universal measures computable in the limit | journal=International Journal of Foundations of Computer Science | volume=13 | issue=4 | pages=587–612 | year=2002 | url=ftp://ftp.idsia.ch/pub/juergen/ijfcspreprint.pdf| doi=10.1142/S0129054102001291}}&lt;/ref&gt; which are kinds of [[super-recursive algorithm]]s.

===Turing machines===

{{Unreferenced section|date=June 2017}}

The third mathematically based direction of inductive inference makes use of the theory of automata and computation. In this context, the process of inductive inference is performed by an abstract automaton called an inductive [[Turing machine]] (Burgin, 2005).
''Inductive Turing machines'' represent the next step in the development of computer science providing better models for contemporary computers and computer networks (Burgin, 2001) and forming an important class of super-recursive algorithms as they satisfy all conditions in the definition of [[algorithm]]. Namely, each inductive Turing machines is a type of effective method in which a definite list of well-defined instructions for completing a task, when given an initial state, will proceed through a well-defined series of successive states, eventually terminating in an end-state. The difference between an inductive Turing machine and a [[Turing machine]] is that to produce the result a Turing machine has to stop, while in some cases an inductive Turing machine can do this without stopping. [[Stephen Kleene]] called procedures that could run forever without stopping by the name ''calculation procedure or algorithm'' (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit &quot;some object&quot; (Kleene 1952:137). This condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps, but inductive Turing machines do not always tell at which step the result has been obtained.

Simple inductive Turing machines are equivalent to other models of computation. More advanced inductive Turing machines are much more powerful. It is proved (Burgin, 2005) that limiting partial recursive functions, trial and error predicates, general Turing machines, and simple inductive Turing machines are equivalent models of computation. However, simple inductive Turing machines and general Turing machines give direct constructions of computing automata, which are thoroughly grounded in physical machines. In contrast, trial and error predicates, limiting recursive functions and limiting partial recursive functions present syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial and error predicates as Turing machines are related to partial recursive functions and lambda-calculus.

Note that only simple inductive Turing machines have the same structure (but different functioning semantics of the output mode) as Turing machines. Other types of inductive Turing machines have an essentially more advanced structure due to the structured memory and more powerful instructions. Their utilization for inference and learning allows achieving higher efficiency and better reflects learning of people (Burgin and Klinger, 2004).

Some researchers confuse computations of inductive Turing machines with non-stopping computations or with infinite time computations. First, some of computations of inductive Turing machines halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not give. Second, some non-stopping computations of inductive Turing machines give results, while others do not give. Rules of inductive Turing machines determine when a computation (stopping or non-stopping) gives a result. Namely, an inductive Turing machine produces output from time to time and once this output stops changing, it is considered the result of the computation. It is necessary to know that descriptions of this rule in some papers are incorrect. For instance, Davis (2006: 128) formulates the rule when result is obtained without stopping as &quot;… once the correct output has been produced any subsequent output will simply repeat this correct result.&quot; Third, in contrast to the widespread misconception, inductive Turing machines give results (when it happens) always after a finite number of steps (in finite time) in contrast to infinite and infinite-time computations.
There are two main distinctions between conventional Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine always informs (by halting or by coming to a final state) when the result is obtained, while a simple inductive Turing machine in some cases does inform about reaching the result, while in other cases (where the conventional Turing machine is helpless), it does not inform. People have an illusion that a computer always itself informs (by halting or by other means) when the result is obtained. In contrast to this, users themselves have to decide in many cases whether the computed result is what they need or it is necessary to continue computations. Indeed, everyday desktop computer applications like word processors and spreadsheets spend most of their time waiting in [[event loop]]s, and do not terminate until directed to do so by users.

====Evolutionary inductive Turing machines====
Evolutionary approach to inductive inference is accomplished by another class of automata called evolutionary inductive Turing machines (Burgin and Eberbach, 2009; 2012). An ‘’’evolutionary inductive Turing machine’’’ is a (possibly infinite) sequence ''E'' = {''A''[''t'']; ''t'' = 1, 2, 3, ... } of inductive Turing machines ''A''[''t''] each working on generations X[t] which are coded as words in the alphabet of the machines ''A''[''t'']. The goal is to build a “population” ''Z'' satisfying the inference condition. The automaton ''A''[''t''] called a component, or a level automaton, of E represents (encodes) a one-level evolutionary algorithm that works with input generations ''X''[''i''] of the population by applying the variation operators v and selection operator s. The first generation ''X''[0] is given as input to ''E'' and is processed by the automaton ''A''[1], which generates/produces the first generation ''X''[1] as its transfer output, which goes to the automaton ''A''[2]. For all ''t'' =&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;3,&amp;nbsp;..., the automaton ''A''[''t''] receives the generation ''X''[''t''&amp;nbsp;−&amp;nbsp;1] as its input from ''A''[''t''&amp;nbsp;−&amp;nbsp;1] and then applies the variation operator v and selection operator ''s'', producing the generation ''X''[''i''&amp;nbsp;+&amp;nbsp;1] and sending it to ''A''[''t''&amp;nbsp;+&amp;nbsp;1] to continue evolution.

==See also==
* [[Algorithmic information theory]]
* [[Bayesian inference]]
* [[Language identification in the limit]]
* [[Inductive inference]]
* [[Inductive probability]]
* [[Mill's methods]]
* [[Minimum description length]]
* [[Minimum message length]]
* For a philosophical viewpoint, see: [[Problem of induction]] and [[New riddle of induction]]

==Notes==
{{reflist}}

==References==
* {{cite journal| last=Angluin | first=Dana | last2= Smith| first2= Carl H.| title=Inductive Inference: Theory and Methods| journal=Computing Surveys|date=Sep 1983| volume=15| number=3| pages=237–269| url=http://users.dsic.upv.es/asignaturas/facultad/apr/AngluinSmith83.pdf| doi=10.1145/356914.356918}}
* Burgin, M. (2005), ''Super-recursive Algorithms'', Monographs in computer science, Springer. {{ISBN|0-387-95569-0}}
* Burgin, M., &quot;How We Know What Technology Can Do&quot;, ''Communications of the ACM'', v. 44, No. 11, 2001, pp.&amp;nbsp;82–88.
* Burgin, M.; Eberbach, E., &quot;Universality for Turing Machines, Inductive Turing Machines and Evolutionary Algorithms&quot;, ''Fundamenta Informaticae'', v. 91, No. 1, 2009, 53–77.
* Burgin, M.; Eberbach, E., &quot;On Foundations of Evolutionary Computation: An Evolutionary Automata Approach&quot;, in ''Handbook of Research on Artificial Immune Systems and Natural Computing: Applying Complex Adaptive Technologies'' (Hongwei Mo, Ed.), IGI Global, Hershey, Pennsylvania, 2009, 342–360.
* Burgin, M.; Eberbach, E., &quot;Evolutionary Automata: Expressiveness and Convergence of Evolutionary Computation&quot;, ''Computer Journal'', v. 55, No. 9, 2012, pp.&amp;nbsp;1023–1029.
* Burgin, M.; Klinger, A. Experience, Generations, and Limits in Machine Learning, ''Theoretical Computer Science'', v. 317, No. 1/3, 2004, pp.&amp;nbsp;71–91
* [[Martin Davis|Davis, Martin]] (2006) &quot;The Church–Turing Thesis: Consensus and opposition]&quot;. Proceedings, Computability in Europe 2006.  Lecture Notes in Computer Science, 3988 pp.&amp;nbsp;125–132.
* [[William Gasarch|Gasarch, W.]]; [[Carl Herbert Smith|Smith, C. H.]] (1997) &quot;A survey of inductive inference with an emphasis on queries&quot;. ''Complexity, logic, and recursion theory'', Lecture Notes in Pure and Appl. Math., 187, Dekker, New York, pp.&amp;nbsp;225–260.
* Hay, Nick. &quot;[http://www.cs.auckland.ac.nz/CDMTCS/researchreports/300nick.pdf Universal Semimeasures: An Introduction],&quot; CDMTCS Research Report Series, University of Auckland, Feb. 2007.
* Jain, Sanjay ; Osherson, Daniel ; Royer, James ; Sharma, Arun, ''Systems that Learn: An Introduction to Learning Theory'' (second edition), [[MIT Press]], 1999.
* {{Citation | last1=Kleene | first1=Stephen C. | author1-link=Stephen C. Kleene| title=Introduction to Metamathematics | publisher=North-Holland | location=Amsterdam | year=1952|edition=First}}.
* Li Ming; Vitanyi, Paul, ''An Introduction to Kolmogorov Complexity and Its Applications'', 2nd Edition, Springer Verlag, 1997.
* Osherson, Daniel ; Stob, Michael ; Weinstein, Scott, ''Systems That Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists'', [[MIT Press]], 1986.
* {{cite journal| last = Solomonoff | first = Ray J.| title=Two Kinds of Probabilistic Induction| journal=The Computer Journal| year=1999| volume=42| number=4|doi=10.1093/comjnl/42.4.256 | page=256| url=http://world.std.com/~rjs/compj99.pdf}}
* {{cite journal | doi=10.1016/S0019-9958(64)90223-2 | last=Solomonoff | first= Ray | title=A Formal Theory of Inductive Inference Part I | journal = Information and Control | url=http://world.std.com/~rjs/1964pt1.pdf | volume=7 | issue= 1 | pages= 1&amp;ndash;22 | date=March 1964}}
* {{cite journal | doi=10.1016/S0019-9958(64)90131-7 | last=Solomonoff |first= Ray | title=A Formal Theory of Inductive Inference Part II | journal = Information and Control | url=http://world.std.com/~rjs/1964pt2.pdf |volume=7 |issue= 2 |pages= 224&amp;ndash;254 |date=June 1964}}

==External links==
*[http://www.scholarpedia.org/article/Algorithmic_probability Algorithmic probability – Scholarpedia]





</text>
      <sha1>pa1bmvsj0bm2eimfpn5grf4paw0ezow</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Datasets in machine learning</title>
    <ns>14</ns>
    <id>42320378</id>
    <revision>
      <id>601428596</id>
      <timestamp>2014-03-26T23:33:22Z</timestamp>
      <contributor>
        <username>Kri</username>
        <id>253188</id>
      </contributor>
      <comment>Created category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="68">[[Category:Datasets|Machine learning]]
</text>
      <sha1>9e9amm06w9m0jhho39gaem1cv9kv81o</sha1>
    </revision>
  </page>
  <page>
    <title>Inductive probability</title>
    <ns>0</ns>
    <id>42579971</id>
    <revision>
      <id>811144208</id>
      <parentid>798304787</parentid>
      <timestamp>2017-11-19T20:17:12Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>Journal cites:,  using [[Project:AWB|AWB]] (12158)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="44194">'''Inductive probability''' attempts to give the probability of future events based on past events. It is the basis for [[inductive reasoning]], and gives the mathematical basis for [[learning]] and the perception of patterns. It is a source of [[knowledge]] about the world.

There are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods.  Deduction establishes new facts based on existing facts.  Only inference establishes new facts from data.

The basis of inference is [[Bayes' theorem]]. But this theorem is sometimes hard to apply and understand. The simpler method to understand inference is in terms of quantities of information.

Information describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters.  But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements.

[[Occam's razor]] says the &quot;simplest theory, consistent with the data is most likely to be  correct&quot;. The &quot;simplest theory&quot; is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct.

== History ==

Probability and statistics was focused on probability distributions and tests of significance.  Probability was formal, well defined, but limited in scope.  In particular its application was limited to situations that could be defined as an experiment or trial, with a well defined population.

[[Bayes's theorem]] is named after Rev. [[Thomas Bayes]] 1701–1761. [[Bayesian inference]] broadened the application of probability to many situations where a population was not well defined.  But Bayes' theorem always depended on prior probabilities, to generate new probabilities. It was unclear where these prior probabilities should come from.

[[Ray Solomonoff]] developed [[algorithmic probability]] which gave an explanation for what randomness is and how patterns in the data may be represented by computer programs, that give shorter representations of the data circa 1964.

[[Chris Wallace (computer scientist)|Chris Wallace]] and D. M. Boulton developed [[minimum message length]] circa 1968. Later [[Jorma Rissanen]] developed the [[minimum description length]] circa 1978. These methods allow [[information theory]] to be related to probability, in a way that can be compared to the application of Bayes' theorem, but which give a source and explanation for the role of prior probabilities.

[[Marcus Hutter]] combined [[decision theory]] with the work of Ray Solomonoff and [[Andrey Kolmogorov]] to give a theory for the [[Pareto efficiency|Pareto optimal]] behavior for an [[Intelligent agent]], circa 1998.

===Minimum description/message length===

The program with the shortest length that matches the data is the most likely to predict future data. This is the thesis behind the [[Minimum message length]]&lt;ref&gt;{{cite journal|last=Wallace|first=Chris| author2=Boulton |title=An information measure for classification |journal=Computer Journal|year=1968|volume=11|issue=2|pages=185–194}}&lt;/ref&gt; and [[Minimum description length]]&lt;ref&gt;{{Cite journal | last1 = Rissanen | first1 = J. | title = Modeling by shortest data description | doi = 10.1016/0005-1098(78)90005-5 | journal = Automatica | volume = 14 | issue = 5 | pages = 465–658 | year = 1978 | pmid =  | pmc = }}&lt;/ref&gt; methods.

At first sight [[Bayes' theorem]] appears different from the minimimum message/description length principle. At closer inspection it turns out to be the same. Bayes' theorem is about conditional probabilities. What is the probability that event ''B'' happens if firstly event ''A'' happens?

:&lt;math&gt;P(A \and B) = P(B) \cdot P(A |  B) = P(A) \cdot P(B |  A)&lt;/math&gt;

Becomes in terms of message length ''L'',
:&lt;math&gt;L(A \and B) = L(B) + L(A |  B) = L(A) + L(B |  A)&lt;/math&gt;

What this means is that in describing an event, if all the information is given describing the event then the length of the information may be used to give the raw probability of the event. So if the information describing the occurrence of ''A'' is given, along with the information describing ''B'' given ''A'', then all the information describing ''A'' and ''B'' has been given.&lt;ref&gt;
{{cite web
|last=Allison
|first=Lloyd
|title=Minimum Message Length (MML) – LA's MML introduction
|url=http://www.csse.monash.edu.au/~lloyd/tildeMML
}}&lt;/ref&gt;
&lt;ref&gt;
{{cite web
|last=Oliver
|first=J. J.
|last2=Baxter
|first2=Rohan A.
|title=MML and Bayesianism: Similarities and Differences (Introduction to Minimum Encoding Inference – Part II)
|url=http://citeseerx.ist.psu.edu/viewdoc/similar;jsessionid=65475C44F4C425AFE77BCAE59D49CE92?doi=10.1.1.1.7367&amp;type=ab}}&lt;/ref&gt;

====Overfitting====

[[Overfitting]] is where the model matches the random noise and not the pattern in the data. For example, take the situation where a curve is fitted to a set of points. If polynomial with many terms is fitted then it can more closely represent the data. Then the fit will be better, and the information needed to describe the deviances from the fitted curve will be smaller. Smaller information length means more probable.

However the information needed to describe the curve must also be considered. The total information for a curve with many terms may be greater than for a curve with fewer terms, that has not as good a fit, but needs less information to describe the polynomial.

===Inference based on program complexity===

[[Solomonoff's theory of inductive inference]] is also inductive inference.  A bit string x is observed.  Then consider all programs that generate strings starting with x.  Cast in the form of inductive inference, the programs are theories that imply the observation of the bit string ''x''.

The method used here to give probabilities for inductive inference is based on [[Solomonoff's theory of inductive inference]].

====Detecting patterns in the data====

If all the bits are 1, then people infer that there is a bias in the coin and that it is more likely also that the next bit is 1 also. This is described as learning from, or detecting a pattern in the data.

Such a pattern may be represented by a [[computer program]]. A short computer program may be written that produces a series of bits which are all 1. If the length of the program ''K'' is &lt;math&gt;L(K)&lt;/math&gt; bits then its prior probability is,
:&lt;math&gt;P(K) = 2^{-L(K)}&lt;/math&gt;

The length of the shortest program that represents the string of bits is called the [[Kolmogorov complexity]].

Kolmogorov complexity is not computable.  This is related to the [[halting problem]].  When searching for the shortest program some programs may go into an infinite loop.

====Considering all theories====

The Greek philosopher [[Epicurus]] is quoted as saying &quot;If more than one theory is consistent with the observations, keep all theories&quot;.&lt;ref&gt;Li, M. and Vitanyi, P., ''An Introduction to Kolmogorov Complexity and Its Applications'', 3rd Edition, Springer Science and Business Media, N.Y., 2008, p 347&lt;/ref&gt;

As in a crime novel all theories must be considered in determining the likely murderer, so with inductive probability all programs must be considered in determining the likely future bits arising from the stream of bits.

Programs that are already longer than ''n'' have no predictive power. The raw (or prior) probability that the pattern of bits is random (has no pattern) is &lt;math&gt;2^{-n}&lt;/math&gt;.

Each program that produces the sequence of bits, but is shorter than the ''n'' is a theory/pattern about the bits with a probability of &lt;math&gt;2^{-k}&lt;/math&gt; where ''k'' is the length of the program.

The probability of receiving a sequence of bits ''y'' after receiving a series of bits ''x'' is then the conditional probability of receiving ''y'' given ''x'', which is the probability of ''x'' with ''y'' appended, divided by the probability of ''x''.&lt;ref&gt;Solomonoff, R., &quot;[http://world.std.com/~rjs/rayfeb60.pdf A Preliminary Report on a General Theory of Inductive Inference]&quot;, Report V-131, Zator Co., Cambridge, Ma. Feb 4, 1960, [http://world.std.com/~rjs/z138.pdf revision], Nov., 1960.&lt;/ref&gt;&lt;ref&gt;Solomonoff, R., &quot;[http://world.std.com/~rjs/1964pt1.pdf A Formal Theory of Inductive Inference, Part I]&quot; ''Information and Control'', Vol 7, No. 1 pp 1–22, March 1964.&lt;/ref&gt;&lt;ref&gt;Solomonoff, R., &quot;[http://world.std.com/~rjs/1964pt2.pdf A Formal Theory of Inductive Inference, Part II]&quot; ''Information and Control'', Vol 7, No. 2 pp 224–254, June 1964.&lt;/ref&gt;

====Universal priors====

The programming language affects the predictions of the next bit in the string.  The language acts as a [[prior probability]].  This is particularly a problem where the programming language codes for numbers and other data types.  Intuitively we think that 0 and 1 are simple numbers, and that prime numbers are somehow more complex the numbers may be factorized.

Using the [[Kolmogorov complexity]] gives an unbiased estimate (a universal prior) of the prior probability of a number.  As a thought experiment an [[intelligent agent]] may be fitted with a data input device giving a series of numbers, after applying some transformation function to the raw numbers.  Another agent might have the same input device with a different transformation function.  The agents do not see or know about these transformation functions.  Then there appears no rational basis for preferring one function over another.  A universal prior insures that although two agents may have different initial probability distributions for the data input, the difference will be bounded by a constant.

So universal priors do not eliminate an initial bias, but they reduce and limit it.  Whenever we describe an event in a language, either using a natural language or other, the language has encoded in it our prior expectations. So some reliance on prior probabilities are inevitable.

A problem arises where an intelligent agent's prior expectations interact with the environment to form a self reinforcing feed back loop. This is the problem of bias or prejudice.  Universal priors reduce but do not eliminate this problem.

===Universal artificial intelligence===

The theory of [[universal artificial intelligence]] applies [[decision theory]]  to inductive probabilities.  The theory shows how the best actions to optimize a reward function may be chosen.  The result is a theoretical model of intelligence.&lt;ref&gt;{{cite book |last=Hutter|first=Marcus |title=Sequential Decisions Based on Algorithmic Probability |year=1998|publisher=Springer |isbn=3-540-22139-5}}&lt;/ref&gt;

It is a fundamental theory of intelligence, which optimizes the agents behavior in,
* Exploring the environment; performing actions to get responses that broaden the agents knowledge.
* Competing or co-operating with another agent; games.
* Balancing short and long term rewards.

In general no agent will always provide the best actions in all situations.  A particular choice made by an agent may be wrong, and the environment may provide no way for the agent to recover from an initial bad choice.  However the agent is [[Pareto optimal]] in the sense that no other agent will do better than this agent in this environment, without doing worse in another environment.  No other agent may, in this sense, be said to be better.

At present the theory is limited by incomputability (the [[halting problem]]).  Approximations may be used to avoid this.  Processing speed and [[combinatorial explosion]] remain the primary limiting factors for [[artificial intelligence]].

== Probability ==

Probability is the representation of uncertain or partial knowledge about the truth of statements. Probabilities are subjective and personal estimates of likely outcomes based on past experience and inferences made from the data.

This description of probability may seem strange at first.  In natural language we refer to &quot;the probability&quot; that the sun will rise tomorrow.  We do not refer to &quot;your probability&quot; that the sun will rise.  But in order for inference to be correctly modeled probability must be personal, and the act of inference generates new posterior probabilities from prior probabilities.

Probabilities are personal because they are conditional on the knowledge of the individual.  Probabilities are subjective because they always depend, to some extent, on prior probabilities assigned by the individual. Subjective should not be taken here to mean vague or undefined.

The term [[intelligent agent]] is used to refer to the holder of the probabilities.  The intelligent agent may be a human or a machine. If the intelligent agent does not interact with the environment then the probability will converge over time to the frequency of the event.

If however the agent uses the probability to interact with the environment there may be a feedback, so that two agents in the identical environment starting with only slightly different priors, end up with completely different probabilities. In this case optimal [[decision theory]] as in [[Marcus Hutter|Marcus Hutter's]] Universal Artificial Intelligence will give [[Pareto optimal]] performance for the agent. This means that no other intelligent agent could do better in one environment without doing worse in another environment.

=== Comparison to deductive probability ===

In deductive probability theories, probabilities are absolutes, independent of the individual making the assessment. But deductive probabilities are based on,
* Shared knowledge.
* Assumed facts, that should be inferred from the data.

For example, in a trial the participants are aware the outcome of all the previous history of trials. They also assume that each outcome is equally probable.  Together this allows a single unconditional value of probability to be defined.

But in reality each individual does not have the same information. And in general the probability of each outcome is not equal.  The dice may be loaded, and this loading needs to be inferred from the data.

=== Probability as estimation ===

The [[principle of indifference]] has played a key role in probability theory. It says that if N statements are symmetric so that one condition cannot be preferred over another then all statements are equally probable.&lt;ref&gt;{{cite web|last1=[[Carnap]]|first1=Rudolf|title=STATISTICAL AND INDUCTIVE PROBABILITY|url=http://fitelson.org/probability/carnap_saip.pdf}}&lt;/ref&gt;

Taken seriously, in evaluating probability this principle leads to contradictions.  Suppose there are 3 bags of gold in the distance and one is asked to select one. Then because of the distance one cannot see the bag sizes. You estimate using the principle of indifference that each bag has equal amounts of gold, and each bag has one third of the gold.

Now, while one of us is not looking, the other takes one of the bags and divide it into 3 bags.  Now there are 5 bags of gold. The principle of indifference now says each bag has one fifth of the gold. A bag that was estimated to have one third of the gold is now estimated to have one fifth of the gold.

Taken as a value associated with the bag the values are different therefore contradictory. But taken as an estimate given under a particular scenario, both values are separate estimates given under different circumstances and there is no reason to believe they are equal.

Estimates of prior probabilities are particularly suspect. Estimates will be constructed that do not follow any consistent frequency distribution. For this reason prior probabilities are considered as estimates of probabilities rather than probabilities.

A full theoretical treatment would associate with each probability,
* The statement
* Prior knowledge
* Prior probabilities
* The estimation procedure used to give the probability.

===Combining probability approaches===

Inductive probability combines two different approaches to probability.
* Probability and information
* Probability and frequency

Each approach gives a slightly different viewpoint.  Information theory is used in relating probabilities to quantities of information.  This approach is often used in giving estimates of prior probabilities.

[[Frequentist probability]] defines probabilities as objective statements about how often an event occurs.  This approach may be stretched by defining the [[Experiment (probability theory)|trials]] to be over [[possible world]]s.  Statements about possible worlds define [[Event (probability theory)|events]].

== Probability and information ==

Whereas logic represents only two values; true and false as the values of statement, probability associates a number in [0,1] to each statement. If the probability of a statement is 0, the statement is false.  If the probability of a statement is 1 the statement is true.

In considering some data as a string of bits the prior probabilities for a sequence of 1s and 0s, the probability of 1 and 0 is equal. Therefore, each extra bit halves the probability of a sequence of bits.
This leads to the conclusion that,
:&lt;math&gt;P(x) = 2^{-L(x)}&lt;/math&gt;
Where &lt;math&gt;P(x)&lt;/math&gt; is the probability of the string of bits &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;L(x)&lt;/math&gt; is its length.

The prior probability of any statement is calculated from the number of bits needed to state it.  See also [[information theory]].

=== Combining information ===

Two statements &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; may be represented by two separate encodings. Then the length of the encoding is,

: &lt;math&gt;L(A \and B) = L(A) + L(B)&lt;/math&gt;

or in terms of probability,

: &lt;math&gt;P(A \and B) = P(A) P(B)&lt;/math&gt;

But this law is not always true because there may be a shorter method of encoding &lt;math&gt;B&lt;/math&gt; if we assume &lt;math&gt;A&lt;/math&gt;. So the above probability law applies only if &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are &quot;independent&quot;.

===The internal language of information===

The primary use of the information approach to probability is to provide estimates of the complexity of statements. Recall that Occam's razor states that &quot;All things being equal, the simplest theory is the most likely to be correct&quot;. In order to apply this rule, first there needs to be a definition of what &quot;simplest&quot; means. Information theory defines simplest to mean having the shortest encoding.

Knowledge is represented as [[Statement (logic)|statements]]. Each statement is a [[Boolean algebra|Boolean]] [[Expression (mathematics)|expression]]. Expressions are encoded by a function that takes a description (as against the value) of the expression and encodes it as a bit string.

The length of the encoding of a statement gives an estimate of the probability of a statement. This probability estimate will often be used as the prior probability of a statement.

Technically this estimate is not a probability because it is not constructed from a frequency distribution. The probability estimates given by it do not always obey [[#The law of total of probability|the law of total of probability]]. Applying the law of total probability to various scenarios will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement.

====Encoding expressions====

An expression is constructed from sub expressions,
* Constants (including function identifier).
* Application of functions.
* [[Quantifier (logic)|quantifiers]].

A [[Huffman coding|Huffman code]] must distinguish the 3 cases. The length of each code is based on the frequency of each type of sub expressions.

Initially constants are all assigned the same length/probability. Later constants may be assigned a probability using the Huffman code based on the number of uses of the function id in all expressions recorded so far. In using a Huffman code the goal is to estimate probabilities, not to compress the data.

The length of a function application is the length of the function identifier constant plus the sum of the sizes of the expressions for each parameter.

The length of a quantifier is the length of the expression being quantified over.

====Distribution of numbers====

No explicit representation of natural numbers is given.  However natural numbers may be constructed by applying the successor function to 0, and then applying other arithmetic functions.  A distribution of natural numbers is implied by this, based on the complexity of constructing each number.

Rational numbers are constructed by the division of natural numbers. The simplest representation has no common factors between the numerator and the denominator. This allows the probability distribution of natural numbers may be extended to rational numbers.

== Probability and frequency ==

The probability of an [[Event (probability theory)|event]] may be interpreted as the frequencies of [[Outcome (probability)|outcomes]] where the statement is true divided by the total number of outcomes.  If the outcomes form a continuum the frequency may need to be replaced with a [[Probability measure|measure]].

Events are sets of outcomes. Statements may be related to events. A Boolean statement B about outcomes defines a set of outcomes b,
: &lt;math&gt; b = \{x : B(x)\} &lt;/math&gt;

=== Conditional probability ===

Each probability is always associated with the state of knowledge at a particular point in the argument. Probabilities before an inference are known as prior probabilities, and probabilities after are known as posterior probabilities.

Probability depends on the facts known. The truth of a fact limits the domain of outcomes to the outcomes consistent with the fact. Prior probabilities are the probabilities before a fact is known. Posterior probabilities are after a fact is known. The posterior probabilities are said to be conditional on the fact. the probability that &lt;math&gt;B&lt;/math&gt; is true given that &lt;math&gt;A&lt;/math&gt; is true is written as: &lt;math&gt;P(B  |  A).&lt;/math&gt;

All probabilities are in some sense conditional.  The prior probability of &lt;math&gt;B&lt;/math&gt; is,
: &lt;math&gt;P(B) = P(B  | \top)&lt;/math&gt;

=== The frequentist approach applied to possible worlds ===

In the [[Frequentist inference|frequentist approach]], probabilities are defined as the ratio of the number of [[Outcome (probability)|outcomes]] within an event to the total number of outcomes. In the [[possible world]] model each possible world is an outcome, and statements about possible worlds define events. The probability of a statement being true is the number of possible worlds divided by the total number of worlds. The probability of a statement &lt;math&gt;A&lt;/math&gt; being true about possible worlds is then,
: &lt;math&gt; P(A) = \frac{|\{x : A(x)\}|}{|x : \top|} &lt;/math&gt;

For a conditional probability.
: &lt;math&gt; P(B  |  A) = \frac{|\{x : A(x) \and B(x)\}|}{|x : A(x)|} &lt;/math&gt;

then

: &lt;math&gt; \begin{align} P(A \and B) &amp;= \frac{|\{x : A(x) \and B(x)\}|}{|x : \top|} \\[8pt]
&amp;= \frac{|\{x : A(x) \and B(x)\}|}{|\{x : A(x)\}|} \frac{|\{x : A(x)\}|}{|x : \top|} \\[8pt]
&amp;= P(A) P(B  |  A)
\end{align}&lt;/math&gt;

Using symmetry this equation may be written out as Bayes' law.
: &lt;math&gt; P(A \and B) = P(A) P(B  |  A) = P(B) P(A  |  B)&lt;/math&gt;

This law describes the relationship between prior and posterior probabilities when new facts are learnt.

Written as quantities of information [[Bayes' Theorem]] becomes,
: &lt;math&gt;L(A \and B) = L(A) + L(B  |  A) = L(B) + L(A  |  B)&lt;/math&gt;

Two statements A and B are said to be independent if knowing the truth of A does not change the probability of B. Mathematically this is,
: &lt;math&gt;P(B) = P(B  |  A)&lt;/math&gt;

then [[Bayes' Theorem]] reduces to,
: &lt;math&gt;P(A \and B) = P(A) P(B)&lt;/math&gt;

=== The law of total of probability ===

For a set of mutually exclusive possibilities &lt;math&gt;A_i&lt;/math&gt;, the sum of the posterior probabilities must be 1.
: &lt;math&gt;\sum_i{P(A_i  |  B)} = 1&lt;/math&gt;

Substituting using Bayes' theorem gives the [[law of total probability]]
: &lt;math&gt;\sum_i{P(B  |  A_i)P(A_i)} = \sum_i{P(A_i  |  B)P(B)}&lt;/math&gt;

: &lt;math&gt;P(B) = \sum_i{P(B  |  A_i) P(A_i)}&lt;/math&gt;

This result is used to give the [[Bayes' theorem#extended form|extended form of Bayes' theorem]],
: &lt;math&gt;P(A_i  |  B) = \frac{P(B  |  A_i) P(A_i)}{\sum_j{P(B  |  A_j) P(A_j)}}&lt;/math&gt;

This is the usual form of Bayes' theorem used in practice, because it guarantees the sum of all the posterior probabilities for &lt;math&gt;A_i&lt;/math&gt; is 1.

=== Alternate possibilities ===

For mutually exclusive possibilities, the probabilities add.
:&lt;math&gt; P(A \or B) = P(A) + P(B), \qquad \text{if }  P(A \and B) = 0 &lt;/math&gt;

Using
: &lt;math&gt; A \or B = (A \and \neg (A \and B)) \or (B \and \neg (A \and B)) \or (A \and B)&lt;/math&gt;
Then the alternatives
: &lt;math&gt; A \and \neg (A \and B), \quad  B \and \neg (A \and B), \quad A \and B &lt;/math&gt;
are all mutually exclusive. Also,
: &lt;math&gt; (A \and \neg (A \and B)) \or (A \and B) = A &lt;/math&gt;
: &lt;math&gt; P(A \and \neg (A \and B)) + P(A \and B) = P(A) &lt;/math&gt;
: &lt;math&gt; P(A \and \neg (A \and B)) = P(A) - P(A \and B)  &lt;/math&gt;

so, putting it all together,

: &lt;math&gt; \begin{align}
P(A \or B) &amp;= P((A \and \neg (A \and B)) \or (B \and \neg (A \and B)) \or (A \and B))  \\
&amp; = P(A \and \neg (A \and B) + P(B \and \neg (A \and B)) + P(A \and B) \\
&amp;= P(A) - P(A \and B) + P(B) - P(A \and B) + P(A \and B) \\
&amp;= P(A) + P(B) - P(A \and B)
\end{align}&lt;/math&gt;

=== Negation ===

As,
: &lt;math&gt; A \or \neg A = \top &lt;/math&gt;
then
: &lt;math&gt; P(A) + P(\neg A) = 1&lt;/math&gt;

=== Implication and condition probability ===

Implication is related to conditional probability by the following equation,
:&lt;math&gt;A \to B \iff P(B  |  A) = 1&lt;/math&gt;

Derivation,

:&lt;math&gt;\begin{align}
A \to B &amp; \iff P(A \to B) = 1 \\
&amp;\iff P(A \and B \or \neg A) = 1 \\
&amp;\iff P(A \and B) + P(\neg A) = 1 \\
&amp;\iff P(A \and B) = P(A) \\
&amp;\iff P(A) \cdot P(B  |  A) = P(A) \\
&amp;\iff P(B  |  A) = 1
\end{align}&lt;/math&gt;

== Bayesian hypothesis testing ==

Bayes' theorem may be used to estimate the probability of a hypothesis or theory H, given some facts F. The posterior probability of H is then

: &lt;math&gt;P(H  |  F) = \frac{P(H)P(F  |  H)}{P(F)}&lt;/math&gt;

or in terms of information,
: &lt;math&gt;P(H  |  F) = 2^{-(L(H) + L(F  |  H) - L(F))} &lt;/math&gt;

By assuming the hypothesis is true, a simpler representation of the statement F may be given. The length of the encoding of this simpler representation is &lt;math&gt;L(F  |  H).&lt;/math&gt;

&lt;math&gt;L(H) + L(F  |  H) &lt;/math&gt; represents the amount of information needed to represent the facts F, if H is true. &lt;math&gt;L(F)&lt;/math&gt; is the amount of information needed to represent F without the hypothesis H. The difference is how much the representation of the facts has been compressed by assuming that H is true.  This is the evidence that the hypothesis H is true.

If &lt;math&gt;L(F)&lt;/math&gt; is estimated from [[#Probability priors from encoding length|encoding length]] then the probability obtained will not be between 0 and 1. The value obtained is proportional to the probability, without being a good probability estimate. The number obtained is sometimes referred to as a relative probability, being how much more probable the theory is than not holding the theory.

If a full set of mutually exclusive hypothesis that provide evidence is known, a proper estimate may be given for the prior probability &lt;math&gt;P(F)&lt;/math&gt;.

===Set of hypothesis===

Probabilities may be calculated from the extended form of Bayes' theorem. Given all mutually exclusive hypothesis &lt;math&gt;H_i&lt;/math&gt; which give evidence, such that,
: &lt;math&gt;L(H_i) + L(F  |  H_i) &lt; L(F)&lt;/math&gt;

and also the hypothesis R, that none of the hypothesis is true, then,
: &lt;math&gt; \begin{align}
P(H_i  |  F) &amp;= \frac{P(H_i) P(F  |  H_i)}{P(F|R) + \sum_j{P(H_j) P(F  |  H_j)}} \\[8pt]
P(R  |  F) &amp;= \frac{P(F  |  R)}{P(F  |  R) + \sum_j{P(H_j) P(F  |  H_j)}}
\end{align}&lt;/math&gt;

In terms of information,

: &lt;math&gt;\begin{align}
P(H_i | F) &amp;= \frac{2^{-(L(H_i) + L(F  |  H_i))}}{2^{-L(F  |  R)} + \sum_j 2^{-(L(H_j) + L(F |  H_j)) }} \\[8pt]
P(R| F) &amp;= \frac{2^{-L(F  |  R)}}{2^{-L(F  |  R)} + \sum_j{2^{-(L(H_j) + L(F  |  H_j))}}}
\end{align}&lt;/math&gt;

In most situations it is a good approximation to assume that &lt;math&gt;F&lt;/math&gt; is independent of &lt;math&gt;R&lt;/math&gt;, which means &lt;math&gt;P(F  |  R) = P(F)&lt;/math&gt; giving,

: &lt;math&gt;\begin{align}
P(H_i  |  F) &amp;\approx \frac{2^{-(L(H_i) + L(F  |  H_i))}}{2^{-L(F)} + \sum_j{2^{-(L(H_j) + L(F|H_j))}}} \\[8pt]
P(R  |  F) &amp;\approx \frac{2^{-L(F)}}{2^{-L(F)} + \sum_j{2^{-(L(H_j) + L(F  |  H_j))}}}
\end{align}&lt;/math&gt;

==Boolean inductive inference==

[[Abductive reasoning#Probabilistic abduction|Abductive inference]] &lt;ref&gt;{{cite web |title=Abduction |url=http://plato.stanford.edu/entries/abduction/ }}&lt;/ref&gt;&lt;ref&gt;{{cite journal| first1=Niki| last1=Pfeifer|first2=Gernot D.|last2=Kleiter|title=INFERENCE IN CONDITIONAL PROBABILITY LOGIC|journal=Kybernetika |date=2006 |volume=42|issue=4 |pages=391–404}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=Conditional Probability |url=http://artint.info/html/ArtInt_142.html |work=Artificial Intelligence - Foundations of computational agents}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=Introduction to the theory of Inductive Logic Programming (ILP) |url=http://www.cs.ox.ac.uk/activities/machlearn/ilp_theory.html}}&lt;/ref&gt; starts with a set of facts ''F'' which is a statement (Boolean expression).  [[Abductive reasoning]] is of the form,

:''A theory T implies the statement F.  As the theory T is simpler than F, abduction says that there is a probability that the theory T is implied by F''.

The theory ''T'', also called an explanation of the condition ''F'', is an answer to the ubiquitous factual &quot;why&quot; question.  For example, for the condition ''F'' is &quot;Why do apples fall?&quot;. The answer is a theory ''T'' that implies that apples fall;
:&lt;math&gt;F = G \frac{m_1 m_2}{r^2}&lt;/math&gt;

Inductive inference is of the form,
:''All observed objects in a class C have a property P.  Therefore there is a probability that all objects in a class C have a property P''.

In terms of abductive inference, ''all objects in a class C or set have a property P'' is a theory that implies the observed condition, ''All observed objects in a class C have a property P''.

So [[inductive inference]] is a special case of abductive inference.  In common usage the term inductive inference is often used to refer to both abductive and inductive inference.

===Generalization and specialization===

Inductive inference is related to [[generalization]].  Generalizations may be formed from statements by replacing a specific value with membership of a category, or by replacing membership of a category with membership of a broader category.  In deductive logic, generalization is a powerful method of generating new theories that may be true. In inductive inference generalization generates theories that have a probability of being true.

The opposite of generalization is specialization.  Specialization is used in applying a general rule to a specific case.  Specializations are created from generalizations by replacing membership of a category by a specific value, or by replacing a category with a sub category.

The [[Carl Linnaeus|Linnaen]] classification of living things and objects forms the basis for generalization and specification. The ability to identify, recognize and classify is the basis for generalization. Perceiving the world as a collection of objects appears to be a key aspect of human intelligence.  It is the object oriented model, in the non [[computer science]] sense.

The object oriented model is constructed from our [[perception]]. In particularly [[Visual perception|vision]] is based on the ability to compare two images and calculate how much information is needed to morph or map one image into another.  [[Computer vision]] uses this mapping to construct 3D images from [[Stereoscopy|stereo image pairs]].

[[Inductive logic programming]] is a means of constructing theory that implies a condition.  Plotkin's &lt;ref&gt;{{cite journal|first1=Gordon D.|last1=Plotkin|title=A Note on Inductive Generalization|editor1-first=B.|editor1-last=Meltzer|editor2-first=D.|editor2-last=Michie|publisher=Edinburgh University Press|journal=Machine Intelligence|volume=5|pages=153–163|year=1970}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|first1=Gordon D.|last1=Plotkin|title=A Further Note on Inductive Generalization|editor1-first=B.|editor1-last=Meltzer|editor2-first=D.|editor2-last=Michie|publisher=Edinburgh University Press|journal=Machine Intelligence|volume=6|pages=101–124|year=1971}}&lt;/ref&gt; &quot;''relative least general generalization (rlgg)''&quot; approach constructs the simplest generalization consistent with the condition.

===Newton's use of induction===

[[Isaac Newton]] used inductive arguments in constructing his [[Newton's law of universal gravitation|law of universal gravitation]].&lt;ref&gt;Isaac Newton: &quot;In [experimental] philosophy particular propositions are inferred from the phenomena and afterwards rendered general by induction&quot;: &quot;[[Philosophiae Naturalis Principia Mathematica|Principia]]&quot;, Book 3, General Scholium, at p.392 in Volume 2 of Andrew Motte's English translation published 1729.&lt;/ref&gt; Starting with the statement,
* The center of an apple falls towards the center of the earth.

Generalizing by replacing apple for object, and earth for object gives, in a two body system,
* The center of an object falls towards the center of another object.

The theory explains all objects falling, so there is strong evidence for it. The second observation,
* The planets appear to follow an elliptical path.

After some complicated mathematical [[calculus]], it can be seen that if the acceleration follows the inverse square law then objects will follow an ellipse. So induction gives evidence for the inverse square law.

Using [[Galileo Galilei|Galileo's]] observation that all objects drop with the same speed,
:&lt;math&gt;F_1 = m_1 a_1 = \frac{m_1 k_1}{r^2} i_1&lt;/math&gt;
:&lt;math&gt;F_2 = m_2 a_2 = \frac{m_2 k_2}{r^2} i_2&lt;/math&gt;

where &lt;math&gt;i_1&lt;/math&gt; and &lt;math&gt;i_2&lt;/math&gt; vectors towards the center of the other object. Then using [[Newton's laws of motion#Newton's third law|Newton's third law]] &lt;math&gt;F_1 = -F_2&lt;/math&gt;
:&lt;math&gt;F = G\frac{m_1 m_2}{r^2}&lt;/math&gt;

===Probabilities for inductive inference===

[[#Implication and condition probability|Implication determines condition probability]] as,
:&lt;math&gt;T \to F \iff P(F  |  T) = 1&lt;/math&gt;

So,
: &lt;math&gt;P(F  |  T) = 1&lt;/math&gt;
: &lt;math&gt;L(F  |  T) = 0&lt;/math&gt;

This result may be used in the probabilities given for Bayesian hypothesis testing. For a single theory, H = T and,
: &lt;math&gt;P(T  |  F) = \frac{P(T)}{P(F)}&lt;/math&gt;

or in terms of information, the relative probability is,
: &lt;math&gt;P(T  |  F) = 2^{-(L(T) - L(F))} &lt;/math&gt;

Note that this estimate for P(T|F) is not a true probability. If &lt;math&gt;L(T_i) &lt; L(F)&lt;/math&gt; then the theory has evidence to support it. Then for a set of theories &lt;math&gt;T_i = H_i&lt;/math&gt;, such that &lt;math&gt;L(T_i) &lt; L(F)&lt;/math&gt;,

: &lt;math&gt;P(T_i  |  F) = \frac{P(T_i)}{P(F  |  R) + \sum_j{P(T_j)}}&lt;/math&gt;
: &lt;math&gt;P(R  |  F) = \frac{P(F  |  R)}{P(F  |  R) + \sum_j{P(T_j)}}&lt;/math&gt;

giving,
: &lt;math&gt;P(T_i  |  F) \approx \frac{2^{-L(T_i)}}{2^{-L(F)} + \sum_j{2^{-L(T_j)}}}&lt;/math&gt;
: &lt;math&gt;P(R  |  F) \approx \frac{2^{-L(F)}}{2^{-L(F)} + \sum_j{2^{-L(T_j)}}}&lt;/math&gt;

==Derivations==

===Derivation of inductive probability===

Make a list of all the shortest programs &lt;math&gt;K_i&lt;/math&gt; that each produce a distinct infinite string of bits, and satisfy the relation,

:&lt;math&gt;T_n(R(K_i)) = x&lt;/math&gt;

where &lt;math&gt;R(K_i)&lt;/math&gt; is the result of running the program &lt;math&gt;K_i&lt;/math&gt; and &lt;math&gt;T_n&lt;/math&gt; truncates the string after ''n'' bits.

The problem is to calculate the probability that the source is produced by program &lt;math&gt;K_i,&lt;/math&gt; given that the truncated source after n bits is ''x''. This is represented by the conditional probability,

:&lt;math&gt;P(s = R(K_i) |  T_n(s) = x)&lt;/math&gt;

Using the [[Bayes' theorem#Extended form|extended form of Bayes' theorem]]

:&lt;math&gt;P(s = R(K_i) |T_n(s) = x) = \frac{P(T_n(s) = x|s = R(K_i))P(s = R(K_i))}{\sum_j P(T_n(s) = x|s = R(K_j)) P(s = R(K_j))}.&lt;/math&gt;

The extended form relies on the [[law of total probability]]. This means that the &lt;math&gt;s = R(K_i) &lt;/math&gt; must be distinct possibilities, which is given by the condition that each &lt;math&gt;K_i&lt;/math&gt; produce a different infinite string. Also one of the conditions &lt;math&gt;s = R(K_i) &lt;/math&gt; must be true. This must be true, as in the limit as &lt;math&gt;n \to \infty,&lt;/math&gt; there is always at least one program that produces &lt;math&gt;T_n(s)&lt;/math&gt;.

As &lt;math&gt;K_i&lt;/math&gt; are chosen so that &lt;math&gt;T_n(R(K_i)) = x,&lt;/math&gt; then,
:&lt;math&gt;P(T_n(s) = x |  s = R(K_i)) = 1 &lt;/math&gt;

The apriori probability of the string being produced from the program, given no information about the string, is based on the size of the program,
:&lt;math&gt;P(s = R(K_i)) = 2^{-I(K_i)}&lt;/math&gt;

giving,
:&lt;math&gt;P(s = R(K_i) |  T_n(s) = x) = \frac{2^{-I(K_i)}}{\sum_j 2^{-I(K_j)}}.&lt;/math&gt;

Programs that are the same or longer than the length of ''x'' provide no predictive power. Separate them out giving,
:&lt;math&gt;P(s = R(K_i) |  T_n(s) = x) = \frac{2^{-I(K_i)}}{\sum_{j:I(K_j)&lt;n} 2^{-I(K_j)}+\sum_{j:I(K_j)\geqslant n} 2^{-I(K_j)}}.&lt;/math&gt;

Then identify the two probabilities as,
:&lt;math&gt;P(x \text{ has pattern}) = \sum_{j:I(K_j)&lt;n} 2^{-I(K_j)}&lt;/math&gt;
:&lt;math&gt;P(x \text{ is random}) = \sum_{j:I(K_j)\geqslant n} 2^{-I(K_j)}&lt;/math&gt;

But the prior probability that ''x'' is a random set of bits is &lt;math&gt;2^{-n}&lt;/math&gt;. So,
:&lt;math&gt;P(s = R(K_i) |  T_n(s) = x) = \frac{2^{-I(K_i)}}{2^{-n} + \sum_{j:I(K_j)&lt;n} 2^{-I(K_j)}}.&lt;/math&gt;

The probability that the source is random, or unpredictable is,
:&lt;math&gt;P(\operatorname{random}(s) |  T_n(s) = x) = \frac{2^{-n}}{2^{-n} + \sum_{j:I(K_j)&lt;n} 2^{-I(K_j)}}.&lt;/math&gt;

===A model for inductive inference===

A model of how worlds are constructed is used in determining the probabilities of theories,
* A random bit string is selected.
* A condition is constructed from the bit string.
* A world is constructed that is consistent with the condition.

If ''w'' is the bit string then the world is created such that &lt;math&gt;R(w)&lt;/math&gt; is true.  An [[intelligent agent]] has some facts about the word, represented by the bit string ''c'', which gives the condition,
:&lt;math&gt;C = R(c)&lt;/math&gt;

The set of bit strings identical with any condition ''x'' is &lt;math&gt;E(x)&lt;/math&gt;.
:&lt;math&gt;\forall x, E(x) = \{w : R(w) \equiv x \}&lt;/math&gt;

A theory is a simpler condition that explains (or implies) ''C''.  The set of all such theories is called ''T'',
:&lt;math&gt; T(C) = \{t : t \to C \}&lt;/math&gt;

====Applying Bayes' theorem====

[[Bayes' theorem#Extended form|extended form of Bayes' theorem]] may be applied
:&lt;math&gt;P(A_i |  B) = \frac{P(B |  A_i)\,P(A_i)}{\sum_j P(B |  A_j)\,P(A_j)},&lt;/math&gt;
where,
:&lt;math&gt;B = E(C)&lt;/math&gt;
:&lt;math&gt;A_i = E(t)&lt;/math&gt;

To apply Bayes' theorem the following must hold: &lt;math&gt;A_i&lt;/math&gt; is a [[partition of a set|partition]] of the event space.

For &lt;math&gt;T(C)&lt;/math&gt; to be a partition, no bit string ''n'' may belong to two theories.  To prove this assume they can and derive a contradiction,
:&lt;math&gt;(N \in T) \and (N \in M) \and (N \ne M) \and (n \in E(N) \and n \in E(M))&lt;/math&gt;
:&lt;math&gt;\implies (N \ne M) \and R(n) \equiv N \and R(n) \equiv M&lt;/math&gt;
:&lt;math&gt;\implies \bot&lt;/math&gt;

Secondly prove that ''T'' includes all outcomes consistent with the condition. As all theories consistent with ''C'' are included then &lt;math&gt;R(w)&lt;/math&gt; must be in this set.

So Bayes theorem may be applied as specified giving,
:&lt;math&gt;\forall t \in T(C), P(E(t) | E(C)) = \frac{P(E(t)) \cdot P(E(C) | E(t))}{\sum_{j \in T(C)} P(E(j)) \cdot P(E(C) | E(j))}  &lt;/math&gt;

Using the [[#Implication and condition probability|implication and condition probability law]], the definition of &lt;math&gt;T(C)&lt;/math&gt; implies,
:&lt;math&gt;\forall t \in T(C), P(E(C) | E(t)) = 1&lt;/math&gt;

The probability of each theory in ''T'' is given by,
:&lt;math&gt; \forall t \in T(C), P(E(t)) = \sum_{n: R(n) \equiv t} 2^{-L(n)}&lt;/math&gt;

so,
:&lt;math&gt;\forall t \in T(C), P(E(t) | E(C)) = \frac{\sum_{n: R(n) \equiv t} 2^{-L(n)}}{\sum_{j \in T(C)} \sum_{m: R(m) \equiv j} 2^{-L(m)}}  &lt;/math&gt;

Finally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfy,
:&lt;math&gt;\forall t \in T(C), P(E(t) | E(C)) = P(t | C)&lt;/math&gt;

giving
:&lt;math&gt;\forall t \in T(C), P(t | C) = \frac{\sum_{n: R(n) \equiv t} 2^{-L(n)}}{\sum_{j \in T(C)} \sum_{m: R(m) \equiv j} 2^{-L(m)}}  &lt;/math&gt;

This is the probability of the theory ''t'' after observing that the condition ''C'' holds.

====Removing theories without predictive power====

Theories that are less probable than the condition ''C'' have no predictive power.  Separate them out giving,
:&lt;math&gt;\forall t \in T(C), P(t | C) = \frac{P(E(t))}{(\sum_{j : j \in T(C) \and P(E(j)) &gt; P(E(C))} P(E(j))) + (\sum_{j : j \in T(C) \and P(E(j)) \le P(E(C))} P(j))}  &lt;/math&gt;

The probability of the theories without predictive power on ''C'' is the same as the probability of ''C''.  So,
:&lt;math&gt;P(E(C)) = \sum_{j : j \in T(C) \and P(E(j)) \le P(E(C))} P(j)&lt;/math&gt;

So the probability
:&lt;math&gt;\forall t \in T(C), P(t | C) = \frac{P(E(t))}{P(E(C)) + \sum_{j : j \in T(C) \and P(E(j)) &gt; P(E(C))} P(E(j))}  &lt;/math&gt;

and the probability of no prediction for C, written as &lt;math&gt;\operatorname{random}(C)&lt;/math&gt;,
:&lt;math&gt;P(\text{random}(C) | C) = \frac{P(E(C))}{P(E(C)) + \sum_{j : j \in T(C) \and P(E(j)) &gt; P(E(C))} P(E(j))}  &lt;/math&gt;

The probability of a condition was given as,
:&lt;math&gt; \forall t, P(E(t)) = \sum_{n: R(n) \equiv t} 2^{-L(n)}&lt;/math&gt;

Bit strings for theories that are more complex than the bit string given to the agent as input have no predictive power.  There probabilities are better included in the ''random'' case.  To implement this a new definition is given as ''F'' in,

:&lt;math&gt; \forall t, P(F(t, c)) = \sum_{n: R(n) \equiv t \and L(n) &lt; L(c)} 2^{-L(n)}&lt;/math&gt;

Using ''F'', an improved version of the abductive probabilities is,
:&lt;math&gt;\forall t \in T(C), P(t | C) = \frac{P(F(t, c))}{P(F(C, c)) + \sum_{j : j \in T(C) \and P(F(j, c)) &gt; P(F(C, c))} P(E(j, c))}  &lt;/math&gt;
:&lt;math&gt;P(\operatorname{random}(C) | C) = \frac{P(F(C, c))}{P(F(C, c)) + \sum_{j : j \in T(C) \and P(F(j, c)) &gt; P(F(C, c))} P(F(j, c))}  &lt;/math&gt;

==Key people==

* [[William of Ockham]]
* [[Thomas Bayes]]
* [[Ray Solomonoff]]
* [[Andrey Kolmogorov]]
* [[Chris Wallace (computer scientist)|Chris Wallace]]
* D. M. Boulton
* [[Jorma Rissanen]]
* [[Marcus Hutter]]

==See also==

* [[Abductive reasoning]]
* [[Algorithmic probability]]
* [[Algorithmic information theory]]
* [[Bayesian inference]]
* [[Information theory]]
* [[Inductive inference]]
* [[Inductive logic programming]]
* [[Inductive reasoning]]
* [[Learning]]
* [[Minimum message length]]
* [[Minimum description length]]
* [[Occam's razor]]
* [[Solomonoff's theory of inductive inference]]
* [[Universal artificial intelligence]]

==References==
{{Reflist}}

==External links==
* Rathmanner, S and Hutter, M., &quot;A Philosophical Treatise of Universal Induction&quot; in Entropy 2011, 13, 1076–1136: A very clear philosophical and mathematical analysis of Solomonoff's Theory of Inductive Inference.
* [[Chris Wallace (computer scientist)|C.S. Wallace]], [http://www.springeronline.com/sgw/cda/frontpage/0,11855,4-10129-22-35893962-0,00.html Statistical and Inductive Inference by Minimum Message Length], Springer-Verlag (Information Science and Statistics), {{ISBN|0-387-23795-X}}, May 2005 – [https://www.springer.com/west/home/statistics/theory?SGWID=4-10129-22-35893962-detailsPage=ppmmedia|toc chapter headings], [http://www.csse.monash.edu.au/mml/toc.pdf table of contents] and [https://books.google.com/books?ie=ISO-8859-1&amp;id=3NmFwNHaNbUC&amp;q=wallace+%22statistical+and+inductive+inference+by+minimum+message+length%22&amp;dq=wallace+%22statistical+and+inductive+inference+by+minimum+message+length%22 sample pages].





</text>
      <sha1>mqcj80fhhckertl4c12mamf6beict1x</sha1>
    </revision>
  </page>
  <page>
    <title>Universal portfolio algorithm</title>
    <ns>0</ns>
    <id>37787103</id>
    <revision>
      <id>807757629</id>
      <parentid>799529805</parentid>
      <timestamp>2017-10-29T23:53:37Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>/* top */[[WP:AWB/GF|General fixes]], removed orphan tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1086">The '''universal portfolio algorithm''' is a portfolio selection algorithm from the field of [[machine learning]] and [[information theory]]. The algorithm learns adaptively from historical data and maximizes the log-optimal growth rate in the long run. It was introduced by the late [[Stanford University]] information theorist [[Thomas M. Cover]].&lt;ref&gt;
{{cite journal
  |url=http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9965.1991.tb00002.x/abstract
  |title=Universal Portfolios
  |first=Thomas M.|last=Cover
  |journal=Mathematical Finance
  |volume=1
  |issue=1
  |pages=1–29
  |year=1991
  |doi=10.1111/j.1467-9965.1991.tb00002.x
}}&lt;/ref&gt;

The algorithm rebalances the portfolio at the beginning of each trading period. At the beginning of the first trading period it starts with a naive diversification. In the following trading periods the portfolio composition depends on the historical total return of all possible constant-rebalanced portfolios.

==References==
{{reflist}}



</text>
      <sha1>g11p45brpb82eq1qzkaefqks70lc8l3</sha1>
    </revision>
  </page>
  <page>
    <title>Kernel embedding of distributions</title>
    <ns>0</ns>
    <id>41370976</id>
    <revision>
      <id>815511545</id>
      <parentid>810155035</parentid>
      <timestamp>2017-12-15T08:01:19Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v478)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="53714">In [[machine learning]], the '''kernel embedding of distributions''' (also called the '''kernel mean''' or '''mean map''') comprises a class of [[nonparametric]] methods in which a [[probability distribution]] is represented as an element of a [[reproducing kernel Hilbert space]]  (RKHS).&lt;ref name = &quot;Smola2007&quot;&gt;A. Smola, A. Gretton, L. Song, B. Schölkopf. (2007). [http://eprints.pascal-network.org/archive/00003987/01/SmoGreSonSch07.pdf A Hilbert Space Embedding for Distributions]. ''Algorithmic Learning Theory: 18th International Conference''. Springer: 13–31.&lt;/ref&gt;   A generalization of the individual data-point feature mapping done in classical [[kernel methods]], the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as [[inner product]]s, distances, [[projection (linear algebra)|projections]], [[linear transformation]]s, and [[spectral theory|spectral analysis]].&lt;ref name = &quot;Song2013&quot;&gt;L. Song, K. Fukumizu, F. Dinuzzo, A. Gretton (2013). [http://www.gatsby.ucl.ac.uk/~gretton/papers/SonFukGre13.pdf Kernel Embeddings of Conditional Distributions: A unified kernel framework for nonparametric inference in graphical models]. ''IEEE Signal Processing Magazine'' '''30''': 98–111.&lt;/ref&gt;    This [[machine learning|learning]] framework is very general and can be applied to distributions over any space &lt;math&gt;\Omega &lt;/math&gt; on which a sensible [[kernel function]] (measuring similarity between elements of &lt;math&gt;\Omega &lt;/math&gt;) may be defined.  For example, various kernels have been proposed for learning from data which are: [[Vector (mathematics and physics)|vectors]] in &lt;math&gt;\mathbb{R}^d&lt;/math&gt;, discrete classes/categories, [[string (computer science)|string]]s, [[Graph (discrete mathematics)|graph]]s/[[network theory|networks]], images, [[time series]], [[manifold]]s, [[dynamical systems]], and other structured objects.&lt;ref&gt;J. Shawe-Taylor, N. Christianini. (2004). ''Kernel Methods for Pattern Analysis''. Cambridge University Press, Cambridge, UK.&lt;/ref&gt;&lt;ref&gt;T. Hofmann, B. Schölkopf, A. Smola. (2008). [http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1211819561 Kernel Methods in Machine Learning]. ''The Annals of Statistics'' '''36'''(3):1171–1220.&lt;/ref&gt;  The theory behind kernel embeddings of distributions has been primarily developed by  [http://alex.smola.org/ Alex Smola], [http://www.cc.gatech.edu/~lsong/ Le Song ], [http://www.gatsby.ucl.ac.uk/~gretton/ Arthur Gretton], and [[Bernhard Schölkopf]]. A review of recent works on kernel embedding of distributions can be found in &lt;ref&gt;{{Cite journal|last=Muandet|first=Krikamol|last2=Fukumizu|first2=Kenji|last3=Sriperumbudur|first3=Bharath|last4=Schölkopf|first4=Bernhard|date=2017-06-28|title=Kernel Mean Embedding of Distributions: A Review and Beyond|url=https://dx.doi.org/10.1561/2200000060|journal=Foundations and Trends® in Machine Learning|language=English|volume=10|issue=1-2|pages=1–141|doi=10.1561/2200000060|issn=1935-8237}}&lt;/ref&gt;.

The analysis of distributions is fundamental in [[machine learning]] and [[statistics]],  and many algorithms in these fields rely on information theoretic approaches such as [[entropy]], [[mutual information]], or [[Kullback–Leibler divergence]].  However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data.&lt;ref name = &quot;SongThesis&quot;&gt;L. Song. (2008) [http://www.cc.gatech.edu/~lsong/papers/lesong_thesis.pdf Learning via Hilbert Space Embedding of Distributions]. PhD Thesis, University of Sydney.&lt;/ref&gt;  Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. [[Mixture model#Gaussian mixture model|Gaussian mixture models]]), while nonparametric methods like [[kernel density estimation]] (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or [[characteristic function (probability theory)|characteristic function]] representation (via the [[Fourier transform]] of the distribution) break down in high-dimensional settings.&lt;ref name = &quot;Song2013&quot; /&gt;

Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages:&lt;ref name = &quot;SongThesis&quot; /&gt;
# Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables
#  Intermediate density estimation is not needed
#  Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel)
# If a ''characteristic'' kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the [[kernel trick]], computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple [[Gramian matrix|Gram]] matrix operations
# Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution)  to the kernel embedding of the true underlying distribution can be proven.
# Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methods
Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.

==Definitions==

Let &lt;math&gt; X &lt;/math&gt; denote a random variable with codomain &lt;math&gt; \Omega &lt;/math&gt; and distribution &lt;math&gt; P(X) &lt;/math&gt;.  Given a kernel &lt;math&gt; k &lt;/math&gt; on &lt;math&gt; \Omega \times \Omega &lt;/math&gt;, the [[Reproducing kernel Hilbert space#Moore-Aronszajn Theorem|Moore-Aronszajn Theorem]] asserts the existence of a RKHS &lt;math&gt; \mathcal{H} &lt;/math&gt; (a [[Hilbert space]] of functions &lt;math&gt; f: \Omega \rightarrow \mathbb{R} &lt;/math&gt; equipped with inner products  &lt;math&gt; \langle \cdot, \cdot \rangle_\mathcal{H} &lt;/math&gt; and norms &lt;math&gt; || \cdot ||_\mathcal{H} &lt;/math&gt;) in which the element &lt;math&gt; \ k(x,\cdot) &lt;/math&gt; satisfies the reproducing property &lt;math&gt; \langle f, k(x,\cdot) \rangle_\mathcal{H} = f(x) \ \forall f \in \mathcal{H}, \forall x \in \Omega &lt;/math&gt;.  One may alternatively consider &lt;math&gt; \ k(x,\cdot)&lt;/math&gt; an implicit feature mapping &lt;math&gt; \phi(x) &lt;/math&gt; from  &lt;math&gt; \Omega &lt;/math&gt; to &lt;math&gt; \mathcal{H} &lt;/math&gt; (which is therefore also called the feature space), so that  &lt;math&gt;\ k(x, x') = \langle \phi(x), \phi(x')\rangle_\mathcal{H} &lt;/math&gt; can be viewed as a measure of similarity between points &lt;math&gt; x, x' \in \Omega &lt;/math&gt;.  While the [[similarity measure]] is linear in the feature space, it may be highly nonlinear in the original space depending on the choice of kernel.

===Kernel embedding===
The kernel embedding of the distribution &lt;math&gt; P(X) &lt;/math&gt; in &lt;math&gt; \mathcal{H} &lt;/math&gt; (also called the '''kernel mean''' or '''mean map''') is given by:&lt;ref name = &quot;Smola2007&quot; /&gt;

:: &lt;math&gt;\mu_X := \mathbb{E}_X [k(X, \cdot) ] = \mathbb{E}_X [\phi(X) ] = \int_\Omega \phi(x) \ \mathrm{d}P(x) &lt;/math&gt;

If &lt;math&gt;P&lt;/math&gt; allows a square integrable density &lt;math&gt;p&lt;/math&gt;, then &lt;math&gt;\mu_X = \mathcal{E}_k p&lt;/math&gt;, where &lt;math&gt;\mathcal{E}_k&lt;/math&gt; is the [[Hilbert–Schmidt integral operator]]. A kernel is ''characteristic'' if the mean embedding &lt;math&gt;\mu: \{\text{family of distributions over }\Omega \} \rightarrow \mathcal{H} &lt;/math&gt;  is injective.&lt;ref name = &quot;Fukumizu2008&quot;&gt;K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf (2008). [http://papers.nips.cc/paper/3340-kernel-measures-of-conditional-dependence.pdf Kernel measures of conditional independence]. ''Advances in Neural Information Processing Systems'' '''20''', MIT Press, Cambridge, MA.&lt;/ref&gt;  Each distribution can thus be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used.

===Empirical kernel embedding===
Given &lt;math&gt;n&lt;/math&gt; training examples &lt;math&gt; \{x_1, \dots, x_n\} &lt;/math&gt; drawn [[Independent and identically distributed random variables|independently and identically distributed]] (i.i.d.) from &lt;math&gt; P &lt;/math&gt;, the kernel embedding of &lt;math&gt; P &lt;/math&gt; can be empirically estimated as
:: &lt;math&gt; \widehat{\mu}_X = \frac{1}{n} \sum_{i=1}^n \phi(x_i) &lt;/math&gt;

===Joint distribution embedding===
If &lt;math&gt; Y &lt;/math&gt; denotes another random variable (for simplicity, assume the co-domain of &lt;math&gt; Y &lt;/math&gt; is also &lt;math&gt; \Omega &lt;/math&gt; with the same kernel &lt;math&gt; k &lt;/math&gt; which satisfies &lt;math&gt; \langle \phi(x) \otimes \phi(y), \phi(x') \otimes \phi(y') \rangle = k(x,x') \otimes k(y,y')&lt;/math&gt;), then the [[Joint probability distribution|joint distribution]] &lt;math&gt; P(X,Y) &lt;/math&gt; can be mapped into a [[tensor product]] feature space &lt;math&gt; \mathcal{H} \otimes \mathcal{H} &lt;/math&gt; via &lt;ref name = &quot;Song2013&quot;/&gt;
:: &lt;math&gt; \mathcal{C}_{XY} = \mathbb{E}_{XY} [\phi(X) \otimes \phi(Y)] = \int_{\Omega \times \Omega} \phi(x) \otimes \phi(y) \ \mathrm{d} P(x,y) &lt;/math&gt;

By the equivalence between a [[tensor]] and a [[linear map]], this joint embedding may be interpreted as an uncentered [[cross-covariance]] operator &lt;math&gt; \mathcal{C}_{XY}: \mathcal{H} \rightarrow \mathcal{H} &lt;/math&gt; from which the cross-covariance of mean-zero functions &lt;math&gt;f,g \in \mathcal{H}&lt;/math&gt; can be computed as &lt;ref name = &quot;SongCDE&quot;&gt;L. Song, J. Huang, A. J. Smola, K. Fukumizu. (2009). [http://www.stanford.edu/~jhuang11/research/pubs/icml09/icml09.pdf Hilbert space embeddings of conditional distributions]. ''Proc. Int. Conf. Machine Learning''. Montreal, Canada: 961-968.&lt;/ref&gt;
:: &lt;math&gt; \text{Cov}_{XY} (f(X), g(Y)) := \mathbb{E}_{XY} [f(X) g(Y)] = \langle f , \mathcal{C}_{XY} g \rangle_{\mathcal{H}} = \langle f \otimes g , \mathcal{C}_{XY} \rangle_{\mathcal{H} \otimes \mathcal{H}} &lt;/math&gt;

Given &lt;math&gt;n&lt;/math&gt; pairs of training examples &lt;math&gt;\{(x_1, y_1), \dots, (x_n, y_n)\} &lt;/math&gt; drawn i.i.d. from &lt;math&gt; P &lt;/math&gt;, we can also empirically estimate the joint distribution kernel embedding via
:: &lt;math&gt; \widehat{\mathcal{C}}_{XY} = \frac{1}{n} \sum_{i=1}^n \phi(x_i) \otimes \phi(y_i) &lt;/math&gt;

===Conditional distribution embedding===
Given a [[conditional distribution]] &lt;math&gt;P(Y \mid X) &lt;/math&gt;, one can define the corresponding RKHS embedding as &lt;ref name = &quot;Song2013&quot;/&gt;
:: &lt;math&gt; \mu_{Y \mid x} = \mathbb{E}_{Y \mid x} [ \phi(Y) ] = \int_\Omega \phi(y) \ \mathrm{d}P(y \mid x)    &lt;/math&gt;
Note that the embedding of &lt;math&gt;P(Y \mid X) &lt;/math&gt; thus defines a family of points in the RKHS indexed by the values &lt;math&gt;x&lt;/math&gt; taken by conditioning variable &lt;math&gt;X &lt;/math&gt;.  By fixing &lt;math&gt;X&lt;/math&gt; to a particular value, we obtain a single element in &lt;math&gt;\mathcal{H}&lt;/math&gt;, and thus it is natural to define the operator
:: &lt;math&gt;\mathcal{C}_{Y \mid X}: \mathcal{H} \rightarrow \mathcal{H}&lt;/math&gt; as &lt;math&gt; \mathcal{C}_{Y \mid X} = \mathcal{C}_{YX} \mathcal{C}_{XX}^{-1} &lt;/math&gt;
which given the feature mapping of &lt;math&gt;x &lt;/math&gt; outputs the conditional embedding of &lt;math&gt;Y&lt;/math&gt; given &lt;math&gt;X = x&lt;/math&gt;.  Assuming that for all &lt;math&gt; g \in \mathcal{H}: \ \mathbb{E}_{Y \mid X} [g(Y)] \in \mathcal{H} &lt;/math&gt;, it can be shown that &lt;ref name = &quot;SongCDE&quot; /&gt;
:: &lt;math&gt; \mu_{Y \mid x} = \mathcal{C}_{Y \mid X} \phi(x) &lt;/math&gt;
This assumption is always true for finite domains with characteristic kernels, but may not necessarily hold for continuous domains.&lt;ref name = &quot;Song2013&quot;/&gt;  Nevertheless, even in cases where the assumption fails, &lt;math&gt; \mathcal{C}_{Y \mid X} \phi(x) &lt;/math&gt; may still be used to approximate the conditional kernel embedding &lt;math&gt; \mu_{Y \mid x} &lt;/math&gt;, and in practice, the inversion operator is replaced with a regularized version of itself &lt;math&gt; (\mathcal{C}_{XX} + \lambda \mathbf{I})^{-1} &lt;/math&gt; (where &lt;math&gt;\mathbf{I}&lt;/math&gt; denotes the [[identity matrix]]).

Given training examples &lt;math&gt; \{(x_1, y_1),\dots, (x_n, y_n)\} &lt;/math&gt;, the empirical kernel conditional embedding operator may be estimated as &lt;ref name = &quot;Song2013&quot; /&gt;
:: &lt;math&gt;\widehat{C}_{Y\mid X} = \boldsymbol{\Phi} (\mathbf{K} + \lambda \mathbf{I})^{-1} \boldsymbol{\Upsilon}^T&lt;/math&gt;
where &lt;math&gt;\boldsymbol{\Phi} = \left(\phi(y_i),\dots, (y_n)\right), \boldsymbol{\Upsilon} = \left(\phi(x_i),\dots, (x_n)\right) &lt;/math&gt; are implicitly formed feature matrices, &lt;math&gt;\mathbf{K} =\boldsymbol{\Upsilon}^T \boldsymbol{\Upsilon} &lt;/math&gt; is the Gram matrix for samples of &lt;math&gt;X&lt;/math&gt;, and &lt;math&gt;\lambda&lt;/math&gt; is a [[Regularization (mathematics)|regularization]] parameter needed to avoid [[overfitting]].

Thus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of &lt;math&gt;Y&lt;/math&gt; in the feature space:
::&lt;math&gt; \widehat{\mu}_{Y\mid x} = \sum_{i=1}^n \beta_i (x) \phi(y_i) = \boldsymbol{\Phi} \boldsymbol{\beta}(x) &lt;/math&gt; where &lt;math&gt;  \boldsymbol{\beta}(x)   = (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{K}_x&lt;/math&gt; and &lt;math&gt;  \mathbf{K}_x = \left( k(x_1, x), \dots, k(x_n, x)  \right)^T &lt;/math&gt;

==Properties==

* The expectation of any function &lt;math&gt; f &lt;/math&gt; in the RKHS can be computed as an inner product with the kernel embedding:
:: &lt;math&gt; \mathbb{E}_X [f(X)] = \langle f, \mu_X \rangle_\mathcal{H} &lt;/math&gt;

* In the presence of large sample sizes, manipulations of the &lt;math&gt;n \times n&lt;/math&gt; Gram matrix may be computationally demanding.  Through use of a low-rank approximation of the Gram matrix (such as the [[incomplete Cholesky factorization]]), running time and memory requirements of kernel-embedding-based learning algorithms can be drastically reduced without suffering much loss in approximation accuracy.&lt;ref name = &quot;Song2013&quot;/&gt;

=== Convergence of empirical kernel mean to the true distribution embedding  ===
* If &lt;math&gt; k &lt;/math&gt; is defined such that &lt;math&gt; f \in [0, 1] &lt;/math&gt; for all &lt;math&gt; f \in \mathcal{H}&lt;/math&gt; with &lt;math&gt; || f ||_\mathcal{H} \le 1 &lt;/math&gt; (as is the case for the widely used [[radial basis function]] kernels), then with probability at least  &lt;math&gt;\  1-\delta &lt;/math&gt;:&lt;ref name=&quot;SongThesis&quot; /&gt; &lt;br&gt;  {{spaces|7}} &lt;math&gt; ||\mu_X - \widehat{\mu}_X  ||_\mathcal{H} = \sup_{f \in \mathcal{B}(0,1)} \left| \mathbb{E}_X[f(X)] - \frac{1}{n} \sum_{i=1}^n f(x_i) \right| \le \frac{2}{n} \mathbb{E}_X \left[ \sqrt{\text{tr} K} \right] + \sqrt{\frac{\log (2/\delta)}{2n}} &lt;/math&gt; &lt;br&gt; where &lt;math&gt; \mathcal{B}(0,1) &lt;/math&gt; denotes the unit ball in &lt;math&gt; \mathcal{H} &lt;/math&gt;  and &lt;math&gt; \mathbf{K} &lt;/math&gt; is the Gram matrix whose &lt;math&gt; i,j &lt;/math&gt;th entry is &lt;math&gt; k(x_i, x_j) &lt;/math&gt;.
* The rate of convergence (in RKHS norm) of the empirical kernel embedding to its distribution counterpart is &lt;math&gt; O(n^{-1/2}) &lt;/math&gt; and does ''not''  depend on the dimension of &lt;math&gt; X &lt;/math&gt;.
* Statistics based on kernel embeddings thus avoid the [[curse of dimensionality]], and though the true underlying distribution is unknown in practice, one can (with high probability) obtain an approximation within &lt;math&gt; O(n^{-1/2})&lt;/math&gt; of the true kernel embedding based on a finite sample of size &lt;math&gt;n&lt;/math&gt;.
* For the embedding of conditional distributions, the empirical estimate can be seen as a ''weighted'' average of feature mappings (where the weights &lt;math&gt;\beta_i(x) &lt;/math&gt; depend on the value of the conditioning variable and capture the effect of the conditioning on the kernel embedding).  In this case, the empirical estimate converges to the conditional distribution RKHS embedding with rate &lt;math&gt; O\left(n^{-1/4} \right) &lt;/math&gt; if the regularization parameter &lt;math&gt; \lambda &lt;/math&gt; is decreased as &lt;math&gt;O\left( n^{-1/2} \right) &lt;/math&gt;, though faster rates of convergence may be achieved by placing additional assumptions on the joint distribution.&lt;ref name=&quot;Song2013&quot;/&gt;

=== Universal kernels ===
* Letting &lt;math&gt;C(\mathcal{X})&lt;/math&gt; denote the space of [[Continuous function|continuous]] [[Bounded function|bounded]] functions on [[Compact space|compact]] domain &lt;math&gt;\mathcal{X}&lt;/math&gt;, we call a kernel &lt;math&gt; k &lt;/math&gt;  ''universal'' if &lt;math&gt;k(x,\cdot)&lt;/math&gt; is continuous for all &lt;math&gt;x&lt;/math&gt; and the RKHS induced by &lt;math&gt;k&lt;/math&gt; is [[Dense set|dense]] in &lt;math&gt;C(\mathcal{X})&lt;/math&gt;.
* If &lt;math&gt;k&lt;/math&gt; induces a strictly positive definite kernel matrix for any set of distinct points, then it is a universal kernel.&lt;ref name = &quot;SongThesis&quot; /&gt;  For example, the widely used Gaussian RBF kernel
:: &lt;math&gt; k(x,x') = \exp\left(-\frac{1}{2\sigma^2} ||x-x'||^2 \right)  &lt;/math&gt;
on compact subsets of &lt;math&gt;\mathbb{R}^d &lt;/math&gt; is universal.

* If &lt;math&gt;k&lt;/math&gt; is shift-invariant &lt;math&gt;h(x-y)=k(x, y)&lt;/math&gt; and its representation in Fourier domain is
:: &lt;math&gt;h(t) = \int e^{-i\langle t, \omega \rangle} \mu(d\omega)&lt;/math&gt;
and [[Support_(mathematics)|support]] of &lt;math&gt;\mu&lt;/math&gt; is an entire space, then &lt;math&gt;k&lt;/math&gt; is universal. &lt;ref&gt; [https://web.stanford.edu/class/cs229t/notes.pdf] page 139 &lt;/ref&gt; For example, Gaussian RBF is universal, [[sinc]] kernel is not universal.

* If &lt;math&gt; k &lt;/math&gt; is universal, then it is ''characteristic'', i.e. the kernel embedding is one-to-one.&lt;ref&gt;A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, A. Smola. (2007). [http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf A kernel method for the two-sample-problem]. ''Advances in Neural Information Processing Systems'' '''19''', MIT Press, Cambridge, MA.&lt;/ref&gt;

=== Parameter selection for conditional distribution kernel embeddings ===
* The empirical kernel conditional distribution embedding operator &lt;math&gt;\widehat{\mathcal{C}}_{Y|X} &lt;/math&gt; can alternatively be viewed as the solution of the following regularized least squares  (function-valued) regression problem &lt;ref&gt;S. Grunewalder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton, M. Pontil. (2012). [http://icml.cc/2012/papers/898.pdf Conditional mean embeddings as regressors]. ''Proc. Int. Conf. Machine Learning'': 1823–1830.&lt;/ref&gt;
:: &lt;math&gt; \min_{\mathcal{C}: \mathcal{H} \rightarrow \mathcal{H}} \sum_{i=1}^n || \phi(y_i) - \mathcal{C} \phi(x_i) ||_\mathcal{H}^2 + \lambda ||\mathcal{C} ||_{HS}^2  &lt;/math&gt; where &lt;math&gt; ||\cdot ||_{HS} &lt;/math&gt; is the [[Hilbert–Schmidt operator|Hilbert-Schmidt norm]].
* One can thus select the regularization parameter &lt;math&gt; \lambda &lt;/math&gt; by performing [[cross-validation (statistics)|cross-validation]] based on the squared loss function of the regression problem.

== Rules of probability as operations in the RKHS ==

This section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al.&lt;ref name = &quot;Song2013&quot; /&gt;&lt;ref name = &quot;SongCDE&quot; /&gt;  The following notation is adopted:
* &lt;math&gt; P(X,Y) = &lt;/math&gt; joint distribution over random variables &lt;math&gt; X, Y &lt;/math&gt;
* &lt;math&gt; P(X) = \int_\Omega P(X, \mathrm{d}y) = &lt;/math&gt; marginal distribution of &lt;math&gt; X &lt;/math&gt;; &lt;math&gt; P(Y) = &lt;/math&gt; marginal distribution of &lt;math&gt; Y &lt;/math&gt;
* &lt;math&gt; P(Y \mid X)  = \frac{P(X,Y)}{P(X)} = &lt;/math&gt; conditional distribution of &lt;math&gt; Y &lt;/math&gt; given &lt;math&gt; X &lt;/math&gt; with corresponding conditional embedding operator &lt;math&gt;  \mathcal{C}_{Y \mid X}&lt;/math&gt;
* &lt;math&gt; \pi(Y) = &lt;/math&gt; prior distribution over &lt;math&gt; Y &lt;/math&gt;
* &lt;math&gt; Q &lt;/math&gt; is used to distinguish distributions which incorporate the prior from distributions &lt;math&gt; P &lt;/math&gt;  which do not rely on the prior

In practice, all embeddings are empirically estimated from data  &lt;math&gt; \{(x_1,y_1),\dots, (x_n, y_n)\} &lt;/math&gt; and it assumed that a set of samples &lt;math&gt;\{\widetilde{y}_1, \dots, \widetilde{y}_{\widetilde{n}} \} &lt;/math&gt; may be used to estimate the kernel embedding of the prior distribution &lt;math&gt; \pi(Y) &lt;/math&gt;.

=== Kernel sum rule ===
In probability theory, the marginal distribution of &lt;math&gt; X &lt;/math&gt; can be computed by integrating out &lt;math&gt; Y &lt;/math&gt; from the joint density (including the prior distribution on &lt;math&gt; Y &lt;/math&gt;)
:: &lt;math&gt; Q(X) = \int_\Omega P(X \mid Y) \mathrm{d} \pi(Y) &lt;/math&gt;
The analog of this rule in the kernel embedding framework states that &lt;math&gt; \mu_X^\pi &lt;/math&gt;, the RKHS embedding of &lt;math&gt; Q(X)&lt;/math&gt;, can be computed via
:: &lt;math&gt;  \mu_X^\pi = \mathbb{E}_{Y} [\mathcal{C}_{X \mid Y} \phi(Y) ] = \mathcal{C}_{X\mid Y} \mathbb{E}_{Y} [\phi(Y)] = \mathcal{C}_{X\mid Y}  \mu_Y^\pi    &lt;/math&gt;   where  &lt;math&gt;\mu_Y^\pi&lt;/math&gt; is the kernel embedding of  &lt;math&gt; \pi(Y) &lt;/math&gt;
In practical implementations, the kernel sum rule takes the following form
:: &lt;math&gt; \widehat{\mu}_X^\pi  =  \widehat{\mathcal{C}}_{X \mid Y} \widehat{\mu}_Y^\pi = \boldsymbol{\Upsilon} (\mathbf{G} + \lambda \mathbf{I})^{-1} \widetilde{\mathbf{G}} \boldsymbol{\alpha}  &lt;/math&gt;
where  &lt;math&gt;\mu_Y^\pi = \sum_{i=1}^{\widetilde{n}} \alpha_i \phi(\widetilde{y}_i)&lt;/math&gt; is the empirical kernel embedding of the prior distribution, &lt;math&gt;\boldsymbol{\alpha}  = (\alpha_1, \dots, \alpha_{\widetilde{n}} )^T&lt;/math&gt;, &lt;math&gt;\boldsymbol{\Upsilon} = \left(\phi(x_1), \dots, \phi(x_n) \right) &lt;/math&gt;, and &lt;math&gt;\mathbf{G}, \widetilde{\mathbf{G}} &lt;/math&gt; are Gram matrices with entries &lt;math&gt;\mathbf{G}_{ij} = k(y_i, y_j), \widetilde{\mathbf{G}}_{ij} = k(y_i, \widetilde{y}_j) &lt;/math&gt; respectively.

=== Kernel chain rule ===
In probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions
:: &lt;math&gt; Q(X,Y) = P(X \mid Y) \pi(Y) &lt;/math&gt;
The analog of this rule in the kernel embedding framework states that &lt;math&gt; \mathcal{C}_{XY}^\pi &lt;/math&gt;, the joint embedding of &lt;math&gt; Q(X,Y) &lt;/math&gt;, can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with &lt;math&gt; \pi(Y) &lt;/math&gt;
:: &lt;math&gt; \mathcal{C}_{XY}^\pi = \mathcal{C}_{X \mid Y} \mathcal{C}_{YY}^\pi &lt;/math&gt; where &lt;math&gt; \mathcal{C}_{XY}^\pi = \mathbb{E}_{XY} [\phi(X) \otimes \phi(Y) ] &lt;/math&gt; and &lt;math&gt; \mathcal{C}_{YY}^\pi = \mathbb{E}_Y [\phi(Y) \otimes \phi(Y)] &lt;/math&gt;
In practical implementations, the kernel chain rule takes the following form
:: &lt;math&gt; \widehat{\mathcal{C}}_{XY}^\pi = \widehat{\mathcal{C}}_{X \mid Y} \widehat{\mathcal{C}}_{YY}^\pi = \boldsymbol{\Upsilon} (\mathbf{G} + \lambda \mathbf{I})^{-1} \widetilde{\mathbf{G}} \text{diag}(\boldsymbol{\alpha}) \boldsymbol{\widetilde{\Phi}}^T &lt;/math&gt;

=== Kernel Bayes' rule ===
In probability theory, a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as
:: &lt;math&gt; Q(Y \mid x) = \frac{P(x \mid Y) \pi(Y)}{Q(x)}  &lt;/math&gt; where &lt;math&gt; Q(x) = \int_\Omega P(x \mid y) \mathrm{d} \pi(y) &lt;/math&gt;
The analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution
:: &lt;math&gt; \mu_{Y \mid x}^\pi = \mathcal{C}_{Y \mid X}^\pi \phi(x) = \mathcal{C}_{YX}^\pi ( \mathcal{C}_{XX}^\pi )^{-1} \phi(x) &lt;/math&gt; where from the chain rule: &lt;math&gt;  \mathcal{C}_{YX}^\pi = \left( \mathcal{C}_{X \mid Y} \mathcal{C}_{YY}^\pi \right)^T  &lt;/math&gt;.
In practical implementations, the kernel Bayes' rule takes the following form
:: &lt;math&gt; \widehat{\mu}_{Y \mid x}^\pi =   \widehat{\mathcal{C}}_{YX}^\pi \left( (\widehat{\mathcal{C}}_{XX})^2 + \widetilde{\lambda} \mathbf{I} \right)^{-1} \widehat{\mathcal{C}}_{XX}^\pi  \phi(x) = \widetilde{\boldsymbol{\Phi}} \boldsymbol{\Lambda}^T \left(  (\mathbf{D} \mathbf{K})^2 + \widetilde{\lambda} \mathbf{I} \right)^{-1} \mathbf{K} \mathbf{D} \mathbf{K}_x  &lt;/math&gt;
where    &lt;math&gt; \boldsymbol{\Lambda} = \left(\mathbf{G} + \widetilde{\lambda} \mathbf{I} \right)^{-1} \widetilde{\mathbf{G}} \text{diag}(\boldsymbol{\alpha}), \mathbf{D} = \text{diag}\left(\left(\mathbf{G} + \widetilde{\lambda} \mathbf{I} \right)^{-1} \widetilde{\mathbf{G}} \boldsymbol{\alpha} \right)   &lt;/math&gt;.
Two regularization parameters are used in this framework: &lt;math&gt;\lambda &lt;/math&gt; for the estimation of &lt;math&gt; \widehat{\mathcal{C}}_{YX}^\pi, \widehat{\mathcal{C}}_{XX}^\pi = \boldsymbol{\Upsilon} \mathbf{D} \boldsymbol{\Upsilon}^T &lt;/math&gt; and &lt;math&gt;\widetilde{\lambda} &lt;/math&gt;  for the estimation of the final conditional embedding operator &lt;math&gt; \widehat{\mathcal{C}}_{Y \mid X}^\pi = \widehat{\mathcal{C}}_{YX}^\pi \left( (\widehat{\mathcal{C}}_{XX}^\pi )^2 + \widetilde{\lambda} \mathbf{I}  \right)^{-1} \widehat{\mathcal{C}}_{XX}^\pi  &lt;/math&gt;.  The latter regularization is done on square of &lt;math&gt; \widehat{\mathcal{C}}_{XX}^\pi  &lt;/math&gt;  because &lt;math&gt; D &lt;/math&gt; may not be [[Positive-definite matrix|positive definite]].

==Applications==

=== Measuring distance between distributions ===
The '''maximum mean discrepancy''' (MMD) is a distance-measure between distributions &lt;math&gt;P(X)&lt;/math&gt; and &lt;math&gt;Q(Y)&lt;/math&gt; which is defined as the squared distance between their embeddings in the RKHS &lt;ref name = &quot;SongThesis&quot; /&gt;
:: &lt;math&gt;\text{MMD}(P,Q) = \left| \left| \mu_X - \mu_Y \right| \right|_{\mathcal{H}}^2 &lt;/math&gt;
While most distance-measures between distributions such as the widely used [[Kullback–Leibler divergence]] either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies,&lt;ref name = &quot;SongThesis&quot; /&gt; the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD.  The characterization of this distance as the ''maximum mean discrepancy'' refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions
:: &lt;math&gt;\text{MMD}(P,Q) = \sup_{||f ||_\mathcal{H} \le 1} \left( \mathbb{E}_X[f(X)] - \mathbb{E}_Y[f(Y)]  \right)   &lt;/math&gt;

=== Kernel two sample test ===
Given ''n'' training examples from &lt;math&gt;P(X)&lt;/math&gt; and ''m'' samples from &lt;math&gt;Q(Y)&lt;/math&gt;, one can formulate a test statistic based on the empirical estimate of the MMD
:: &lt;math&gt;\widehat{\text{MMD}}(P,Q) = \left| \left| \frac{1}{n}\sum_{i=1}^n \phi(x_i) - \frac{1}{m}\sum_{i=1}^m \phi(y_i) \right| \right|_{\mathcal{H}}^2 = \frac{1}{nm} \sum_{i=1}^n\sum_{j=1}^m \left[ k(x_i, x_j) + k(y_i, y_j) - 2 k(x_i, y_j) \right] &lt;/math&gt;
to obtain a '''two-sample test''' &lt;ref&gt;A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, A. Smola. (2012). [http://jmlr.org/papers/volume13/gretton12a/gretton12a.pdf A kernel two-sample test]. ''Journal of Machine Learning Research'', '''13''': 723-773.&lt;/ref&gt; of the null hypothesis that both samples stem from the same distribution (i.e. &lt;math&gt;P = Q&lt;/math&gt;) against the broad alternative &lt;math&gt;P \neq Q&lt;/math&gt;.

=== Density estimation via kernel embeddings ===
Although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on ''n'' samples drawn from an underlying distribution &lt;math&gt;P_X^*&lt;/math&gt;.  This can be done by solving the following optimization problem &lt;ref name =&quot;SongThesis&quot;/&gt;&lt;ref&gt;M. Dudík, S. J. Phillips, R. E. Schapire. (2007). [http://classes.soe.ucsc.edu/cmps242/Winter08/lect/15/maxent_genreg_jmlr.pdf Maximum Entropy Distribution Estimation with Generalized Regularization and an Application to Species Distribution Modeling]. ''Journal of Machine Learning Research'', '''8''': 1217-1260.&lt;/ref&gt;
:: &lt;math&gt; \max_{P_X} H(P_X) &lt;/math&gt;  subject to  &lt;math&gt; ||\widehat{\mu}_X - \mu_X[P_X] ||_\mathcal{H} \le \epsilon &lt;/math&gt;
where the maximization is done over the entire space of distributions on &lt;math&gt; \Omega &lt;/math&gt;.  Here, &lt;math&gt; \mu_X[P_X] &lt;/math&gt; is the kernel embedding of the proposed density &lt;math&gt; P_X &lt;/math&gt; and &lt;math&gt;H&lt;/math&gt; is an entropy-like quantity (e.g. [[Entropy (information theory)|Entropy]], [[Kullback–Leibler divergence|KL divergence]], [[Bregman divergence]]).   The  distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples).  In practice, a  good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of  ''M'' candidate distributions with regularized mixing proportions.  Connections between  the ideas underlying [[Gaussian process]]es and [[conditional random fields]] may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) [[exponential family|exponential families]].&lt;ref name = &quot;SongThesis&quot;/&gt;

=== Measuring dependence of random variables ===
A measure of the statistical dependence between random variables &lt;math&gt; X &lt;/math&gt; and &lt;math&gt; Y &lt;/math&gt; (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert–Schmidt Independence Criterion &lt;ref&gt;A. Gretton, O. Bousquet, A. Smola, B. Schölkopf. (2005). [http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBouSmoSch05.pdf Measuring statistical dependence with Hilbert–Schmidt norms]. ''Proc. Intl. Conf. on Algorithmic Learning Theory'': 63–78.&lt;/ref&gt;
:: &lt;math&gt; \text{HSIC}(X, Y) = \left| \left| \mathcal{C}_{XY} - \mu_X \otimes \mu_Y  \right| \right|_{\mathcal{H} \otimes \mathcal{H}}^2 &lt;/math&gt;
and can be used as a principled replacement for [[mutual information]], [[Pearson correlation]] or any other dependence measure used in learning algorithms.  Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are [[independence (probability theory)|independent]]), and can be used to measure dependence between different types of data (e.g. images and text captions).  Given ''n'' i.i.d. samples of each random variable, a  simple parameter-free [[Bias of an estimator|unbiased]] estimator of HSIC which exhibits [[Concentration of measure|concentration]] about the true value can be computed in &lt;math&gt;O(n(d_f^2 +d_g^2))&lt;/math&gt; time,&lt;ref name = &quot;SongThesis&quot;/&gt; where the Gram matrices of the two datasets are approximated using &lt;math&gt;\mathbf{A} \mathbf{A}^T, \mathbf{B} \mathbf{B}^T &lt;/math&gt; with &lt;math&gt; \mathbf{A} \in \mathbb{R}^{n \times d_f},  \mathbf{B} \in \mathbb{R}^{n \times d_g}&lt;/math&gt;.  The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as: [[feature selection]] (BAHSIC &lt;ref&gt;L. Song, A. Smola , A. Gretton, K. Borgwardt, J. Bedo. (2007). [http://www.machinelearning.org/proceedings/icml2007/papers/244.pdf  Supervised feature selection via dependence estimation]. ''Proc. Intl. Conf. Machine Learning'', Omnipress: 823–830.&lt;/ref&gt;), [[Cluster analysis|clustering]] (CLUHSIC &lt;ref&gt;L. Song, A. Smola, A. Gretton, K. Borgwardt. (2007). [http://machinelearning.wustl.edu/mlpapers/paper_files/icml2007_SongSGB07.pdf A dependence maximization view of clustering]. ''Proc. Intl. Conf. Machine Learning''. Omnipress: 815–822.&lt;/ref&gt;), and [[dimensionality reduction]] (MUHSIC &lt;ref&gt;L. Song, A. Smola, K. Borgwardt, A. Gretton. (2007). [http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_492.pdf Colored maximum variance unfolding]. ''Neural Information Processing Systems''.&lt;/ref&gt;).

HSIC can be extended to measure the dependence of multiple random variables. The question of when HSIC captures independence in this case has recently been studied &lt;ref name = &quot;CharAndUniv&quot;&gt;Zoltán Szabó, Bharath K. Sriperumbudur. [http://arxiv.org/abs/1708.08157 Characteristic and Universal Tensor Product Kernels]. ''Technical Report'', 2017.&lt;/ref&gt;: for
more than two variables
* on &lt;math&gt; \mathbb{R}^d &lt;/math&gt;: the characteristic property of the individual kernels remains an equivalent condition.
* on general domains: the characteristic property of the kernel components is necessary but ''not sufficient''.

=== Kernel belief propagation ===
[[Belief propagation]] is a fundamental algorithm for inference in [[graphical models]] in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations.  In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates.  Given  ''n'' samples of random variables represented by nodes in a [[Markov Random Field]], the incoming message to node  ''t'' from node  ''u'' can be expressed as &lt;math&gt; m_{ut}(\cdot) = \sum_{i=1}^n \beta_{ut}^i \phi(x_t^i) &lt;/math&gt; if it assumed to lie in the RKHS.  The '''kernel belief propagation update''' message from  ''t'' to node  ''s'' is then given by &lt;ref name = &quot;Song2013&quot;/&gt;
:: &lt;math&gt; \widehat{m}_{ts} = \left( \odot_{u \in N(t) \backslash s} \mathbf{K}_t \boldsymbol{\beta}_{ut} \right)^T (\mathbf{K}_s + \lambda \mathbf{I})^{-1} \boldsymbol{\Upsilon}_s^T \phi(x_s)&lt;/math&gt;
where &lt;math&gt; \odot &lt;/math&gt; denotes the element-wise vector product, &lt;math&gt;N(t) \backslash s &lt;/math&gt; is the set of nodes connected to  ''t'' excluding node  ''s'', &lt;math&gt; \boldsymbol{\beta}_{ut} = \left(\beta_{ut}^1, \dots, \beta_{ut}^n \right) &lt;/math&gt;,  &lt;math&gt;\mathbf{K}_t, \mathbf{K}_s &lt;/math&gt; are the Gram matrices of the samples from variables &lt;math&gt;X_t, X_s &lt;/math&gt;, respectively, and &lt;math&gt;\boldsymbol{\Upsilon}_s = \left(\phi(x_s^1),\dots, \phi(x_s^n)\right)&lt;/math&gt; is the feature matrix for the samples from &lt;math&gt;X_s&lt;/math&gt;.

Thus, if the incoming messages to node ''t'' are linear combinations of feature mapped samples from &lt;math&gt; X_t &lt;/math&gt;, then the outgoing message from this node is also a linear combination of feature mapped samples from  &lt;math&gt; X_s &lt;/math&gt;.  This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the [[Markov Random Field#Clique factorization|potentials]] are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled.&lt;ref name = &quot;Song2013&quot;/&gt;

=== Nonparametric filtering in hidden Markov models ===
In the [[hidden Markov model]] (HMM), two key quantities of interest are the transition probabilities between hidden states &lt;math&gt; P(S^t \mid S^{t-1})&lt;/math&gt; and the emission probabilities &lt;math&gt; P(O^t \mid S^t) &lt;/math&gt; for observations.  Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM.  A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible.

One common use of HMMs is [[Hidden Markov Model#Filtering|filtering]] in which the goal is to estimate posterior distribution over the hidden state &lt;math&gt; s^{t}&lt;/math&gt; at time step ''t'' given a history of previous observations &lt;math&gt;h^t = (o^1, \dots, o^t)&lt;/math&gt; from the system. In filtering, a '''belief state''' &lt;math&gt; P(S^{t+1} \mid h^{t+1})  &lt;/math&gt; is recursively maintained via a prediction step (where updates &lt;math&gt;P(S^{t+1} \mid h^t) = \mathbb{E}_{S^t \mid h^t} [P(S^{t+1} \mid S^t)] &lt;/math&gt; are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates &lt;math&gt; P(S^{t+1} \mid h^t, o^{t+1}) \propto P(o^{t+1} \mid S^{t+1}) P(S^{t+1} \mid h^t) &lt;/math&gt; are computed by applying Bayes' rule to condition on a new observation).&lt;ref name = &quot;Song2013&quot;/&gt;  The RKHS embedding of the belief state at time ''t+1'' can be recursively expressed as
:: &lt;math&gt; \mu_{S^{t+1} \mid h^{t+1}} = \mathcal{C}_{S^{t+1} O^{t+1}}^\pi  \left(\mathcal{C}_{O^{t+1} O^{t+1}}^\pi \right)^{-1} \phi(o^{t+1})   &lt;/math&gt;
by computing the embeddings of the prediction step via the [[#Kernel Sum Rule|kernel sum rule]] and the embedding of the conditioning step via [[#Kernel Bayes' Rule|kernel Bayes' rule]].  Assuming a training sample &lt;math&gt;(\widetilde{s}^1, \dots, \widetilde{s}^T, \widetilde{o}^1, \dots, \widetilde{o}^T) &lt;/math&gt; is given, one can in practice estimate &lt;math&gt; \widehat{\mu}_{S^{t+1} \mid h^{t+1}} = \sum_{i=1}^T \alpha_i^t \phi(\widetilde{s}^t) &lt;/math&gt; and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights &lt;math&gt;\boldsymbol{\alpha} = (\alpha_1, \dots, \alpha_T)&lt;/math&gt;  &lt;ref name = &quot;Song2013&quot;/&gt;
:: &lt;math&gt; \mathbf{D}^{t+1} = \text{diag}\left((G+\lambda \mathbf{I})^{-1} \widetilde{G} \boldsymbol{\alpha}^t  \right)&lt;/math&gt;
:: &lt;math&gt; \boldsymbol{\alpha}^{t+1} = \mathbf{D}^{t+1} \mathbf{K} \left( (\mathbf{D}^{t+1}  K)^2 + \widetilde{\lambda} \mathbf{I} \right)^{-1} \mathbf{D}^{t+1} \mathbf{K}_{o^{t+1}} &lt;/math&gt;
where  &lt;math&gt; \mathbf{G}, \mathbf{K} &lt;/math&gt; denote the Gram matrices of &lt;math&gt;\widetilde{s}^1, \dots, \widetilde{s}^T &lt;/math&gt; and &lt;math&gt; \widetilde{o}^1, \dots, \widetilde{o}^T&lt;/math&gt; respectively,  &lt;math&gt; \widetilde{\mathbf{G}} &lt;/math&gt; is a transfer Gram matrix defined as &lt;math&gt; \widetilde{\mathbf{G}}_{ij} = k(\widetilde{s}_i, \widetilde{s}_{j+1}) &lt;/math&gt;, and &lt;math&gt; \mathbf{K}_{o^{t+1}} = (k(\widetilde{o}^1, o^{t+1}), \dots, k(\widetilde{o}^T, o^{t+1}))^T &lt;/math&gt;.

=== Support measure machines ===
The '''support measure machine''' (SMM) is a generalization of the [[support vector machine]] (SVM) in which the training examples are probability distributions paired with labels &lt;math&gt; \{P_i, y_i\}_{i=1}^n, \ y_i \in \{+1, -1\} &lt;/math&gt;.&lt;ref name = &quot;SMM&quot;&gt;K.  Muandet, K. Fukumizu, F. Dinuzzo, B. Schölkopf. (2012). [http://books.nips.cc/papers/files/nips25/NIPS2012_0015.pdf Learning from Distributions via Support Measure Machines]. ''Advances in Neural Information Processing Systems'': 10–18.&lt;/ref&gt;
SMMs solve the standard SVM [[Support vector machine#Dual form|dual optimization problem]] using the following '''expected kernel'''
:: &lt;math&gt; K\left(P(X), Q(Z)\right) = \langle \mu_X , \mu_Z \rangle_\mathcal{H} = \mathbb{E}_{XZ} [k(x,z)] &lt;/math&gt;
which is computable in closed form for many common specific distributions &lt;math&gt; P_i &lt;/math&gt; (such as the Gaussian distribution) combined with popular embedding kernels &lt;math&gt;k&lt;/math&gt; (e.g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples &lt;math&gt;\{x_i\}_{i=1}^n \sim P(X), \{z_j\}_{j=1}^m \sim Q(Z) &lt;/math&gt; via
::&lt;math&gt; \widehat{K} \left(X, Z\right) = \frac{1}{n m} \sum_{i=1}^n \sum_{j=1}^m k(x_i, z_j) &lt;/math&gt;
Under certain choices of the embedding kernel &lt;math&gt;k&lt;/math&gt;, the SMM applied to training examples &lt;math&gt;\{P_i, y_i\}_{i=1}^n &lt;/math&gt; is equivalent to a SVM trained on samples &lt;math&gt;\{x_i, y_i\}_{i=1}^n&lt;/math&gt;, and thus the SMM can  be viewed as a ''flexible'' SVM in which a different data-dependent kernel (specified by the assumed form of the distribution &lt;math&gt; P_i &lt;/math&gt;)   may be placed on each training point.&lt;ref name = &quot;SMM&quot; /&gt;

=== Domain adaptation under covariate, target, and conditional shift ===
The goal of [[Domain Adaptation|domain adaptation]] is the formulation of learning algorithms which generalize well when the training and test data have different distributions.  Given training examples &lt;math&gt; \{ (x_i^{tr}, y_i^{tr}) \}_{i=1}^n &lt;/math&gt; and a test set &lt;math&gt; \{ (x_j^{te}, y_j^{te}) \}_{j=1}^m &lt;/math&gt; where the &lt;math&gt; y_j^{te} &lt;/math&gt;  are unknown, three types of differences are commonly assumed  between the distribution of the training examples &lt;math&gt; P^{tr}(X,Y)&lt;/math&gt; and the test distribution &lt;math&gt; P^{te}(X,Y)&lt;/math&gt;:&lt;ref name = &quot;DA&quot;&gt;K. Zhang, B. Schölkopf, K. Muandet, Z. Wang. (2013). [http://jmlr.org/proceedings/papers/v28/zhang13d.pdf  Domain adaptation under target and conditional shift]. ''Journal of Machine Learning Research, '''28'''(3): 819–827.&lt;/ref&gt;&lt;ref name = &quot;CovS&quot;&gt;A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, B. Schölkopf. (2008). Covariate shift and local learning by distribution matching. ''In J. Quinonero-Candela, M. Sugiyama, A. Schwaighofer, N. Lawrence (eds.). Dataset shift in machine learning'', MIT Press, Cambridge, MA: 131–160.&lt;/ref&gt;
# '''Covariate Shift''' in which the marginal distribution of the covariates changes across domains: &lt;math&gt; P^{tr}(X) \neq P^{te}(X)&lt;/math&gt;
# '''Target Shift''' in which the marginal distribution of the outputs changes across domains: &lt;math&gt; P^{tr}(Y) \neq P^{te}(Y)&lt;/math&gt;
# '''Conditional Shift''' in which &lt;math&gt; P(Y)&lt;/math&gt; remains the same across domains, but the conditional distributions differ: &lt;math&gt; P^{tr}(X \mid Y) \neq P^{te}(X \mid Y)&lt;/math&gt;.  In general, the presence of conditional shift leads to an [[Well-posed problem|ill-posed]] problem, and the additional assumption that  &lt;math&gt; P(X \mid Y) &lt;/math&gt; changes only under [[Location parameter|location]]-[[Scale parameter|scale]] (LS) transformations on &lt;math&gt; X &lt;/math&gt; is commonly imposed to make the problem tractable.

By utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated.  Covariate shift may be accounted for by reweighting examples via estimates of the ratio &lt;math&gt;P^{te}(X) / P^{tr}(X) &lt;/math&gt; obtained directly from the  kernel embeddings of the marginal distributions of &lt;math&gt;X&lt;/math&gt; in each domain without any need for explicit estimation of the distributions.&lt;ref name = &quot;CovS&quot;/&gt;  Target shift, which cannot be similarly dealt with since no samples from &lt;math&gt;Y&lt;/math&gt; are available in the test domain, is accounted for by weighting training examples using the vector &lt;math&gt;\boldsymbol{\beta}^*(\mathbf{y}^{tr}) &lt;/math&gt; which solves  the following optimization problem (where in practice, empirical approximations must be used) &lt;ref name = &quot;DA&quot;/&gt;
:: &lt;math&gt; \min_{\boldsymbol{\beta}(y)} \left|\left|\mathcal{C}_{{(X \mid Y)}^{tr}} \mathbb{E}_{Y^{tr}} [ \boldsymbol{\beta}(y) \phi(y)] - \mu_{X^{te}} \right|\right|_\mathcal{H}^2  &lt;/math&gt; subject to &lt;math&gt; \boldsymbol{\beta}(y) \ge 0, \mathbb{E}_{Y^{tr}} [ \boldsymbol{\beta}(y)] = 1 &lt;/math&gt;

To deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data &lt;math&gt; \mathbf{X}^{new} =  \mathbf{X}^{tr} \odot \mathbf{W} + \mathbf{B} &lt;/math&gt; (where &lt;math&gt;\odot &lt;/math&gt; denotes the element-wise vector product).  To ensure similar distributions between the new transformed training samples and the test data, &lt;math&gt;\mathbf{W},\mathbf{B}&lt;/math&gt; are estimated by minimizing the following empirical kernel embedding distance &lt;ref name =
&quot;DA&quot;/&gt;
:: &lt;math&gt; \left| \left| \widehat{\mu}_{X^{new}} - \widehat{\mu}_{X^{te}}    \right| \right|_{\mathcal{H}}^2  =  \left| \left| \widehat{\mathcal{C}}_{(X \mid Y)^{new}} \widehat{\mu}_{Y^{tr}} -  \widehat{\mu}_{X^{te}} \right| \right|_{\mathcal{H}}^2 &lt;/math&gt;
In general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted   transformation of the training data which mimics the test distribution, and these methods may perform well even in the presence of conditional shifts other than location-scale changes.&lt;ref name = &quot;DA&quot;/&gt;

=== Domain generalization via invariant feature representation ===
Given ''N'' sets of training examples sampled i.i.d. from distributions &lt;math&gt;P^{(1)}(X,Y), P^{(2)}(X,Y), \dots, P^{(N)}(X,Y)&lt;/math&gt;, the goal of '''domain generalization''' is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain &lt;math&gt;P^*(X,Y)&lt;/math&gt; where no data from the test domain is available at training time.  If conditional distributions &lt;math&gt;P(Y \mid X)&lt;/math&gt; are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals &lt;math&gt;P(X)&lt;/math&gt;.  Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains.&lt;ref name = &quot;DICA&quot;&gt;K. Muandet, D. Balduzzi, B. Schölkopf. (2013).[http://jmlr.org/proceedings/papers/v28/muandet13.pdf Domain Generalization Via Invariant Feature Representation]. ''30th International Conference on Machine Learning''.&lt;/ref&gt;  DICA thus extracts ''invariants'', features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as [[kernel principal component analysis]], transfer component analysis, and covariance operator inverse regression.&lt;ref name = &quot;DICA&quot;/&gt;

Defining a probability distribution &lt;math&gt;\mathcal{P}&lt;/math&gt; on the RKHS &lt;math&gt;\mathcal{H}&lt;/math&gt; with &lt;math&gt;\mathcal{P}(\mu_{X^{(i)}Y^{(i)}}) = 1/N \text{ for } i=1,\dots, N&lt;/math&gt;, DICA measures dissimilarity between domains via '''distributional variance''' which is computed as
:: &lt;math&gt;V_\mathcal{H} (\mathcal{P}) = \frac{1}{N} \text{tr}(\mathbf{G}) - \frac{1}{N^2} \sum_{i,j=1}^N \mathbf{G}_{ij} &lt;/math&gt;  where  &lt;math&gt;\mathbf{G}_{ij}  = \langle \mu_{X^{(i)}}, \mu_{X^{(j)}} \rangle_\mathcal{H} &lt;/math&gt;
so &lt;math&gt;\mathbf{G}&lt;/math&gt; is a &lt;math&gt;N \times N&lt;/math&gt; Gram matrix over the distributions from which the training data are sampled.   Finding an [[Orthogonal matrix|orthogonal transform]] onto a low-dimensional [[Linear subspace|subspace]] ''B'' (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that ''B'' aligns with the [[Basis function|bases]] of a '''central subspace''' ''C'' for which &lt;math&gt;Y&lt;/math&gt; becomes independent of &lt;math&gt;X&lt;/math&gt; given &lt;math&gt;C^T X&lt;/math&gt; across all domains.  In the absence of target values &lt;math&gt;Y&lt;/math&gt;, an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of &lt;math&gt;X&lt;/math&gt; (in the feature space) across all domains (rather than preserving a central subspace).&lt;ref name = &quot;DICA&quot;/&gt;

=== Distribution regression ===
In distribution regression, the goal is to regress from probability distributions to reals (or vectors). Many important [[machine learning]] and statistical tasks fit into this framework, including [[Multiple-instance learning|multi-instance learning]], and [[point estimation]] problems without analytical solution (such as [[hyperparameter]] or [[entropy estimation]]). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between ''sets of points''. Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images.&lt;ref name = &quot;MERR&quot;&gt;Z. Szabó, B. Sriperumbudur, B. Póczos, A. Gretton. [http://jmlr.org/papers/v17/14-510.html Learning Theory for Distribution Regression]. ''Journal of Machine Learning Research'', 17(152):1-40, 2016.&lt;/ref&gt;

Given &lt;math&gt;{\left(\{X_{i,n}\}_{n=1}^{N_i}, y_i\right)}_{i=1}^{\ell}&lt;/math&gt; training data, where the &lt;math&gt;\hat{X_i} := \{X_{i,n}\}_{n=1}^{N_i}&lt;/math&gt; bag contains samples from a probability distribution &lt;math&gt;X_i&lt;/math&gt; and the &lt;math&gt;i^{th}&lt;/math&gt; output label is &lt;math&gt;y_i\in \mathbb{R}&lt;/math&gt;, one can tackle the distribution regression task by taking the embeddings of the distributions, and learning the regressor from the embeddings to the outputs. In other words, one can consider the following kernel [[Tikhonov regularization|ridge regression]] problem &lt;math&gt;(\lambda&gt;0)&lt;/math&gt;
:: &lt;math&gt;  J(f) = \frac{1}{\ell} \sum_{i=1}^{\ell} \left[f\left(\mu_{\hat{X_i}}\right)-y_i\right]^2 + \lambda \left\|f\right\|_{\mathcal{H}(K)}^2 \rightarrow \min_{f\in \mathcal{H}(K)}, &lt;/math&gt;
where &lt;math&gt;\mu_{\hat{X}_i} = \int_{\Omega} k(\cdot,u) \mathrm{d} \hat{X}_i(u)= \frac{1}{N_i} \sum_{n=1}^{N_i} k(\cdot, X_{i,n})&lt;/math&gt; with a &lt;math&gt;k&lt;/math&gt; kernel on the domain of &lt;math&gt;X_i&lt;/math&gt;-s &lt;math&gt;(k:\Omega\times \Omega \rightarrow \mathbb{R})&lt;/math&gt;, &lt;math&gt;K&lt;/math&gt; is a kernel on the embedded distributions, and &lt;math&gt;\mathcal{H}(K)&lt;/math&gt; is the RKHS determined by &lt;math&gt;K&lt;/math&gt;. Examples for &lt;math&gt;K&lt;/math&gt; include the linear kernel &lt;math&gt; \left[ K(\mu_P,\mu_Q) = \langle\mu_P,\mu_Q\rangle_{\mathcal{H}(k)} \right] &lt;/math&gt;, the Gaussian kernel &lt;math&gt; \left[ K(\mu_P,\mu_Q) = e^{-\left\|\mu_P-\mu_Q\right\|_{H(k)}^2/(2\sigma^2)} \right] &lt;/math&gt;, the exponential kernel &lt;math&gt; \left[ K(\mu_P,\mu_Q) = e^{-\left\|\mu_P-\mu_Q\right\|_{H(k)}/(2\sigma^2)} \right] &lt;/math&gt;, the Cauchy kernel &lt;math&gt; \left[ K(\mu_P,\mu_Q) =  \left(1+ \left\|\mu_P-\mu_Q\right\|_{H(k)}^2/\sigma^2 \right)^{-1} \right] &lt;/math&gt;, the generalized t-student kernel &lt;math&gt; \left[ K(\mu_P,\mu_Q) =  \left(1+ \left\|\mu_P-\mu_Q\right\|_{H(k)}^{\sigma} \right)^{-1}, (\sigma \le 2) \right] &lt;/math&gt;, or the inverse multiquadrics kernel &lt;math&gt; \left[ K(\mu_P,\mu_Q) =  \left(\left\|\mu_P-\mu_Q\right\|_{H(k)}^2 + \sigma^2 \right)^{-\frac{1}{2}} \right] &lt;/math&gt;.

The prediction on a new distribution &lt;math&gt;(\hat{X})&lt;/math&gt; takes the simple, analytical form
:: &lt;math&gt; \hat{y}\big(\hat{X}\big) = \mathbf{k} [\mathbf{G} + \lambda \ell]^{-1}\mathbf{y}, &lt;/math&gt;
where &lt;math&gt;\mathbf{k}=\big[K \big(\mu_{\hat{X}_i},\mu_{\hat{X}}\big)\big]\in \mathbb{R}^{1\times \ell}&lt;/math&gt;, &lt;math&gt;\mathbf{G}=[G_{ij}]\in \mathbb{R}^{\ell\times \ell}&lt;/math&gt;, &lt;math&gt;G_{ij} = K\big(\mu_{\hat{X}_i},\mu_{\hat{X}_j}\big)\in \mathbb{R}&lt;/math&gt;, &lt;math&gt;\mathbf{y}=[y_1;...;y_l]\in \R^{\ell}&lt;/math&gt;. Under mild regularity conditions this estimator can be shown to be consistent and it can achieve the one-stage sampled (as if one had access to the true &lt;math&gt;X_i&lt;/math&gt;-s) [[Minimax estimator|minimax optimal]] rate.&lt;ref name = &quot;MERR&quot; /&gt; In the &lt;math&gt;J&lt;/math&gt; objective function &lt;math&gt;y_i&lt;/math&gt;-s are real numbers; the results can also be extended to the case when &lt;math&gt;y_i&lt;/math&gt;-s are &lt;math&gt;d&lt;/math&gt;-dimensional vectors, or more generally elements of a [[Separable space|separable]] [[Hilbert space]] using operator-valued &lt;math&gt;K&lt;/math&gt; kernels.

== Example ==
In this simple example, which is taken from Song et al.,&lt;ref name = &quot;Song2013&quot;/&gt; &lt;math&gt;X, Y&lt;/math&gt; are assumed to be [[Probability distribution#Discrete probability distribution|discrete random variables]] which take values in the set &lt;math&gt;\{1,\dots,K\} &lt;/math&gt; and the kernel is chosen to be the [[Kronecker delta]] function, so &lt;math&gt;k(x,x') = \delta(x,x')&lt;/math&gt;.  The feature map corresponding to this kernel is the [[standard basis]] vector &lt;math&gt;\phi(x) = \mathbf{e}_x&lt;/math&gt;.  The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are &lt;math&gt;K\times K &lt;/math&gt; matrices specifying joint probability tables, and the explicit form of these embeddings is
:: &lt;math&gt;\mu_X = \mathbb{E}_X [\mathbf{e}_x] = \left(
\begin{array}{c}
P(X=1) \\
\vdots \\
P(X=K) \\
\end{array}
\right) &lt;/math&gt;
:: &lt;math&gt; \mathcal{C}_{XY} = \mathbb{E}_{XY} [\mathbf{e}_X \otimes e_Y] = \bigg( P(X=s, Y=t) \bigg)_{s,t \in \{1,\dots,K\}} &lt;/math&gt;

The conditional distribution embedding operator &lt;math&gt; \mathcal{C}_{Y\mid X} = \mathcal{C}_{YX} \mathcal{C}_{XX}^{-1} &lt;/math&gt;  is in this setting a conditional probability table
:: &lt;math&gt; \mathcal{C}_{Y \mid X} = \bigg( P(Y=s \mid X=t) \bigg)_{s,t \in \{1,\dots,K\}} &lt;/math&gt;
: and &lt;math&gt; \mathcal{C}_{XX} =\left(
\begin{array}{c c c}
P(X=1) &amp; \dots &amp; 0 \\
\vdots &amp; \ddots  &amp; \vdots \\
0 &amp; \dots &amp; P(X=K) \\
\end{array}
\right)
&lt;/math&gt;
Thus, the embeddings of the conditional distribution under a fixed value of &lt;math&gt;X&lt;/math&gt; may be computed as
:: &lt;math&gt; \mu_{Y \mid x} = \mathcal{C}_{Y \mid X} \phi(x) = \left(
\begin{array}{c}
P(Y=1 \mid X = x) \\
\vdots \\
P(Y=K \mid X = x) \\
\end{array}
\right) &lt;/math&gt;

In this discrete-valued setting with the Kronecker delta kernel, the [[#Rules of probability as operations in the RKHS|kernel sum rule]] becomes
:: &lt;math&gt; \underbrace{ \left(
\begin{array}{c}
Q(X=1) \\
\vdots \\
P(X = N) \\
\end{array}
\right) }_{\mu_X^\pi} = \underbrace{ \left( \begin{array}{c} \\  P(X=s \mid Y=t) \\ \\ \end{array} \right) }_{ \mathcal{C}_{X\mid Y} } \underbrace{ \left(
\begin{array}{c}
\pi(Y=1) \\
\vdots \\
\pi(Y = N) \\
\end{array}
\right) }_{ \mu_Y^\pi} &lt;/math&gt;

The [[#Rules of probability as operations in the RKHS|kernel chain rule]] in this case is given by
:: &lt;math&gt; \underbrace{ \left( \begin{array}{c} \\ Q(X=s,Y=t) \\ \\ \end{array} \right) }_{\mathcal{C}_{XY}^\pi} =
 \underbrace{ \left( \begin{array}{c} \\ P(X=s \mid Y=t) \\ \\ \end{array} \right) }_{\mathcal{C}_{X \mid Y}}
\underbrace{ \left(
\begin{array}{c c c}
\pi(Y=1) &amp; \dots &amp; 0 \\
\vdots &amp; \ddots  &amp; \vdots \\
0 &amp; \dots &amp; \pi(Y=K) \\
\end{array}
\right) }_{\mathcal{C}_{YY}^\pi} &lt;/math&gt;

==References==
{{reflist}}

==External links==
* [https://bitbucket.org/szzoli/ite/ Information Theoretical Estimators toolbox] (distribution regression demonstration).


</text>
      <sha1>mic5cuzfb12krxp13tz0spfbiuwdp1c</sha1>
    </revision>
  </page>
  <page>
    <title>Stability (learning theory)</title>
    <ns>0</ns>
    <id>33886025</id>
    <revision>
      <id>814045323</id>
      <parentid>802540851</parentid>
      <timestamp>2017-12-06T16:19:24Z</timestamp>
      <contributor>
        <ip>108.31.200.202</ip>
      </contributor>
      <comment>changed &quot;milestones were made&quot; to &quot;milestones were reached&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16290">'''Stability''', also known as '''algorithmic stability''', is a notion in [[computational learning theory]] of how a [[machine learning| machine learning algorithm]] is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (&quot;A&quot; to &quot;Z&quot;) as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar [[statistical classification|classifier]] with both the 1000-element and 999-element training sets.

Stability can be studied for many types of learning problems, from [[Natural language processing|language learning]] to [[inverse problem]]s in physics and engineering, as it is a property of the learning process rather than the type of information being learned. The study of stability gained importance in [[computational learning theory]] in the 2000s when it was shown to have a connection with [[Machine learning#Generalization|generalization]]. It was shown that for large classes of learning algorithms, notably [[empirical risk minimization]] algorithms, certain types of stability ensure good generalization.

== History ==

A central goal in designing a [[machine learning| machine learning system]] is to guarantee that the learning algorithm will [[Machine learning#Generalization|generalize]], or perform accurately on new examples after being trained on a finite number of them. In the 1990s, milestones were reached in obtaining generalization bounds for [[supervised learning| supervised learning algorithms]]. The technique historically used to prove generalization was to show that an algorithm was [[consistent estimator|consistent]], using the [[uniform convergence]] properties of empirical quantities to their means. This technique was used to obtain generalization bounds for the large class of [[empirical risk minimization]] (ERM) algorithms. An ERM algorithm is one that selects a solution from a hypothesis space &lt;math&gt;H&lt;/math&gt; in such a way to minimize the empirical error on a training set &lt;math&gt;S&lt;/math&gt;.

A general result, proved by [[Vladimir Vapnik]] for an ERM binary classification algorithms, is that for any target function and input distribution, any hypothesis space &lt;math&gt;H&lt;/math&gt; with [[VC dimension|VC-dimension]] &lt;math&gt;d&lt;/math&gt;, and &lt;math&gt;n&lt;/math&gt; training examples, the algorithm is consistent and will produce a training error that is at most &lt;math&gt;O\left(\sqrt{\frac{d}{n}}\right)&lt;/math&gt; (plus logarithmic factors) from the true training error. The result was later extended to almost-ERM algorithms with function classes that do not have unique minimizers.

Vapnik's work, using what became known as [[VC theory]], established a relationship between generalization of a learning algorithm and properties of the hypothesis space &lt;math&gt;H&lt;/math&gt; of functions being learned. However, these results could not be applied to algorithms with hypothesis spaces of unbounded VC-dimension. Put another way, these results could not be applied when the information being learned had a complexity that was too large to measure. Some of the simplest machine learning algorithms—for instance, for regression—have hypothesis spaces with unbounded VC-dimension. Another example is language learning algorithms that can produce sentences of arbitrary length.

Stability analysis was developed in the 2000s for [[computational learning theory]] and is an alternative method for obtaining generalization bounds. The stability of an algorithm is a property of the learning process, rather than a direct property of the hypothesis space &lt;math&gt;H&lt;/math&gt;, and it can be assessed in algorithms that have hypothesis spaces with unbounded or undefined VC-dimension such as nearest neighbor. A stable learning algorithm is one for which the learned function does not change much when the training set is slightly modified, for instance by leaving out an example. A measure of [[Leave one out error]] is used in a Cross Validation Leave One Out (CVloo) algorithm to evaluate a learning algorithm's stability with respect to the loss function. As such, stability analysis is the application of [[sensitivity analysis]] to machine learning.

== Summary of classic results ==

* '''Early 1900s''' - Stability in learning theory was earliest described in terms of continuity of the learning map &lt;math&gt;L&lt;/math&gt;, traced to [[Andrey Nikolayevich Tikhonov]].
* '''1979''' - Devroye and Wagner observed that the leave-one-out behavior of an algorithm is related to its sensitivity to small changes in the sample.&lt;ref&gt;L. Devroye and Wagner, Distribution-free performance bounds for potential function rules, IEEE Trans. Inform. Theory 25(5) (1979) 601–604.&lt;/ref&gt;
* '''1999''' - Kearns and Ron discovered a connection between finite VC-dimension and stability.&lt;ref&gt;M. Kearns and [[Dana Ron|D. Ron]], Algorithmic stability and sanity-check bounds for leave-one-out cross-validation, Neural Comput. 11(6) (1999) 1427–1453.&lt;/ref&gt;
* '''2002''' - In a landmark paper, Bousquet and Elisseeff proposed the notion of ''uniform hypothesis stability'' of a learning algorithm and showed that it implies low generalization error. Uniform hypothesis stability, however, is a strong condition that does not apply to large classes of algorithms, including ERM algorithms with a hypothesis space of only two functions.&lt;ref&gt;O. Bousquet and A. Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526, 2002.&lt;/ref&gt;
* '''2002''' - Kutin and Niyogi extended Bousquet and Elisseeff's results by providing generalization bounds for several weaker forms of stability which they called ''almost-everywhere stability''. Furthermore, they took an initial step in establishing the relationship between stability and consistency in ERM algorithms in the Probably Approximately Correct (PAC) setting.&lt;ref&gt;S. Kutin and P. Niyogi, Almost-everywhere algorithmic stability and generalization error, Technical Report TR-2002-03, University of Chicago (2002).&lt;/ref&gt;
* '''2004''' - In an unusual publication (on a theorem!) for the journal [[nature journal|Nature]], Poggio et al. proved the relationship between stability and ERM consistency in the general case. They proposed a statistical form of leave-one-out-stability which they called ''CVEEEloo stability'', and showed that it is a) sufficient for generalization in bounded loss classes, and b) necessary and sufficient for consistency (and thus generalization) of ERM algorithms for certain loss functions (such as the square loss, the absolute value and the binary classification loss).&lt;ref&gt;S. Mukherjee, P. Niyogi, T. Poggio, and R. M. Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Adv. Comput. Math., 25(1-3):161–193, 2006.&lt;/ref&gt;
* '''2010''' - Shalev Shwartz noticed problems with the original results of Vapnik due to the complex relations between hypothesis space and loss class. They discuss stability notions that capture different loss classes and different types of learning, supervised and unsupervised.&lt;ref&gt;Shalev Shwartz, S., Shamir, O., Srebro, N., Sridharan, K.,  Learnability, Stability and Uniform Convergence, Journal of Machine Learning Research, 11(Oct):2635-2670, 2010.&lt;/ref&gt;

== Preliminary definitions ==

We define several terms related to learning algorithms training sets, so that we can then define stability in multiple ways and present theorems from the field.

A machine learning algorithm, also known as a learning map &lt;math&gt;L&lt;/math&gt;, maps a training data set, which is a set of labeled examples &lt;math&gt;(x,y)&lt;/math&gt;, onto a function &lt;math&gt;f&lt;/math&gt; from &lt;math&gt;X&lt;/math&gt; to &lt;math&gt;Y&lt;/math&gt;, where &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are in the same space of the training examples. The functions &lt;math&gt;f&lt;/math&gt; are selected from a hypothesis space of functions called &lt;math&gt;H&lt;/math&gt;.

The training set from which an algorithm learns is defined as

&lt;math&gt;S = \{z_1 = (x_1,\ y_1)\ ,..,\ z_m = (x_m,\ y_m)\}&lt;/math&gt;

and is of size &lt;math&gt;m&lt;/math&gt; in &lt;math&gt;Z = X \times Y&lt;/math&gt;

drawn i.i.d. from an unknown distribution D.

Thus, the learning map &lt;math&gt;L&lt;/math&gt; is defined as a mapping from &lt;math&gt;Z_m&lt;/math&gt; into &lt;math&gt;H&lt;/math&gt;, mapping a training set &lt;math&gt;S&lt;/math&gt; onto a function &lt;math&gt;f_S&lt;/math&gt; from &lt;math&gt;X&lt;/math&gt; to &lt;math&gt;Y&lt;/math&gt;. Here, we consider only deterministic algorithms where &lt;math&gt;L&lt;/math&gt; is symmetric with respect to &lt;math&gt;S&lt;/math&gt;, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable.

The loss &lt;math&gt;V&lt;/math&gt; of a hypothesis &lt;math&gt;f&lt;/math&gt; with respect to an example &lt;math&gt;z = (x,y)&lt;/math&gt; is then defined as &lt;math&gt;V(f,z) = V(f(x),y)&lt;/math&gt;.

The empirical error of &lt;math&gt;f&lt;/math&gt; is &lt;math&gt;I_S[f] = \frac{1}{n}\sum V(f,z_i)&lt;/math&gt;.

The true error of &lt;math&gt;f&lt;/math&gt; is &lt;math&gt;I[f] = \mathbb{E}_z V(f,z)&lt;/math&gt;

Given a training set S of size m, we will build, for all i = 1....,m, modified training sets as follows:
* By removing the i-th element
&lt;math&gt;S^{|i} = \{z_1 ,...,\ z_{i-1},\ z_{i+1},...,\ z_m\}&lt;/math&gt;
* By replacing the i-th element
&lt;math&gt;S^i = \{z_1 ,...,\ z_{i-1},\ z_i^',\ z_{i+1},...,\ z_m\}&lt;/math&gt;

== Definitions of stability ==

===Hypothesis Stability===
An algorithm &lt;math&gt;L&lt;/math&gt; has hypothesis stability β with respect to the loss function V if the following holds:

&lt;math&gt;\forall i\in \{1,...,m\}, \mathbb{E}_{S,z} [|V(f_S,z)-V(f_{S^{|i}},z)|]\leq\beta.&lt;/math&gt;

===Point-wise Hypothesis Stability===
An algorithm &lt;math&gt;L&lt;/math&gt; has point-wise hypothesis stability β with respect to the loss function V if the following holds:

&lt;math&gt;\forall i\in\ \{1,...,m\}, \mathbb{E}_{S} [|V(f_S,z_i)-V(f_{S^{|i}},z_i)|]\leq\beta.&lt;/math&gt;

===Error Stability===
An algorithm &lt;math&gt;L&lt;/math&gt; has error stability β with respect to the loss function V if the following holds:

&lt;math&gt;\forall S\in Z^m, \forall i\in\{1,...,m\}, |\mathbb{E}_z[V(f_S,z)]-\mathbb{E}_z[V(f_{S^{|i}},z)]|\leq\beta&lt;/math&gt;

===Uniform Stability===
An algorithm &lt;math&gt;L&lt;/math&gt; has uniform stability β with respect to the loss function V if the following holds:

&lt;math&gt;\forall S\in Z^m, \forall i\in\{1,...,m\}, \sup_{z\in Z}|V(f_S,z)-V(f_{S^{|i}},z)|\leq\beta&lt;/math&gt;

A probabilistic version of uniform stability β is:

&lt;math&gt;\forall S\in Z^m, \forall i\in\{1,...,m\}, \mathbb{P}_S\{\sup_{z\in Z}|V(f_S,z)-V(f_{S^{|i}},z)|\leq\beta\}\geq1-\delta&lt;/math&gt;

An algorithm is said to be '''stable''', when the value of &lt;math&gt;\beta&lt;/math&gt; decreases as &lt;math&gt;O(\frac{1}{m})&lt;/math&gt;.

===Leave-one-out cross-validation (CVloo) Stability===
An algorithm &lt;math&gt;L&lt;/math&gt; has CVloo stability β with respect to the loss function V if the following holds:

&lt;math&gt;\forall i\in\{1,...,m\}, \mathbb{P}_S\{ |V(f_S,z_i)-V(f_{S^{|i}},z_i)|\leq\beta_{CV}\}\geq1-\delta_{CV}&lt;/math&gt;

The definition of (CVloo) Stability is '''equivalent''' to Pointwise-hypothesis  stability seen earlier.

===Expected-leave-one-out error (&lt;math&gt;Eloo_{err}&lt;/math&gt;) Stability===
An algorithm &lt;math&gt;L&lt;/math&gt; has &lt;math&gt;Eloo_{err}&lt;/math&gt; stability if for each n there exists a &lt;math&gt;\beta_{EL}^m&lt;/math&gt; and a &lt;math&gt;\delta_{EL}^m&lt;/math&gt; such that:

&lt;math&gt;\forall i\in\{1,...,m\}, \mathbb{P}_S\{|I[f_S]-\frac{1}{m}\sum_{i=1}^m V(f_{S^{|i}},z_i)|\leq\beta_{EL}^m\}\geq1-\delta_{EL}^m&lt;/math&gt;, with &lt;math&gt;\beta_{EL}^m&lt;/math&gt; and &lt;math&gt;\delta_{EL}^m&lt;/math&gt; going to zero for &lt;math&gt;m,\rightarrow\infty&lt;/math&gt;

== Classic theorems ==

'''From Bousquet and Elisseeff (02)''':

For symmetric learning algorithms with bounded loss, if the algorithm has Uniform Stability with the probabilistic definition above, then the algorithm generalizes.

Uniform Stability is a strong condition which is not met by all algorithms but is, surprisingly, met by the large and important class of Regularization algorithms.
The generalization bound is given in the article.

'''From Mukherjee et al. (06)''':

*For symmetric learning algorithms with bounded loss, if the algorithm has ''both'' Leave-one-out cross-validation (CVloo) Stability and Expected-leave-one-out error (&lt;math&gt;Eloo_{err}&lt;/math&gt;) Stability as defined above, then the algorithm generalizes.
*Neither condition alone is sufficient for generalization. However, both together ensure generalization (while the converse is not true).
*For ERM algorithms specifically (say for the square loss), Leave-one-out cross-validation (CVloo) Stability is both necessary and sufficient for consistency and generalization.

This is an important result for the foundations of learning theory, because it shows that two previously unrelated properties of an algorithm, stability and consistency, are equivalent for ERM (and certain loss functions).
The generalization bound is given in the article.

==Algorithms that are stable==
This is a list of algorithms that have been shown to be stable, and the article where the associated generalization bounds are provided.

* [[Linear regression]]&lt;ref&gt;Elisseeff, A. A study about algorithmic stability and
 their relation to generalization performances. Technical
 report. (2000)
&lt;/ref&gt;
*k-NN classifier with a {0-1} loss function.&lt;ref&gt;L. Devroye and Wagner, Distribution-free performance bounds for potential function rules, IEEE Trans. Inform. Theory 25(5) (1979) 601–604.&lt;/ref&gt;
*[[Support Vector Machine]] (SVM) classification with a bounded kernel and where the regularizer is a norm in a Reproducing Kernel Hilbert Space. A large regularization constant &lt;math&gt;C&lt;/math&gt; leads to good stability.&lt;ref&gt;O. Bousquet and A. Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526, 2002.&lt;/ref&gt;
*Soft margin SVM classification.&lt;ref&gt;O. Bousquet and A. Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526, 2002.&lt;/ref&gt;
*[[regularization (machine learning)|Regularized]] Least Squares regression.&lt;ref&gt;O. Bousquet and A. Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526, 2002.&lt;/ref&gt;
*The minimum relative entropy algorithm for classification.&lt;ref&gt;O. Bousquet and A. Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526, 2002.&lt;/ref&gt;
*A version of [[bootstrap aggregating|bagging]] regularizers with the number &lt;math&gt;k&lt;/math&gt; of regressors increasing with &lt;math&gt;n&lt;/math&gt;.&lt;ref&gt;Rifkin, R. Everything Old is New Again: A fresh
 look at historical approaches in machine learning. Ph.D. Thesis, MIT, 2002&lt;/ref&gt;
*Multi-class SVM classification.&lt;ref&gt;Rifkin, R. Everything Old is New Again: A fresh
 look at historical approaches in machine learning. Ph.D. Thesis, MIT, 2002&lt;/ref&gt;
*All learning algorithms with Tikhonov regularization satisfies Uniform Stability criteria and are, thus, generalizable.&lt;ref&gt;http://www.mit.edu/~9.520/spring09/Classes/class10_stability.pdf&lt;/ref&gt;

== References ==
{{Reflist}}

==Further reading==
{{Refbegin}}
*S.Kutin and P.Niyogi.Almost-everywhere algorithmic stability and generalization error. In Proc. of UAI 18, 2002
*S. Rakhlin, S. Mukherjee, and T. Poggio. Stability results in learning theory. Analysis and Applications, 3(4):397–419, 2005
*V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995
*Vapnik, V., Statistical Learning Theory. Wiley, New York, 1998
*Poggio, T., Rifkin, R., Mukherjee, S. and Niyogi, P., &quot;Learning Theory: general conditions for predictivity&quot;, Nature, Vol. 428, 419-422, 2004
*Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, Stability of Randomized Learning Algorithms, Journal of Machine Learning Research 6, 55–79, 2010
*Elisseeff, A. Pontil, M., Leave-one-out Error and Stability of Learning Algorithms with Applications, NATO SCIENCE SERIES SUB SERIES III COMPUTER AND SYSTEMS SCIENCES, 2003, VOL 190, pages 111-130
*Shalev Shwartz, S., Shamir, O., Srebro, N., Sridharan, K.,  Learnability, Stability and Uniform Convergence, Journal of Machine Learning Research, 11(Oct):2635-2670, 2010
{{Refend}}


</text>
      <sha1>d6nfm9av4adkb5cr886jta9cz2zx0aq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Artificial neural networks</title>
    <ns>14</ns>
    <id>42936114</id>
    <revision>
      <id>717605771</id>
      <parentid>619964805</parentid>
      <timestamp>2016-04-28T17:57:08Z</timestamp>
      <contributor>
        <username>Dimadick</username>
        <id>24198</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="427">{{Commons cat|Artificial neural network}}
{{Cat main|Artificial neural networks}}

This category are for articles about artificial neural networks (ANN).








</text>
      <sha1>nalnt1o5lrmak9biiwwytapz2fdme76</sha1>
    </revision>
  </page>
  <page>
    <title>Bayesian interpretation of kernel regularization</title>
    <ns>0</ns>
    <id>35867897</id>
    <revision>
      <id>806917401</id>
      <parentid>795337553</parentid>
      <timestamp>2017-10-24T22:59:59Z</timestamp>
      <contributor>
        <username>KenFehling</username>
        <id>767830</id>
      </contributor>
      <minor/>
      <comment>Estimator link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17301">{{context|date=May 2012}}
In [[machine learning]], [[kernel methods]] arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as [[support vector machine]]s (SVMs), the original formulation and its [[regularization (mathematics)|regularization]] were not Bayesian in nature. It is helpful to understand them from a [[Bayesian probability|Bayesian]] perspective.  Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general [[reproducing kernel Hilbert space]]s.  In Bayesian probability kernel methods are a key component of [[Gaussian processes]], where the kernel function is known as the covariance function.  Kernel methods have traditionally been used in [[supervised learning]] problems where the ''input space'' is usually a ''space of vectors'' while the ''output space'' is a ''space of scalars''. More recently these methods have been extended to problems that deal with [[Kernel methods for vector output|multiple outputs]] such as in [[multi-task learning]].&lt;ref name=AlvRosLaw11&gt;{{cite arxiv|last=Álvarez|first=Mauricio A.|author2=Rosasco, Lorenzo |author3=Lawrence, Neil D. |title=Kernels for Vector-Valued Functions: A Review|eprint=1106.6251 |date=June 2011|class=stat.ML}}&lt;/ref&gt;

In this article we analyze the connections between the regularization and the Bayesian point of view for kernel methods in the case of scalar outputs.  A mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is ''finite-dimensional''. The infinite-dimensional case raises subtle mathematical issues; we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent [[estimator|estimators]], and show the connection that ties them together.

==The Supervised Learning Problem==

The classical [[supervised learning]] problem requires estimating the output for some new input point &lt;math&gt;\mathbf{x}'&lt;/math&gt; by learning a scalar-valued estimator &lt;math&gt;\hat{f}(\mathbf{x}')&lt;/math&gt; on the basis of a training set &lt;math&gt;S&lt;/math&gt; consisting of &lt;math&gt;n&lt;/math&gt; input-output pairs, &lt;math&gt;S = (\mathbf{X},\mathbf{Y}) = (\mathbf{x}_1,y_1),\ldots,(\mathbf{x}_n,y_n)&lt;/math&gt;.&lt;ref name=Vap98&gt;{{cite book|last=Vapnik|first=Vladimir|title=Statistical learning theory|year=1998|publisher=Wiley|isbn=9780471030034|url=https://books.google.com/?id=GowoAQAAMAAJ&amp;q=statistical+learning+theory&amp;dq=statistical+learning+theory}}&lt;/ref&gt;  Given a symmetric and positive bivariate function &lt;math&gt;k(\cdot,\cdot)&lt;/math&gt; called a ''kernel'', one of the most popular estimators in machine learning is given by

:{{NumBlk|:|&lt;math&gt;
\hat{f}(\mathbf{x}') = \mathbf{k}^\top(\mathbf{K} + \lambda n \mathbf{I})^{-1} \mathbf{Y},
&lt;/math&gt;|{{EquationRef|1}}}}

where &lt;math&gt;\mathbf{K} \equiv k(\mathbf{X},\mathbf{X})&lt;/math&gt; is the [[Gramian matrix|kernel matrix]] with entries &lt;math&gt;\mathbf{K}_{ij} = k(\mathbf{x}_i,\mathbf{x}_j)&lt;/math&gt;, &lt;math&gt; \mathbf{k} = [k(\mathbf{x}_1,\mathbf{x}'),\ldots,k(\mathbf{x}_n,\mathbf{x}')]^\top&lt;/math&gt;, and &lt;math&gt;\mathbf{Y} = [y_1,\ldots,y_n]^\top&lt;/math&gt;.  We will see how this estimator can be derived both from a regularization and a Bayesian perspective.

==A Regularization Perspective==

The main assumption in the regularization perspective is that the set of functions &lt;math&gt;\mathcal{F}&lt;/math&gt; is assumed to belong to a reproducing kernel Hilbert space &lt;math&gt;\mathcal{H}_k&lt;/math&gt;.&lt;ref name=Vap98 /&gt;&lt;ref name=Wah90 /&gt;&lt;ref name=SchSmo02&gt;{{cite book|last=Schölkopf|first=Bernhard|title=Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond|year=2002|publisher=MIT Press|isbn=9780262194754|author2=Smola, Alexander J.}}&lt;/ref&gt;&lt;ref name=GirPog90&gt;{{cite journal|last=Girosi|first=F.|author2=Poggio, T.|title=Networks and the best approximation property|journal=Biological Cybernetics|year=1990|volume=63|issue=3|pages=169–176|publisher=Springer|doi=10.1007/bf00195855}}&lt;/ref&gt;

===Reproducing Kernel Hilbert Space===

A [[reproducing kernel Hilbert space]] (RKHS) &lt;math&gt;\mathcal{H}_k&lt;/math&gt; is a [[Hilbert space]] of functions defined by a [[Symmetry in mathematics|symmetric]], [[positive-definite function]] &lt;math&gt;k : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}&lt;/math&gt; called the ''reproducing kernel'' such that the function &lt;math&gt;k(\mathbf{x},\cdot)&lt;/math&gt; belongs to &lt;math&gt;\mathcal{H}_k&lt;/math&gt; for all &lt;math&gt;\mathbf{x} \in \mathcal{X}&lt;/math&gt;.&lt;ref name=Aro50&gt;{{cite journal|last=Aronszajn|first=N|title=Theory of Reproducing Kernels|journal=Transactions of the American Mathematical Society|date=May 1950|volume=68|issue=3|pages=337–404|doi=10.2307/1990404|jstor=1990404}}&lt;/ref&gt;&lt;ref name=Sch64&gt;{{cite journal|last=Schwartz|first=Laurent|title=Sous-espaces hilbertiens d’espaces vectoriels topologiques et noyaux associés (noyaux reproduisants)|journal=Journal d'analyse mathématique|year=1964|volume=13|issue=1|pages=115–256|publisher=Springer|doi=10.1007/bf02786620}}&lt;/ref&gt;&lt;ref name=CucSma01&gt;{{cite journal|last=Cucker|first=Felipe|author2=Smale, Steve|title=On the mathematical foundations of learning|journal=Bulletin of the American Mathematical Society|date=October 5, 2001|volume=39|issue=1|pages=1–49|doi=10.1090/s0273-0979-01-00923-5}}&lt;/ref&gt; There are three main properties make an RKHS appealing:

1. The ''reproducing property'', which gives name to the space,

:&lt;math&gt;
f(\mathbf{x}) = \langle f,k(\mathbf{x},\cdot) \rangle_k, \quad \forall \ f \in \mathcal{H}_k,
&lt;/math&gt;

where &lt;math&gt;\langle \cdot,\cdot \rangle_k&lt;/math&gt; is the inner product in &lt;math&gt;\mathcal{H}_k&lt;/math&gt;.

2. Functions in an RKHS are in the closure of the linear combination of the kernel at given points,

:&lt;math&gt;
f(\mathbf{x}) = \sum_i k(\mathbf{x}_i,\mathbf{x})c_i
&lt;/math&gt;.

This allows the construction in a unified framework of both linear and generalized linear models.

3. The squared norm in an RKHS can be written as

:&lt;math&gt;\|f\|_k^2 = \sum_{i,j} k(\mathbf{x}_i,\mathbf{x}_j) c_i c_j
&lt;/math&gt;

and could be viewed as measuring the ''complexity'' of the function.

===The Regularized Functional===

The estimator is derived as the minimizer of the regularized functional

:{{NumBlk|:|&lt;math&gt;
\frac{1}{n} \sum_{i=1}^{n}(f(\mathbf{x}_i)-y_i)^2 + \lambda \|f\|_k^2,
&lt;/math&gt;|{{EquationRef|2}}}}

where &lt;math&gt;f \in \mathcal{H}_k&lt;/math&gt; and &lt;math&gt;\|\cdot\|_k&lt;/math&gt; is the norm in &lt;math&gt;\mathcal{H}_k&lt;/math&gt;.  The first term in this functional, which measures the average of the squares of the errors between the &lt;math&gt;f(\mathbf{x}_i)&lt;/math&gt; and the &lt;math&gt;y_i&lt;/math&gt;, is called the ''empirical risk'' and represents the cost we pay by predicting &lt;math&gt;f(\mathbf{x}_i)&lt;/math&gt; for the true value &lt;math&gt;y_i&lt;/math&gt;.  The second term in the functional is the squared norm in a RKHS multiplied by a weight &lt;math&gt;\lambda&lt;/math&gt; and serves the purpose of stabilizing the problem&lt;ref name=Wah90 /&gt;&lt;ref name=GirPog90 /&gt; as well as of adding a trade-off between fitting and complexity of the estimator.&lt;ref name=Vap98 /&gt;  The weight &lt;math&gt;\lambda&lt;/math&gt;, called the ''regularizer'', determines the degree to which instability and complexity of the estimator should be penalized (higher penalty for increasing value of &lt;math&gt;\lambda&lt;/math&gt;).

===Derivation of the Estimator===

The explicit form of the estimator in equation ({{EquationNote|1}}) is derived in two steps.  First, the representer theorem&lt;ref name=KimWha70&gt;{{cite journal|last=Kimeldorf|first=George S.|author2=Wahba, Grace|title=A correspondence between Bayesian estimation on stochastic processes and smoothing by splines|journal=The Annals of Mathematical Statistics|year=1970|volume=41|issue=2|pages=495–502|doi=10.1214/aoms/1177697089}}&lt;/ref&gt;&lt;ref name=SchHerSmo01&gt;{{cite journal|last=Schölkopf|first=Bernhard|author2=Herbrich, Ralf |author3=Smola, Alex J. |title=A Generalized Representer Theorem|journal=COLT/EuroCOLT 2001, LNCS|year=2001|volume=2111/2001|pages=416–426|doi=10.1007/3-540-44581-1_27|series=Lecture Notes in Computer Science|isbn=978-3-540-42343-0}}&lt;/ref&gt;&lt;ref name=DevEtal04&gt;{{cite journal|last=De Vito|first=Ernesto|author2=Rosasco, Lorenzo |author3=Caponnetto, Andrea |author4=Piana, Michele |author5= Verri, Alessandro |title=Some Properties of Regularized Kernel Methods|journal=Journal of Machine Learning Research|date=October 2004|volume=5|pages=1363–1390}}&lt;/ref&gt; states that the minimizer of the functional ({{EquationNote|2}}) can always be written as a linear combination of the kernels centered at the training-set points,

:{{NumBlk|:|&lt;math&gt;
\hat{f}(\mathbf{x}') = \sum_{i=1}^n c_i k(\mathbf{x}_i,\mathbf{x}') = \mathbf{k}^\top \mathbf{c},
&lt;/math&gt;|{{EquationRef|3}}}}

for some &lt;math&gt;\mathbf{c} \in \mathbb{R}^n&lt;/math&gt;.  The explicit form of the coefficients &lt;math&gt;\mathbf{c} = [c_1,\ldots,c_n]^\top&lt;/math&gt; can be found by substituting for &lt;math&gt;f(\cdot)&lt;/math&gt; in the functional ({{EquationNote|2}}).  For a function of the form in equation ({{EquationNote|3}}), we have that

:&lt;math&gt;\begin{align}
\|f\|_k^2 &amp; = \langle f,f \rangle_k, \\
&amp; = \left\langle \sum_{i=1}^N c_i k(\mathbf{x}_i,\cdot), \sum_{j=1}^N c_j k(\mathbf{x}_j,\cdot) \right\rangle_k, \\
&amp; = \sum_{i=1}^N \sum_{j=1}^N c_i c_j \langle k(\mathbf{x}_i,\cdot), k(\mathbf{x}_j,\cdot) \rangle_k, \\
&amp; = \sum_{i=1}^N \sum_{j=1}^N c_i c_j k(\mathbf{x}_i,\mathbf{x}_j), \\
&amp; = \mathbf{c}^\top \mathbf{K} \mathbf{c}.
\end{align}&lt;/math&gt;

We can rewrite the functional ({{EquationNote|2}}) as

:&lt;math&gt;
\frac{1}{n} \| \mathbf{y} - \mathbf{K} \mathbf{c} \|^2 + \lambda \mathbf{c}^\top \mathbf{K} \mathbf{c}.
&lt;/math&gt;

This functional is convex in &lt;math&gt;\mathbf{c}&lt;/math&gt; and therefore we can find its minimum by setting the gradient with respect to &lt;math&gt;\mathbf{c}&lt;/math&gt; to zero,

:&lt;math&gt;\begin{align}
-\frac{1}{n} \mathbf{K} (\mathbf{Y} - \mathbf{K} \mathbf{c}) + \lambda \mathbf{K} \mathbf{c} &amp; = 0, \\
(\mathbf{K} + \lambda n \mathbf{I}) \mathbf{c} &amp; = \mathbf{Y}, \\
\mathbf{c} &amp; = (\mathbf{K} + \lambda n \mathbf{I})^{-1} \mathbf{Y}.
\end{align}&lt;/math&gt;

Substituting this expression for the coefficients in equation ({{EquationNote|3}}), we obtain the estimator stated previously in equation ({{EquationNote|1}}),

:&lt;math&gt;
\hat{f}(\mathbf{x}') = \mathbf{k}^\top(\mathbf{K} + \lambda n \mathbf{I})^{-1} \mathbf{Y}.
&lt;/math&gt;

==A Bayesian Perspective==

The notion of a kernel plays a crucial role in Bayesian probability as the covariance function of a stochastic process called the ''[[Gaussian process]]''.

===A Review of Bayesian Probability===

As part of the Bayesian framework, the Gaussian process specifies the [[Prior probability|''prior distribution'']] that describes the prior beliefs about the properties of the function being modeled.  These beliefs are updated after taking into account observational data by means of a ''[[likelihood function]]'' that relates the prior beliefs to the observations.  Taken together, the prior and likelihood lead to an updated distribution called the [[Posterior probability|''posterior distribution'']] that is customarily used for predicting test cases.

===The Gaussian Process===

A [[Gaussian process]] (GP) is a stochastic process in which any finite number of random variables that are sampled follow a joint [[Multivariate normal distribution|Normal distribution]].&lt;ref name=RasWil06 /&gt;  The mean vector and covariance matrix of the Gaussian distribution completely specify the GP.  GPs are usually used as a priori distribution for functions, and as such the mean vector and covariance matrix can be viewed as functions, where the covariance function is also called the ''kernel'' of the GP.  Let a function &lt;math&gt;f&lt;/math&gt; follow a Gaussian process with mean function &lt;math&gt;m&lt;/math&gt; and kernel function &lt;math&gt;k&lt;/math&gt;,

:&lt;math&gt;
f \sim \mathcal{GP}(m,k).
&lt;/math&gt;

In terms of the underlying Gaussian distribution, we have that for any finite set &lt;math&gt;\mathbf{X} = \{\mathbf{x}_i\}_{i=1}^{n}&lt;/math&gt; if we let &lt;math&gt;f(\mathbf{X}) = [f(\mathbf{x}_1),\ldots,f(\mathbf{x}_n)]^\top&lt;/math&gt; then

:&lt;math&gt;
f(\mathbf{X}) \sim \mathcal{N}(\mathbf{m},\mathbf{K}),
&lt;/math&gt;

where &lt;math&gt;\mathbf{m} = m(\mathbf{X}) = [m(\mathbf{x}_1),\ldots,m(\mathbf{x}_N)]^\top&lt;/math&gt; is the mean vector and &lt;math&gt;\mathbf{K} = k(\mathbf{X},\mathbf{X})&lt;/math&gt; is the covariance matrix of the multivariate Gaussian distribution.

===Derivation of the Estimator===
{{further|Minimum mean square error#Linear MMSE estimator for linear observation process}}
In a regression context, the likelihood function is usually assumed to be a Gaussian distribution and the observations to be independent and identically distributed (iid),

:&lt;math&gt;
p(y|f,\mathbf{x},\sigma^2) = \mathcal{N}(f(\mathbf{x}),\sigma^2).
&lt;/math&gt;

This assumption corresponds to the observations being corrupted with zero-mean Gaussian noise with variance &lt;math&gt;\sigma^2&lt;/math&gt;. The iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs &lt;math&gt;\mathbf{X}&lt;/math&gt; and the variance of the noise &lt;math&gt;\sigma^2&lt;/math&gt;, and thus the posterior distribution can be computed analytically. For a test input vector &lt;math&gt;\mathbf{x}'&lt;/math&gt;, given the training data &lt;math&gt;S = \{\mathbf{X},\mathbf{Y}\}&lt;/math&gt;, the posterior distribution is given by

:&lt;math&gt;
p(f(\mathbf{x}')|S,\mathbf{x}',\boldsymbol{\phi}) = \mathcal{N}(m(\mathbf{x}'),\sigma^2(\mathbf{x}')),
&lt;/math&gt;

where &lt;math&gt;\boldsymbol{\phi}&lt;/math&gt; denotes the set of parameters which include the variance of the noise &lt;math&gt;\sigma^2&lt;/math&gt; and any parameters from the covariance function &lt;math&gt;k&lt;/math&gt; and where

:&lt;math&gt;\begin{align}
m(\mathbf{x}') &amp; = \mathbf{k}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{Y}, \\
\sigma^2(\mathbf{x}') &amp; = k(\mathbf{x}',\mathbf{x}') - \mathbf{k}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{k}.
\end{align}&lt;/math&gt;

==The Connection Between Regularization and Bayes==

A connection between regularization theory and Bayesian theory can only be achieved in the case of ''finite dimensional RKHS''. Under this assumption, regularization theory and Bayesian theory are connected through Gaussian process prediction.&lt;ref name=Wah90&gt;{{cite book|last=Wahba|first=Grace|title=Spline models for observational data|year=1990|publisher=SIAM}}&lt;/ref&gt;&lt;ref name=RasWil06&gt;{{cite book|last=Rasmussen|first=Carl Edward|title=Gaussian Processes for Machine Learning|year=2006|publisher=The MIT Press|isbn=0-262-18253-X|url=http://www.gaussianprocess.org/gpml/|author2=Williams, Christopher K. I.}}&lt;/ref&gt;

In the finite dimensional case, every RKHS can be described in terms of a feature map &lt;math&gt;\Phi : \mathcal{X} \rightarrow \mathbb{R}^p&lt;/math&gt; such that&lt;ref name=Vap98 /&gt;

:&lt;math&gt;
k(\mathbf{x},\mathbf{x}') = \sum_{i=1}^p \Phi^i(\mathbf{x})\Phi^i(\mathbf{x}').
&lt;/math&gt;

Functions in the RKHS with kernel &lt;math&gt;\mathbf{K}&lt;/math&gt; can be then be written as

:&lt;math&gt;
f_{\mathbf{w}}(\mathbf{x}) = \sum_{i=1}^p \mathbf{w}^i \Phi^i(\mathbf{x}) = \langle \mathbf{w},\Phi(\mathbf{x}) \rangle,
&lt;/math&gt;

and we also have that

:&lt;math&gt;
\|f_{\mathbf{w}} \|_k = \|\mathbf{w}\|.
&lt;/math&gt;

We can now build a Gaussian process by assuming &lt;math&gt; \mathbf{w} = [w^1,\ldots,w^p]^\top &lt;/math&gt; to be distributed according to a multivariate Gaussian distribution with zero mean and identity covariance matrix,

:&lt;math&gt;
\mathbf{w} \sim \mathcal{N}(0,\mathbf{I}) \propto \exp(-\|\mathbf{w}\|^2).
&lt;/math&gt;

If we assume a Gaussian likelihood we have

:&lt;math&gt;
P(\mathbf{Y}|\mathbf{X},f) = \mathcal{N}(f(\mathbf{X}),\sigma^2 \mathbf{I}) \propto \exp\left(-\frac{1}{\sigma^2} \| f_{\mathbf{w}}(\mathbf{X}) - \mathbf{Y} \|^2\right),
&lt;/math&gt;

where &lt;math&gt; f_{\mathbf{w}}(\mathbf{X}) = (\langle\mathbf{w},\Phi(\mathbf{x}_1)\rangle,\ldots,\langle\mathbf{w},\Phi(\mathbf{x}_n \rangle) &lt;/math&gt;. The resulting posterior distribution is the given by

:&lt;math&gt;
P(f|\mathbf{X},\mathbf{Y}) \propto \exp\left(-\frac{1}{\sigma^2} \|f_{\mathbf{w}}(\mathbf{X}) - \mathbf{Y}\|_n^2 + \|\mathbf{w}\|^2\right)
&lt;/math&gt;

We can see that a ''maximum posterior (MAP)'' estimate is equivalent to the minimization problem defining [[Tikhonov regularization]], where in the Bayesian case the regularization parameter is related to the noise variance.

From a philosophical perspective, the loss function in a regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting &lt;math&gt;f(\mathbf{x})&lt;/math&gt; in place of &lt;math&gt;y&lt;/math&gt;, the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process. From a mathematical perspective, however, the formulations of the regularization and Bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions &lt;math&gt;f&lt;/math&gt; that approximate the labels &lt;math&gt;y&lt;/math&gt; as much as possible.

==References==
{{Reflist}}


</text>
      <sha1>b4cf5mz4fnfqa3zxn0u86d35ofl1dmk</sha1>
    </revision>
  </page>
  <page>
    <title>AIXI</title>
    <ns>0</ns>
    <id>30511763</id>
    <revision>
      <id>813159006</id>
      <parentid>805110629</parentid>
      <timestamp>2017-12-02T03:52:36Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* Optimality */[[User:JCW-CleanerBot#Logic|task]], replaced: Proceedings of The  → Proceedings of the using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6489">{{multiple issues|
{{COI|date=November 2016}}
{{expert needed|1=WikiProject Artificial intelligence|date=November 2016}}
{{overly detailed|date=November 2016}}
{{primary sources|date=November 2016}}
}}
'''AIXI''' {{IPA-all|'ai̯k͡siː|}} is a theoretical [[mathematical formalism]] for [[artificial general intelligence]].
It combines [[Solomonoff induction]] with [[Decision theory|sequential decision theory]].
AIXI was first proposed by [[Marcus Hutter]] in 2000&lt;ref&gt;{{cite book |author=Marcus Hutter |title=A Theory of Universal Artificial Intelligence based on Algorithmic Complexity |arxiv=cs.AI/0004001 |year=2000 }}&lt;/ref&gt; and the results below are proved in Hutter's 2005 book ''Universal Artificial Intelligence''.&lt;ref name=&quot;uaibook&quot;&gt;{{cite book |author=Marcus Hutter |title=Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability |url=https://books.google.com/books?id=NP53iZGt4KUC |year=2004 |publisher=Springer |isbn=978-3-540-22139-5 |doi=10.1007/b138233 |ref=harv |authormask=1}}&lt;/ref&gt;

AIXI is a [[Reinforcement learning|reinforcement learning agent]];
it maximizes the expected total rewards received from the environment.
Intuitively, it simultaneously considers every computable hypothesis.
In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken.
The promised rewards are then weighted by the [[Subjective logic|subjective belief]] that this program constitutes the true environment.
This belief is computed from the length of the program: longer programs are considered less likely, in line with [[Occam's razor]].
AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.

== Definition ==

The AIXI agent interacts sequentially with some (stochastic and unknown to AIXI) environment &lt;math&gt;\mu&lt;/math&gt;.
In step ''t'', the agent outputs an action &lt;math&gt;a_t&lt;/math&gt; and
the environment responds with an observation &lt;math&gt;o_t&lt;/math&gt; and a reward &lt;math&gt;r_t&lt;/math&gt; distributed according to the conditional probability
&lt;math&gt;\mu(o_t r_t | a_1 o_1 r_1 ... a_{t-1} o_{t-1} r_{t-1} a_t)&lt;/math&gt;.
Then this cycle repeats for ''t + 1''.
The agent tries to maximize cumulative future reward &lt;math&gt;r_t + \ldots + r_m&lt;/math&gt; for a fixed lifetime ''m''.

Given a current time ''t'' and history &lt;math&gt;a_1 o_1 r_1 ... a_{t-1} o_{t-1} r_{t-1}&lt;/math&gt;,
the action AIXI outputs is defined as&lt;ref&gt;[http://hutter1.net/ai/uaibook.htm Universal Artificial Intelligence&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

:&lt;math&gt;
\arg \max_{a_t} \sum_{o_t r_t} \ldots \max_{a_m} \sum_{o_m r_m}
  [r_t + \ldots + r_m] \sum_{q:\; U(q, a_1 \ldots a_m) = o_1 r_1 \ldots o_m r_m} 2^{-\textrm{length}(q)},
&lt;/math&gt;

where ''U'' denotes a [[monotone class theorem|monotone]] [[universal Turing machine]], and
''q'' ranges over all programs on the universal machine ''U''.

The parameters to AIXI are the universal Turing machine and the agent's lifetime ''m''.
The latter dependence can be removed by the use of [[discounting]].

== Optimality ==

AIXI's performance is measured by the expected total number of rewards it receives.
AIXI has been proven to be optimal in the following ways.&lt;ref name=&quot;uaibook&quot; /&gt;

* [[Pareto optimality]]: there is no other agent that performs at least as well as AIXI in all environments while performing strictly better in at least one environment.{{citation needed|date=June 2014}}
* Balanced Pareto optimality: Like Pareto optimality, but considering a weighted sum of environments.
* Self-optimizing: a policy ''p'' is called self-optimizing for an environment &lt;math&gt;\mu&lt;/math&gt; if the performance of ''p'' approaches the theoretical maximum for &lt;math&gt;\mu&lt;/math&gt; when the length of the agent's lifetime (not time) goes to infinity. For environment classes where self-optimizing policies exist, AIXI is self-optimizing.

It was later shown that balanced Pareto optimality is subjective and that any policy can be considered Pareto optimal, which undermines all previous optimality claims for AIXI.&lt;ref&gt;{{cite conference|conference=Proceedings of the 28th Conference on Learning Theory|last1=Leike|first1=Jan|last2=Hutter|first2=Marcus|title=Bad Universal Priors and Notions of Optimality|date=2015|url=http://proceedings.mlr.press/v40/Leike15.pdf}}&lt;/ref&gt;

However, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesn't consider itself to be contained by the environment it interacts with. It also assumes the environment is computable.&lt;ref&gt;{{cite web|last1=Soares|first1=Nate|title=Formalizing Two Problems of Realistic World-Models|url=https://intelligence.org/files/RealisticWorldModels.pdf|website=Intelligence.org|accessdate=2015-07-19|ref=MIRI}}&lt;/ref&gt; Since AIXI is incomputable (see below), it assigns zero probability to its own existence{{citation needed|date=October 2017}}.

== Computational aspects ==

Like [[Solomonoff induction]], AIXI is [[Undecidable problem|incomputable]].
However, there are computable approximations of it.
One such approximation is AIXI''tl'',
which performs as least as well as the provably best time ''t'' and space ''l'' limited agent.&lt;ref name=&quot;uaibook&quot; /&gt;
Another approximation to AIXI with a restricted environment class is MC-AIXI(FAC-CTW),
which has had some success playing simple games such as [[Partially observable system|partially observable]] [[Pac-Man]].&lt;ref&gt;{{cite arXiv |last1=Veness |first1=Joel |author2=Kee Siong Ng |last3=Hutter |first3=Marcus |last4=Uther |first4=William  |last5=Silver |first5=David   |eprint=0909.0801 |title=A Monte Carlo AIXI Approximation |year=2009 |class=cs.AI}}&lt;/ref&gt;&lt;ref&gt;[https://www.youtube.com/watch?v=yfsMHtmGDKE Playing Pacman using AIXI Approximation - YouTube&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

== See also ==
* [[Gödel machine]]

== References ==

{{reflist}}
* &quot;Universal Algorithmic Intelligence: A mathematical top-&gt;down approach&quot;, Marcus Hutter, {{arXiv|cs/0701125}}; also in ''Artificial General Intelligence'', eds. B. Goertzel and C. Pennachin, Springer, 2007, {{ISBN|9783540237334}}, pp.&amp;nbsp;227–290, {{doi|10.1007/978-3-540-68677-4_8}}.



</text>
      <sha1>hxmv5g0w49j18npbtckb7qcpqkv6uua</sha1>
    </revision>
  </page>
  <page>
    <title>Sample complexity</title>
    <ns>0</ns>
    <id>43269516</id>
    <revision>
      <id>777478012</id>
      <parentid>757736169</parentid>
      <timestamp>2017-04-27T11:55:55Z</timestamp>
      <contributor>
        <username>E.pajouheshgar</username>
        <id>30437011</id>
      </contributor>
      <comment>/* An example of a PAC-learnable hypothesis space */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12138">{{Machine learning bar}}
The '''sample complexity''' of a [[machine learning]] algorithm represents the number of training-samples that it needs in order to successfully learn a target function.

More precisely, the sample complexity is the number of training-samples that we need to supply to the algorithm, so that the function returned by the algorithm is within an arbitrarily small error of the best possible function, with probability arbitrarily close to 1.

There are two variants of sample complexity:
* The weak variant fixes a particular input-output distribution;
* The strong variant takes the worst-case sample complexity over all input-output distributions.

The No Free Lunch theorem, discussed below, proves that, in general, the strong sample complexity is infinite. I.e, there is no algorithm that can learn the globally-optimal target function using a finite number of training samples.

However, if we are only interested in a particular class of target functions (e.g, only linear functions) then the sample complexity is finite, and it depends linearly on the [[VC dimension]] on the class of target functions.
&lt;ref name=&quot;:0&quot; /&gt;

==Definition==
Let &lt;math&gt;X&lt;/math&gt; be a space which we call the input space, and &lt;math&gt;Y&lt;/math&gt; be a space which we call the output space, and let &lt;math&gt;Z&lt;/math&gt; denote the product &lt;math&gt;X\times Y&lt;/math&gt;. For example, in the setting of binary classification, &lt;math&gt;X&lt;/math&gt; is typically a finite-dimensional vector space and &lt;math&gt;Y&lt;/math&gt; is the set &lt;math&gt;\{-1,1\}&lt;/math&gt;.

Fix a hypothesis space &lt;math&gt;\mathcal H&lt;/math&gt; of functions &lt;math&gt;h\colon X\to Y&lt;/math&gt;. A learning algorithm over &lt;math&gt;\mathcal H&lt;/math&gt; is a computable map from &lt;math&gt;Z^*&lt;/math&gt; to &lt;math&gt;\mathcal H&lt;/math&gt;. In other words, it is an algorithm that takes as input a finite sequence of training samples and outputs a function from &lt;math&gt;X&lt;/math&gt; to &lt;math&gt;Y&lt;/math&gt;. Typical learning algorithms include [[empirical risk minimization]], without or with [[Tikhonov regularization]].

Fix a loss function &lt;math&gt;Loss\colon Y\times Y\to\R_{\geq 0}&lt;/math&gt;, for example, the square loss &lt;math&gt; Loss(y,y')=(y-y')^2&lt;/math&gt;. For a given distribution &lt;math&gt;\rho&lt;/math&gt; on &lt;math&gt;X\times Y&lt;/math&gt;, the expected risk of a hypothesis (a function) &lt;math&gt;h\in\mathcal H&lt;/math&gt; is

:&lt;math&gt;\mathcal E(h) :=\mathbb E_\rho[Loss(h(x),y)]=\int_{X\times Y} Loss(h(x),y)\,d\rho(x,y)&lt;/math&gt;

In our setting, we have &lt;math&gt;h=ALG(S_n)&lt;/math&gt; where &lt;math&gt;ALG&lt;/math&gt; is a learning algorithm and &lt;math&gt;S_n = ((x_1,y_1),\ldots,(x_n,y_n))\sim \rho^n&lt;/math&gt; is a sequence of vectors which are all drawn independently from &lt;math&gt;\rho&lt;/math&gt;. Define the optimal risk&lt;math display=&quot;block&quot;&gt;
\mathcal E^*_\mathcal{H} = \underset{h \in \mathcal H}{\inf}\mathcal E(h).
&lt;/math&gt;Set &lt;math&gt;h_n=Alg(S_n)&lt;/math&gt;  for each &lt;math&gt;n&lt;/math&gt;. Note that
&lt;math&gt;h_n&lt;/math&gt; is a random variable and depends on the random variable &lt;math&gt;S_n&lt;/math&gt;, which is drawn from the distribution &lt;math&gt;\rho^n&lt;/math&gt;. The algorithm &lt;math&gt;ALG&lt;/math&gt; is called consistent if &lt;math&gt;
\mathcal E(h_n)
&lt;/math&gt; probabilistically converges to &lt;math&gt;
\mathcal E_\mathcal H^*
&lt;/math&gt;, in other words, for all ''ε'', ''δ'' &gt; 0, there exists a positive integer ''N'' such that for all ''n'' ≥ ''N'', we have&lt;math display=&quot;block&quot;&gt;
\Pr_{\rho^n}[\mathcal E(h_n) - \mathcal E^*_\mathcal{H}\geq\varepsilon]&lt;\delta.
&lt;/math&gt;The sample complexity of &lt;math&gt;ALG&lt;/math&gt; is then the minimum ''N'' for which this holds, as a function of ''ρ'', ''ε'', and ''δ''. We write the sample complexity as &lt;math&gt;N(\rho,\epsilon,\delta)&lt;/math&gt; to emphasize that this value of ''N'' depends on ''ρ'', ''ε'', and ''δ''. If &lt;math&gt;ALG&lt;/math&gt; is not consistent, then we set &lt;math&gt;N(\rho,\epsilon,\delta)=\infty&lt;/math&gt;. If there exists an algorithm for which &lt;math&gt;N(\rho,\epsilon,\delta)&lt;/math&gt; is finite, then we say that the hypothesis space &lt;math&gt; \mathcal H&lt;/math&gt; is '''learnable'''.

In words, the sample complexity &lt;math&gt;N(\rho,\epsilon,\delta)&lt;/math&gt; defines the rate of consistency of the algorithm: given a desired accuracy ''ε'' and confidence ''δ'', one needs to sample &lt;math&gt;N(\rho,\epsilon,\delta)&lt;/math&gt; data points to guarantee that the risk of the output function is within ''ε'' of the best possible, with probability at least 1 - ''δ''.&lt;ref name=&quot;Rosasco&quot;&gt;{{citation |last = Rosasco | first = Lorenzo | title = Consistency, Learnability, and Regularization | series = Lecture Notes for MIT Course 9.520. | year = 2014 }}&lt;/ref&gt;

In [[Probably approximately correct learning|probabilistically approximately correct (PAC) learning]], one is concerned with whether the sample complexity is ''polynomial'', that is, whether &lt;math&gt;N(\rho,\epsilon,\delta)&lt;/math&gt; is bounded by a polynomial in 1/''ε'' and 1/''δ''. If &lt;math&gt;N(\rho,\epsilon,\delta)&lt;/math&gt; is polynomial for some learning algorithm, then one says that the hypothesis space
&lt;math&gt; \mathcal H &lt;/math&gt; is '''PAC-learnable'''. Note that this is a stronger notion than being learnable.

==Unrestricted hypothesis space: infinite sample complexity==
&lt;span id='No Free Lunch Theorem'&gt;&lt;/span&gt;
One can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense, that is, there is a bound on the number of samples needed so that the algorithm can learn any distribution over the input-output space with a specified target error. More formally, one asks whether there exists a learning algorithm &lt;math&gt;ALG&lt;/math&gt; such that, for all ''ε'', ''δ'' &gt; 0, there exists a positive integer ''N'' such that for all ''n'' ≥ ''N'', we have&lt;math display=&quot;block&quot;&gt;
\sup_\rho\left(\Pr_{\rho^n}[\mathcal E(h_n) - \mathcal E^*_\mathcal{H}\geq\varepsilon]\right)&lt;\delta,
&lt;/math&gt;where &lt;math&gt;h_n=ALG(S_n)&lt;/math&gt;, with &lt;math&gt;S_n = ((x_1,y_1),\ldots,(x_n,y_n))\sim \rho^n&lt;/math&gt; as above. The [[No free lunch in search and optimization|No Free Lunch Theorem]] says that without restrictions on the hypothesis space &lt;math&gt;\mathcal H&lt;/math&gt;, this is not the case, i.e., there always exist &quot;bad&quot; distributions for which the sample complexity is arbitrarily large.&lt;ref name=&quot;:0&quot;&gt;{{citation |last = Vapnik | first = Vladimir | title = Statistical Learning Theory | place = New York | publisher = Wiley. | year = 1998}}&lt;/ref&gt;

Thus, in order to make statements about the rate of convergence of the quantity
&lt;math display=&quot;block&quot;&gt;
\sup_\rho\left(\Pr_{\rho^n}[\mathcal E(f_n) - \mathcal E^*_\mathcal{H}\geq\varepsilon]\right),
&lt;/math&gt;
one must either
*constrain the space of probability distributions &lt;math&gt;\rho&lt;/math&gt;, e.g. via a parametric approach, or
*constrain the space of hypotheses &lt;math&gt;\mathcal H&lt;/math&gt;, as in distribution-free approaches.

==Restricted hypothesis space: finite sample-complexity==
The latter approach leads to concepts such as [[VC dimension]] and [[Rademacher complexity]] which control the complexity of the space &lt;math&gt;\mathcal H&lt;/math&gt;. A smaller hypothesis space introduces more bias into the inference process, meaning that &lt;math&gt;\mathcal E^*_\mathcal{H}&lt;/math&gt; may be greater than the best possible risk in a larger space. However, by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions. This trade-off leads to the concept of [[regularization (mathematics)|regularization]].&lt;ref name = &quot;Rosasco&quot; /&gt;

It is a theorem from [[Vapnik–Chervonenkis theory|VC theory]] that the following three statements are equivalent for a hypothesis space &lt;math&gt;\mathcal H&lt;/math&gt;:
# &lt;math&gt;\mathcal H&lt;/math&gt; is PAC-learnable.
# The VC dimension of &lt;math&gt;\mathcal H&lt;/math&gt; is finite.
# &lt;math&gt;\mathcal H&lt;/math&gt; is a uniform [[Glivenko-Cantelli class]].
This gives a way to prove that certain hypothesis spaces are PAC learnable, and by extension, learnable.

=== An example of a PAC-learnable hypothesis space ===
Let ''X'' = '''R'''&lt;sup&gt;''d''&lt;/sup&gt;, ''Y'' = {-1, 1}, and let &lt;math&gt;\mathcal H&lt;/math&gt; be the space of affine functions on ''X'', that is, functions of the form &lt;math&gt;x\mapsto \langle w,x\rangle+b&lt;/math&gt; for some &lt;math&gt;w\in\R^d,b\in\R&lt;/math&gt;. This is the linear classification with offset learning problem. Now, note that four coplanar points in a square cannot be shattered by any affine function, since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two. Thus, the VC dimension of &lt;math&gt;\mathcal H&lt;/math&gt; is &lt;math&gt;d + 1&lt;/math&gt;, in particular finite. It follows by the above characterization of PAC-learnable classes that &lt;math&gt;\mathcal H&lt;/math&gt; is PAC-learnable, and by extension, learnable.

=== Sample-complexity bounds ===
&lt;span id='bounds'&gt;&lt;/span&gt;
Suppose &lt;math&gt;\mathcal H&lt;/math&gt; is a class of binary functions (functions to {0,1}). Then, &lt;math&gt;\mathcal H&lt;/math&gt; is &lt;math&gt;(\epsilon,\delta)&lt;/math&gt;-PAC-learnable with a sample of size:
&lt;ref&gt;{{Cite journal|title=The optimal sample complexity OF PAC learning
|author=Steve Hanneke|year=2016|url=http://dl.acm.org/citation.cfm?id=2946683}}&lt;/ref&gt;
&lt;math display=&quot;block&quot;&gt;
N = O\bigg(\frac{VC(\mathcal H) + \ln{1\over \delta}}{\epsilon}\bigg)
&lt;/math&gt;
where &lt;math&gt;VC(\mathcal H)&lt;/math&gt; is the [[VC dimension]] of &lt;math&gt;\mathcal H&lt;/math&gt;.
Moreover, any &lt;math&gt;(\epsilon,\delta)&lt;/math&gt;-PAC-learning algorithm for &lt;math&gt;\mathcal H&lt;/math&gt; must have sample-complexity:&lt;ref&gt;{{Cite journal|doi=10.1016/0890-5401(89)90002-3|title=A general lower bound on the number of examples needed for learning|journal=Information and Computation|volume=82|issue=3|pages=247|year=1989|last1=Ehrenfeucht|first1=Andrzej|last2=Haussler|first2=David|last3=Kearns|first3=Michael|last4=Valiant|first4=Leslie}}&lt;/ref&gt;
&lt;math display=&quot;block&quot;&gt;
N = \Omega\bigg(\frac{VC(\mathcal H) + \ln{1\over \delta}}{\epsilon}\bigg)
&lt;/math&gt;
Thus, the sample-complexity is a linear function of the [[VC dimension]] of the hypothesis space.

Suppose &lt;math&gt;\mathcal H&lt;/math&gt; is a class of real-valued functions with range in [0,T]. Then, &lt;math&gt;\mathcal H&lt;/math&gt; is &lt;math&gt;(\epsilon,\delta)&lt;/math&gt;-PAC-learnable with a sample of size:
&lt;ref name=mr15&gt;{{cite book|first1=Martin|last1=Anthony|first2=Peter L.|last2=Bartlett|title=Neural Network Learning: Theoretical Foundations|year=2009|isbn=9780521118620}}&lt;/ref&gt;&lt;ref&gt;{{cite conference|title=On the Pseudo-Dimension of Nearly Optimal Auctions|year=2015|conference=NIPS|url=http://papers.nips.cc/paper/5766-on-the-pseudo-dimension-of-nearly-optimal-auctions|arxiv=1506.03684}}&lt;/ref&gt;
&lt;math display=&quot;block&quot;&gt;
N = O\bigg(T^2\frac{PD(\mathcal H)\ln{T\over \epsilon} + \ln{1\over \delta}}{\epsilon^2}\bigg)
&lt;/math&gt;
where &lt;math&gt;PD(\mathcal H)&lt;/math&gt; is [[VC dimension#Generalizations|Pollard's pseudo-dimension]] of &lt;math&gt;\mathcal H&lt;/math&gt;.

==Other Settings==
In addition to the supervised learning setting, sample complexity is relevant to [[semi-supervised learning]] problems including [[active learning]],&lt;ref name=&quot;Balcan&quot;&gt;{{cite journal |doi = 10.1007/s10994-010-5174-y|title = The true sample complexity of active learning|journal = Machine Learning|date = 2010|volume = 80|issue = 2–3|pages = 111–139|first = Maria-Florina|last = Balcan}}&lt;/ref&gt; where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels. The concept of sample complexity also shows up in [[reinforcement learning]],&lt;ref&gt;{{citation |last = Kakade | first = Sham | title = On the Sample Complexity of Reinforcement Learning | place = University College London | publisher = Gatsby Computational Neuroscience Unit. | series = PhD Thesis | year = 2003 | url = http://www.ias.tu-darmstadt.de/uploads/Research/NIPS2006/SK.pdf}}&lt;/ref&gt; [[online machine learning|online learning]], and unsupervised algorithms, e.g. for [[dictionary learning]].&lt;ref&gt;{{cite journal |last1 = Vainsencher | first1 = Daniel | last2 = Mannor | first2 = Shie | last3 = Bruckstein | first3 = Alfred | title = The Sample Complexity of Dictionary Learning | journal = Journal of Machine Learning Research | volume = 12 | pages = 3259–3281 | date = 2011 | url = http://www.jmlr.org/papers/volume12/vainsencher11a/vainsencher11a.pdf}}&lt;/ref&gt;

==References==
{{Reflist}}

</text>
      <sha1>ss6dd75sgdbqqoxbkv3dh05s6mpaun9</sha1>
    </revision>
  </page>
  <page>
    <title>Evaluation of binary classifiers</title>
    <ns>0</ns>
    <id>43218024</id>
    <revision>
      <id>805887482</id>
      <parentid>805886638</parentid>
      <timestamp>2017-10-18T07:59:56Z</timestamp>
      <contributor>
        <ip>91.64.93.57</ip>
      </contributor>
      <comment>/* Sensitivity and specificity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16565">{| class=&quot;wikitable&quot; align=&quot;right&quot; width=35% style=&quot;font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;&quot;
|+ Terminology and derivations&lt;br&gt;from a confusion matrix
|- valign=top
|
; true positive (TP)
:eqv. with hit
; true negative (TN)
:eqv. with correct rejection
; false positive (FP)
:eqv. with [[false alarm]], [[Type I error]]
; false negative (FN)
:eqv. with miss, [[Type II error]]
----
; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)
:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]
:&lt;math&gt;\mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})&lt;/math&gt;
; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate
:&lt;math&gt;\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{FP} + \mathit{TN}) &lt;/math&gt;
; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)
:&lt;math&gt;\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP})&lt;/math&gt;
; [[negative predictive value]] (NPV)
:&lt;math&gt;\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN})&lt;/math&gt;
; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)
:&lt;math&gt;\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN}) = 1 - \mathit{TNR} &lt;/math&gt;
; [[false discovery rate]] (FDR)
:&lt;math&gt;\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} &lt;/math&gt;
; Miss Rate or [[Type I and type II errors#False positive and false negative rates|False Negative Rate]] (FNR)
:&lt;math&gt;\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) = 1 - \mathit{TPR} &lt;/math&gt;
----
; [[accuracy]] (ACC)
:&lt;math&gt;\mathit{ACC} = (\mathit{TP} + \mathit{TN}) / (P + N)&lt;/math&gt;
; [[Accuracy and precision#In binary classification|balanced accuracy]] (BACC)
:&lt;math&gt;\mathit{BACC} = (\mathit{TP}/P + \mathit{TN}/N)/2&lt;/math&gt;
;[[F1 score]]
: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]
:&lt;math&gt;\mathit{F1} = 2 \mathit{TP} / (2 \mathit{TP} + \mathit{FP} + \mathit{FN})&lt;/math&gt;
; [[Matthews correlation coefficient]] (MCC)
:&lt;math&gt; \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;
; [[Uncertainty coefficient]], aka Proficiency
:&lt;math&gt;\begin{align} L &amp;= (P+N)\times \log(P+N) \\
LTP &amp;= TP \times \log\frac{TP}{(TP+FP)(TP+FN)} \\
LFP &amp;= FP \times \log\frac{FP}{(FP+TP)(FP+TN)} \\
LFN &amp;= FN \times \log\frac{FN}{(FN+TP)(FN+TN)} \\
LTN &amp;= TN \times \log\frac{TN}{(TN+FP)(TN+FN)} \\
LP &amp;= P \times \log \frac{P}{P+N} \\
LN &amp;= N \times \log\frac{N}{P+N} \\
UC &amp;= \frac{L + LTP + LFP + LFN + LTN}{L + LP + LN}
\end{align}&lt;/math&gt;

;Informedness = Sensitivity + Specificity − 1
;Markedness = PPV + NPV − 1
;
&lt;span style=&quot;font-size:90%;&quot;&gt;''Sources: Fawcett (2006) and Powers (2011).''&lt;ref name=Fawcett2006&gt;{{cite journal|last=Fawcett|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861–874|doi=10.1016/j.patrec.2005.10.010}}&lt;/ref&gt;&lt;ref name=Powers2011 /&gt;&lt;/span&gt;
|}

[[Image:binary-classification-labeled.svg|thumb|220px|right|From the [[confusion matrix]] you can derive four basic measures]]

The '''evaluation of binary classifiers''' compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated.  There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine [[sensitivity and specificity]] are often used, while in computer science [[precision and recall]] are preferred. An important distinction is between metrics that are independent on the [[prevalence]] (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.

==Contingency table==
{{main article|Confusion matrix}}

Given a data set, a classification (the output of a classifier on that set) gives two numbers: the number of positives and the number of negatives, which add up to the total size of the set. To evaluate a classifier, one compares its output to another reference classification – ideally a perfect classification, but in practice the output of another [[gold standard (test)|gold standard]] test – and [[cross tabulation|cross tabulates]] the data into a 2×2 [[contingency table]], comparing the two classifications. One then evaluates the classifier ''relative'' to the gold standard by computing [[summary statistic]]s of these 4 numbers. Generally these statistics will be [[scale invariant]] (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of [[homogeneous function]]s, most simply [[homogeneous linear]] or [[homogeneous quadratic]] functions.

Say we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called ''[[true positive]]s'' (TP). Some have the disease, but the test incorrectly claims they don't. They are called ''[[false negative]]s'' (FN). Some don't have the disease, and the test says they don't – ''[[true negative]]s'' (TN). Finally, there might be healthy people who have a positive test result – ''[[false positive]]s'' (FP). These can be arranged into a 2×2 contingency table ([[confusion matrix]]), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis.

These numbers can then be totaled, yielding both a [[grand total]] and [[marginal total]]s. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2×2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions.

The contingency table and the most common derived ratios are summarized below; see sequel for details.

{{DiagnosticTesting Diagram}}

Note that the columns correspond to the ''condition actually'' being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the rows correspond to the ''test'' being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above.

== Sensitivity and specificity ==
{{main article|Sensitivity and specificity}}
The fundamental prevalence-independent statistics are [[sensitivity and specificity]].

'''[[Sensitivity (tests)|Sensitivity]]''' or [[True Positive Rate]] (TPR), also known as [[Recall (information retrieval)|recall]], is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, CP = TP + FN). It can be seen as ''the probability that the test is positive given that the patient is sick''. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market).

'''[[Specificity (tests)|Specificity]]''' (SPC) or [[True Negative Rate]] (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, CN = TN + FP). As with sensitivity, it can be looked at as ''the probability that the test result is negative given that the patient is not sick''. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarded).

The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the [[Receiver Operating Characteristic]] (ROC) curve.

In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a [[surrogate endpoint|surrogate marker]]. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator.

Modern pregnancy tests ''do not'' use the pregnancy itself to determine pregnancy status; rather, [[human chorionic gonadotropin]] is used, or hCG, present in the urine of [[gravid]] females, as a ''surrogate marker to indicate'' that a woman is pregnant.  Because hCG can also be produced by a [[neoplasm|tumor]], the specificity of modern pregnancy tests cannot be 100% (in that false positives are possible).  Also, because hCG is present in the urine in such small concentrations after fertilization and early [[embryogenesis]], the sensitivity of modern pregnancy tests cannot be 100% (because false negatives are possible).

===Likelihood ratios===
{{main article|Likelihood ratios in diagnostic testing}}
{{empty section|date=July 2014}}

==Positive and negative predictive values==
{{main article|Positive and negative predictive values}}
In addition to sensitivity and specificity, the performance of a binary classification test can be measured with [[positive predictive value]] (PPV), also known as [[Precision (information retrieval)|precision]], and [[negative predictive value]] (NPV). The positive prediction value answers the question &quot;If the test result is ''positive'', how well does that ''predict'' an actual presence of disease?&quot;. It is calculated as TP/(TP + FP); that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally.

=== Impact of prevalence on prediction values ===
Prevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested and the prevalence (in the sample) is 50%, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result.

However, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease – that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result.

===Likelihood ratios===
{{empty section|date=July 2014}}

==Precision and recall==
{{main article|Precision and recall}}
{{empty section|date=July 2014}}

===Relationships===
There are various relationships between these ratios.

If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity:

::&lt;math&gt; \text{PPV} = \frac{(\text{sensitivity}) (\text{prevalence})}{(\text{sensitivity}) (\text{prevalence}) + (1 - \text{specificity}) (1-\text{prevalence})} &lt;/math&gt;

If the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity:

::&lt;math&gt;
\text{NPV} = \frac{(\text{specificity}) (1 - \text{prevalence})}{(\text{specificity}) (1 - \text{prevalence}) + (1 - \text{sensitivity}) (\text{prevalence})}. &lt;/math&gt;

==Single metrics==
In addition to the paired metrics, there are also single metrics that give a single number to evaluate the test.

Perhaps the simplest statistic is [[Accuracy and precision#In binary classification|accuracy]] or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/Total Population = (TP + TN)/(TP + TN + FP + FN). This is often not very useful, compared to the marginal ratios, as it does not yield useful marginal interpretations, due to mixing true positives (test positive, condition positive) and true negatives (test negative, condition negative) – in terms of the condition table, it sums the diagonal; further, it is prevalence-dependent. The complement is the Fraction Incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) – this is the sum of the [[antidiagonal]], divided by the total population.

The [[diagnostic odds ratio]] (DOR) is a more useful overall metric, which can be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of True Rates or Prediction Values). This has a useful interpretation – as an [[odds ratio]] – and is prevalence-independent.

An [[F-score]] is a combination of the [[Precision (information retrieval)|precision]] and the [[Recall (information retrieval)|recall]], providing a single score. There is a one-parameter family of statistics, with parameter ''β,'' which determines the relative weights of precision and recall. The traditional or balanced F-score ([[F1 score]]) is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of precision and recall:

:&lt;math&gt;F_1 = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}} &lt;/math&gt;.

===Alternative metrics===
Note, however, that the F-scores do not take the true negative rate into account, and that measures such as the Phi coefficient, [[Matthews correlation coefficient]], Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.&lt;ref name=&quot;Powers2011&quot;&gt;{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=https://dl.dropboxusercontent.com/u/27743223/201101-Evaluation_JMLT_Postprint-Colour.pdf}}&lt;/ref&gt; As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[markedness]] (deltap) and informedness (deltap').&lt;ref name=&quot;Perruchet2004&quot;&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119}}&lt;/ref&gt;

Other metrics include [[Youden's J statistic]].

==See also==
* [[Population Impact Measures]]
* [[Attributable risk]]
* [[Attributable risk percent]]

==References==
{{Reflist}}


</text>
      <sha1>sk6smsl6bsv75z8h64pv6osk65uo27x</sha1>
    </revision>
  </page>
  <page>
    <title>Vanishing gradient problem</title>
    <ns>0</ns>
    <id>43502368</id>
    <revision>
      <id>815472815</id>
      <parentid>815045207</parentid>
      <timestamp>2017-12-15T01:13:53Z</timestamp>
      <contributor>
        <ip>128.115.190.34</ip>
      </contributor>
      <comment>Derivative of hyperbolic tangent has range (0,1), not (-1,1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11821">{{machine learning bar}}
[[machine learning|In machine learning]], the '''vanishing gradient problem''' is a difficulty found in training [[artificial neural network]]s with [[Stochastic gradient descent|gradient-based learning methods]] and [[backpropagation]]. In such methods, each of the neural network's weights receives an update proportional to the [[gradient]] of the [[error function]] with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional [[activation function]]s such as the [[hyperbolic tangent]] function have gradients in the range {{math|(0, 1)}}, and backpropagation computes gradients by the [[chain rule]]. This has the effect of multiplying {{mvar|n}} of these small numbers to compute gradients of the &quot;front&quot; layers in an {{mvar|n}}-layer network, meaning that the gradient (error signal) decreases exponentially with {{mvar|n}} while the front layers train very slowly.

Back-propagation allowed researchers to train [[Supervised learning|supervised]] deep artificial neural networks from scratch, initially with little success. [[Sepp Hochreiter|Hochreiter]]'s diploma thesis of 1991&lt;ref&gt;[[Sepp Hochreiter|S. Hochreiter]]. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f. Informatik, Technische Univ. Munich, 1991. Advisor: [[Jürgen Schmidhuber|J. Schmidhuber]].&lt;/ref&gt;&lt;ref&gt;[[Sepp Hochreiter|S. Hochreiter]], Y. Bengio, P. Frasconi, and [[Jürgen Schmidhuber|J. Schmidhuber]]. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.&lt;/ref&gt; formally identified the reason for this failure in the &quot;vanishing gradient problem&quot;, which not only affects [[Deep learning|many-layered]] [[Feedforward neural network|feedforward networks]] &lt;ref&gt;{{Cite journal|last=Goh|first=Garrett B.|last2=Hodas|first2=Nathan O.|last3=Vishnu|first3=Abhinav|date=2017-06-15|title=Deep learning for computational chemistry|url=http://onlinelibrary.wiley.com/doi/10.1002/jcc.24764/abstract|journal=Journal of Computational Chemistry|language=en|volume=38|issue=16|pages=1291–1307|doi=10.1002/jcc.24764|issn=1096-987X}}&lt;/ref&gt;, but also [[recurrent neural network|recurrent network]]s&lt;ref&gt;{{Cite journal|last=Pascanu|first=Razvan|last2=Mikolov|first2=Tomas|last3=Bengio|first3=Yoshua|date=2012-11-21|title=On the difficulty of training Recurrent Neural Networks|url=http://arxiv.org/abs/1211.5063|journal=arXiv:1211.5063 [cs]}}&lt;/ref&gt;. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network.

When activation functions are used whose derivatives can take on larger values, one risks encountering the related [[exploding gradient problem]].
{{toclimit|3}}

== Solutions ==
{{refimprove science | section | date= December 2017}}
{{unreliable sources | section | date= December 2017}}
=== Multi-level hierarchy ===
To overcome this problem, several methods were proposed. One is [[Jürgen Schmidhuber]]'s  multi-level hierarchy of networks (1992) pre-trained one level at a time through [[unsupervised learning]], fine-tuned through [[backpropagation]].&lt;ref name=&quot;SCHMID1992&quot;&gt;J. Schmidhuber., &quot;Learning complex, extended sequences using the principle of history compression,&quot; ''Neural Computation'', 4, pp. 234–242, 1992.&lt;/ref&gt; Here each level learns a compressed representation of the observations that is fed to the next level.

==== Related approach ====
Similar ideas have been used in feed-forward neural network for unsupervised pre-training to structure a neural network, making it first learn generally useful [[feature detection (nervous system)|feature detectors]]. Then the network is trained further by supervised [[back-propagation]] to classify labeled data. The [[Deep belief network]] model by Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued  [[latent variable]]s. It uses a [[restricted Boltzmann machine]] to model each new layer of higher level features. Each new layer guarantees an increase on the [[Lower bound|lower-bound]] of the [[log likelihood]] of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a [[generative model]] by reproducing the data when sampling down the model (an &quot;ancestral pass&quot;) from the top level feature activations.&lt;ref name=&quot;hinton2006&quot;&gt;{{cite journal|last2=Osindero|first2=S.|last3=Teh|first3=Y.|year=2006|title=A fast learning algorithm for deep belief nets|url=http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf|journal=[[Neural Computation (journal)|Neural Computation]]|volume=18|issue=7|pages=1527–1554|doi=10.1162/neco.2006.18.7.1527|pmid=16764513|last1=Hinton|first1=G. E.|authorlink1=Geoffrey Hinton}}&lt;/ref&gt;
Hinton reports that his models are effective feature extractors over high-dimensional, structured data.&lt;ref&gt;{{Cite journal|year=2009|title=Deep belief networks|url=http://www.scholarpedia.org/article/Deep_belief_networks|journal=Scholarpedia|volume=4|issue=5|pages=5947|doi=10.4249/scholarpedia.5947|pmc=|pmid=|last1=Hinton|first1=G.}}&lt;/ref&gt; This work plays a key role in reintroducing the interests in deep neural network research and consequently leads to the developments of [[Deep learning]], although deep belief network is no longer the main deep learning technique.

=== Long short-term memory ===
{{Main|Long short-term memory}}

Another method particularly used for [[Recurrent neural network]] is the [[long short-term memory]] (LSTM) network of 1997 by [[Sepp Hochreiter|Hochreiter]] &amp; [[Jürgen Schmidhuber|Schmidhuber]].&lt;ref name=lstm&gt;{{cite journal | last1 = Hochreiter | first1 = Sepp | authorlink = Sepp Hochreiter | authorlink2 = Jürgen Schmidhuber | last2 = Schmidhuber | first2 = Jürgen | year = 1997 | title = Long Short-Term Memory | url = | journal = Neural Computation | volume = 9 | issue = 8| pages = 1735–1780 | doi=10.1162/neco.1997.9.8.1735 | pmid=9377276}}&lt;/ref&gt; In 2009, deep multidimensional LSTM networks demonstrated the power of deep learning with many nonlinear layers, by winning three ICDAR 2009 competitions in connected [[handwriting recognition]], without any prior knowledge about the three different languages to be learned.&lt;ref&gt;Graves, Alex; and Schmidhuber, Jürgen; ''Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks'', in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), ''Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC'', Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Graves | first1 = A. | last2 = Liwicki | first2 = M. | last3 = Fernandez | first3 = S. | last4 = Bertolami | first4 = R. | last5 = Bunke | first5 = H. | last6 = Schmidhuber | first6 = J. | title = A Novel Connectionist System for Improved Unconstrained Handwriting Recognition | url = | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | volume = 31 | issue = 5| year = 2009 }}&lt;/ref&gt;

===Faster hardware===
Hardware advances have meant that from 1991 to 2015, computer power (especially as delivered by [[General-purpose computing on graphics processing units|GPUs]]) has increased around a million-fold, making standard backpropagation feasible for networks several layers deeper than when the vanishing gradient problem was recognized. Schmidhuber notes that this &quot;is basically what is winning many of the image recognition competitions now&quot;, but that it &quot;does not really
overcome the problem in a fundamental way&quot;.&lt;ref&gt;{{cite journal |last=Schmidhuber |first=Jürgen |title=Deep learning in neural networks: An overview |journal=Neural Networks |volume=61 |year=2015 |pages=85–117 |arxiv=1404.7828 |doi=10.1016/j.neunet.2014.09.003}}&lt;/ref&gt;

=== Residual networks ===
One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks (ResNets&lt;ref&gt;{{cite web|url=https://blog.init.ai/residual-neural-networks-are-an-exciting-area-of-deep-learning-research-acf14f4912e9|title=Residual neural networks are an exciting area of deep learning research|date=28 April 2016|publisher=}}&lt;/ref&gt; - not to be confused with recurrent neural networks&lt;ref&gt;http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf&lt;/ref&gt;). It was noted prior to ResNets that a deeper network would actually have higher ''training'' error than the shallow network. This intuitively can be understood as data disappearing through too many layers of the network, meaning output from a shallow layer was diminished through the greater number of layers in the deeper network, yielding a worse result. Going with this intuitive hypothesis, Microsoft research found that splitting a deep network into e.g., three layer chunks and passing the input into each chunk straight through to the next chunk (along with the residual—output of the chunk minus the input to the chunk that is reintroduced) helped eliminate much of this disappearing signal problem. No extra parameters or changes to the learning algorithm were needed. ResNets&lt;ref&gt;{{cite web|url=https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32|title=ResNets, HighwayNets, and DenseNets, Oh My! – Chatbot’s Life|date=14 October 2016|publisher=}}&lt;/ref&gt; yielded lower training error (and test error) than their shallower counterparts simply by reintroducing outputs from shallower layers in the network to compensate for the vanishing data.&lt;ref&gt;{{cite web|title=Deep Residual Learning for Image Recognition|url=https://arxiv.org/pdf/1512.03385.pdf|accessdate=13 February 2017}}&lt;/ref&gt;

Note that ResNets are an ensemble of relatively shallow Nets and do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network – rather, they avoid the problem simply by constructing ensembles of many short networks together. (Ensemble by Construction&lt;ref&gt;{{Cite journal|last=Veit|first=Andreas|last2=Wilber|first2=Michael|last3=Belongie|first3=Serge|date=2016-05-20|title=Residual Networks Behave Like Ensembles of Relatively Shallow Networks|url=http://arxiv.org/abs/1605.06431|journal=arXiv:1605.06431 [cs]}}&lt;/ref&gt;)
=== Other ===
Behnke relied only on the sign of the gradient ([[Rprop]]) when training his [[Neural Abstraction Pyramid]]&lt;ref&gt;{{cite book|url=http://www.ais.uni-bonn.de/books/LNCS2766.pdf|title=Hierarchical Neural Networks for Image Interpretation.|publisher=Springer|year=2003|series=Lecture Notes in Computer Science|volume=2766|author=Sven Behnke}}&lt;/ref&gt; to solve problems like image reconstruction and face localization.{{Citation needed|date=June 2017}}

Neural networks can also be optimized by using a universal search algorithm on the space of neural network's weights, e.g.,  [[random guess]] or more systematically [[genetic algorithm]]. This approach is not based on gradient and avoids the vanishing gradient problem.&lt;ref&gt;{{Cite web|url=http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html|title=Sepp Hochreiter's Fundamental Deep Learning Problem (1991)|website=people.idsia.ch|access-date=2017-01-07}}&lt;/ref&gt;

== See also ==
* [[Spectral radius]]

== References ==
{{reflist|30em}}


</text>
      <sha1>bq4fxodmpk2pxotwwcpny4pxpgbsatd</sha1>
    </revision>
  </page>
  <page>
    <title>Query-level feature</title>
    <ns>0</ns>
    <id>41929726</id>
    <revision>
      <id>814119833</id>
      <parentid>802151967</parentid>
      <timestamp>2017-12-07T00:13:27Z</timestamp>
      <contributor>
        <ip>129.59.122.20</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="435">A '''query-level feature''' or '''QLF''' is a ranking feature utilized in a [[machine-learned ranking]] algorithm.

Example QLFs:
* How many times has this query been run in the last month?
* How many words are in the query?
* What is the sum/average/min/max/median of the [[Probabilistic relevance model (BM25)|BM25F]] values for the query?




{{compu-ai-stub}}


</text>
      <sha1>qpk7bsb0hjh61sx56psb5qv96ebp9bb</sha1>
    </revision>
  </page>
  <page>
    <title>Random projection</title>
    <ns>0</ns>
    <id>43932548</id>
    <revision>
      <id>802414295</id>
      <parentid>784482747</parentid>
      <timestamp>2017-09-26T01:06:16Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>clean up spacing around punctuation, replaced: ,I → , I, ,j → , j using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10061">{{multiple issues|
{{refimprove|date=November 2014}}
{{expert needed|date=November 2014}}
}}

In mathematics and statistics, '''random projection''' is a technique used to [[dimensionality reduction|reduce the dimensionality]] of a set of points which lie in [[Euclidean space]]. Random projection methods are powerful methods known for their simplicity and less erroneous output compared with other methods{{Citation needed|reason=What other methods? Says who?|date=June 2017}}. According to experimental results, random projection preserve distances well, but empirical results are sparse.&lt;ref&gt;{{cite conference
 | first = Bingham | last = Ella
 | first2 = Mannila | last2 = Heikki
 | title = Random projection in dimensionality reduction: Applications to image and text data
 | book-title = KDD-2001: Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
 | pages = 245–250
 | publisher = Association for Computing Machinery | date = 2001 | location = New York
 | url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.5135&amp;rep=rep1&amp;type=pdf
 | doi = 10.1145/502512.502546 | access-date = 22 Dec 2015}}&lt;/ref&gt;
They have been applied to many natural language tasks under the name of [[random indexing]].

==Dimensionality reduction==
{{main|Dimensionality reduction}}
Dimensionality reduction, as the name suggests, is reducing the number of random variables using various mathematical methods from statistics and machine learning. Dimensionality reduction is often used to reduce the problem of managing and manipulating large data sets. Dimensionality reduction techniques generally use linear transformations in determining the intrinsic dimensionality of the manifold as well as extracting its principal directions. For this purpose there are various related techniques, including: [[principal component analysis]], [[linear discriminant analysis]], [[canonical correlation analysis]], [[discrete cosine transform]], random projection, etc.

Random projection is a simple and computationally efficient way to reduce the dimensionality of data by trading a controlled amount of error for faster processing times and smaller model sizes. The dimensions and distribution of random projection matrices are controlled so as to approximately preserve the pairwise distances between any two samples of the dataset.

==Method==

The core idea behind random projection is given in the [[Johnson-Lindenstrauss lemma]],&lt;ref&gt;{{cite book
 | last1 = Johnson | first1 = William B. | author1-link = William B. Johnson (mathematician)| last2 = Lindenstrauss | first2 = Joram | author2-link = Joram Lindenstrauss
 | contribution = Extensions of Lipschitz mappings into a Hilbert space
 | doi = 10.1090/conm/026/737400
 | location = Providence, RI
 | mr = 737400
 | pages = 189–206
 | publisher = American Mathematical Society
 | series = Contemporary Mathematics
 | title = Conference in Modern Analysis and Probability (New Haven, Conn., 1982)
 | volume = 26
 | year = 1984}}.
&lt;/ref&gt; which states that if points in a vector space are of sufficiently high dimension, then they may be projected into a suitable lower-dimensional space in a way which approximately preserves the distances between the points.

In random projection, the original d-dimensional data is projected to a k-dimensional (k &lt;&lt; d) subspace, using a random &lt;math&gt;k \times d &lt;/math&gt; - dimensional matrix R whose rows have unit lengths. Using matrix notation: If &lt;math&gt;X_{d \times N}&lt;/math&gt; is the original set of N d-dimensional observations, then &lt;math&gt;X_{k \times N}^{RP}=R_{k \times d}X_{d \times N}&lt;/math&gt; is the projection of the data onto a lower k-dimensional subspace. Random projection is computationally simple: form the random matrix &quot;R&quot; and project the &lt;math&gt;d \times N&lt;/math&gt; data matrix X onto K dimensions of order &lt;math&gt;O(dkN)&lt;/math&gt;. If the data matrix X is sparse with about c nonzero entries per column, then the complexity of this operation is of order &lt;math&gt;O(ckN)&lt;/math&gt;.&lt;ref&gt;{{Cite web|url =http://www.ime.unicamp.br/~wanderson/Artigos/randon_projection_kdd.pdf |title =Random projection in dimensionality reduction: Applications to image and text data |date = May 6, 2014|accessdate = |website = |publisher = | last1 = Bingham | first1 = Ella | last2 = Mannila | first2 = Heikki  }}&lt;/ref&gt;

===Gaussian random projection===

The random matrix R can be generated using a Gaussian distribution.&lt;ref&gt;{{Cite journal | doi = 10.1137/060673096| title = The Fast Johnson–Lindenstrauss Transform and Approximate Nearest Neighbors| journal = SIAM Journal on Computing| volume = 39| pages = 302| year = 2009| last1 = Ailon | first1 = N. | last2 = Chazelle | first2 = B. }}&lt;/ref&gt; The first row is a random unit vector uniformly chosen from &lt;math&gt;S^{d-1}&lt;/math&gt;. The second row is a random unit vector from the space orthogonal to the first row, the third row is a random unit vector from the space orthogonal to the first two rows, and so on. In this way of choosing R, the following properties are satisfied:
* Spherical symmetry: For any orthogonal matrix &lt;math&gt;A \in O(d)&lt;/math&gt;, RA and R have the same distribution.
* Orthogonality: The rows of R are orthogonal to each other.
* Normality: The rows of R are unit-length vectors.

===More computationally efficient random projections===

Achlioptas&lt;ref&gt;{{cite book|doi=10.1145/375551.375608|chapter=Database-friendly random projections|title=Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems  - PODS '01|pages=274|year=2001|last1=Achlioptas|first1=Dimitris|isbn=1581133618}}&lt;/ref&gt; has shown that the Gaussian distribution can be replaced by a much simpler distribution such as
:&lt;math&gt;R_{i,j} = \sqrt{3} \begin{cases}
+1 &amp; \text{with probability }\frac{1}{6}\\
0 &amp; \text{with probability }\frac{2}{3}\\
-1 &amp; \text{with probability }\frac{1}{6} \end{cases} &lt;/math&gt;
This is efficient for database applications because the computations can be performed using integer arithmetic.

It was later shown how to use integer arithmetic while making the distribution even sparser, having very few nonzeroes per column, in work on the Sparse JL Transform.&lt;ref&gt;{{cite journal
 | first1 = Daniel M. | last1 = Kane | last2 = Nelson | first2 = Jelani
 | doi = 10.1145/2559902
 | issue = 1
 | journal = [[Journal of the ACM]]
 | title = Sparser Johnson-Lindenstrauss Transforms
 | volume = 61
 | year = 2014
 | mr = 3167920}}
&lt;/ref&gt; This is advantageous since a sparse embedding matrix means being able to project the data to lower dimension even faster.

==Large quasiorthogonal [[Basis (linear algebra)|bases]]==
The Johnson-Lindenstrauss lemma states that large sets of vectors in a high-dimensional space can be linearly mapped in a space of much lower (but still high) dimension ''n''  with approximate preservation of distances.  One of the explanations of this effect is the exponentially high quasiorthogonal dimension of ''n''-dimensional [[Euclidean space]].&lt;ref&gt;P.C. Kainen, V. Kurkova, Quasiorthogonal dimension of Euclidian spaces, Appl. Math. Lett. 6(3) (1993) 7–10.&lt;/ref&gt; There are exponentially large (in dimension ''n'')    sets of almost [[Orthogonality|orthogonal]] vectors (with small value of [[Inner product space|inner products]]) in ''n''–dimensional Euclidean space. This observation is useful in [[Database index|indexing]] of high-dimensional data.&lt;ref&gt;R. Hecht-Nielsen, Context vectors: General-purpose approximate meaning representations self-organized from raw data,  in: J. Zurada,  R. Marks, C. Robinson  (Eds.), computational Intelligence: Imitating Life, IEEE Press, 1994, pp. 43–56.&lt;/ref&gt;

Quasiorthogonality of large random sets is important for methods of random approximation in [[machine learning]]. In high dimensions, exponentially large numbers of randomly and independently chosen vectors from equidistribution on a sphere (and from many other distributions) are almost orthogonal with probability close to one.&lt;ref name = &quot;GorbanTyukin2016&quot;&gt;A.N. Gorban, I.Yu. Tyukin, D.V. Prokhorov, K.I. Sofeikov, [https://www.researchgate.net/publication/278413384_Approximation_with_Random_Bases_Pro_et_Contra Approximation with Random Bases: Pro et Contra], Information Sciences, 364–365 (2016), 129-145.&lt;/ref&gt;  This implies that in order to represent an element of such a high-dimensional space by linear combinations of randomly and independently chosen vectors, it may often be necessary to generate samples of exponentially large length if we use bounded coefficients in linear combinations. On the other hand, if coefficients with arbitrarily large values are allowed, the number of randomly generated elements that are sufficient for approximation is even less than dimension of the data space.

==Applications==
Random projections have been applied for human activity recognition to reduce feature dimensionality of sensor data.&lt;ref name = &quot;Damasevicius2016&quot;&gt;R. Damaševičius, M. Vasiljevas, J. Šalkevičius, and M. Woźniak, [https://www.researchgate.net/publication/304187762_Human_Activity_Recognition_in_AAL_Environments_Using_Random_Projections Human Activity Recognition in AAL Environments Using Random Projections], Computational and Mathematical Methods in Medicine, vol. 2016, Article ID 4073584, 17 pages, 2016. doi:10.1155/2016/4073584&lt;/ref&gt;

==See also==
* [[Locality-sensitive hashing]]

==References==
{{Reflist}}
* Fodor, I. (2002) [http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.8.5098 &quot;A survey of dimension reduction techniques&quot;].  Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report UCRL-ID-148494
* ADITYA KRISHNA MENON (2007) [http://cseweb.ucsd.edu/~akmenon/HonoursThesis.pdf &quot;Random projections and applications to dimensionality reduction&quot;].  School of Information Technologies, The University of Sydney, Australia
* ADITYA Ramdas [http://www.cs.cmu.edu/~aramdas/reports/MLTreport.pdf &quot;A Random Introduction To Random Projections&quot;].  Carnegie Mellon University


</text>
      <sha1>qprxah9daqbf23itus2lwgi42o0f42j</sha1>
    </revision>
  </page>
  <page>
    <title>Action model learning</title>
    <ns>0</ns>
    <id>43808044</id>
    <revision>
      <id>797622145</id>
      <parentid>727678106</parentid>
      <timestamp>2017-08-28T08:29:28Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Bots/Requests for approval/KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7285">{{Machine learning bar}}

'''Action model learning''' (sometimes abbreviated '''action learning''') is an area of [[machine learning]] concerned with creation and modification of [[software agent]]'s knowledge about ''effects'' and ''preconditions'' of the ''actions'' that can be executed within its ''environment''. This knowledge is usually represented in logic-based [[action language|action description language]] and used as the input for [[automated planning|automated planners]].

Learning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from [[reinforcement learning]]. It enables reasoning about actions instead of expensive trials in the world.&lt;ref name=&quot;amir2008&quot;&gt;
{{cite journal
  | last = Amir | first = Eyal
  | last2 = Chang | first2 = Allen
  | title = Learning Partially Observable Deterministic Action Models
  | journal = Journal of Artificial Intelligence Research (JAIR)
  | volume = 33
  | pages = 349&amp;ndash;402
  | year = 2008
  | url = http://dl.acm.org/citation.cfm?id=1622708
}}
&lt;/ref&gt; Action model learning is a form of [[inductive reasoning]], where new knowledge is generated based on agent's ''observations''. It differs from standard [[supervised learning]] in that correct input/output pairs are never presented, nor imprecise action models explicitly corrected.

Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments).

== Action models ==

Given a [[training set]] &lt;math&gt;E&lt;/math&gt; consisting of examples &lt;math&gt;e = (s,a,s')&lt;/math&gt;, where &lt;math&gt;s,s'&lt;/math&gt; are observations of a world state from two consecutive time steps &lt;math&gt;t, t'&lt;/math&gt; and &lt;math&gt;a&lt;/math&gt; is an ''action instance'' observed in time step &lt;math&gt;t&lt;/math&gt;, the goal of action model learning in general is to construct an ''action model'' &lt;math&gt;\langle D,P \rangle&lt;/math&gt;, where &lt;math&gt;D&lt;/math&gt; is a description of domain dynamics in action description formalism like [[STRIPS]], [[Architecture description language|ADL]] or [[PDDL]] and &lt;math&gt;P&lt;/math&gt; is a probability function defined over the elements of &lt;math&gt;D&lt;/math&gt;.
&lt;ref name=&quot;certicky2013&quot;&gt;
{{cite journal
  | doi = 10.1080/08839514.2014.927692
  | last = Čertický | first = Michal
  | title = Real-Time Action Model Learning with Online Algorithm 3SG
  | journal = Applied Artificial Intelligence
  | volume = 28
  | pages = 690&amp;ndash;711
  | publisher = Taylor &amp; Francis
  | year = 2014
  | url = https://dx.doi.org/10.1080/08839514.2014.927692}}
&lt;/ref&gt;
However, many state of the art ''action learning methods'' assume determinism and do not induce &lt;math&gt;P&lt;/math&gt;. In addition to determinism, individual methods differ in how they deal with other attributes of domain (e.g. partial observability or sensoric noise).

== Action learning methods ==

=== State of the art ===
Recent action learning methods take various approaches and employ a wide variety of tools from different areas of [[artificial intelligence]] and [[computational logic]]. As an example of a method based on propositional logic, we can mention SLAF (Simultaneous Learning and Filtering) algorithm,&lt;ref name=&quot;amir2008&quot;/&gt; which uses agent's observations to construct a long propositional formula over time and subsequently interprets it using a [[SAT solver|satisfiability (SAT) solver]]. Another technique, in which learning is converted into a satisfiability problem (weighted [[MAX-SAT]] in this case) and SAT solvers are used, is implemented in ARMS (Action-Relation Modeling System).&lt;ref name=&quot;yang2007&quot;&gt;
{{cite journal
|last1=Yang
|first1=Qiang
|last2=Kangheng
|first2=Wu
|last3=Yunfei
|first3=Jiang
|title=Learning action models from plan examples using weighted MAX-SAT
|journal=Artificial Intelligence
|date=2007
|volume=171
|pages=107–143
|url=http://www.sciencedirect.com/science/article/pii/S0004370206001408
|doi=10.1016/j.artint.2006.11.005}}&lt;/ref&gt;
Two mutually similar, fully declarative approaches to action learning were based on logic programming paradigm [[Answer Set Programming]] (ASP)&lt;ref&gt;{{cite journal|last1=Balduccini|first1=Marcelo|title=Learning Action Descriptions with A-Prolog: Action Language C|journal=AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning|date=2007|pages=13–18|url=http://www.aaai.org/Library/Symposia/Spring/2007/ss07-05-004.php}}&lt;/ref&gt; and its extension, Reactive ASP.&lt;ref&gt;{{cite journal|last1=Čertický|first1=Michal|title=Action Learning with Reactive Answer Set Programming: Preliminary Report|journal=ICAS 2012, The Eighth International Conference on Autonomic and Autonomous Systems|date=2012|pages=107–111|url=http://www.thinkmind.org/index.php?view=article&amp;articleid=icas_2012_5_20_20056}}&lt;/ref&gt; In another example, bottom-up [[inductive logic programming]] approach was employed.&lt;ref&gt;{{cite journal|last1=Benson|first1=Scott|title=Inductive learning of reactive action models|journal=Machine Learning: Proceedings of the Twelfth International Conference (ICML)|date=1995}}&lt;/ref&gt; Several different solutions are not directly logic-based. For example, the action model learning using a [[perceptron algorithm]] &lt;ref&gt;{{cite journal|last1=Mourao|first1=Kira|last2=Petrick|first2=Ronald|last3=Steedman|first3=Mark|title=Learning action effects in partially observable domains|journal=European Conference on Artificial Intelligence (ECAI)|date=2010|volume=215|pages=973–974|doi=10.3233/978-1-60750-606-5-973|url=http://www.ebooks.iospress.nl/volumearticle/5920}}&lt;/ref&gt; or the multi level [[greedy search]] over the space of
possible action models.&lt;ref&gt;{{cite journal|last1=Zettlemoyer|first1=Luke|last2=Pasula|first2=Hanna|last3=Kaelblin|first3=Leslie Pack|title=Learning planning rules in noisy stochastic worlds|journal=AAAI|date=2005|pages=911–918|url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.65.3417}}&lt;/ref&gt; In the older paper from 1992,&lt;ref&gt;{{cite journal|last1=Lin|first1=Long-Ji|title=Self-improving reactive agents based on reinforcement learning, planning and teaching|journal=Machine Learning|date=1992|volume=8|issue=3-4|pages=293–321|doi=10.1023/A:1022628806385|url=https://link.springer.com/article/10.1023/A:1022628806385}}&lt;/ref&gt; the action model learning was studied as an extension of [[reinforcement learning]].

=== Literature ===
Most action learning research papers are published in journals and conferences focused on [[artificial intelligence]] in general (e.g. Journal of Artificial Intelligence Research (JAIR), Artificial Intelligence, Applied Artificial Intelligence (AAI) or AAAI conferences). Despite mutual relevance of the topics, action model learning is usually not addressed on [[Automated planning and scheduling|planning]] conferences like ICAPS.

==See also==
* [[Machine learning]]
* [[Automated planning and scheduling]]
* [[Action language]]
* [[STRIPS]]
* [[PDDL]]
* [[Architecture description language]]
* [[Inductive reasoning]]
* [[Computational logic]]
* [[Knowledge representation]]

== References ==
{{reflist}}



</text>
      <sha1>shb36e6c728pkhjw0iyyktw8h20b9ag</sha1>
    </revision>
  </page>
  <page>
    <title>Bradley–Terry model</title>
    <ns>0</ns>
    <id>44439173</id>
    <revision>
      <id>811083026</id>
      <parentid>753816683</parentid>
      <timestamp>2017-11-19T12:35:11Z</timestamp>
      <contributor>
        <username>Michael Fourman</username>
        <id>1386530</id>
      </contributor>
      <minor/>
      <comment>/* Estimating the parameters */ limits on sum</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7303">The '''Bradley–Terry model''' is a [[probability theory|probability model]] that can predict the outcome of a comparison. Given a pair of individuals {{mvar|i}} and {{mvar|j}} drawn from some [[Population (statistics)|population]], it estimates the probability that the [[pairwise comparison]] {{math|''i'' &gt; ''j''}} turns out true, as

:&lt;math&gt;P(i &gt; j) = \frac{p_i}{p_i + p_j}&lt;/math&gt;

where {{mvar|p&lt;sub&gt;i&lt;/sub&gt;}} is a positive [[real number|real-valued]] score assigned to individual {{mvar|i}}. The comparison {{math|''i'' &gt; ''j''}} can be read as &quot;{{mvar|i}} is preferred to {{mvar|j}}&quot;, &quot;{{mvar|i}} ranks higher than {{mvar|j}}&quot;, or &quot;{{mvar|i}} beats {{mvar|j}}&quot;, depending on the application.

For example, {{mvar|p&lt;sub&gt;i&lt;/sub&gt;}} may represent the skill of a team in a sports tournament, estimated from the number of times {{mvar|i}} has won a match. &lt;math&gt;P(i&gt;j)&lt;/math&gt; then represents the probability that {{mvar|i}} will win a match against {{mvar|j}}.&lt;ref name=&quot;hunter&quot; /&gt;&lt;ref name=&quot;agresti&quot; /&gt; Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.&lt;ref name=&quot;agresti&quot; /&gt;

== History and applications ==
The model is named after R. A. Bradley and M. E. Terry,&lt;ref&gt;{{cite encyclopedia |author=E.E.M. van Berkum |title=Bradley-Terry model |encyclopedia=Encyclopedia of Mathematics |url=http://www.encyclopediaofmath.org/index.php?title=Bradley-Terry_model&amp;oldid=22181 |accessdate=18 November 2014}}&lt;/ref&gt; who presented it in 1952,&lt;ref&gt;{{Cite journal | doi = 10.2307/2334029| jstor = 2334029| title = Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons| journal = Biometrika| volume = 39| issue = 3/4| pages = 324| year = 1952| last1 = Bradley | first1 = Ralph Allan | last2 = Terry | first2 = Milton E. }}&lt;/ref&gt; although it had already been studied by [[Ernst Zermelo|Zermelo]] in the 1920s.&lt;ref name=&quot;hunter&quot;&gt;{{Cite journal| first = David R. | last = Hunter| title = MM algorithms for generalized Bradley–Terry models| journal = The Annals of Statistics| volume = 32 | issue = 1| year = 2004| pages = 384–406| doi = 10.2307/3448514| jstor = 3448514| url = http://projecteuclid.org/euclid.aos/1079120141}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |last=Zermelo |first=Ernst |title=Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung |journal=[[Mathematische Zeitschrift]] |volume=29 |number=1 |year=1929 |pages=436–460|doi=10.1007/BF01180541}}&lt;/ref&gt;&lt;ref&gt;{{citation |title=Ernst Zermelo: An Approach to His Life and Work |author=Heinz-Dieter Ebbinghaus |year=2007 |isbn=9783540495536 |pages=268–269}}&lt;/ref&gt;

Real-world applications of the model include estimation of the influence of [[Statistics|statistical]] [[scientific journal|journals]], or ranking documents by relevance in [[Learning to rank|machine-learned]] [[search engine]]s.&lt;ref&gt;{{cite conference |last1=Szummer |first1=Martin |first2=Emine |last2=Yilmaz |title=Semi-supervised learning to rank with preference regularization |conference=CIKM |year=2011 |url=http://research.microsoft.com/pubs/154323/SzummerYilmaz-semisupervised-ranking-cikm11.pdf}}&lt;/ref&gt;
In the latter application, &lt;math&gt;P(i &gt; j)&lt;/math&gt; may reflect that document {{mvar|i}} is more relevant to the user's [[Web search query|query]] than document {{mvar|j}}, so it should be displayed earlier in the results list. The individual {{mvar|p&lt;sub&gt;i&lt;/sub&gt;}} then express the relevance of the document, and can be estimated from the frequency with which users click particular &quot;hits&quot; when presented with a result list.&lt;ref&gt;{{cite conference |first1=Filip |last1=Radlinski |first2=Thorsten |last2=Joachims |title=Active Exploration for Learning Rankings from Clickthrough Data |conference=KDD '07 Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining |year=2007 |url=http://www.cs.cornell.edu/People/tJ/publications/radlinski_joachims_07a.pdf|doi=10.1145/1281192.1281254|pages=570–579 }}&lt;/ref&gt;

== Definition ==
The Bradley–Terry model can be parametrized in various ways. One way to do so is to pick a single parameter per observation, leading to a model of {{mvar|n}} parameters {{math|''p''&lt;sub&gt;1&lt;/sub&gt;, ..., ''p&lt;sub&gt;n&lt;/sub&gt;''}}.&lt;ref name=&quot;wu&quot;&gt;{{cite conference |title=Ranking Optimization with Constraints |conference=CIKM '14 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management |author1=Fangzhao Wu |author2=Jun Xu |author3=Hang Li |author4=Xin Jiang |year=2014|doi=10.1145/2661829.2661895|pages=1049–1058 }}&lt;/ref&gt;
Another variant, in fact the version considered by Bradley and Terry,&lt;ref name=&quot;agresti&quot;&gt;{{cite book |last=Agresti |first=Alan |title=Categorical Data Analysis |publisher=John Wiley &amp; Sons |year=2014 |pages=436–439}}&lt;/ref&gt; uses exponential score functions &lt;math&gt;p_i = e^{\beta_i}&lt;/math&gt; so that

:&lt;math&gt;P(i &gt; j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}&lt;/math&gt;

or, using the [[logit]] (and disallowing ties),&lt;ref name=&quot;hunter&quot; /&gt;

:&lt;math&gt;\operatorname{logit}(P(i &gt; j)) = \log\left(\frac{P(i &gt; j)}{1 - P(i &gt; j)}\right) = \log\left(\frac{P(i &gt; j)}{P(j &gt; i)}\right) = \beta_i - \beta_j&lt;/math&gt;

reducing the model to [[logistic regression]] on pairs of individuals.

=== Estimating the parameters ===
The following [[algorithm]] computes the parameters {{mvar|p&lt;sub&gt;i&lt;/sub&gt;}} of the basic version of the model from a sample of observations. Formally, it computes a [[maximum likelihood estimation|maximum likelihood estimate]], i.e., it maximizes the [[likelihood]] of the observed data. The algorithm dates back to the work of Zermelo.&lt;ref name=&quot;hunter&quot; /&gt;

The observations required are the outcomes of previous comparisons, for example, pairs {{math|(''i'', ''j'')}} where {{mvar|i}} beats {{mvar|j}}. Summarizing these outcomes as {{mvar|w&lt;sub&gt;ij&lt;/sub&gt;}}, the number of times {{mvar|i}} has beaten {{mvar|j}}, we obtain the [[log-likelihood]] of the parameter vector {{math|'''p''' {{=}} ''p''&lt;sub&gt;1&lt;/sub&gt;, ..., ''p&lt;sub&gt;n&lt;/sub&gt;''}} as&lt;ref name=&quot;hunter&quot; /&gt;

:&lt;math&gt;L(\mathbf{p}) = \sum_i^n \sum_j^n w_{ij} \ln p_i - w_{ij} \ln(p_i + p_j).&lt;/math&gt;

Denote the number of comparisons &quot;won&quot; by {{mvar|i}} as {{mvar|W&lt;sub&gt;i&lt;/sub&gt;}}, and the number of comparisons made between {{mvar|i}} and {{mvar|j}} as {{mvar|N&lt;sub&gt;ij&lt;/sub&gt;}}. Starting from an arbitrary vector {{math|'''p'''}}, the algorithm iteratively performs the update

:&lt;math&gt;p'_i = W_i \left( \sum_{j \ne i} \frac{N_{ij}}{p_i + p_j} \right)^{-1}&lt;/math&gt;

for all {{mvar|i}}. After computing all of the new parameters, they should be renormalized,

:&lt;math&gt;p_i \leftarrow \frac{p'_i}{\sum_{j=1}^n p'_j}.&lt;/math&gt;

This estimation procedure improves the log-likelihood in every iteration, and eventually converges to a unique maximum.

== See also ==
*[[Ordinal regression]]
*[[Rasch model]]
*[[Scale (social sciences)]]

== References ==
{{reflist|30em}}

{{DEFAULTSORT:Bradley-Terry model}}



</text>
      <sha1>52ug2ky3fns1mstgl149anlxynlkqmg</sha1>
    </revision>
  </page>
  <page>
    <title>Quantum machine learning</title>
    <ns>0</ns>
    <id>44108758</id>
    <revision>
      <id>815068368</id>
      <parentid>804040376</parentid>
      <timestamp>2017-12-12T15:44:52Z</timestamp>
      <contributor>
        <ip>192.16.184.34</ip>
      </contributor>
      <comment>/* Linear algebra simulation with quantum amplitudes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="48640">{{Quantum mechanics}}
'''Quantum machine learning''' is an emerging interdisciplinary research area at the intersection of [[quantum physics]] and [[machine learning]].&lt;ref&gt;{{cite journal |doi=10.1080/00107514.2014.964942 |arxiv=1409.3097 |title=An introduction to quantum machine learning |journal=Contemporary Physics |volume=56 |issue=2 |pages=172 |year=2014 |last1=Schuld |first1=Maria |last2=Sinayskiy |first2=Ilya |last3=Petruccione |first3=Francesco |bibcode=2015ConPh..56..172S}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Wittek |first=Peter |title=Quantum Machine Learning: What Quantum Computing Means to Data Mining |publisher=Academic Press |year=2014 |isbn=978-0-12-800953-6|url=http://www.sciencedirect.com/science/book/9780128009536}}&lt;/ref&gt;&lt;ref&gt;{{cite arxiv |eprint=1512.02900 |first1=Jeremy|last1=Adcock|first2=Euan|last2=Allen|first3=Matthew|last3=Day|first4=Stefan|last4=Frick|first5=Janna|last5=Hinchliff|first6=Mack|last6=Johnson|first7=Sam|last7=Morley-Short|first8=Sam|last8=Pallister|first9=Alasdair|last9=Price|first10=Stasja|last10=Stanisic|title=Advances in quantum machine learning |class=quant-ph |year=2015}}&lt;/ref&gt;&lt;ref&gt;{{cite arxiv |eprint=1611.09347 |first1=Jacob|last1=Biamonte|first2=Peter|last2=Wittek|first3=Nicola|last3=Pancotti|first4=Patrick|last4=Rebentrost|first5=Nathan|last5=Wiebe|first6=Seth|last6=Lloyd|title=Quantum machine learning |class=quant-ph |year=2016}}&lt;/ref&gt; One can distinguish four different ways of merging the two parent disciplines.&lt;ref name=&quot;AimeurEtAl_2006&quot;&gt;{{Cite journal|last=Aïmeur|first=Esma|last2=Brassard|first2=Gilles|last3=Gambs|first3=Sébastien|date=2006-06-07|title=Machine Learning in a Quantum World|url=https://link.springer.com/chapter/10.1007/11766247_37|journal=Advances in Artificial Intelligence|language=en|publisher=Springer, Berlin, Heidelberg|pages=431–442|doi=10.1007/11766247_37}}&lt;/ref&gt;&lt;ref name=&quot;DunjkoTaylorBriegel&quot;&gt;{{Cite journal|last=Dunjko|first=Vedran|last2=Taylor|first2=Jacob M.|last3=Briegel|first3=Hans J.|date=2016-09-20|title=Quantum-Enhanced Machine Learning|url=http://link.aps.org/doi/10.1103/PhysRevLett.117.130501|journal=Physical Review Letters|volume=117|issue=13|pages=130501|doi=10.1103/PhysRevLett.117.130501|bibcode=2016PhRvL.117m0501D|arxiv=1610.08251}}&lt;/ref&gt; Quantum machine learning algorithms can use the advantages of [[Quantum computing|quantum computation]] in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a [[quantum computer]].&lt;ref name=&quot;Nathan Wiebe 2014&quot;&gt;{{Cite journal |arxiv=1401.2142 |last1=Wiebe |first1=Nathan |title=Quantum Algorithms for Nearest-Neighbor Methods for Supervised and  Unsupervised Learning |journal=Quantum Information &amp; Computation ( &amp; 4): (20) |volume=15 |issue=3 |pages=0318–0358 |last2=Kapoor |first2=Ashish |last3=Svore |first3=Krysta |year=2014|bibcode=2014arXiv1401.2142W }}&lt;/ref&gt;&lt;ref&gt;{{cite arxiv |eprint=1307.0411 |last1=Lloyd |first1=Seth |title=Quantum algorithms for supervised and unsupervised machine learning |last2=Mohseni |first2=Masoud |last3=Rebentrost |first3=Patrick |class=quant-ph |year=2013}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal |arxiv=1303.6055 |last1=Yoo |first1=Seokwon |title=A quantum speedup in machine learning: Finding a N-bit Boolean function  for a classification |journal=New Journal of Physics |volume=16 |issue=10 |pages=103014 |last2=Bang |first2=Jeongho |last3=Lee |first3=Changhyoup |last4=Lee |first4=Jinhyoung |year=2013 |doi=10.1088/1367-2630/16/10/103014|bibcode=2014NJPh...16j3014Y }}&lt;/ref&gt; On the other hand, one can apply classical methods of machine learning to analyse quantum systems. Most generally, one can consider situations wherein both the learning device and the system under study are fully quantum.

A related branch of research explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks, which has revealed, for example, that certain mathematical and numerical techniques from quantum physics carry over to classical deep learning.&lt;ref&gt;{{cite arxiv|last=Bény|first=Cédric|date=2013-01-14|title=Deep learning and the renormalization group|eprint=1301.3124 |class=quant-ph}}&lt;/ref&gt;

[[File:Qml approaches.tif |thumb|Four different approaches to combine the disciplines of quantum computing and machine learning.&lt;ref name=&quot;AimeurEtAl_2006&quot;/&gt;&lt;ref name=&quot;DunjkoTaylorBriegel&quot;/&gt; The first letter refers to whether the system under study is classical or quantum, while the second letter defines whether a classical or quantum information processing device is used.]]

==Quantum-enhanced machine learning==

Quantum-enhanced machine learning refers to [[quantum algorithm]]s that solve tasks in machine learning, thereby improving a classical machine learning method. Such algorithms typically require one to encode the given classical dataset into a quantum computer, so as to make it accessible for quantum information processing. After this, quantum information processing routines can be applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit could reveal the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal [[quantum computer]] to be tested, others have been implemented on small-scale or special purpose quantum devices.

===Linear algebra simulation with quantum amplitudes===

One line of approaches is based on the idea of ''amplitude encoding'', that is, to associate the [[Probability amplitude|amplitudes]] of a quantum state with the inputs and outputs of computations.&lt;ref name=&quot;Patrick Rebentrost 2014&quot;&gt;{{cite journal |doi=10.1103/PhysRevLett.113.130503|pmid=25302877|title=Quantum Support Vector Machine for Big Data Classification|journal=Physical Review Letters|volume=113|issue=13|pages=130503|year=2014|last1=Rebentrost|first1=Patrick|last2=Mohseni|first2=Masoud|last3=Lloyd|first3=Seth|bibcode=2014PhRvL.113m0503R|arxiv=1307.0471}}&lt;/ref&gt;&lt;ref name=&quot;Nathan Wiebe 2012&quot;&gt;{{cite journal |doi=10.1103/PhysRevLett.109.050505|pmid=23006156|title=Quantum Algorithm for Data Fitting|journal=Physical Review Letters|volume=109|issue=5|pages=050505|year=2012|last1=Wiebe|first1=Nathan|last2=Braun|first2=Daniel|last3=Lloyd|first3=Seth|bibcode=2012PhRvL.109e0505W|arxiv=1204.5242}}&lt;/ref&gt;&lt;ref name=&quot;Maria Schuld 2016&quot;&gt;{{cite journal |doi=10.1103/PhysRevA.94.022342|title=Prediction by linear regression on a quantum computer|journal=Physical Review A|volume=94|issue=2|pages=022342|year=2016|last1=Schuld|first1=Maria|last2=Sinayskiy|first2=Ilya|last3=Petruccione|first3=Francesco|bibcode=2016PhRvA..94b2342S|arxiv=1601.07823}}&lt;/ref&gt;&lt;ref name=&quot;ReferenceA&quot;&gt;{{cite arxiv |eprint=1512.03929 |last1=Zhao |first1=Zhikuan |title=Quantum assisted Gaussian process regression |last2=Fitzsimons |first2=Jack K. |last3=Fitzsimons |first3=Joseph F. |class=quant-ph |year=2015}}&lt;/ref&gt; Since a state of &lt;math&gt;n&lt;/math&gt; qubits is described by &lt;math&gt;2^n&lt;/math&gt; complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits &lt;math&gt;n&lt;/math&gt;, which amounts to a logarithmic growth in the number of amplitudes and thereby the dimension of the input.

Many quantum machine learning algorithms in this category are based on variations of the [[quantum algorithm for linear systems of equations]]&lt;ref&gt;{{Cite journal |arxiv=0811.3171 |last1=Harrow |first1=Aram W. |title=Quantum algorithm for solving linear systems of equations |journal=Physical Review Letters |volume=103 |issue=15 |pages=150502 |last2=Hassidim |first2=Avinatan |last3=Lloyd |first3=Seth |year=2008 |doi=10.1103/PhysRevLett.103.150502|pmid=19905613 |bibcode=2009PhRvL.103o0502H }}&lt;/ref&gt; which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entrywise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse&lt;ref&gt;{{cite conference |url= |title=Hamiltonian simulation with nearly optimal dependence on all parameters |last1=Berry |first1= Dominic W.|last2=Childs |first2=Andrew M.|last3=Kothari|first3=Robin|date=2015 |publisher=IEEE |pages=792–809 |conference= 56th Annual Symposium on Foundations of Computer Science|doi=10.1109/FOCS.2015.54|arxiv=1501.01715}}&lt;/ref&gt; or low rank.&lt;ref&gt;{{cite journal |doi=10.1038/nphys3029|title=Quantum principal component analysis|journal=Nature Physics|volume=10|issue=9|pages=631|year=2014|last1=Lloyd|first1=Seth|last2=Mohseni|first2=Masoud|last3=Rebentrost|first3=Patrick|bibcode=2014NatPh..10..631L|arxiv=1307.0401}}&lt;/ref&gt; For reference, any known classical algorithm for [[matrix inversion]] requires a number of operations that grows [[Computational complexity of mathematical operations#Matrix algebra|at least quadratically in the dimension of the matrix]].

Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a [[System of linear equations|linear system of equations]], for example in least-squares linear regression,&lt;ref name=&quot;Nathan Wiebe 2012&quot; /&gt;&lt;ref name=&quot;Maria Schuld 2016&quot; /&gt; the least-squares version of support vector machines,&lt;ref name=&quot;Patrick Rebentrost 2014&quot; /&gt; and Gaussian processes.&lt;ref name=&quot;ReferenceA&quot;/&gt;

A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases,&lt;ref&gt;{{cite journal |doi=10.1103/PhysRevA.73.012307|title=Efficient state preparation for a register of quantum bits|journal=Physical Review A|volume=73|pages=012307|year=2006|last1=Soklakov|first1=Andrei N.|last2=Schack|first2=Rüdiger|bibcode=2006PhRvA..73a2307S}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |doi=10.1103/PhysRevLett.100.160501|pmid=18518173|title=Quantum Random Access Memory|journal=Physical Review Letters|volume=100|issue=16|pages=160501|year=2008|last1=Giovannetti|first1=Vittorio|last2=Lloyd|first2=Seth|last3=MacCone|first3=Lorenzo|bibcode=2008PhRvL.100p0501G|arxiv=0708.1879}}&lt;/ref&gt; this step easily hides the complexity of the task.&lt;ref&gt;{{Cite journal |doi=10.1038/nphys3272|title=Read the fine print|journal=Nature Physics|volume=11|issue=4|pages=291|year=2015|last1=Aaronson|first1=Scott|bibcode=2015NatPh..11..291A}}&lt;/ref&gt;

===Quantum machine learning algorithms based on Grover search===

Another approach to improving classical machine learning with quantum information processing uses [[amplitude amplification]] methods based on [[Grover's algorithm|Grover's search]] algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the [[K-medians clustering|k-medians]]&lt;ref&gt;{{Cite journal|last=Aïmeur|first=Esma|last2=Brassard|first2=Gilles|last3=Gambs|first3=Sébastien|date=2013-02-01|title=Quantum speed-up for unsupervised learning|url=https://link.springer.com/article/10.1007/s10994-012-5316-5|journal=Machine Learning|language=en|volume=90|issue=2|pages=261–287|doi=10.1007/s10994-012-5316-5|issn=0885-6125}}&lt;/ref&gt; and the [[k-nearest neighbour|k-nearest neighbors algorithms]].&lt;ref name=&quot;Nathan Wiebe 2014&quot; /&gt; Another application is a quadratic speedup in the training of [[perceptrons|perceptron]].&lt;ref name=wiebe2016nips&gt;{{cite conference |arxiv=1602.04799|class=quant-ph|last1=Wiebe|first1=Nathan|title=Quantum Perceptron Models|last2=Kapoor|first2=Ashish|last3=Svore|first3=Krysta M.|conference=Advances in Neural Information Processing Systems|volume=29||pages=3999–4007|url=https://papers.nips.cc/paper/6401-quantum-perceptron-models||bibcode=2016arXiv160204799W|year=2016}}&lt;/ref&gt;

Amplitude amplification is often combined with [[quantum walk]]s to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm&lt;ref&gt;{{cite journal |doi= 10.1038/srep00444|pmid= 22685626|pmc= 3370332|title= Google in a Quantum Network|journal= Scientific Reports|volume= 2|pages= 444|year= 2012|last1= Paparo|first1= Giuseppe Davide|last2= Martin-Delgado|first2= Miguel Angel|bibcode= 2012NatSR...2E.444P|arxiv= 1112.2079}}&lt;/ref&gt; as well as the performance of reinforcement learning agents in the projective simulation framework.&lt;ref name=paparo2014quantum&gt;{{cite journal |doi=10.1103/PhysRevX.4.031002|title=Quantum Speedup for Active Learning Agents|journal=Physical Review X|volume=4|issue=3|pages=031002|year=2014|last1=Paparo|first1=Giuseppe Davide|last2=Dunjko|first2=Vedran|last3=Makmal|first3=Adi|last4=Martin-Delgado|first4=Miguel Angel|last5=Briegel|first5=Hans J.|bibcode=2014PhRvX...4c1002P|arxiv=1401.4997}}&lt;/ref&gt;

===Quantum-enhanced reinforcement learning===
[[Reinforcement learning]] is a third branch of machine learning, distinct from supervised and unsupervised learning, which also admits quantum enhancements.&lt;ref&gt;{{Cite journal|last2=Chen|first2=Chunlin|last3=Li|first3=Hanxiong|last4=Tarn|first4=Tzyh-Jong|year=2008|title=Quantum Reinforcement Learning|journal=IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)|volume=38|issue=5|pages=1207|doi=10.1109/TSMCB.2008.925743|first1=Daoyi|last1=Dong}}&lt;/ref&gt;&lt;ref name=&quot;paparo2014quantum&quot; /&gt;&lt;ref&gt;{{cite arxiv|eprint=1612.05695|last1=Crawford|first1=Daniel|title=Reinforcement Learning Using Quantum Boltzmann Machines|last2=Levit|first2=Anna|last3=Ghadermarzy|first3=Navid|last4=Oberoi|first4=Jaspreet S.|last5=Ronagh|first5=Pooya|class=quant-ph|year=2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Briegel|first=Hans J.|last2=Cuevas|first2=Gemma De las|date=2012-05-15|title=Projective simulation for artificial intelligence|url=http://www.nature.com/articles/srep00400|journal=Scientific Reports|language=en|volume=2|doi=10.1038/srep00400|issn=2045-2322|pmc=3351754|pmid=22590690|arxiv=1104.3787|bibcode=2012NatSR...2E.400B}}&lt;/ref&gt; In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical environment and occasionally receives rewards for its actions, which allows the agent to adapt its behaviour—in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent,&lt;ref name=&quot;paparo2014quantum&quot; /&gt; or due to the possibility to probe the environment in [[Quantum superposition|superpositions]],&lt;ref name=&quot;DunjkoTaylorBriegel&quot; /&gt; a quantum speedup may be achieved. Implementations of these kinds of protocols in superconducting circuits&lt;ref&gt;{{cite journal|last1=Lamata|first1=Lucas|title=Basic protocols in quantum reinforcement learning with superconducting circuits|journal=Scientific Reports|volume=7|pages=1609|doi=10.1038/s41598-017-01711-6|year=2017|arxiv=1701.05131|bibcode=2017NatSR...7.1609L}}&lt;/ref&gt; and in systems of trapped ions&lt;ref&gt;{{Cite journal|last=Dunjko|first=V.|last2=Friis|first2=N.|last3=Briegel|first3=H. J.|date=2015-01-01|title=Quantum-enhanced deliberation of learning agents using trapped ions|url=http://stacks.iop.org/1367-2630/17/i=2/a=023006|journal=New Journal of Physics|language=en|volume=17|issue=2|pages=023006|doi=10.1088/1367-2630/17/2/023006|issn=1367-2630|arxiv=1407.2830|bibcode=2015NJPh...17b3006D}}&lt;/ref&gt; have been proposed.

===Quantum sampling techniques===

Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include [[deep learning]], [[probabilistic programming]], and other machine learning and artificial intelligence applications.

A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a [[Boltzmann distribution]]. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though [[quantum annealing|quantum annealers]], like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.&lt;ref&gt;{{cite journal |last1=Biswas |first1=Rupak |last2=Jiang |first2=Zhang |last3=Kechezi |first3=Kostya |last4=Knysh |first4=Sergey |last5=Mandrà |first5=Salvatore |last6=O’Gorman |first6=Bryan |last7=Perdomo-Ortiz |first7=Alejando |last8=Pethukov |first8=Andre |last9=Realpe-Gómez |first9=John |last10=Rieffel |first10=Eleanor|last11=Venturelli|first11=Davide|last12=Vasko|first12=Fedir|last13=Wang|first13=Zhihui |title=A NASA perspective on quantum computing: Opportunities and challenges |year=2016|doi=10.1016/j.parco.2016.11.002|journal=Parallel Computing}}&lt;/ref&gt;

Some research groups have recently explored the use of quantum annealing hardware for training [[Boltzmann machine]]s and [[deep neural networks]].&lt;ref name=Adachi2015&gt;{{cite arXiv |last1=Adachi |first1=Steven H. |last2=Henderson |first2=Maxwell P. |date=2015 |title=Application of quantum annealing to training of deep neural networks |eprint=1510.06356 |class=quant-ph}}&lt;/ref&gt;&lt;ref name=Benedetti2016a&gt;{{cite arXiv |last1=Benedetti |first1=Marcello |last2=Realpe-Gómez |first2=John |last3=Biswas |first3=Rupak |last4=Perdomo-Ortiz |first4=Alejandro |date=2016 |title=Quantum-assisted learning of graphical models with arbitrary pairwise connectivity |eprint=1609.02542 |class=quant-ph}}&lt;/ref&gt;&lt;ref name=Benedetti2016b&gt;{{cite journal |last1=Benedetti |first1=Marcello |last2=Realpe-Gómez |first2=John |last3=Biswas |first3=Rupak |last4=Perdomo-Ortiz |first4=Alejandro |date=2016 |title=Estimation of effective temperatures in quantum annealers for sampling applications: A case study with possible applications in deep learning |journal=Physical Review A |doi=10.1103/PhysRevA.94.022308 |volume=94 |issue=2 |pages=022308 |bibcode=2016PhRvA..94b2308B|arxiv=1510.07611 }}&lt;/ref&gt;&lt;ref name=&quot;William G 1611&quot;&gt;{{cite arXiv |last1=Korenkevych |first1=Dmytro |last2=Xue |first2=Yanbo |last3=Bian |first3=Zhengbing |last4=Chudak |first4=Fabian |last5=Macready |first5=William G. |last6=Rolfe |first6=Jason |last7=Andriyash |first7=Evgeny |date=2016 |title=Benchmarking quantum hardware for training of fully visible Boltzmann machines |eprint=1611.04528 |class=quant-ph}}&lt;/ref&gt; The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard [[Gibbs sampling|sampling]] techniques, such as [[Markov chain Monte Carlo]] algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.

The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures.&lt;ref name=Benedetti2016b /&gt; Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks.&lt;ref name=Adachi2015 /&gt; The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets.&lt;ref name=Benedetti2016a /&gt; In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.

Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed.&lt;ref&gt;{{cite arXiv |last1=Amin |first1=Mohammad H. |last2=Andriyash |first2=Evgeny |last3=Rolfe |first3=Jason |last4=Kulchytskyy |first4=Bohdan |last5=Melko |first5=Roger |date=2016 |title=Quantum Boltzmann machines |eprint=1601.02036 |class=quant-ph}}&lt;/ref&gt; Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.&lt;ref name=Benedetti2016a /&gt;&lt;ref name=&quot;William G 1611&quot;/&gt;

Quantum annealing is not the only technology for sampling. In a prepare and measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing.&lt;ref&gt;{{cite arXiv |last1=Wiebe |first1=Nathan |last2=Kapoor |first2=Ashish |last3=Svore |first3=Krysta M. |date=2014 |title=Quantum deep learning |eprint=1412.3489 |class=quant-ph}}&lt;/ref&gt; The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced [[Markov logic network]]s exploit the symmetries and the locality structure of the [[Graphical model|probabilistic graphical model]] generated by a [[first-order logic]] template.&lt;ref&gt;{{cite journal |last1=Wittek |first1=Peter |last2=Gogolin |first2=Christian |date=2017 |title=Quantum Enhanced Inference in Markov Logic Networks |journal=Scientific Reports|doi=10.1038/srep45672|volume=7|page=45672&lt;!--|eprint=1611.08104--&gt; |class=stat.ML|arxiv=1611.08104|bibcode=2017NatSR...745672W}}&lt;/ref&gt; This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.

===Quantum neural networks===
{{Main article|Quantum neural network|}}

There are quantum analogues or generalisations of classical neural nets&lt;ref name=&quot;WanDKGK16&quot;&gt;{{cite journal|last2=Dahlsten|first2=Oscar|last3=Kristjansson|first3=Hler|last4=Gardner|first4=Robert|last5=Kim|first5=Myungshik|year=2016|title=Quantum generalisation of feedforward neural networks|journal=|volume=|issue=|pages=|arxiv=1612.01045|bibcode=2016arXiv161201045W|last1=Wan|first1=Kwok-Ho}}&lt;/ref&gt; which are known as [[quantum neural network]]s.

==Quantum learning theory==

Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical [[computational learning theory]], but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider ''specific problems'' and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.&lt;ref&gt;{{cite arXiv|class=quant-ph|first2=Ronald|last2=de Wolf|title=A Survey of Quantum Learning Theory|date=2017|last1=Arunachalam|first1=Srinivasan|eprint=1701.06806}}&lt;/ref&gt;

The starting point in learning theory is typically a ''concept class'', a set of possible concepts. Usually a concept is a function on some domain, such as &lt;math&gt;\{0,1\}^n&lt;/math&gt;. For example, the concept class could be the set of [[Disjunctive normal form|disjunctive normal form (DNF)]] formulas on ''n'' bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn
(exactly or approximately) an unknown ''target concept'' from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.

In active learning, a learner can make ''membership queries'' to the target concept ''c'', asking for its value ''c(x)'' on inputs ''x'' chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of ''quantum exact learning'', the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more.&lt;ref name=&quot;gortlerservedioquantum&quot;&gt;{{cite journal|last2=Gortler|first2=Steven J.|year=2004|title=Equivalences and Separations Between Quantum and Classical Learnability|journal=SIAM Journal on Computing|volume=33|issue=5|pages=1067–1092|doi=10.1137/S0097539704412910|last1=Servedio|first1=Rocco A.}}&lt;/ref&gt; If complexity is measured by the amount of ''time''
the learner uses, then there are concept classes that can be learned efficiently by
quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).&lt;ref name=&quot;gortlerservedioquantum&quot; /&gt;

A natural model of passive learning is Valiant's [[probably approximately correct learning|probably approximately correct (PAC) learning]]. Here the learner receives random examples ''(x,c(x))'', where ''x'' is distributed according to some unknown distribution ''D''. The learner's goal is to output a hypothesis function ''h'' such that ''h(x)=c(x)'' with high probability when ''x'' is drawn according to ''D''. The learner has to be able to produce such an 'approximately correct' ''h'' for every ''D'' and every target concept ''c'' in its concept class.
We can consider replacing the random examples by potentially more powerful quantum examples &lt;math&gt;\sum_x \sqrt{D(x)}|x,c(x)\rangle&lt;/math&gt;. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and
quantum sample complexity are the same up to constant factors.&lt;ref&gt;{{cite arXiv|class=quant-ph|first2=Ronald|last2=de Wolf|title=Optimal Quantum Sample Complexity of Learning Algorithms|date=2016|last1=Arunachalam|first1=Srinivasan|eprint=1607.00932}}&lt;/ref&gt; However, for learning under some
fixed distribution ''D'', quantum examples can be very helpful, for example for learning DNF under
the uniform distribution.&lt;ref&gt;{{cite journal|last2=Jeffrey|first2=Jackson C.|year=1999|title=Learning DNF over the Uniform Distribution Using a Quantum Example Oracle|journal=SIAM Journal on Computing|volume=28|issue=3|pages=1136–1153|doi=10.1137/S0097539795293123|last1=Nader|first1=Bshouty H.}}&lt;/ref&gt; When considering ''time'' complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).&lt;ref name=&quot;gortlerservedioquantum&quot; /&gt;

This passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis ''h'' is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.&lt;ref&gt;{{cite journal|first2=Gael|last2=Sentís|title=Inductive supervised quantum learning|date=2017|last1=Monràs|first1=Alex|last3=Wittek|first3=Peter|journal=Physical Review Letters|volume=118|issue=19|pages=190503|doi=10.1103/PhysRevLett.118.190503&lt;!--|eprint=1605.07541--&gt; |bibcode=2017PhRvL.118s0503M|arxiv=1605.07541}}&lt;/ref&gt;

==Classical learning applied to quantum systems==
The term quantum machine learning is also used for approaches that apply classical methods of machine learning to the study of quantum systems. A prime example is the use of classical learning techniques to process large amounts of experimental data in order to characterize an unknown quantum system (for instance in the context of [[Quantum information |quantum information theory]] and for the development of quantum technologies), but there are also more exotic applications.

The ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example, [[Bayesian statistics|Bayesian]] methods and concepts of [[Algorithmic learning theory|algorithmic learning]] can be fruitfully applied to tackle quantum state classification,&lt;ref name=&quot;sentis2012quantum&quot;&gt;{{cite journal |last1=Sentís |first1=Gael|last2=Calsamiglia |first2=John |last3=Muñoz-Tapia |first3=Raúl |last4=Bagan |first4=Emilio |year=2012 |title=Quantum learning without quantum memory |url=|journal=Scientific Reports |volume=2 |issue=|page=708 |arxiv=1106.2742 |bibcode=2012NatSR...2E.708S |doi=10.1038/srep00708}}&lt;/ref&gt; Hamiltonian learning,&lt;ref&gt;{{cite journal |last2=Granade |first2=Christopher |last3=Ferrie |first3=Christopher |last4=Cory |first4=David |year=2014 |title=Quantum Hamiltonian learning using imperfect quantum resources |url=|journal=Physical Review A |volume=89 |issue=4 |page=042314 |arxiv=1311.5269 |bibcode=2014PhRvA..89d2314W |doi=10.1103/physreva.89.042314 |last1=Wiebe |first1=Nathan}}&lt;/ref&gt; and the characterization of an unknown [[Unitary matrix|unitary transformation]].&lt;ref name=&quot;bisio2010&quot;&gt;{{Cite journal |arxiv=0903.0543 |last1=Bisio |first1=Alessandro |title=Optimal quantum learning of a unitary transformation |journal=Physical Review A |volume=81 |issue=3 |pages=032324 |last2= Chiribella |first2=Giulio |last3= D'Ariano |first3=Giacomo Mauro |last4= Facchini |first4=Stefano |last5= Perinotti |first5=Paolo |year=2010 |doi=10.1103/PhysRevA.81.032324|bibcode=2010PhRvA..81c2324B }}&lt;/ref&gt;&lt;ref name=&quot;:3&quot;&gt;{{cite journal |last2=Junghee Ryu |first2=Bang |last3=Yoo |first3=Seokwon |last4=Pawłowski |first4=Marcin |last5=Lee |first5=Jinhyoung |year=2014 |title=A strategy for quantum algorithm design assisted by machine learning |url=|journal=New Journal of Physics |volume=16 |issue=|page=073017 |arxiv=1304.2169 |bibcode=2014NJPh...16a3017K |doi=10.1088/1367-2630/16/1/013017 |last1=Jeongho |first1=}}&lt;/ref&gt; Other problems that have been addressed with this approach are given in the following list:
* Identifying an accurate model for the dynamics of a quantum system, through the reconstruction of the [[Hamiltonian (quantum mechanics)|Hamiltonian]];&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|last=Granade|first=Christopher E.|last2=Ferrie|first2=Christopher|last3=Wiebe|first3=Nathan|last4=Cory|first4=D. G.|date=2012-10-03|title=Robust Online Hamiltonian Learning|journal=New Journal of Physics|volume=14|issue=10|pages=103013|arxiv=1207.1655|bibcode=2012NJPh...14j3013G|doi=10.1088/1367-2630/14/10/103013|issn=1367-2630}}&lt;/ref&gt;&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|last=Wiebe|first=Nathan|last2=Granade|first2=Christopher|last3=Ferrie|first3=Christopher|last4=Cory|first4=D. G.|date=2014-05-14|title=Hamiltonian Learning and Certification Using Quantum Resources|journal=Physical Review Letters|volume=112|issue=19|pages=190501|arxiv=1309.0876|bibcode=2014PhRvL.112s0501W|doi=10.1103/PhysRevLett.112.190501|issn=0031-9007|pmid=24877920}}&lt;/ref&gt;&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|last=Wiebe|first=Nathan|last2=Granade|first2=Christopher|last3=Ferrie|first3=Christopher|last4=Cory|first4=David G.|date=2014-04-17|title=Quantum Hamiltonian Learning Using Imperfect Quantum Resources|journal=Physical Review A|volume=89|issue=4|pages=042314|arxiv=1311.5269|bibcode=2014PhRvA..89d2314W|doi=10.1103/PhysRevA.89.042314|issn=1050-2947}}&lt;/ref&gt;
* Extracting information on unknown states;&lt;ref name=&quot;sasaki2010a&quot;&gt;{{Cite journal|last=Sasaki|first=Madahide|last2=Carlini|first2=Alberto|last3=Jozsa|first3=Richard|date=2001|title=Quantum Template Matching|journal=Physical Review A|volume=64|issue=2|pages=022317|arxiv=quant-ph/0102020|bibcode=2001PhRvA..64b2317S|doi=10.1103/PhysRevA.64.022317}}&lt;/ref&gt;&lt;ref name=&quot;sasaki2010b&quot;&gt;{{Cite journal|last=Sasaki|first=Masahide|date=2002|title=Quantum learning and universal quantum matching machine|journal=Physical Review A|volume=66|issue=2|pages=022303|arxiv=quant-ph/0202173|bibcode=2002PhRvA..66b2303S|doi=10.1103/PhysRevA.66.022303}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal |last=Sentís |first=Gael |last2=Guţă |first2=Mădălin |last3=Adesso |first3=Gerardo |date=2015-07-09 |title=Quantum learning of coherent states |url=https://link.springer.com/article/10.1140/epjqt/s40507-015-0030-4 |journal=EPJ Quantum Technology |language=en |volume=2 |issue=1 |pages=17 |doi=10.1140/epjqt/s40507-015-0030-4 |issn=2196-0763}}&lt;/ref&gt;&lt;ref name=sentis2012quantum /&gt;
* Learning unknown unitary transformations and measurements;&lt;ref name=bisio2010/&gt;&lt;ref name=&quot;:3&quot; /&gt;
* Engineering of quantum gates from qubit networks with pairwise interactions, using time dependent&lt;ref&gt;{{Cite journal |last=Zahedinejad |first=Ehsan |last2=Ghosh |first2=Joydip |last3=Sanders |first3=Barry C. |date=2016-11-16 |title=Designing High-Fidelity Single-Shot Three-Qubit Gates: A Machine Learning Approach |arxiv=1511.08862 |journal=Physical Review Applied |volume=6 |issue=5 |pages=054005 |doi=10.1103/PhysRevApplied.6.054005 |issn=2331-7019 |bibcode=2016PhRvP...6e4005Z}}&lt;/ref&gt; or independent&lt;ref name=&quot;Banchi&quot;&gt;{{Cite journal|last=Banchi|first=Leonardo|last2=Pancotti|first2=Nicola|last3=Bose|first3=Sougato|date=2016-07-19|title=Quantum gate learning in qubit networks: Toffoli gate without time-dependent control|url=http://www.nature.com/articles/npjqi201619|journal=[[npj Quantum Information]] |volume=2|pages=16019|bibcode=2016npjQI...216019B|doi=10.1038/npjqi.2016.19}}&lt;/ref&gt; Hamiltonians.
However, the characterization of quantum states and processes is not the only application of classical machine learning techniques. Some additional applications include
* Inferring molecular energies;&lt;ref&gt;{{Cite journal |last=Rupp |first=Matthias |last2=Tkatchenko |first2=Alexandre | last3=Muller | first3=Klaus-Robert | last4=von Lilienfeld | first4=O. Anatole |date=2012-01-31 |title=Fast and Accurate Modeling of Molecular Atomization Energies With Machine Learning |url=https://doi.org/10.1103/PhysRevLett.108.058301 |journal=Physical Review Letters |volume=355 |issue=6325 |pages=602 |doi=10.1103/PhysRevLett.108.058301 |arxiv=1109.2618}}&lt;/ref&gt;
* Automatic generation of new quantum experiments;&lt;ref&gt;{{Cite journal |last=Krenn |first=Mario |date=2016-01-01 |title=Automated Search for new Quantum Experiments |url=http://link.aps.org/doi/10.1103/PhysRevLett.116.090405 |journal=Physical Review Letters |volume=116 |issue=9 |pages=090405 |doi=10.1103/PhysRevLett.116.090405 |pmid=26991161 |bibcode=2016PhRvL.116i0405K|arxiv=1509.02749 }}&lt;/ref&gt;
* Solving the many-body, static and time-dependent Schrödinger equation;&lt;ref&gt;{{Cite journal |last=Carleo |first=Giuseppe |last2=Troyer |first2=Matthias |date=2017-02-09 |title=Solving the quantum many-body problem with artificial neural networks |url=http://science.sciencemag.org/content/355/6325/602 |journal=Science |volume=355 |issue=6325 |pages=602 |doi=10.1126/science.aag2302 |arxiv=1606.02318 |bibcode=2017Sci...355..602C }}&lt;/ref&gt;
* Identifying phase transitions from entanglement spectra;&lt;ref&gt;{{Cite journal|last=van Nieuwenburg|first=Evert|last2=Liu|first2=Ye-Hua|last3=Huber|first3=Sebastian|year=2017|title=Learning phase transitions by confusion|url=http://www.nature.com/nphys/journal/vaop/ncurrent/full/nphys4037.html|journal=Nature Physics|volume=|pages=|doi=10.1038/nphys4037|via=|arxiv=1610.02048|bibcode=2017NatPh..13..435V}}&lt;/ref&gt;
* Generating adaptive feedback schemes for [[quantum metrology]].&lt;ref&gt;{{Cite journal|last=Hentschel|first=Alexander|date=2010-01-01|title=Machine Learning for Precise Quantum Measurement|url=http://link.aps.org/doi/10.1103/PhysRevLett.104.063603|journal=Physical Review Letters|volume=104|issue=6|doi=10.1103/PhysRevLett.104.063603|bibcode=2010PhRvL.104f3603H|arxiv=0910.0762}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Palittpongarnpim|first=Pantita|last2=Wittek|first2=Peter|last3=Sanders|first3=Barry C.|date=2016-09-13|title=Single-shot adaptive measurement for quantum-enhanced metrology|url=http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2553344|publisher=International Society for Optics and Photonics|pages=99800H–99800H-11|doi=10.1117/12.2237355|journal=Quantum Communications and Quantum Imaging XIV}}&lt;/ref&gt;

==Fully quantum machine learning==
In the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.

One class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case.&lt;ref&gt;{{cite journal|last1=Sentís|first1=Gael|last2=Guţă|first2=Mădălin|last3=Adesso|first3=Gerardo|title=Quantum learning of coherent states|journal=EPJ Quantum Technology|date=9 July 2015|volume=2|issue=1|doi=10.1140/epjqt/s40507-015-0030-4}}&lt;/ref&gt; (This also relates to work on quantum pattern matching.&lt;ref&gt;{{cite journal|last1=Sasaki|first1=Masahide|last2=Carlini|first2=Alberto|title=Quantum learning and universal quantum matching machine|journal=Physical Review A|date=6 August 2002|volume=66|issue=2|doi=10.1103/PhysRevA.66.022303|bibcode=2002PhRvA..66b2303S|arxiv=quant-ph/0202173}}&lt;/ref&gt;) The problem of learning unitary transformations can be approached in a similar way.&lt;ref&gt;{{cite journal|last1=Bisio|first1=Alessandro|last2=Chiribella|first2=Giulio|last3=D’Ariano|first3=Giacomo Mauro|last4=Facchini|first4=Stefano|last5=Perinotti|first5=Paolo|title=Optimal quantum learning of a unitary transformation|journal=Physical Review A|date=25 March 2010|volume=81|issue=3|doi=10.1103/PhysRevA.81.032324|bibcode=2010PhRvA..81c2324B|arxiv=0903.0543}}&lt;/ref&gt;

Going beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum.&lt;ref&gt;{{cite journal|last1=Aïmeur|first1=Esma|last2=Brassard|first2=Gilles|last3=Gambs|first3=Sébastien|title=Quantum Clustering Algorithms|journal=Proceedings of the 24th International Conference on Machine Learning|date=1 January 2007|pages=1–8|doi=10.1145/1273496.1273497|url=http://doi.acm.org/10.1145/1273496.1273497|publisher=ACM}}&lt;/ref&gt; Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in,&lt;ref name=&quot;DunjkoTaylorBriegel&quot;/&gt; where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning.

==Implementations and experiments==

The earliest experiments were conducted using the adiabatic [[D-Wave Systems|D-Wave]] quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009.&lt;ref&gt;{{cite web|url=http://static.googleusercontent.com/media/www.google.com/de//googleblogs/pdfs/nips_demoreport_120709_research.pdf|title=NIPS 2009 Demonstration: Binary Classification using Hardware Implementation of Quantum Annealing|publisher=Static.googleusercontent.com|format=PDF|accessdate=26 November 2014}}&lt;/ref&gt; Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, [[NASA]], and the [[Universities Space Research Association]] launched the [[Quantum Artificial Intelligence Lab]] which explores the use of the adiabatic D-Wave quantum computer.&lt;ref&gt;{{cite web|url=https://plus.google.com/+QuantumAILab|title=Google Quantum A.I. Lab Team|date=31 January 2017|website=Google Plus|publisher=Google|access-date=31 January 2017|quote=|author=&lt;!--Staff writer(s); no by-line.--&gt;}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://ti.arc.nasa.gov/tech/dash/physics/quail/|title=NASA Quantum Artificial Intelligence Laboratory|date=31 January 2017|website=NASA|publisher=NASA|access-date=31 January 2017|quote=|author=&lt;!--Staff writer(s); no by-line.--&gt;}}&lt;/ref&gt; A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.&lt;ref name=&quot;Benedetti2016a&quot; /&gt;

Using a different annealing technology based on [[nuclear magnetic resonance]] (NMR), a quantum [[Hopfield network]] was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation.&lt;ref&gt;{{cite journal|last2=Neves|first2=Jorge L.|last3=Sollacher|first3=Rudolf|last4=Glaser|first4=Steffen J.|year=2009|title=Quantum pattern recognition with liquid-state nuclear magnetic resonance|journal=Physical Review A|volume=79|issue=4|pages=042321|arxiv=0802.1592|bibcode=2009PhRvA..79d2321N|doi=10.1103/PhysRevA.79.042321|last1=Neigovzen|first1=Rodion}}&lt;/ref&gt; NMR technology also enables universal quantum computing{{Citation needed|date=February 2017}}, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state  quantum computer in 2015.&lt;ref&gt;{{cite journal|last2=Liu|first2=Xiaomei|last3=Xu|first3=Nanyang|last4=Du|first4=Jiangfeng|year=2015|title=Experimental Realization of a Quantum Support Vector Machine|journal=Physical Review Letters|volume=114|issue=14|pages=140504|arxiv=1410.1054|bibcode=2015PhRvL.114n0504L|doi=10.1103/PhysRevLett.114.140504|pmid=25910101|last1=Li|first1=Zhaokai}}&lt;/ref&gt; The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly [[quantum tomography]] by reading out the final state in terms of direction (up/down) of the NMR signal.

Photonic implementations are attracting more attention,&lt;ref name=&quot;WanDKGK16&quot; /&gt; not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013.&lt;ref&gt;{{cite journal|last2=Soriano|first2=Miguel C.|last3=Mirasso|first3=Claudio R.|last4=Fischer|first4=Ingo|year=2013|title=Parallel photonic information processing at gigabyte per second data rates using transient states|journal=Nature Communications|volume=4|pages=1364|bibcode=2013NatCo...4E1364B|doi=10.1038/ncomms2368|pmc=3562454|pmid=23322052|last1=Brunner|first1=Daniel}}&lt;/ref&gt; Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule.&lt;ref&gt;{{cite journal|last2=Mabuchi|first2=Hideo|year=2015|title=A coherent perceptron for all-optical learning|journal=EPJ Quantum Technology|volume=2|arxiv=1501.01608|doi=10.1140/epjqt/s40507-015-0023-3|last1=Tezak|first1=Nikolas}}&lt;/ref&gt; A core building block in many learning algorithms is to calculate the distance between two vectors: this was first to experimentally demonstrated up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.&lt;ref&gt;{{cite journal|last2=Wu|first2=D.|last3=Su|first3=Z.-E.|last4=Chen|first4=M.-C.|last5=Wang|first5=X.-L.|last6=Li|first6=Li|last7=Liu|first7=N.-L.|last8=Lu|first8=C.-Y.|last9=Pan|first9=J.-W.|year=2015|title=Entanglement-Based Machine Learning on a Quantum Computer|journal=Physical Review Letters|volume=114|issue=11|pages=110504|arxiv=1409.7770|bibcode=2015PhRvL.114k0504C|doi=10.1103/PhysRevLett.114.110504|pmid=25839250|last1=Cai|first1=X.-D.}}&lt;/ref&gt;

Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical [[memristor]].&lt;ref&gt;{{cite journal |doi=10.1038/srep29507 |pmid=27381511 |pmc=4933948 |title=Quantum memristors |journal=Scientific Reports |volume=6 |pages=29507 |year=2016 |last1=Pfeiffer |first1=P. |last2=Egusquiza |first2=I. L. |last3=Di Ventra |first3=M. |last4=Sanz |first4=M. |last5=Solano |first5=E. |bibcode=2016NatSR...629507P|arxiv=1511.02192 }}&lt;/ref&gt; This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed,&lt;ref&gt;{{cite journal |doi=10.1038/srep42044|last1= Salmilehto|first1= J.|title= Quantum Memristors with Superconducting Circuits|last2=  Deppe|first2= F.|last3=  Di Ventra|first3= M.|last4=  Sanz|first4= M.|last5=  Solano|first5= E.|journal=Scientific Reports |volume=7 |pages=42044|year= 2017|arxiv=1603.04487|bibcode=2017NatSR...742044S}}&lt;/ref&gt; and an experiment with quantum dots performed.&lt;ref&gt;{{cite arxiv|eprint=1612.08409|last1=Li|first1=Ying|title=A simple and robust quantum memristor|last2= Holloway|first2=Gregory W.|last3= Benjamin|first3=Simon C.|last4= Briggs|first4=G. Andrew D.|last5=Baugh|first5=Jonathan|last6= Mol|first6=Jan A.|class=cond-mat.mes-hall|year=2016}}&lt;/ref&gt; A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional [[quantum neural network]].

== See also ==
*[[Quantum computing]]
*[[Quantum algorithm for linear systems of equations]]
*[[Quantum annealing]]
*[[Quantum neural network]]
*[[Quantum image]]

==References==
{{Reflist |30em}}

{{Quantum computing}}




</text>
      <sha1>bt4wwt2107beeqtvrun3pdd4bmmgfnw</sha1>
    </revision>
  </page>
  <page>
    <title>M-Theory (learning framework)</title>
    <ns>0</ns>
    <id>44632031</id>
    <revision>
      <id>805671468</id>
      <parentid>802257352</parentid>
      <timestamp>2017-10-16T22:00:14Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* Relation to biology */[[User:JCW-CleanerBot#Logic|task]], replaced: J. Neurophysiol → J. Neurophysiol. using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26031">{{about|machine learning|the physics term|M-theory}}
{{Orphan|date=December 2014}}

In [[Machine Learning]] and [[Computer Vision]], '''M-Theory''' is a learning framework inspired by feed-forward processing in the [[Two-streams hypothesis|ventral stream]] of [[visual cortex]] and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as [[speech recognition]]. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance.&lt;ref&gt;Serre T., Oliva A., Poggio T. (2007) A feedforward architecture accounts for rapid categorization. ''PNAS'', vol. 104, no. 15, pp. 6424-6429&lt;/ref&gt;

The core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with [[Compressed Sensing]]. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex.

==Intuition==

===Invariant representations===

A great challenge in visual recognition tasks is that the same object can be seen in a variety of conditions. It can be seen from different distances, different viewpoints, under different lighting, partially occluded, etc. In addition, for particular classes objects, such as faces, highly complex specific transformations may be relevant, such as changing facial expressions. For learning to recognize images, it is greatly beneficial to factor out these variations. It results in much simpler classification problem and, consequently, in great reduction of [[sample complexity]] of the model.

A simple computational experiment illustrates this idea. Two instances of a classifier were trained to distinguish images of planes from those of cars. For training and testing of the first instance, images with arbitrary viewpoints were used. Another instance received only images seen from a particular viewpoint, which was equivalent to training and testing the system on invariant representation of the images. One can see that the second classifier performed quite well even after receiving a single example from each category, while performance of the first classifier was close to random guess even after seeing 20 examples.

Invariant representations has been incorporated into several learning architectures, such as [[neocognitron]]s. Most of these architectures, however, provided invariance through custom-designed features or properties of architecture itself. While it helps to take into account some sorts of transformations, such as translations, it is very nontrivial to accommodate for other sorts of transformations, such as 3D rotations and changing facial expressions. M-Theory provides a framework of how such transformations can be learned. In addition to higher flexibility, this theory also suggests how human brain may have similar capabilities.

===Templates===

Another core idea of M-Theory is close in spirit to ideas from the field of [[compressed sensing]]. An implication from [[Johnson–Lindenstrauss lemma]] says that a particular number of images can be embedded into a low-dimensional feature space with the same distances between images by using random projections. This result suggests that [[dot product]] between the observed image and some other image stored in memory, called template, can be used as a feature helping to distinguish the image from other images. The template need not to be anyhow related to the image, it could be chosen randomly.

===Combining templates and invariant representations===

The two ideas outlined in previous sections can be brought together to construct a framework for learning invariant representations. The key observation is how dot product between image &lt;math&gt;I&lt;/math&gt; and a template &lt;math&gt;t&lt;/math&gt; behaves when image is transformed (by such transformations as translations, rotations, scales, etc.). If transformation &lt;math&gt;g&lt;/math&gt; is a member of a [[unitary group]] of transformations, then the following holds:

&lt;math&gt;\langle gI,t \rangle = \langle I,g^{-1}t \rangle    (1) &lt;/math&gt;

In other words, the dot product of transformed image and a template is equal to the dot product of original image and inversely transformed template. For instance, for image rotated by 90 degrees, the inversely transformed template would be rotated by -90 degrees.

Consider the set of dot products of an image &lt;math&gt;I&lt;/math&gt; to all possible transformations of template: &lt;math&gt;\lbrace \langle I,g^\prime t\rangle | g^\prime \in G \rbrace&lt;/math&gt;. If one applies a transformation &lt;math&gt;g&lt;/math&gt; to &lt;math&gt;I&lt;/math&gt;, the set would become &lt;math&gt;\lbrace \langle gI,g^\prime t\rangle | g^\prime \in G\rbrace&lt;/math&gt;. But because of the property (1), this is equal to &lt;math&gt;\lbrace \langle I,g^{-1}g^\prime t\rangle | g^\prime \in G\rbrace&lt;/math&gt;. The set &lt;math&gt;\lbrace g^{-1}g^\prime | g^\prime \in G \rbrace&lt;/math&gt; is equal to just the set of all elements in &lt;math&gt;G&lt;/math&gt;. To see this, note that every &lt;math&gt;g^{-1}g^\prime&lt;/math&gt; is in &lt;math&gt;G&lt;/math&gt; due to the closure property of [[group (mathematics)|groups]], and for every &lt;math&gt;g^{\prime\prime}&lt;/math&gt; in G there exist its prototype &lt;math&gt;g^\prime&lt;/math&gt; such as &lt;math&gt;g^{\prime\prime} = g^{-1}g^\prime&lt;/math&gt; (namely, &lt;math&gt;g^\prime = gg^{\prime\prime}&lt;/math&gt;). Thus, &lt;math&gt;\lbrace \langle I,g^{-1}g^\prime t\rangle | g^\prime \in G\rbrace = \lbrace\langle I,g^{\prime\prime}t\rangle | g^{\prime\prime} \in G \rbrace&lt;/math&gt;. One can see that the set of dot products remains the same despite that a transformation was applied to the image! This set by itself may serve as a (very cumbersome) invariant representation of an image. More practical representations can be derived from it.

In the introductory section, it was claimed that M-Theory allows to learn invariant representations. This is because templates and their transformed versions can be learned from visual experience - by exposing the system to sequences of transformations of objects. It is plausible that similar visual experiences occur in early period of human life, for instance when infants twiddle toys in their hands. Because templates may be totally unrelated to images that the system later will try to classify, memories of these visual experiences may serve as a basis for recognizing many different kinds of objects in later life. However, as it is shown later, for some kinds of transformations, specific templates are needed.

==Theoretical aspects==

===From orbits to distribution measures ===

To implement the ideas described in previous sections, one need to know how to derive a computationally efficient invariant representation of an image. Such unique representation for each image can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during unsupervised learning). These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below.

Orbit &lt;math&gt;O_I&lt;/math&gt; is a set of images &lt;math&gt;gI&lt;/math&gt; generated from a single image &lt;math&gt;I&lt;/math&gt; under the action of the group &lt;math&gt;G, \forall g \in G&lt;/math&gt;.

In other words, images of an object and of its transformations correspond to a orbit &lt;math&gt;O_I&lt;/math&gt;. If two orbits have a point in common they are identical everywhere,&lt;ref name=&quot;magic_paper&quot;&gt;F Anselmi, JZ Leibo, L Rosasco, J Mutch, A Tacchetti, T Poggio (2014) ''Unsupervised learning of invariant representations in hierarchical architectures''  arXiv preprint arXiv:1311.4158&lt;/ref&gt; i.e. an orbit is an invariant and unique representation of an image. So, two images are called equivalent when they belong to the same orbit: &lt;math&gt;I \sim I^\prime&lt;/math&gt; if &lt;math&gt;\exists g \in G&lt;/math&gt; such that &lt;math&gt;I^\prime = gI&lt;/math&gt;. Conversely, two orbits are different if none of the images in one orbit coincide with any image in the other.&lt;ref&gt;H. Schulz-Mirbach. Constructing invariant features by averaging techniques. In Pattern Recognition, 1994. Vol. 2 - Conference B: Computer Vision amp; Image Processing., Proceedings of the 12th IAPR International. Conference on, volume 2, pages 387 –390 vol.2, 1994.&lt;/ref&gt;

A natural question arises: how can one compare two orbits? There are several possible approaches. One of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points. Thus, one can consider a probability distribution &lt;math&gt;P_I&lt;/math&gt; induced by the group’s action on images &lt;math&gt;I&lt;/math&gt; (&lt;math&gt;gI&lt;/math&gt; can be seen as a realization of a random variable).

This probability distribution &lt;math&gt;P_I&lt;/math&gt; can be almost uniquely characterized by &lt;math&gt;K&lt;/math&gt; one-dimensional probability distributions &lt;math&gt;P_{\langle I ,t^k \rangle}&lt;/math&gt; induced by the (one-dimensional) results of projections &lt;math&gt;\langle I,t^k\rangle&lt;/math&gt;, where &lt;math&gt;t^k, k = 1,...,K&lt;/math&gt; are a set of templates (randomly chosen images) (based on the Cramer-Wold theorem  &lt;ref&gt;H. Cramer and H. Wold. Some theorems on distribution functions. J. London Math. Soc., 4:290–294, 1936.&lt;/ref&gt; and concentration of measures).

Consider &lt;math&gt;n&lt;/math&gt; images &lt;math&gt;X_n \in X&lt;/math&gt;. Let &lt;math&gt;K \geq \frac{2}{c\epsilon^2} \log \frac{n}{\delta}&lt;/math&gt; , where &lt;math&gt;c&lt;/math&gt; is a universal constant. Then

&lt;math&gt;|d(P_I,P_I^\prime)-dK(P_I,P_I^\prime)| \leq \epsilon,&lt;/math&gt;

with probability &lt;math&gt;1 - \delta^2&lt;/math&gt;, for all &lt;math&gt;I, I^\prime&lt;/math&gt; &lt;math&gt;\in &lt;/math&gt; &lt;math&gt;X_n&lt;/math&gt;.

This result (informally) says that an approximately invariant and unique representation of an image &lt;math&gt;I&lt;/math&gt; can be obtained from the estimates of &lt;math&gt;K&lt;/math&gt; 1-D probability distributions &lt;math&gt;P_{\langle I,t^k \rangle}&lt;/math&gt; for &lt;math&gt;k = 1,...,K&lt;/math&gt;. The number &lt;math&gt;K&lt;/math&gt; of projections needed to discriminate &lt;math&gt;n&lt;/math&gt; orbits, induced by &lt;math&gt;n&lt;/math&gt; images, up to precision &lt;math&gt;\epsilon&lt;/math&gt; (and with confidence &lt;math&gt;1 - \delta^2&lt;/math&gt;) is &lt;math&gt;K \geq \frac{2}{c\epsilon^2} \log \frac{n}{\delta}&lt;/math&gt;, where &lt;math&gt;c&lt;/math&gt; is a universal constant.

To classify an image, the following “recipe” can be used:

#     Memorize a set of images/objects called templates;
#     Memorize observed transformations for each template;
#     Compute dot products of its transformations with image;
#     Compute histogram of the resulting values, called ''signature'' of the image;
#     Compare the obtained histogram with signatures stored in memory.

Estimates of such one-dimensional probability density functions (PDFs) &lt;math&gt;P_{\langle I ,t^k \rangle}&lt;/math&gt; can be written in terms of histograms as &lt;math&gt;\mu^k_n(I) = 1/\left|G\right| \sum_{i=1}^{\left|G\right|} \eta_n(\langle I, g_i t^k \rangle) &lt;/math&gt;, where &lt;math&gt;\eta_n, n = 1, ... , N &lt;/math&gt; is a set of nonlinear functions. These 1-D probability distributions can be characterized with N-bin histograms or set of statistical moments. For example, HMAX represents an architecture in which pooling is done with a max operation.

===Non-compact groups of transformations===

In the &quot;recipe&quot; for image classification, groups of transformations are approximated with finite number of transformations. Such approximation is possible only when the group is [[compact group|compact]].

Such groups as all translations and all scalings of the image are not compact, as they allow arbitrarily big transformations. However, they are [[locally compact group|locally compact]]. For locally compact groups, invariance is achievable within certain range of transformations.&lt;ref name=&quot;magic_paper&quot; /&gt;

Assume that &lt;math&gt;G_0&lt;/math&gt; is a subset of transformations from &lt;math&gt;G&lt;/math&gt; for which the transformed patterns exist in memory. For an image &lt;math&gt;I&lt;/math&gt; and template &lt;math&gt;t_k&lt;/math&gt;, assume that &lt;math&gt;\langle I,g^{-1}t_k\rangle&lt;/math&gt; is equal to zero everywhere except some subset of &lt;math&gt;G_0&lt;/math&gt;. This subset is called [[support (mathematics)|support]] of &lt;math&gt;\langle I,g^{-1}t_k\rangle&lt;/math&gt; and denoted as &lt;math&gt;supp(\langle I, g^{-1}t_k\rangle)&lt;/math&gt;. It can be proven that if for a transformation &lt;math&gt;g^\prime&lt;/math&gt;, support set will also lie within &lt;math&gt;g^\prime G_0&lt;/math&gt;, then signature of &lt;math&gt;I&lt;/math&gt; is invariant with respect to &lt;math&gt;g^\prime&lt;/math&gt;.&lt;ref name=&quot;magic_paper&quot; /&gt; This theorem determines the range of transformations for which invariance is guaranteed to hold.

One can see that the smaller is &lt;math&gt;supp(\langle I, g^{-1}t_k\rangle)&lt;/math&gt;, the larger is the range of transformations for which invariance is guaranteed to hold. It means that for a group that is only locally compact, not all templates would work equally well anymore. Preferable templates are those with a reasonably small &lt;math&gt;supp(\langle gI, t_k\rangle)&lt;/math&gt; for a generic image. This property is called localization: templates are sensitive only to images within a small range of transformations. Note that although minimizing &lt;math&gt;supp(\langle gI, t_k\rangle)&lt;/math&gt; is not absolutely necessary for the system to work, it improves approximation of invariance. Requiring localization simultaneously for translation and scale yields a very specific kind of templates: [[Gabor function]]s.&lt;ref name=&quot;magic_paper&quot; /&gt;

The desirability of custom templates for non-compact group is in conflict with the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex.&lt;ref&gt;F. Anselmi, J.Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, T. Poggio (2013) ''Magic Materials: a theory of deep hierarchical architectures for learning sensory representations.'' CBCL paper, Massachusetts Institute of Technology, Cambridge, MA&lt;/ref&gt; The optimality of Gabor templates for translations and scales is a possible explanation of this phenomenon.

===Non-group transformations===

Many interesting transformations of images do not form groups. For instance, transformations of images associated with 3D rotation of corresponding 3D object do not form a group, because it is impossible to define an inverse transformation (two objects may looks the same from one angle but different from another angle). However, approximate invariance is still achievable even for non-group transformations, if localization condition for templates holds and transformation can be locally linearized.

As it was said in the previous section, for specific case of translations and scaling, localization condition can be satisfied by use of generic Gabor templates. However, for general case (non-group) transformation, localization condition can be satisfied only for specific class of objects.&lt;ref name=&quot;magic_paper&quot; /&gt; More specifically, in order to satisfy the condition, templates must be similar to the objects one would like to recognize. For instance, if one would like to build a system to recognize 3D rotated faces, one need to use other 3D rotated faces as templates. This may explain the existence of such specialized modules in the brain as one responsible for face recognition.&lt;ref name=&quot;magic_paper&quot; /&gt; Even with custom templates, a noise-like encoding of images and templates is necessary for localization. It can be naturally achieved if the non-group transformation is processed on any layer other than the first in hierarchical recognition architecture.

===Hierarchical architectures===

The previous section suggests one motivation for hierarchical image recognition architectures. However, they have other benefits as well.

Firstly, hierarchical architectures best accomplish the goal of ‘parsing’ a complex visual scene with many objects consisting of many parts, whose relative position may greatly vary. In this case, different elements of the system must react to different objects and parts. In hierarchical architectures, representations of parts at different levels of embedding hierarchy can be stored at different layers of hierarchy.

Secondly, hierarchical architectures which have invariant representations for parts of objects may facilitate learning of complex compositional concepts. This facilitation may happen through reusing of learned representations of parts that were constructed before in process of learning of other concepts. As a result, sample complexity of learning compositional concepts may be greatly reduced.

Finally, hierarchical architectures have better tolerance to clutter. Clutter problem arises when the target object is in front of a non-uniform background, which functions as a distractor for the visual task. Hierarchical architecture provides signatures for parts of target objects, which do not include parts of background and are not affected by background variations.&lt;ref&gt;Liao Q., Leibo J., Mroueh Y., Poggio T. (2014) ''Can a biologically-plausible hierarchy effectively replace face detection, alignment, and recognition pipelines?'' CBMM Memo No. 003, Massachusetts Institute of Technology, Cambridge, MA&lt;/ref&gt;

In hierarchical architectures, one layer is not necessarily invariant to all transformations that are handled by the hierarchy as a whole. Some transformations may pass through that layer to upper layers, as in the case of non-group transformations described in the previous section. For other transformations, an element of the layer may produce invariant representations only within small range of transformations. For instance, elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation. For such transformations, the layer should provide ''covariant'' rather than invariant, signatures. The property of covariance can be written as &lt;math&gt;distr(\langle \mu_l(gI),\mu_l(t)\rangle) = distr(\langle \mu_l(I),\mu_l(g^{-1}t)\rangle)&lt;/math&gt;, where &lt;math&gt;l&lt;/math&gt; is a layer, &lt;math&gt;\mu_l(I)&lt;/math&gt; is the signature of image on that layer, and &lt;math&gt;distr&lt;/math&gt; stands for &quot;distribution of values of the expression for all &lt;math&gt;g \in G&lt;/math&gt;&quot;.

==Relation to biology==

M-theory is based on a quantitative theory of the ventral stream of visual cortex.&lt;ref&gt;M. Riesenhuber and T. Poggio ''Hierarchical Models of Object Recognition in Cortex'' (1999) Nature Neuroscience, vol. 2, no. 11, pp. 1019-1025, 1999.&lt;/ref&gt;&lt;ref&gt;T. Serre, M. Kouh, C. Cadieu, U. Knoblich, G. Kreiman, and T. Poggio (2005) ''A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex'' AI Memo 2005-036/CBCL Memo 259, Massachusetts Inst. of Technology, Cambridge.&lt;/ref&gt; Understanding how visual cortex works in object recognition is still a challenging task for neuroscience. Humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any state-of-the art machine vision systems that usually require a lot of data in order to recognize objects. Prior to the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms (e.g.,&lt;ref name=&quot;HW&quot;&gt;D.H. Hubel and T.N. Wiesel (1962) ''Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex'' The Journal of Physiology 160.&lt;/ref&gt;) and to justify the use of DoG (derivative-of-Gaussian) filters and more recently of Gabor filters.&lt;ref&gt;D. Gabor (1946) ''Theory of Communication'' J. IEE, vol. 93, pp. 429-459.&lt;/ref&gt;&lt;ref&gt;J.P. Jones and L.A. Palmer (1987) ''An Evaluation of the Two-Dimensional Gabor Filter Model of Simple Receptive Fields in Cat Striate Cortex'' J. Neurophysiol., vol. 58, pp. 1233-1258.&lt;/ref&gt; No real attention has been given to biologically plausible features of higher complexity. While mainstream computer vision has always been inspired and challenged by human vision, it seems to have never advanced past the very first stages of processing in the simple cells in V1 and V2. Although some of the systems inspired - to various degrees - by neuroscience, have been tested on at least some natural images, neurobiological models of object recognition in cortex have not yet been extended to deal with real-world image databases.&lt;ref name=&quot;Robust_Obj_Recog&quot; &gt;Thomas Serre, Lior Wolf, Stanley Bileschi, Maximilian Riesenhuber, and Tomaso Poggio (2007) ''Robust Object Recognition with Cortex-Like Mechanisms'' IEEE Transactions on pattern analysis and machine intelligence, VOL. 29, NO. 3&lt;/ref&gt;

M-theory learning framework employs a novel hypothesis about the main computational function of the ventral stream: the representation of new objects/images in terms of a signature, which is invariant to transformations learned during visual experience. This allows recognition from very few labeled examples - in the limit, just one.

Neuroscience	suggests that natural functionals for a neuron to compute is a high-dimensional dot product between an “image patch” and	another image	patch (called template)
which is stored in terms of synaptic weights (synapses per neuron). The standard computational model of a neuron is based on a dot product and a threshold. Another important feature of the visual cortex is that it consists of simple and complex cells. This idea was originally proposed by Hubel and Wiesel.&lt;ref name=&quot;HW&quot;/&gt; M-theory employs this idea. Simple cells compute dot products of an image and transformations of templates &lt;math&gt;\langle I,g_it^k\rangle&lt;/math&gt; for &lt;math&gt;i = 1,...,|G|&lt;/math&gt; (&lt;math&gt;|G|&lt;/math&gt; is a number of simple cells). Complex cells are responsible for pooling and computing empirical histograms or statistical moments of it. The following formula for constructing histogram can be computed by neurons:

&lt;math&gt;\frac{1}{|G|}\sum_{i=1}^{|G|}\sigma(\langle I,g_it^k\rangle+n\Delta),&lt;/math&gt;

where &lt;math&gt;\sigma&lt;/math&gt; is a smooth version of step function, &lt;math&gt;\Delta&lt;/math&gt; is the width of a histogram bin, and &lt;math&gt;n&lt;/math&gt; is the number of the bin.

==Applications==

===Applications to computer vision===
In {{clarify|date=December 2014}}&lt;ref&gt;Qianli Liao, Joel Z Leibo, Youssef Mroueh, Tomaso Poggio (2014) ''Can a biologically-plausible hierarchy effectively replace face detection, alignment, and recognition pipelines?'' CBMM Memo No. 003&lt;/ref&gt;&lt;ref&gt;Qianli Liao, Joel Z Leibo, and Tomaso Poggio (2014) ''Learning invariant representations and applications to face verification'' NIPS 2014&lt;/ref&gt; authors applied M-theory to unconstrained face recognition in natural photographs. Unlike the DAR (detection, alignment, and recognition) method, which handles clutter by detecting objects and cropping closely around them so that very little background remains, this approach accomplishes detection and alignment implicitly by storing transformations of training images (templates) rather than explicitly detecting and aligning or cropping faces at test time. This system is built according to the principles of a recent theory of invariance in hierarchical networks and can evade the clutter problem generally problematic for feedforward systems.
The resulting end-to-end system achieves a drastic improvement in the state of the art on this end-to-end task, reaching the same level of performance as the best systems operating on aligned, closely cropped images (no outside training data). It also performs well on two newer datasets, similar to LFW, but more difficult: significantly jittered (misaligned) version of LFW and SUFR-W (for example, the model’s accuracy in the LFW “unaligned &amp; no outside data used” category is 87.55±1.41% compared to state-of-the-art APEM (adaptive probabilistic elastic matching): 81.70±1.78%).

The theory was also applied to a range of recognition tasks: from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (CalTech5, CalTech101, MIT-CBCL) and complex (street) scene understanding tasks that requires the recognition of both shape-based as well as texture-based objects (on 	StreetScenes data set).&lt;ref name=&quot;Robust_Obj_Recog&quot;/&gt; The approach performs really well: It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems  constellation models, the hierarchical SVM-based face- detection system). A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance.

===Applications to speech recognition===

This theory can also be extended for the speech recognition domain.
As an example, in&lt;ref&gt;Georgios Evangelopoulos, Stephen Voinea, Chiyuan Zhang, Lorenzo Rosasco, Tomaso Poggio (2014) ''Learning An Invariant Speech Representation'' CBMM Memo No. 022&lt;/ref&gt; an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.&lt;ref&gt;https://catalog.ldc.upenn.edu/LDC93S1&lt;/ref&gt;

==References==
{{Reflist}}



</text>
      <sha1>fqt4rojfjwwgqse7x2mcsuf5uf4cv4p</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Machine learning portal</title>
    <ns>14</ns>
    <id>44943481</id>
    <revision>
      <id>640998008</id>
      <timestamp>2015-01-04T22:00:36Z</timestamp>
      <contributor>
        <username>Masatran</username>
        <id>364611</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with ';Note: This portal navigation can be placed on articles, and is commonly placed in the '''[[WP:ALSO|See also]]''' section of an article on Wikipedia, using the f...'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="315">;Note:
This portal navigation can be placed on articles, and is commonly placed in the '''[[WP:ALSO|See also]]''' section of an article on Wikipedia, using the following code:
{|
|-
|&lt;nowiki&gt;{{Portal|Machine learning}}&lt;/nowiki&gt;
|-
|This displays as:
|-
|{{Portal|Machine learning}}
|}

</text>
      <sha1>ne39244te507rkwkcpdksnwy3vz5auv</sha1>
    </revision>
  </page>
  <page>
    <title>Instantaneously trained neural networks</title>
    <ns>0</ns>
    <id>470314</id>
    <revision>
      <id>815921057</id>
      <parentid>815909147</parentid>
      <timestamp>2017-12-18T02:14:09Z</timestamp>
      <contributor>
        <username>Arlene47</username>
        <id>19898113</id>
      </contributor>
      <minor/>
      <comment>Added link to paper and name</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4188">'''Instantaneously trained neural networks''' are feedforward [[artificial neural networks]] proposed by [[Subhash Kak]] that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization.&lt;ref&gt;Kak, S. On training feedforward neural networks. Pramana, vol. 40, pp. 35-42, 1993 [https://link.springer.com/article/10.1007/BF02898040] &lt;/ref&gt;&lt;ref&gt;Kak, S. New algorithms for training feedforward neural networks. Pattern Recognition Letters 15: 295-298, 1994.&lt;/ref&gt; This separation is done using the nearest hyperplane that can be written down instantaneously. In the two most important implementations the neighborhood of generalization either varies with the training sample (CC1 network) or remains constant (CC4 network)   These networks use [[unary coding]] for an effective representation of the data sets.&lt;ref&gt;Kak, S. On generalization by neural networks, Information Sciences 111: 293-302, 1998.&lt;/ref&gt;

Instantaneously trained neural networks have been proposed as models of short term [[learning]] and used in [[web search]], and financial [[time series prediction]] applications.&lt;ref&gt;Kak, S. Faster web search and prediction using instantaneously trained neural networks. IEEE Intelligent Systems 14: 79-82, November/December 1999.&lt;/ref&gt; They have also been used in instant [[document classification|classification of documents]]&lt;ref&gt;Zhang, Z. et al., TextCC: New feedforward neural network for classifying documents instantly. Advances in Neural Networks ISNN 2005. [[Lecture Notes in Computer Science]] 3497: 232-237, 2005.&lt;/ref&gt; and for [[deep learning]] and [[data mining]].&lt;ref&gt;Zhang, Z. et al., Document Classification Via TextCC Based on Stereographic Projection and for deep learning, International Conference on Machine Learning and Cybernetics, Dalin, 2006&lt;/ref&gt;&lt;ref&gt;Schmidhuber, J. Deep Learning in Neural Networks: An Overview, arXiv:1404.7828, 2014 http://arxiv.org/abs/1404.7828&lt;/ref&gt;

As in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs&lt;ref&gt;Zhu, J. and G. Milne, Implementing Kak Neural Networks on a Reconfigurable Computing Platform, Lecture Notes in Computer Science Volume 1896: 260-269, 2000.&lt;/ref&gt; and by optical implementation.&lt;ref&gt;Shortt, A., J.G. Keating, L. Moulinier, C.N. Pannell, Optical implementation of the Kak neural network, Information Sciences 171: 273-287, 2005.&lt;/ref&gt;

==CC4 network==

In the CC4 network, which is a three-stage network, the number of input nodes is one more than the size of the training vector, with the extra node serving as the biasing node whose input is always 1. For binary input vectors, the weights from the input nodes to the hidden neuron (say of index j) corresponding to the trained vector is given by the following formula:
:&lt;math&gt;w_{ij} = \begin{cases}
   -1, &amp; \mbox{for } x_i = 0\\
                         +1, &amp; \mbox{for } x_i = 1\\
  r+s-1, &amp; \mbox{for } i = n+1
\end{cases}&lt;/math&gt;

where &lt;math&gt;r &lt;/math&gt; is the radius of generalization and &lt;math&gt;s &lt;/math&gt; is the [[Hamming weight]] (the number of 1s) of the binary sequence. From the hidden layer to the output layer the weights are 1 or -1 depending on whether the vector belongs to a given output class or not. The neurons in the hidden and output layers output 1 if the weighted sum to the input is 0 or positive and 0, if the weighted sum to the input is negative:

:&lt;math&gt;y = \left\{ \begin{matrix} 1 &amp; \mbox{if } \sum x_i \ge 0\\ 0 &amp; \mbox{if } \sum x_i&lt; 0\end{matrix} \right.&lt;/math&gt;
==Other networks==

The CC4 network has also been modified to include non-binary input with varying radii of generalization so that it effectively provides a CC1 implementation.&lt;ref&gt;Tang, K.W. and Kak, S. Fast classification networks for signal processing. ''Circuits, Systems, Signal Processing'' 21, 2002, pp. 207-224.&lt;/ref&gt;

In feedback networks the Willshaw network as well as the [[Hopfield network]] are able to learn instantaneously.

==References==
{{Reflist}}



</text>
      <sha1>dw7y2dzprmbsocgioy73609w19v583p</sha1>
    </revision>
  </page>
  <page>
    <title>Adversarial machine learning</title>
    <ns>0</ns>
    <id>45049676</id>
    <revision>
      <id>813159039</id>
      <parentid>812401462</parentid>
      <timestamp>2017-12-02T03:52:54Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>[[User:JCW-CleanerBot#Logic|task]], replaced: Proceedings of The  → Proceedings of the using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28881">{{Distinguish|Generative adversarial network}}

'''Adversarial machine learning''' is a research field that lies at the intersection of [[machine learning]] and [[computer security]]. It aims to enable the safe adoption of machine learning techniques in adversarial settings like [[spam filtering]], malware detection and [[Biometrics|biometric recognition]].

The problem arises from the fact that machine learning techniques were originally designed for stationary environments in which the training and test data are assumed to be generated from the same (although possibly unknown) distribution. In the presence of intelligent and adaptive adversaries, however, this working hypothesis is likely to be violated to at least some degree (depending on the adversary). In fact, a malicious adversary can carefully manipulate the input data exploiting specific vulnerabilities of learning algorithms to compromise the whole system security.

Examples include: attacks in spam filtering, where spam messages are obfuscated through misspelling of bad words or insertion of good words;&lt;ref name=&quot;Adversarial Machine Learning_21A&quot;&gt;N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. “Adversarial classification”. In Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 99–108, Seattle, 2004.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_33A&quot;&gt;D. Lowd and C. Meek. “[http://research.microsoft.com/pubs/73510/kdd05lowd.pdf Adversarial learning]”. In A. Press, editor, Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 641–647, Chicago, IL., 2005.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_7A&quot;&gt;B. Biggio, I. Corona, G. Fumera, G. Giacinto, and F. Roli. “[http://pralab.diee.unica.it/en/node/736 Bagging classifiers for fighting poisoning attacks in adversarial classification tasks]”. In C. Sansone, J. Kittler, and F. Roli, editors, 10th International Workshop on Multiple Classifier Systems (MCS), volume 6713 of Lecture Notes in Computer Science, pages 350–359. Springer-Verlag, 2011.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_9A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/770 Adversarial pattern classification using multiple classifiers and randomisation]”. In 12th Joint IAPR International Workshop on Structural and Syntactic Pattern Recognition (SSPR 2008), volume 5342 of Lecture Notes in Computer Science, pages 500–509, Orlando, Florida, USA, 2008. Springer-Verlag.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_12A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/671 Multiple classifier systems for robust classifier design in adversarial environments]”. International Journal of Machine Learning and Cybernetics, 1(1):27–41, 2010.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_18A&quot;&gt;M. Bruckner, C. Kanzow, and T. Scheffer. “Static prediction games for adversarial learning problems”. J. Mach. Learn. Res., 13:2617–2654, 2012.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_19A&quot;&gt;M. Bruckner and T. Scheffer. “Nash equilibria of static prediction games”. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 171–179. 2009.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_20A&quot;&gt;M. Bruckner and T. Scheffer. &quot;Stackelberg games for adversarial prediction problems&quot;. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 547–555, New York, NY, USA, 2011. ACM.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_24A&quot;&gt;A. Globerson and S. T. Roweis. “Nightmare at test time: robust learning by feature deletion”. In W. W. Cohen and A. Moore, editors, Proceedings of the 23rd International Conference on Machine Learning, volume 148, pages 353–360. ACM, 2006.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_30A&quot;&gt;A. Kolcz and C. H. Teo. “Feature weighting for improved classifier robustness”. In Sixth Conference on Email and Anti-Spam (CEAS), Mountain View, CA, USA, 2009.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_34A&quot;&gt;B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia. “Exploiting machine learning to subvert your spam filter”. In LEET’08: Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats, pages 1–9, Berkeley, CA, USA, 2008. USENIX Association.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_43A&quot;&gt;G. L. Wittel and S. F. Wu. “On attacking statistical spam filters”. In First Conference on Email and Anti-Spam (CEAS), Microsoft Research Silicon Valley, Mountain View, California, 2004.&lt;/ref&gt; attacks in computer security, e.g., to obfuscate malware code within network packets &lt;ref name=&quot;Adversarial Machine Learning_23A&quot;&gt;P. Fogla, M. Sharif, R. Perdisci, O. Kolesnikov, and W. Lee. Polymorphic blending attacks. In USENIX- SS’06: Proc. of the 15th Conf. on USENIX Security Symp., CA, USA, 2006. USENIX Association.&lt;/ref&gt; or mislead signature detection;&lt;ref name=&quot;Adversarial Machine Learning_37A&quot;&gt;J. Newsome, B. Karp, and D. Song. [http://repository.cmu.edu/cgi/viewcontent.cgi?article=1026&amp;context=ece Paragraph: Thwarting signature learning by training maliciously]. In Recent Advances in Intrusion Detection, LNCS, pages 81–105. Springer, 2006.&lt;/ref&gt; attacks in biometric recognition, where fake biometric traits may be  exploited to impersonate a legitimate user (biometric spoofing) &lt;ref name=&quot;Adversarial Machine Learning_44A&quot;&gt;R. N. Rodrigues, L. L. Ling, and V. Govindaraju. &quot;Robustness of multimodal biometric fusion methods against spoof attacks&quot;. J. Vis. Lang. Comput., 20(3):169–179, 2009.&lt;/ref&gt; or to compromise users’ template galleries that are adaptively updated over time.&lt;ref name=&quot;Adversarial Machine Learning_6A&quot;&gt;B. Biggio, L. Didaci, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/715 Poisoning attacks to compromise face templates]”. In 6th IAPR Int’l Conf. on Biometrics (ICB 2013), pages 1–7, Madrid, Spain, 2013.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_113A&quot;&gt;M. Torkamani and D. Lowd “[http://jmlr.csail.mit.edu/proceedings/papers/v28/torkamani13.pdf Convex Adversarial Collective Classification]”. In Proceedings of the 30th International Conference on Machine Learning (pp. 642-650), Atlanta, GA., 2013.&lt;/ref&gt;

==Security evaluation==
[[File:Reactive arms race.jpg|thumb|Conceptual representation of the reactive arms race.&lt;ref name=&quot;Adversarial Machine Learning_4A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/657 Security evaluation of pattern classifiers under attack]”. IEEE Transactions on Knowledge and Data Engineering, 26(4):984–996, 2014.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_46A&quot; /&gt;]]
To understand the security properties of learning algorithms in adversarial settings, one should address the following main issues:&lt;ref name=&quot;Adversarial Machine Learning_4A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot;&gt;B. Biggio, I. Corona, B. Nelson, B. Rubinstein, D. Maiorca, G. Fumera, G. Giacinto, and F. Roli. “[http://pralab.diee.unica.it/en/node/1047 Security evaluation of support vector machines in adversarial environments]”. In Y. Ma and G. Guo, editors, Support Vector Machines Applications, pp. 105–153. Springer, 2014.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_46A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/1103 Pattern recognition systems under attack: Design issues and research challenges]”. Int’l J. Patt. Recogn. Artif. Intell., 28(7):1460002, 2014.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_26A&quot;&gt;L. Huang, A. D. Joseph, B. Nelson, B. Rubinstein, and J. D. Tygar. “Adversarial machine learning”. In 4th ACM Workshop on Artificial Intelligence and Security (AISec 2011), pages 43–57, Chicago, IL, USA, October 2011.&lt;/ref&gt;
* identifying potential vulnerabilities of machine learning algorithms during learning and classification;
* devising appropriate attacks that correspond to the identified threats and evaluating their impact on the targeted system;
* proposing countermeasures to improve the security of machine learning algorithms against the considered attacks.

This process amounts to simulating a proactive arms race (instead of a reactive one, as depicted in Figures 1 and 2), where system designers try to anticipate the adversary in order to understand whether there are potential vulnerabilities that should be fixed in advance; for instance, by means of specific countermeasures such as additional features or different learning algorithms. However proactive approaches are not necessarily superior to reactive ones. For instance, in,&lt;ref name=&quot;Adversarial Machine Learning_3A&quot;&gt;A. Barth, B. I. P. Rubinstein, M. Sundararajan, J. C. Mitchell, D. Song, and P. L. Bartlett. &quot;A learning-based approach to reactive security. IEEE Transactions on Dependable and Secure Computing&quot;, 9(4):482–493, 2012.&lt;/ref&gt; the authors showed that under some circumstances, reactive approaches are more suitable for improving system security.
[[File:Proactive arms race.jpg|thumb|Conceptual representation of the proactive arms race.&lt;ref name=&quot;Adversarial Machine Learning_4A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_46A&quot; /&gt;]]

==Attacks against machine learning algorithms (supervised)==
The first step of the above-sketched arms race is identifying potential attacks against machine learning algorithms. A substantial amount of work has been done in this direction.&lt;ref name=&quot;Adversarial Machine Learning_4A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_46A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_26A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning definition&quot;&gt;M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be secure? In ASIACCS ’06: Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pages 16–25, New York, NY, USA, 2006. ACM&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_2&quot;&gt;M. Barreno, B. Nelson, A. Joseph, and J. Tygar. “The security of machine learning”. Machine Learning, 81:121–148, 2010&lt;/ref&gt;&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|last=Vorobeychik|first=Yevgeniy|last2=Li|first2=Bo|date=2014-01-01|title=Optimal Randomized Classification in Adversarial Settings|url=http://dl.acm.org/citation.cfm?id=2615731.2615811|journal=Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems|series=AAMAS '14|location=Richland, SC|publisher=International Foundation for Autonomous Agents and Multiagent Systems|pages=485–492|isbn=9781450327381}}&lt;/ref&gt;&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|last=Li|first=Bo|last2=Vorobeychik|first2=Yevgeniy|date=2014-01-01|title=Feature Cross-substitution in Adversarial Classification|url=http://dl.acm.org/citation.cfm?id=2969033.2969060|journal=Proceedings of the 27th International Conference on Neural Information Processing Systems|series=NIPS'14|location=Cambridge, MA, USA|publisher=MIT Press|pages=2087–2095}}&lt;/ref&gt;&lt;ref name=&quot;:2&quot;&gt;{{Cite book|url=http://papers.nips.cc/paper/6142-data-poisoning-attacks-on-factorization-based-collaborative-filtering.pdf|title=Advances in Neural Information Processing Systems 29|last=Li|first=Bo|last2=Wang|first2=Yining|last3=Singh|first3=Aarti|last4=Vorobeychik|first4=Yevgeniy|date=2016-01-01|publisher=Curran Associates, Inc.|editor-last=Lee|editor-first=D. D.|pages=1885–1893|editor-last2=Sugiyama|editor-first2=M.|editor-last3=Luxburg|editor-first3=U. V.|editor-last4=Guyon|editor-first4=I.|editor-last5=Garnett|editor-first5=R.}}&lt;/ref&gt;&lt;ref name=&quot;:3&quot;&gt;{{Cite journal|last=Bo|first=Li,|last2=Yevgeniy|first2=Vorobeychik,|last3=Xinyun|first3=Chen,|date=2016-04-09|title=A General Retraining Framework for Scalable Adversarial Classification|url=https://arxiv.org/abs/1604.02606|language=en}}&lt;/ref&gt;

===A taxonomy of potential attacks against machine learning===
Attacks against (supervised) machine learning algorithms have been categorized along three primary axes:&lt;ref name=&quot;Adversarial Machine Learning_26A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning definition&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_2&quot; /&gt; their ''influence'' on the classifier, the ''security violation'' they cause, and their ''specificity''.

* ''Attack influence''. It can be '''causative''', if the attack aims to introduce vulnerabilities (to be exploited at classification phase) by manipulating training data; or '''exploratory''', if the attack aims to find and subsequently exploit vulnerabilities at classification phase.
* ''Security violation''. It can be an '''integrity''' violation, if it aims to get malicious samples misclassified as legitimate; or an  '''availability''' violation, if the goal is to increase the misclassification rate of legitimate samples, making the classifier unusable (e.g., a denial of service).
* ''Attack specificity''. It can be '''targeted''', if specific samples are considered (e.g., the adversary aims to allow a specific intrusion or she wants a given spam email to get past the filter); or '''indiscriminate'''.

This taxonomy has been extended into a more comprehensive threat model that allows one to make explicit assumptions on the adversary’s goal, knowledge of the attacked system, capability of manipulating the input data and/or the system components, and on the corresponding (potentially, formally-defined) attack strategy. Details can be found here.&lt;ref name=&quot;Adversarial Machine Learning_4A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt; Two of the main attack scenarios identified according to this threat model are sketched below.

===Evasion attacks===
Evasion attacks &lt;ref name=&quot;Adversarial Machine Learning_4A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt;&lt;ref name=&quot;:0&quot; /&gt;&lt;ref name=&quot;:1&quot; /&gt;&lt;ref name=&quot;:3&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_8A&quot;&gt;B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G. Giacinto, and F. Roli. “[http://pralab.diee.unica.it/en/node/950 Evasion attacks against machine learning at test time]”. In H. Blockeel, K. Kersting, S. Nijssen, and F. Zelezny, editors, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Part III, volume 8190 of Lecture Notes in Computer Science, pages 387– 402. Springer Berlin Heidelberg, 2013.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_36A&quot;&gt;B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, S. J. Lee, S. Rao, and J. D. Tygar. &quot;Query strategies for evading convex-inducing classifiers&quot;. J. Mach. Learn. Res., 13:1293–1332, 2012&lt;/ref&gt; are the most prevalent type of attack that may be encountered in adversarial settings during system operation. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware code. In the evasion setting, malicious samples are modified at test time to evade detection; that is, to be misclassified as legitimate. No influence over the training data is assumed.
A clear example of evasion is [[Image spam|image-based spam]] in which the spam content is embedded within an attached image to evade the textual analysis performed by anti-spam filters.
Another example of evasion is given by spoofing attacks against biometric verification systems.&lt;ref name=&quot;Adversarial Machine Learning_44A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_6A&quot; /&gt;

===Poisoning attacks===
Machine learning algorithms are often re-trained on data collected during operation to adapt to changes in the underlying data distribution. For instance, intrusion detection systems (IDSs) are often re-trained on a set of samples collected during network operation. Within this scenario, an attacker may poison the training data by injecting carefully designed samples to eventually compromise the whole learning process. Poisoning may thus be regarded as an adversarial contamination of the training data. Examples of poisoning attacks against machine learning algorithms (including learning in the presence of worst-case adversarial label flips in the training data) can be found in.&lt;ref name=&quot;Adversarial Machine Learning_37A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_6A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_4A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_26A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning definition&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_2&quot; /&gt;&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_15A&quot;&gt;B. Biggio, B. Nelson, and P. Laskov. “[http://pralab.diee.unica.it/en/node/751 Support vector machines under adversarial label noise]”. In Journal of Machine Learning Research - Proc. 3rd Asian Conf. Machine Learning, volume 20, pp. 97–112, 2011.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_16A&quot;&gt;B. Biggio, B. Nelson, and P. Laskov. “[http://pralab.diee.unica.it/en/node/729 Poisoning attacks against support vector machines]”. In J. Langford and J. Pineau, editors, 29th Int’l Conf. on Machine Learning. Omnipress, 2012.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_28A&quot;&gt;M. Kloft and P. Laskov. &quot;Online anomaly detection under adversarial impact&quot;. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 405–412, 2010.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_29A&quot;&gt;M. Kloft and P. Laskov. “Security analysis of online centroid anomaly detection”. Journal of Machine Learning Research, 13:3647–3690, 2012.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_31A&quot;&gt;P. Laskov and M. Kloft. “A framework for quantitative security analysis of machine learning”. In AISec ’09: Proceedings of the 2nd ACM workshop on Security and artificial intelligence, pages 1–4, New York, NY, USA, 2009. ACM.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_39A&quot;&gt;H. X. Han Xiao and C. Eckert. “Adversarial label flips attack on support vector machines”. In 20th European Conference on Artificial Intelligence, 2012.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_40A&quot;&gt;B. I. P. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar. “[https://people.eng.unimelb.edu.au/brubinstein/papers/imc206-rubinstein.pdf Antidote: understanding and defending against poisoning of anomaly detectors]”. In Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, IMC ’09, pages 1–14, New York, NY, USA, 2009. ACM.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_35A&quot;&gt;B. Nelson, B. Biggio, and P. Laskov. &quot;[http://pralab.diee.unica.it/en/node/753 Understanding the risk factors of learning in adversarial environments]&quot;. In 4th ACM Workshop on Artificial Intelligence and Security, AISec ’11, pages 87–92, Chicago, IL, USA, October 2011&lt;/ref&gt;

==Attacks against clustering algorithms==
Clustering algorithms have been increasingly adopted in security applications to find dangerous or illicit activities. For instance, clustering of malware and computer viruses aims to identify and categorize different existing malware families, and to generate specific signatures for their detection by anti-viruses, or signature-based intrusion detection systems like Snort.
However, clustering algorithms have not been originally devised to deal with deliberate attack attempts that are designed to subvert the clustering process itself. Whether clustering can be safely adopted in such settings thus remains questionable. Preliminary work reporting some vulnerability of clustering can be found in.&lt;ref name=&quot;Adversarial Machine Learning_17A&quot;&gt;B. Biggio, I. Pillai, S. R. Bulò, D. Ariu, M. Pelillo, and F. Roli. &quot;[http://pralab.diee.unica.it/en/node/955 Is data clustering in adversarial settings secure?]&quot; In Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security, AISec ’13, pages 87–98, New York, NY, USA, 2013. ACM.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_38A&quot;&gt;J. G. Dutrisac and D. Skillicorn. “Hiding clusters in adversarial settings”. In IEEE International Conference on Intelligence and Security Informatics (ISI 2008), pages 185–187, 2008.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_42A&quot;&gt;D. B. Skillicorn. “Adversarial knowledge discovery”. IEEE Intelligent Systems, 24:54–61, 2009.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_47A&quot;&gt;B. Biggio, K. Rieck, D. Ariu, C. Wressnegger, I. Corona, G. Giacinto, and F. Roli. “[http://pralab.diee.unica.it/en/node/1121 Poisoning behavioral malware clustering]”. In Proc. 2014 Workshop on Artificial Intelligent and Security Workshop, AISec ’14, pages 27–36, New York, NY, USA, 2014. ACM.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_48A&quot;&gt;B. Biggio, S. R. Bulò, I. Pillai, M. Mura, E. Z. Mequanint, M. Pelillo, and F. Roli. “[http://pralab.diee.unica.it/en/node/1089 Poisoning complete-linkage hierarchical clustering]”. In P. Franti, G. Brown, M. Loog, F. Escolano, and M. Pelillo, editors, Joint IAPR Int’l Workshop on Structural, Syntactic, and Statistical Pattern Recognition, volume 8621 of Lecture Notes in Computer Science, pages 42–52, Joensuu, Finland, 2014. Springer Berlin Heidelberg.&lt;/ref&gt;

==Secure learning in adversarial settings==
A number of defense mechanisms against evasion, poisoning and privacy attacks have been proposed in the field of adversarial machine learning, including:

# The definition of secure learning algorithms;&lt;ref name=&quot;Adversarial Machine Learning_18A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_19A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_20A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_24A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_22A&quot;&gt;O. Dekel, O. Shamir, and L. Xiao. &quot;Learning to classify with missing and corrupted features&quot;. Machine Learning, 81:149–178, 2010.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_25A&quot;&gt;M. Grosshans, C. Sawade, M. Bruckner, and T. Scheffer. &quot;Bayesian games for adversarial regression problems&quot;. In Journal of Machine Learning Research - Proc. 30th International Conference on Machine Learning (ICML), volume 28, 2013.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_14A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/739 Design of robust classifiers for adversarial environments]”. In IEEE Int’l Conf. on Systems, Man, and Cybernetics (SMC), pages 977–982, 2011.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_45A&quot;&gt;W. Liu and S. Chawla. “Mining adversarial patterns via regularized loss minimization”. Machine Learning, 81(1):69–83, 2010.&lt;/ref&gt;
# The use of multiple classifier systems;&lt;ref name=&quot;Adversarial Machine Learning_7A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_9A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_12A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_30A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_10A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/642 Evade hard multiple classifier systems]”. In O. Okun and G. Valentini, editors, Supervised and Unsupervised Ensemble Methods and Their Applications, volume 245 of Studies in Computational Intelligence, pages 15–38. Springer Berlin / Heidelberg, 2009.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_11A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/765 Multiple classifier systems for adversarial classification tasks]”. In J. A. Benediktsson, J. Kittler, and F. Roli, editors, Proceedings of the 8th International Workshop on Multiple Classifier Systems, volume 5519 of Lecture Notes in Computer Science, pages 132–141. Springer, 2009.&lt;/ref&gt;&lt;ref name=&quot;Adversarial Machine Learning_13A&quot;&gt;B. Biggio, G. Fumera, and F. Roli. “[http://pralab.diee.unica.it/en/node/758 Multiple classifier systems under attack]”. In N. E. Gayar, J. Kittler, and F. Roli, editors, MCS, Lecture Notes in Computer Science, pages 74–83. Springer, 2010.&lt;/ref&gt;
# The use of randomization or disinformation to mislead the attacker while acquiring knowledge of the system;&lt;ref name=&quot;Adversarial Machine Learning_9A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_26A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning definition&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_2&quot; /&gt;&lt;ref&gt;Li, B and Vorobeychik, Y. [http://www.jmlr.org/proceedings/papers/v38/li15a.pdf Scalable Optimization of Randomized Operational Decisions in Adversarial Classification Settings]. AISTATS, 2015.&lt;/ref&gt;
# The study of privacy-preserving learning.&lt;ref name=&quot;Adversarial Machine Learning_5A&quot; /&gt;&lt;ref name=&quot;Adversarial Machine Learning_41A&quot;&gt;B. I. P. Rubinstein, P. L. Bartlett, L. Huang, and N. Taft. “Learning in a large function space: Privacy- preserving mechanisms for svm learning”. Journal of Privacy and Confidentiality, 4(1):65–100, 2012.&lt;/ref&gt;
# Ladder algorithm for Kaggle-style competitions.&lt;ref name=&quot;kaggle_ladder&quot;&gt;Avrim Blum, Moritz Hardt. [http://arxiv.org/abs/1502.04585 &quot;The Ladder: A Reliable Leaderboard for Machine Learning Competitions&quot;]. 2015.&lt;/ref&gt;
# Game theoretic models for adversarial machine learning and data mining.&lt;ref name=&quot;feature_select&quot;&gt;M. Kantarcioglu, B. Xi, C. Clifton. [http://www.stat.purdue.edu/~xbw/research/BoweiXi.AdversarialClassification2010.pdf &quot;Classifier Evaluation and Attribute Selection against Active Adversaries&quot;]. Data Min. Knowl. Discov., 22:291–335, January 2011.&lt;/ref&gt;&lt;ref name=&quot;ad_svm&quot;&gt;Y. Zhou, M. Kantarcioglu, B. Thuraisingham, B. Xi. [http://www.utdallas.edu/~muratk/publications/kdd2012.pdf &quot;Adversarial Support Vector Machine Learning&quot;]. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’12, pages 1059–1067, New
 York, NY, USA, 2012.&lt;/ref&gt;&lt;ref name=&quot;ad_rvm&quot;&gt;Y. Zhou, M. Kantarcioglu, B. M. Thuraisingham. [http://www.utdallas.edu/~mxk055100/publications/icdm2012-adversarial.pdf &quot;Sparse Bayesian Adversarial Learning Using Relevance Vector Machine Ensembles&quot;]. In
ICDM, pages 1206–1211, 2012.&lt;/ref&gt;&lt;ref name=&quot;nested_stackelberg&quot;&gt;Y. Zhou, M. Kantarcioglu. [http://dl.acm.org/citation.cfm?id=2990671 &quot;Modeling Adversarial Learning as Nested Stackelberg Games”]. in Advances in Knowledge Discovery and Data Mining: 20th Pacific-Asia Conference, PAKDD 2016, Auckland, New Zealand, April 19–22, 2016.&lt;/ref&gt;

==Software==
Some software libraries are available, mainly for testing purposes and research.
* [http://pralab.diee.unica.it/en/AdversariaLib AdversariaLib] (includes implementation of evasion attacks from &lt;ref name=&quot;Adversarial Machine Learning_8A&quot; /&gt;).
* [https://github.com/vu-aml/adlib AdLib].  A python library with a scikit-style interface which includes implementations of a number of published evasion attacks and defenses.
* [http://pralab.diee.unica.it/en/ALFASVMLib AlfaSVMLib]. Adversarial Label Flip Attacks against Support Vector Machines.&lt;ref name=&quot;Adversarial Machine Learning_49A&quot;&gt;H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. “[http://pralab.diee.unica.it/en/node/1104 Support vector machines under adversarial label contamination]”. Neurocomputing, Special Issue on Advances in Learning with Label Noise, In Press.&lt;/ref&gt;
* [http://pralab.diee.unica.it/en/BattistaBiggio/Code Poisoning Attacks against Support Vector Machines],&lt;ref name=&quot;Adversarial Machine Learning_16A&quot; /&gt; and [http://pralab.diee.unica.it/en/BattistaBiggio/Code Attacks against Clustering Algorithms]&lt;ref name=&quot;Adversarial Machine Learning_17A&quot; /&gt;
* [https://github.com/cchio/deep-pwning deep-pwning]  Metasploit for deep learning which currently has attacks on deep neural networks using [[TensorFlow|Tensorflow]]&lt;ref&gt;{{Cite web|url=https://github.com/cchio/deep-pwning|title=cchio/deep-pwning|website=GitHub|access-date=2016-08-08}}&lt;/ref&gt;

===Past events===
* NIPS 2007 Workshop on [https://web.archive.org/web/20120108072159/http://nips.cc/Conferences/2007/Program/event.php?ID=615 Machine Learning in Adversarial Environments for Computer Security]
* Special Issue on [https://link.springer.com/article/10.1007%2Fs10994-010-5207-6 “Machine Learning in Adversarial Environments”] in the journal of Machine Learning
* Dagsthul Perspectives Workshop on “[http://www.dagstuhl.de/en/program/calendar/semhp/?semnr=12371 Machine Learning Methods for Computer Security]” &lt;ref name=&quot;Adversarial Machine Learning_27A&quot;&gt;A. D. Joseph, P. Laskov, F. Roli, J. D. Tygar, and B. Nelson. “Machine Learning Methods for Computer Security” (Dagstuhl Perspectives Workshop 12371). Dagstuhl Manifestos, 3(1):1–30, 2013.&lt;/ref&gt;
* Workshop on [http://www.cse.chalmers.se/~aikmitr/AISec2014/Program.html Artificial Intelligence and Security], (AISec) Series

==See also==
* [[Machine learning]]
* [[Pattern Recognition]]

==References==
{{reflist|30em}}


</text>
      <sha1>ps441clujwk7atuf4u7aa3rlzrt5g7w</sha1>
    </revision>
  </page>
  <page>
    <title>Domain adaptation</title>
    <ns>0</ns>
    <id>45390860</id>
    <revision>
      <id>815993881</id>
      <parentid>805903274</parentid>
      <timestamp>2017-12-18T15:23:00Z</timestamp>
      <contributor>
        <username>Geyzou</username>
        <id>16927539</id>
      </contributor>
      <minor/>
      <comment>Uniformizing citation format</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6355">{{technical|date=February 2015}}

'''Domain Adaptation'''&lt;ref&gt;{{cite journal|last1=Bridle|first1=John S.|last2=Cox|first2=Stephen J|title=RecNorm: Simultaneous normalisation and classification applied to speech recognition|journal=Conference on Neural Information Processing Systems (NIPS)|date=1990|pages=234–240}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Ben-David|first1=Shai|last2=Blitzer|first2=John|last3=Crammer|first3=Koby|last4=Kulesza|first4=Alex|last5=Pereira|first5=Fernando|last6=Wortman Vaughan|first6=Jennifer|title=A theory of learning from different domains|journal=Machine Learning Journal|date=2010|volume=79|issue=1-2}}&lt;/ref&gt; is a field associated with [[machine learning]] and [[inductive transfer|transfer learning]].
This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common [[Anti-spam techniques|spam filtering problem]] consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution).
Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.&lt;ref&gt;{{cite journal|last1=Crammer|first1=Koby|last2=Kearns|first2=Michael|last3=Wortman|first3=Jeniifer|title=Learning from Multiple Sources|journal=Journal of Machine Learning Research|date=2008|volume=9|pages=1757-1774}}&lt;/ref&gt;

[[File:Transfer learning and domain adaptation.png|thumb|Distinction between usual machine learning setting and transfer learning, and positioning of domain adaptation.]]

== Formalization ==
Let &lt;math&gt;X&lt;/math&gt; be the input space (or description space) and let &lt;math&gt;Y&lt;/math&gt; be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) &lt;math&gt;h:X\to Y&lt;/math&gt; able to affect a label of &lt;math&gt;Y&lt;/math&gt; to an example from &lt;math&gt;X&lt;/math&gt;. This model is learned from a learning sample &lt;math&gt;S=\{(x_i,y_i)\}_{i=1}^m \in (X\times Y)^m&lt;/math&gt;.

Usually in [[supervised learning]] (without domain adaptation), we suppose that the examples &lt;math&gt;(x_i,y_i)\in S&lt;/math&gt; are drawn i.i.d. from a distribution &lt;math&gt;D_S&lt;/math&gt; of support &lt;math&gt;X\times Y&lt;/math&gt; (unknown and fixed). The objective is then to learn &lt;math&gt;h&lt;/math&gt; (from &lt;math&gt;S&lt;/math&gt;) such that it commits the least error as possible for labelling new examples coming from the distribution &lt;math&gt;D_S&lt;/math&gt;.

The main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions &lt;math&gt;D_S&lt;/math&gt; and &lt;math&gt;D_T&lt;/math&gt; on &lt;math&gt;X\times Y&lt;/math&gt;. The domain adaptation task then consists of the transfer of knowledge from the source domain &lt;math&gt;D_S&lt;/math&gt; to the target one &lt;math&gt;D_T&lt;/math&gt;. The goal is then to learn &lt;math&gt;h&lt;/math&gt; (from labeled or unlabelled samples coming from the two domains) such that it commits as little error as possible on the target domain &lt;math&gt;D_T&lt;/math&gt;.

The major issue is the following: if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain?

== The different types of domain adaptation ==
There are several contexts of domain adaptation. They differ in the informations considered for the target task.
# The '''unsupervised domain adaptation''': the learning sample contains a set of labeled source examples, a set of unlabeled source examples and an unlabeled set of target examples.
# The '''semi-supervised domain adaptation''':  in this situation, we also consider a &quot;small&quot; set of labeled target examples.
# The '''supervised domain adaptation''': all the examples considered are supposed to be labeled.

== Three algorithmic principles ==

=== Reweighting algorithms ===
The objective is to reweight the source labeled sample such that it &quot;looks like&quot; the target sample (in term of the error measure considered)&lt;ref&gt;{{cite journal|last1=Huang|first1=Jiayuan|last2=Smola|first2=Alexander J.|last3=Gretton|first3=Arthur|last4=Borgwardt|first4=Karster M.|last5=Schölkopf|first5=Bernhard|title=Correcting Sample Selection Bias by Unlabeled Data|journal=Conference on Neural Information Processing Systems (NIPS)|date=2006|pages=601-608}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Shimodaira|first1=Hidetoshi|title=Improving predictive inference under covariate shift by weighting the log-likelihood function|journal=Journal of statistical planning and inference|date=2000|pages=227-244|url=https://www.researchgate.net/profile/Hidetoshi_Shimodaira/publication/230710850_Shimodaira_H._Improving_predictive_inference_under_covariate_shift_by_weighting_the_log-likelihood_function._J._Stat._Plan._Infer._90_227-244/links/02bfe50d08c85f1996000000.pdf}}&lt;/ref&gt;

=== Iterative algorithms ===
A method for adapting consists in iteratively &quot;auto-labeling&quot; the target examples. The principle is simple:
# a model &lt;math&gt;h&lt;/math&gt; is learned from the labeled examples;
# &lt;math&gt;h&lt;/math&gt; automatically labels some target examples;
# a new model is learned from the new labeled examples.
Note that there exists other iterative approaches, but they usually need target labeled examples.

=== Search of a common representation space ===
The goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task.
This can be achieved through the use of [[Adversarial machine learning]] techniques where feature representations from samples in different domains are encouraged to be indistinguishable &lt;ref name=&quot;Domain-Adversarial Training&quot;&gt;Ganin, Yaroslav; Ustinova, Evgeniya; Ajakan, Hana; Germain, Pascal; Larochelle, Hugo; Laviolette, François; Marchand, Mario; Lempitsky, Victor (2016). [http://jmlr.org/papers/volume17/15-239/15-239.pdf &quot;Domain-Adversarial Training of Neural Networks&quot;]. Journal of Machine Learning Research, 17:1–35.&lt;/ref&gt;&lt;ref name=&quot;ADA&quot;&gt;Wulfmeier, Markus; Bewley, Alex; Posber, Ingmar (2017). &quot;Addressing Appearance Change in Outdoor Robotics with Adversarial Domain Adaptation&quot;. International Conference on Intelligent Robotics Systems (IROS).&lt;/ref&gt;.

==References==
{{Reflist}}

</text>
      <sha1>eipkc86wafn0hk7rvde532rp2yrz2k6</sha1>
    </revision>
  </page>
  <page>
    <title>Logic learning machine</title>
    <ns>0</ns>
    <id>45378845</id>
    <revision>
      <id>803869704</id>
      <parentid>763811389</parentid>
      <timestamp>2017-10-05T04:45:25Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>adding links using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5053">{{Multiple issues|
{{third-party|date=June 2015}}
{{COI|date=June 2015}}
{{Orphan|date=July 2016}}
}}

{{Machine learning bar}}

'''Logic Learning Machine (LLM)''' is a [[machine learning]] method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm,&lt;ref&gt;{{cite journal
|last=Muselli
|first=Marco
|title=Switching Neural Networks: A new connectionist model for classification|journal=WIRN 2005 and NAIS 2005, Lecture Notes on Computer Science
| year=2006
| volume=3931
| pages=23–30
| url=https://www.rulex-inc.com/site/wp-content/uploads/2014/07/lncs06b.pdf}}&lt;/ref&gt; developed by Marco Muselli, Senior Researcher at the Italian National Research Council CNR-IEIIT in [[Genoa]].
Logic Learning Machine is implemented in the [[Rulex]] suite.

LLM has been employed in different fields, including orthopaedic patient classification,&lt;ref&gt;{{cite journal
| last1=Mordenti |first1=M.
| last2=Ferrari| first2=E.
| last3=Pedrini| first3 = E.
| last4=Fabbri| first4=N.
| last5=Campanacci| first5=L.
| last6=Muselli|first6=M.
| last7=Sangiorgi| first7=L.
| title=Validation of a New Multiple Osteochondromas Classification Through Switching Neural Networks|journal=American Journal of Medical Genetics Part A
| year=2013
| volume=161
| pages=556–560| doi=10.1002/ajmg.a.35819| pmid=23401177}}&lt;/ref&gt; DNA microarray analysis &lt;ref&gt;{{cite journal
| last1=Cangelosi|first1=D.
| last2=Muselli| first2=M.
| last3=Blengio| first3 = F.
| last4=Becherini| first4=P.
| last5=Versteeg| first5=R.
| last6=Conte | first6=M.
| last7=Varesio| first7 = L.
| title=Use of Attribute Driven Incremental Discretization and Logic Learning Machine to build a prognostic classifier for neuroblastoma patients
| journal=BITS2013
| year=2013
| url=https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-S5-S4}}&lt;/ref&gt; and Clinical Decision Support System.&lt;ref&gt;{{cite journal
| last1=Parodi|first1=S.
| last2=Filiberti| first2=R.
| last3=Marroni| first3 = P.
| last4=Montani| first4=E.
| last5=Muselli| first5=M.
| title=Differential diagnosis of pleural mesothelioma using Logic Learning Machine
|journal=BITS2014
| year=2014
| url=https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-16-S9-S3}}&lt;/ref&gt;

== History ==

The Switching Neural Network approach was developed in the 1990s to overcome the drawbacks of the most commonly used machine learning methods. In particular, black box methods, such as [[multilayer perceptron]] and [[support vector machine]], had good accuracy but could not provide deep insight into the studied phenomenon. On the other hand, [[decision tree learning|decision trees]] were able to describe the phenomenon but often lacked accuracy. Switching Neural Networks made use of [[Boolean algebra]] to build sets of intelligible rules able to obtain very good performance. In 2014, an efficient version of Switching Neural Network was developed and implemented in the [[Rulex]] suite with the name Logic Learning Machine.&lt;ref&gt;{{cite web|title=Rulex: a software for knowledge extraction from data|publisher=Italian National Research Council|accessdate=7 March 2015 |url=http://www.cnr.it/istituti/FocusByN_eng.html?cds=029&amp;nfocus=7}}&lt;/ref&gt; Also a LLM version devoted to regression problems was developed.

== General ==

Like other machine learning methods, LLM uses data to build a model able to perform a good forecast about future behaviors. LLM starts from a table including a target variable (output) and some inputs and generates a set of rules that return the output value &lt;math&gt;y&lt;/math&gt; corresponding to a given configuration of inputs. A rule is written in the form:

:&lt;math&gt; \textbf{if }  \text{  } premise \text{  } \textbf{ then } \text{  } consequence&lt;/math&gt;

where ''consequence'' contains the output value whereas ''premise'' includes one or more conditions on the inputs. According to the input type, conditions can have different forms:
* for [[categorical variable]]s the input value must be in a given subset :&lt;math&gt;x_1 \in \{A,B,C,...\}&lt;/math&gt;.
* for [[real number|ordered variables]] the condition is written as an inequality or an interval: &lt;math&gt;x_2 \leq \alpha&lt;/math&gt; or &lt;math&gt;\beta\leq x_3\leq \gamma&lt;/math&gt;

A possible rule is therefore in the form

:&lt;math&gt; \textbf{if }  \text{  } x_1 \in \{A,B,C,...\} \text{  AND  } x_2 \leq \alpha \text{  AND  } \beta\leq x_3\leq \gamma \text{  } \textbf{ then } \text{  } y = \bar{y}&lt;/math&gt;

== Types ==

According to the output type, different versions of Logic Learning Machine have been developed:
* Logic Learning Machine for classification, when the output is a [[categorical variable]], which can assume values in a finite set
* Logic Learning Machine for regression, when the output is an [[integer]] or [[real number]].

== References ==
{{Reflist}}

== External links ==
*[http://www.rulex-inc.com/site/ Rulex] official site

&lt;!--- Categories ---&gt;
[[Category:Machine learning|Machine learning]]

</text>
      <sha1>q3gr4ehhit1b58r49tew6etvdeypwbn</sha1>
    </revision>
  </page>
  <page>
    <title>Native-language identification</title>
    <ns>0</ns>
    <id>45627703</id>
    <revision>
      <id>793044058</id>
      <parentid>717300630</parentid>
      <timestamp>2017-07-30T07:48:06Z</timestamp>
      <contributor>
        <username>Prof.radu</username>
        <id>31616295</id>
      </contributor>
      <comment>/* Methodology */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5527">'''Native-language identification''' ('''NLI''') is the task of determining an author's [[first language|native language]] (L1) based only on their writings in a [[second language]] (L2).&lt;ref&gt;Wong, Sze-Meng Jojo, and Mark Dras. [http://anthology.aclweb.org/D/D11/D11-1148.pdf &quot;Exploiting parse structures for native language identification&quot;]. Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011.&lt;/ref&gt;
NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts.
This is motivated in part by applications in [[second-language acquisition]], language teaching and [[forensic linguistics]], amongst others.

== Overview ==
NLI works under the assumption that an author's L1 will dispose them towards particular language production patterns in their L2, as influenced by their native language. This relates to cross-linguistic influence (CLI), a key topic in the field of second-language acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages.

Using large-scale English data, NLI methods achieve over 80% accuracy in predicting the native language of texts written by authors from 11 different L1 backgrounds. This can be compared to a baseline of 9% for choosing randomly.

==Applications==

===Pedagogy and language transfer===
This identification of L1-specific features has been used to study [[language transfer]] effects in second-language acquisition.&lt;ref&gt;Malmasi, Shervin, and Mark Dras. [http://www.aclweb.org/anthology/D/D14/D14-1144.pdf &quot;Language Transfer Hypotheses with Linear SVM Weights.&quot;] Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.&lt;/ref&gt; This is useful for developing pedagogical material, teaching methods, L1-specific instructions and generating learner feedback that is tailored to their native language.

===Forensic linguistics===
NLI methods can also be applied in [[forensic linguistics]] as a method of performing authorship profiling in order to infer the attributes of an author, including their linguistic background.
This is particularly useful in situations where a text, e.g. an anonymous letter, is the key piece of evidence in an investigation and clues about the native language of a writer can help investigators in identifying the source.
This has already attracted interest and funding from intelligence agencies.&lt;ref&gt;Ria Perkins. 2014. &quot;Linguistic identifiers of L1 Persian speakers writing in English: NLID for authorship analysis&quot;. Ph.D. thesis, Aston University.&lt;/ref&gt;

== Methodology ==

[[Natural language processing]] methods are used to extract and identify language usage patterns common to speakers of an L1-group. This is done using language learner data, usually from a [[learner corpus]]. Next, [[machine learning]] is applied to train classifiers, like [[support vector machine]]s, for predicting the L1 of unseen texts.&lt;ref&gt;Tetreault et al, [http://anthology.aclweb.org/C/C12/C12-1158.pdf &quot;Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification&quot;], In Proc. International Conf. on Computational Linguistics (COLING), 2012&lt;/ref&gt;
A range of ensemble based systems have also been applied to the task and shown to improve performance over single classifier systems.&lt;ref&gt;Malmasi, Shervin, Sze-Meng Jojo Wong, and Mark Dras. [http://anthology.aclweb.org/W/W13/W13-1716.pdf &quot;NLI Shared Task 2013: MQ submission&quot;]. Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications. 2013.&lt;/ref&gt;

Various linguistic feature types have been applied for this task. These include syntactic features such as constituent parses, grammatical dependencies and part-of-speech tags.
Surface level lexical features such as character, word and lemma [[n-gram|n-grams]] have also been found to be quite useful for this task. However, it seems that character n-grams&lt;ref&gt;Radu Tudor Ionescu, Marius Popescu and Aoife Cahill. [http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00256 &quot;String Kernels for Native Language Identification: Insights from Behind the Curtains&quot;], Computational Linguistics, 2016&lt;/ref&gt;&lt;ref&gt;Radu Tudor Ionescu and Marius Popescu. [https://arxiv.org/abs/1707.08349 &quot;Can string kernels pass the test of time in Native Language Identification?&quot;], In Proceedings of BEA12, 2017.&lt;/ref&gt; are the single best feature for the task.

== 2013 shared task ==
The Building Educational Applications (BEA) workshop at [[NAACL]] 2013 hosted the inaugural NLI shared task.&lt;ref&gt;Tetreault et al, [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.5931&amp;rep=rep1&amp;type=pdf &quot;A report on the first native language identification shared task&quot;], 2013&lt;/ref&gt; The competition resulted in 29 entries from teams across the globe, 24 of which also published a paper describing their systems and approaches.

==See also==
{{div col|cols=3}}
*[[Crosslinguistic influence]]
*[[Foreign language writing aid]]
*[[Computer-assisted language learning]]
*[[Language education]]
*[[Natural language processing]]
*[[Language transfer]]
{{div col end}}

==References==
{{reflist}}

{{DEFAULTSORT:Natural Language Processing}}

[[Category:Second-language acquisition]]



</text>
      <sha1>qurfbzj50xto2pty2m22lpkuwmpemd1</sha1>
    </revision>
  </page>
  <page>
    <title>Constrained conditional model</title>
    <ns>0</ns>
    <id>28255458</id>
    <revision>
      <id>795168919</id>
      <parentid>703768989</parentid>
      <timestamp>2017-08-12T13:33:13Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 1 as dead. #IABot (v1.5beta)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11472">A '''constrained conditional model''' (CCM) is a [[machine learning]] and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. The constraint can be used as a way to incorporate expressive{{clarify|date=April 2013}} prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints. The framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference.

Models of this kind have recently{{when|date=March 2013}} attracted much attention{{citation needed|date=March 2013}} within the natural language processing ([[Natural Language Processing|NLP]]) community.
Formulating problems as constrained optimization problems over the output of learned models has several advantages. It allows one to focus on the modeling of problems by providing the opportunity to incorporate domain-specific knowledge as global constraints using a first order language. Using this declarative framework frees the developer from low level [[feature engineering]] while capturing the problem's domain-specific properties and guarantying exact inference. From a machine learning perspective it allows decoupling the stage of model generation (learning) from that of the constrained inference stage, thus helping to simplify the learning stage while improving the quality of the solutions. For example, in the case of generating compressed sentences, rather than simply relying on a language model to retain the most commonly used n-grams in the sentence, constraints can be used to ensure that if a modifier is kept in the compressed sentence, its subject will also be kept.

==Motivation==
Making decisions in many domains (such as natural language processing and computer vision problems) often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate, what assignments are possible. These settings are applicable not only to Structured Learning problems such as semantic role labeling, but also for cases that require making use of multiple pre-learned components, such as summarization, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain- or problem-specific constraints.

Constrained conditional models form a learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints (written, for example, using a first-order representation) as a way to support decisions in an expressive output space while maintaining modularity and tractability of training and inference. These constraints can express either hard restrictions, completely prohibiting some assignments, or soft restrictions, penalizing unlikely assignments. In most applications of this framework in NLP, following,&lt;ref&gt;Dan Roth and Wen-tau Yih, [http://l2r.cs.uiuc.edu/~danr/Papers/RothYi04.pdf &quot;A Linear Programming Formulation for Global Inference in Natural Language Tasks.&quot;] ''CoNLL'', (2004).&lt;/ref&gt; Integer Linear Programming (ILP) was used as the inference framework, although other algorithms can be used for that purpose.

==Formal Definition==
Given a set of feature functions &lt;math&gt;\{ \phi_i(x,y) \}&lt;/math&gt; and a set of constraints &lt;math&gt;\{ C_i (x,y)\}&lt;/math&gt;, defined over an input structure &lt;math&gt;x \in X&lt;/math&gt; and an output structure  &lt;math&gt; y \in Y&lt;/math&gt;, a constraint conditional model is characterized by two weight vectors, w and &lt;math&gt;\rho&lt;/math&gt;, and is defined as the solution to the following optimization problem:
:&lt;math&gt;argmax_{y} \sum_i w_i \phi_i (x,y) - \sum \rho_i C_i (x,y)&lt;/math&gt;.

Each constraint  &lt;math&gt;C_i \in C&lt;/math&gt;  is a boolean mapping indicating if the joint assignment &lt;math&gt;(x,y)&lt;/math&gt; violates a constraint, and &lt;math&gt;\rho&lt;/math&gt; is the penalty incurred for violating the constraints.  Constraints assigned an infinite penalty are known as hard constraints, and represent unfeasible assignments to the optimization problem.

==Training paradigms==

=== Learning local vs. global models ===
The objective function used by CCMs can be decomposed and learned in several ways, ranging from a complete joint training of the model along with the constraints to completely decoupling the learning and the inference stage. In the latter case, several local models are learned independently and the dependency between these models is considered only at decision time via a global decision process. The advantages of each approach are discussed in &lt;ref&gt;Vasin Punyakanok and Dan Roth and Wen-Tau Yih and Dav Zimak, [http://l2r.cs.uiuc.edu/~danr/Papers/PRYZ05.pdf  &quot;Learning and Inference over Constrained Output.&quot;]  ''IJCAI'', (2005).&lt;/ref&gt; which studies the two training paradigms: (1) local models: L+I (learning + inference) and (2) global model: IBT (Inference based training), and shows both theoretically and experimentally that while IBT (joint training) is best in the limit, under some conditions (basically, ”good” components) L+I can generalize better.

The ability of CCM to combine local models is especially beneficial in cases where joint learning is computationally intractable or when training data are not available for joint learning. This flexibility distinguishes CCM from the other learning frameworks that also combine statistical information with declarative constraints, such as [[Markov logic network]], that emphasize joint training.

=== Minimally supervised CCM ===
CCM can help reduce supervision by using domain knowledge (expressed as constraints) to drive learning. These settings were studied in
&lt;ref&gt;Ming-Wei Chang and Lev Ratinov and Dan Roth, [http://l2r.cs.uiuc.edu/~danr/Papers/ChangRaRo07.pdf &quot;Guiding Semi-Supervision with Constraint-Driven Learning.&quot;] ''ACL'', (2007).&lt;/ref&gt; and.&lt;ref&gt;Ming-Wei Chang and Lev Ratinov and Dan Roth, [http://l2r.cs.uiuc.edu/~danr/Papers/ChangRaRo08.pdf &quot;Constraints as Prior Knowledge.&quot;] ''ICML Workshop on Prior Knowledge for Text and Language Processing, (2008).&lt;/ref&gt; These works introduce semi-supervised Constraints Driven Learning
(CODL) and show that by incorporating domain knowledge the performance of the learned model improves significantly.

=== Learning over latent representations ===
CCMs have also been applied to latent learning frameworks, where the learning problem is defined over a latent representation layer. Since the notion of a ''correct representation'' is inherently ill-defined, no gold-standard labeled data regarding the representation decision is available to the learner. Identifying the correct (or optimal) learning representation is viewed as a [[structured prediction]] process and therefore modeled as a CCM.
This problem was covered in several papers, in both supervised&lt;ref&gt;Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar, [http://l2r.cs.uiuc.edu/~danr/Papers/CGRS10.pdf &quot;Discriminative Learning over Constrained Latent Representations.&quot;] NAACL, (2010).&lt;/ref&gt;  and unsupervised &lt;ref&gt;Ming-Wei Chang Dan Goldwasser Dan Roth and Yuancheng Tu, [http://l2r.cs.uiuc.edu/~danr/Papers/CGRT10.pdf &quot;Unsupervised Constraint Driven Learning For Transliteration Discovery.&quot;]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }} NAACL, (2009).&lt;/ref&gt; settings. In all cases research showed that explicitly modeling the interdependencies between representation decisions via constraints results in an improved performance.

== Integer linear programming for natural language processing applications ==
The advantages of the CCM declarative formulation and the availability of off-the-shelf solvers have led to a large variety of [[natural language processing]] tasks being formulated within the framework, including [[semantic role labeling]],&lt;ref&gt;Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zimak, [http://l2r.cs.uiuc.edu/~danr/Papers/PRYZ04.pdf &quot;Semantic Role Labeling via Integer Linear Programming Inference.&quot;] COLING, (2004).&lt;/ref&gt; syntactic parsing,&lt;ref&gt;Kenji Sagae and Yusuke Miyao and Jun’ichi Tsujii, [http://www.aclweb.org/anthology/P07-1079 &quot;HPSG Parsing with Shallow Dependency Constraints.&quot;] ACL, (2007).&lt;/ref&gt; [[coreference]] resolution,&lt;ref&gt;Pascal Denis and Jason Baldridge, [http://www.aclweb.org/anthology-new/N/N07/N07-1030.pdf &quot;Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming.&quot;] NAACL-HLT, (2007).&lt;/ref&gt; summarization,&lt;ref&gt;James Clarke and Mirella Lapata, [http://www.jair.org/media/2433/live-2433-3730-jair.ps &quot;Global Inference for Sentence Compression: An Integer Linear Programming Approach.&quot;]  Journal of Artificial Intelligence Research (JAIR), (2008).&lt;/ref&gt;&lt;ref&gt;Katja Filippova and Michael Strube, [http://www.aclweb.org/anthology-new/W/W08/W08-1105 &quot;Dependency Tree Based Sentence Compression.&quot;] ''INLG'', (2008).&lt;/ref&gt;&lt;ref&gt;Katja Filippova and Michael Strube, [http://www.aclweb.org/anthology/D08-1019 &quot;Sentence Fusion via Dependency Graph Compression.&quot;] ''EMNLP'', (2008).&lt;/ref&gt; [[transliteration]],&lt;ref&gt;Dan Goldwasser and Dan Roth, [http://l2r.cs.uiuc.edu/~danr/Papers/GoldwasserRo08a.pdf &quot;Transliteration as Constrained Optimization.&quot;] EMNLP, (2008).&lt;/ref&gt; natural language generation &lt;ref&gt;Regina Barzilay and Mirrela Lapata, [http://www.aclweb.org/anthology/N/N06/N06-1046 &quot;Aggregation via Set Partitioning for Natural Language Generation.&quot;] ''NAACL'', (2006).&lt;/ref&gt; and [[joint information]] extraction.&lt;ref&gt;Dan Roth and Wen-tau Yih, [http://l2r.cs.uiuc.edu/~danr/Papers/RothYi04.pdf  &quot;A Linear Programming Formulation for Global Inference in Natural Language Tasks.&quot;] ''CoNLL'', (2004).&lt;/ref&gt;&lt;ref&gt;Yejin Choi and Eric Breck and Claire Cardie, [http://portal.acm.org/citation.cfm?id=1610075.1610136 &quot;Joint Extraction of Entities and Relations for Opinion Recognition.&quot;] ''EMNLP'', (2006).&lt;/ref&gt;

Most of these works use an integer linear programming (ILP) solver to solve the decision problem. Although theoretically solving an Integer Linear Program is exponential in the size of the decision problem, in practice using state-of-the-art solvers and [[approximate inference]] techniques &lt;ref&gt;André F. T. Martins, Noah A. Smith, and Eric P. Xing, [http://www.cs.cmu.edu/~nasmith/papers/martins+smith+xing.acl09.pdf &quot;Concise Integer Linear Programming Formulations for Dependency Parsing .&quot;] ACL, (2009).&lt;/ref&gt; large scale problems can be solved efficiently.

The key advantage of using an ILP solver for solving the optimization problem defined by a constrained conditional model is the declarative formulation used as input for the ILP solver, consisting of a linear objective function and a set of linear constraints.

== Resources ==
* '''CCM Tutorial''' [http://l2r.cs.uiuc.edu/~danr/Talks/CCM-NAACL-12-Tutorial.pdf Predicting Structures in NLP: Constrained Conditional Models and Integer Linear Programming in NLP]

== External links==
* [http://l2r.cs.uiuc.edu/~cogcomp/wpt.php?pr_key=CCM University of Illinois Cognitive Computation Group]
* [https://web.archive.org/web/20100530234120/http://www-tsujii.is.s.u-tokyo.ac.jp/ilpnlp/ Workshop on Integer Linear Programming for Natural Language Processing, NAACL-2009]

==References==
&lt;references/&gt;

&lt;!--Categories--&gt;

</text>
      <sha1>ov2kf2i95xh0xjnople0kh0logib7ih</sha1>
    </revision>
  </page>
  <page>
    <title>Feature engineering</title>
    <ns>0</ns>
    <id>46207323</id>
    <revision>
      <id>813144530</id>
      <parentid>813144473</parentid>
      <timestamp>2017-12-02T01:39:47Z</timestamp>
      <contributor>
        <ip>198.0.156.147</ip>
      </contributor>
      <comment>/* Automated Feature Engineering */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7296">{{Machine learning bar}}
'''Feature engineering''' is the process of using domain knowledge of the data to create [[Feature (machine learning)|features]] that make [[machine learning]] algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated [[feature learning]].

Feature engineering is an informal topic, but it is considered essential in applied machine learning.

{{Quote|text=Coming up with features is difficult, time-consuming, requires expert knowledge. &quot;Applied machine learning&quot; is basically feature engineering.|sign=[[Andrew Ng]]|source=''[https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf Machine Learning and AI via Brain simulations]''&lt;ref&gt;{{cite web | title =Machine Learning and AI via Brain simulations | url =https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf  | website =Stanford University | date =  | accessdate =2017-08-03 }}&lt;/ref&gt;}}

== Features ==
A feature is an attribute or property shared by all of the independent units on which analysis or prediction is to be done. Any attribute could be a feature, as long as it is useful to the model.

The purpose of a feature, other than being an attribute, would be much easier to understand in the context of a problem. A feature is a characteristic that might help when solving the problem.&lt;ref name=&quot;:0&quot;&gt;{{Cite web|title = Discover Feature Engineering, How to Engineer Features and How to Get Good at It - Machine Learning Mastery|url = http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/|website = Machine Learning Mastery|accessdate = 2015-11-11}}&lt;/ref&gt;

== Importance of features ==
The features in your data are important to the [[Predictive modelling|predictive models]] you use and will influence the results you are going to achieve. The quality and quantity of the features will have great influence on whether the model is good or not.&lt;ref&gt;{{Cite web|title = Feature Engineering: How to transform variables and create new ones?|url = http://www.analyticsvidhya.com/blog/2015/03/feature-engineering-variable-transformation-creation/|website = Analytics Vidhya|date = 2015-03-12|accessdate = 2015-11-12}}&lt;/ref&gt;

You could say the better the features are, the better the result is. This isn't entirely true, because the results achieved also depend on the model and the data, not just the chosen features. That said, choosing the right features is still very important. Better features can produce simpler and more flexible models, and they often yield better results.&lt;ref name=&quot;:0&quot; /&gt;

{{Quote|text=The algorithms we used are very standard for [[Kaggle]]rs. […]  We spent most of our efforts in feature engineering. [...] We were also very careful to discard features likely to expose us to the risk of [[over-fitting]] our model.|sign=Xavier Conort|source=&quot;Q&amp;A with Xavier Conort&quot;&lt;ref&gt;kaggle.com,(2015).Q&amp;A with Xavier Conort,[Accessed at:]http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/|accessdate=November 2015&lt;/ref&gt;}}

{{Quote|text=…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.|sign=Pedro Domingos|source=&quot;A Few Useful Things to Know about Machine Learning&quot;&lt;ref&gt;{{Cite web|url = http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf|title = A Few Useful Things to Know about Machine Learning|date = |accessdate = 12 November 2015|website = |publisher = |last = Domingos|first = Pedro}}&lt;/ref&gt;}}

== The process of feature engineering&lt;ref&gt;{{cite web|title=Big Data: Week 3 Video 3 - Feature Engineering|url=https://www.youtube.com/watch?v=drUToKxEAUA|website=youtube.com}}&lt;/ref&gt; ==
# [[Brainstorming]] or [[Software testing|Testing]] features;
# Deciding what features to create;
# Creating features;
# Checking how the features work with your model;
# Improving your features if needed;
# Go back to brainstorming/creating more features until the work is done.

== Feature relevance&lt;ref&gt;{{Cite web|url = http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf|title = Feature Engineering|date = 2010-04-22|accessdate = 12 November 2015|website = |publisher = |last = |first = }}&lt;/ref&gt; ==
Depending on a feature it could be strongly relevant (has information that doesn't exist in any other feature), relevant, weakly relevant (some information that other features include) or irrelevant. It is important to create a lot of features. Even if some of them are irrelevant, you can't afford missing the rest. Afterwards, [[feature selection]] can be used in order to prevent overfitting.&lt;ref&gt;{{Cite web|url = http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/slides.pdf|title = Feature engineering and selection|date = |accessdate = 12 November 2015|website = |publisher = Alexandre Bouchard-Côté|last = |first = }}&lt;/ref&gt;

== Feature explosion ==
'''Feature explosion''' can be caused by feature combination or feature templates, both leading to a quick growth in the total number of features.
* Feature templates - implementing features templates instead of coding new features
* Feature combinations - combinations that cannot be represented by the linear system
There are a few solutions to help stop feature explosion such as: [[Regularization (mathematics)|regularisation]], [[kernel method]], [[feature selection]].&lt;ref&gt;{{Cite web|url = https://ufal.mff.cuni.cz/~zabokrtsky/courses/npfl104/html/feature_engineering.pdf|title = Feature engineering in Machine Learning|date = |accessdate = 12 November 2015|website = |publisher = Zdenek Zabokrtsky|last = |first = }}&lt;/ref&gt;

== Automated Feature Engineering ==
Automation of feature engineering has become emerging topic of research in academia. In 2015, researchers at MIT presented the Deep Feature Synthesis algorithm and demonstrated it's effectiveness in online data science competitions where it beat 615 of 906 human teams&lt;ref&gt;{{Cite web| url = https://news.mit.edu/2015/automating-big-data-analysis-1016| title = Automating big-data analysis}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf| title = Deep Feature Synthesis: Towards Automating Data Science Endeavors}}&lt;/ref&gt;. That worked was followed by other researchers including IBM's OneBM &lt;ref&gt;{{Cite web|url = https://arxiv.org/pdf/1706.00327.pdf| title = One button machine for automating feature engineering in relational databases}}&lt;/ref&gt; and Berkeley's ExploreKit&lt;ref&gt;{{Cite web|url = https://people.eecs.berkeley.edu/~dawnsong/papers/icdm-2016.pdf| title = ExploreKit: Automatic Feature Generation and Selection}}&lt;/ref&gt;. The researchers at IBM state that feature engineering automation &quot;helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time and cost.&quot;

==See also==
* [[Covariate]]
* [[Hashing trick]]
* [[Kernel method]]
* [[List of datasets for machine learning research]]

==References==
{{Reflist}}

</text>
      <sha1>gblek11f6g015ji0v44ii1f6sb63x1e</sha1>
    </revision>
  </page>
  <page>
    <title>Formal concept analysis</title>
    <ns>0</ns>
    <id>313845</id>
    <revision>
      <id>815399825</id>
      <parentid>815078224</parentid>
      <timestamp>2017-12-14T16:37:59Z</timestamp>
      <contributor>
        <username>Bernhard Ganter</username>
        <id>25507974</id>
      </contributor>
      <minor/>
      <comment>/* Implications */  added a WP link.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="40150">{{Expand German|Formale Begriffsanalyse|date=February 2012}}
'''Formal concept analysis''' ('''FCA''') is a [[:en:Principle|principled way]] of deriving a ''concept hierarchy'' or formal [[:en:Ontology (computer science)|ontology]] from a collection of [[:en:Mathematical object|objects]] and their [[:en:Property (philosophy)|properties]]. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a [[:en:subset|subset]] of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by [[:en:Rudolf Wille|Rudolf Wille]] in 1980, and builds on the mathematical theory of [[:en:lattice theory|lattices]] and [[:en:order theory|ordered sets]] that was developed by [[:en:Garrett Birkhoff|Garrett Birkhoff]] and others in the 1930s.

Formal concept analysis finds practical application in fields including [[:en:data mining|data mining]], [[:en:text mining|text mining]],  [[:en:machine learning|machine learning]], [[:en:knowledge management|knowledge management]], [[:en:semantic web|semantic web]], [[:en:software development|software development]], [[:en:chemistry|chemistry]] and [[:en:biology|biology]].

== Overview and history ==

The original motivation of formal concept analysis was the search for real-world meaning of mathematical order theory. One such possibility of very general nature is that data tables can be transformed into algebraic structures called ''complete lattices'', and that these can be utilized for data visualization and interpretation. Data tables that represent [[:en:binary relations|binary relations]] between objects and attributes, thus tabulating pairs of the form &quot;object ''g'' has attribute ''m'',&quot; are considered as the basic data type and are referred to as ''formal contexts''. In this theory, a ''formal concept'' is defined to be a pair (''A'', ''B''), where ''A'' is a set of objects (called the ''extent'') and ''B'' is a set of attributes (the ''intent'') such that

*the extent ''A'' consists of all objects that share the attributes in ''B'', and [[:en:dual (math)|dually]]
*the intent ''B'' consists of all attributes shared by the objects in ''A''.

In this way, formal concept analysis formalizes the [[:en:semantics|semantic]] notions of [[:en:Extension (semantics)|extension]] and [[:en:intension|intension]].

The formal concepts of any formal context can—as explained [[#Concept lattice of a context|below]]—be [[:en:partially ordered set|ordered]] in a hierarchy called more formally the context's &quot;concept lattice.&quot; The concept lattice can be graphically visualized as a &quot;line diagram&quot;, which then may be helpful for understanding the data. Often however these lattices get too large for visualization. Then the mathematical theory of formal concept analysis may be helpful, e.g., for decomposing the lattice into smaller pieces without information loss, or for embedding it into another structure which is easier to interpret.

The theory in its present form goes back to the early 1980s and a research group led by [[:en:Rudolf Wille|Rudolf Wille]], Bernhard Ganter and Peter Burmeister at the [[:en:Technische Universität Darmstadt|Technische Universität Darmstadt]]. Its basic mathematical definitions, however, were already introduced in the 1930s by [[:en:Garrett Birkhoff|Garrett Birkhoff]] as part of general lattice theory. Other previous approaches to the same idea arose from various French research groups, but the Darmstadt group normalised  the field and systematically worked out both its mathematical theory and its philosophical foundations. The latter refer in particular to [[:en:Charles S. Peirce|Charles S. Peirce]], but also to the [[:en:Port-Royal Logic|logic of Port-Royal]].

== Motivation and philosophical background ==

In his article ''Restructuring Lattice Theory'' &lt;ref name=&quot;restructuring&quot;&gt;Rudolf Wille ''Restructuring lattice theory: An approach based on hierarchies of concepts.'' Reprinted in {{cite book |editor1-last=Ferré |editor1-first=Sébastien |editor2-last=Rudolph |editor2-first=Sebastian | title= Formal Concept Analysis: 7th International Conference, ICFCA 2009 Darmstadt, Germany, May 21–24, 2009 Proceedings | page=314 |publisher=Springer Science &amp; Business Media |isbn=978-364201814-5 }}&lt;/ref&gt;
(1982) initiating formal concept analysis as a mathematical discipline, Wille starts from a discontent with the current lattice theory and pure mathematics in general: The production of theoretical results - often achieved by &quot;elaborate mental gymnastics&quot; - were impressive, but the connections between neighboring domains, even parts of a theory were getting weaker.

{{Quote |text=Restructuring lattice theory is an attempt to reinvigorate connections with our general culture by interpreting the theory as concretely as possible, and in this way to promote better communication between lattice theorists and potential users of lattice theory |author=Rudolf Wille  |source=&lt;ref name=&quot;restructuring&quot; /&gt; }}

This aim traces back to the educationalist Hartmut von Hentig, who in 1972 pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally (i.e. also without specialized knowledge) critiqueable.&lt;ref&gt;Hartmut von Hentig: ''Magier oder Magister? Über die Einheit der Wissenschaft im Verständigungsprozeß''. Klett 1972 / Suhrkamp 1974. Cited after Karl Erich Wolff: [http://www.fbmn.h-da.de/~wolff/Publikationen/Ordnung_Wille_und_Begriff.doc ''Ordnung, Wille und Begriff''], Ernst Schröder Zentrum für Begriffliche Wissensverarbeitung, Darmstadt 2003.&lt;/ref&gt; Hence, by its origins formal concept analysis aims at interdisciplinarity and democratic control of research.&lt;ref name=&quot;AttrExGeneRegProc&quot;&gt;Johannes Wollbold: ''[http://www.db-thueringen.de/servlets/DerivateServlet/Derivate-24615/Wollbold/Dissertation.pdf Attribute Exploration of Gene Regulatory Processes]''. PhD thesis, University of Jena 2011, p. 9&lt;/ref&gt;

It corrects the starting point of lattice theory during the development of [[formal logic]] in the 19th century. Then - and later in [[model theory]] - a concept as unary [[predicate (logic)|predicate]] had been reduced to its extent. Now again, the philosophy of concepts should become less abstract by considering the intent. Hence, formal concept analysis is oriented towards the categories [[extension (semantics)|extension]] and [[intension]] of [[linguistics]] and classical conceptual logic.&lt;ref name=&quot;GW&quot;&gt;Ganter, Bernhard and Wille, Rudolf: ''Formal Concept Analysis: Mathematical Foundations''. Springer, Berlin, {{ISBN|3-540-62771-5}}&lt;/ref&gt;

Formal Concept Analysis aims at the clarity of concepts according to Charles S. Peirce's [[pragmatic maxim]] by unfolding observable, elementary properties of the [[Minor premise|subsumed]] objects.&lt;ref name=&quot;AttrExGeneRegProc&quot; /&gt; In his late philosophy, Peirce assumed that logical thinking aims at perceiving [[reality]], by the triade concept, [[judgement]] and [[Consequent|conclusion]]. Mathematics is an abstraction of logic, develops patterns of [[Logical possibility|possible]] realities and therefore may support rational [[communication]]. On this background, Wille defines:

{{Quote |text=The aim and meaning of Formal Concept Analysis as mathematical theory of concepts and concept hierarchies is to support the rational communication of humans by mathematically developing appropriate conceptual structures which can be logically activated. |author=Rudolf Wille |source = &lt;ref&gt;Rudolf Wille ''Formal Concept Analysis as Mathematical Theory of Concepts and Concept Hierarchies.'' In {{cite book |editor1-last=Ganter |editor1-first=Bernhard |editor2-last=Stumme |editor2-first=Gerd |editor3-last=Wille |editor3-first=Rudolf | title=Formal Concept Analysis. Foundations and Applications |publisher=Springer Science &amp; Business Media |publication-date=2005 |isbn=978-354027891-7}}&lt;/ref&gt;}}

== Example ==
The data in the example is taken from a semantic field study, where different kinds of [[Body of water|bodies of water]] were systematically categorized by their attributes.&lt;ref&gt;{{citation/core|Surname1=Peter Rolf Lutzeier|Title=Wort und Feld: wortsemantische Fragestellungen mit besonderer Berücksichtigung des Wortfeldbegriffes: Dissertation|Series=Linguistische Arbeiten  103|Publisher=Niemeyer|PublicationPlace=Tübingen|OCLC=8205166|Date=1981|language=German|DOI=10.1515/9783111678726.fm}}&lt;/ref&gt; For the purpose here it has been simplified.

The data table represents a ''formal context'', the ''line diagram'' next to it shows its ''concept lattice''.  Formal definitions follow below.

{| class=&quot;wikitable&quot; style=&quot;text-align: center; margin-right: 2em; float: left; font-size: 80%&quot;
|+ Example for a formal context: “bodies of water”
|-
! rowspan=&quot;2&quot; colspan=&quot;2&quot;| bodies of water !! colspan=&quot;8&quot; | attributes
|-
! ''temporary'' !! ''running'' !! ''natural'' !! ''stagnant'' !! ''constant'' !! ''maritime''
|-
! rowspan=&quot;18&quot; {{verth|va=middle|objects}}
|-
! {{rh}} | [[canal]] || || X || || || X ||
|-
! {{rh}} | [[Channel (geography)|channel]] || || X || || || X ||
|-
! {{rh}} | [[lagoon]] || || || X || X || X || X
|-
! {{rh}} | [[lake]] || || || X || X || X ||
|-
! {{rh}} | [[maar]] || || || X || X || X ||
|-
! {{rh}} | [[puddle]] || X || || X || X || ||
|-
! {{rh}} | [[pond]] || || || X || X || X ||
|-
! {{rh}} | [[pool]] || || || X || X || X ||
|-
! {{rh}} | [[reservoir]] || || || || X || X ||
|-
! {{rh}} | [[river]] || || X || X || || X ||
|-
! {{rh}} | [[rivulet]] || || X || X || || X ||
|-
! {{rh}} | [[runnel]] || || X || X || || X ||
|-
! {{rh}} | [[sea]] || || || X || X || X || X
|-
! {{rh}} | [[stream]] || || X || X || || X ||
|-
! {{rh}} | [[tarn]] || || || X || X || X ||
|-
! {{rh}} | [[torrent]] || || X || X || || X ||
|-
! {{rh}} | [[trickle]] || || X || X || || X ||
|-
|}
&amp;nbsp; &lt;!-- for the diagramm, to be displayed on the same level as the table --&gt;
[[file:FCA body of water.svg|thumb|Line diagram corresponding to the formal context ''bodies of water'' on the left]]
{{Clear}}

The above line diagram consists of circles, connecting line segments, and labels. Circles represent ''formal concepts''. The lines allow to read off the subconcept-superconcept hierarchy. Each object and attribute name is used as a label
exactly once in the diagram, with objects below and attributes above concept circles. This is done in a way that an attribute can be reached from an object via an ascending path if and only if the object has the attribute.

In the diagram shown, e.g. the item ''reservoir'' has the attributes ''stagnant'' and ''perpetual'', but not the attributes ''temporary, running, natural, maritime''. Accordingly, ''puddle'' has exactly the characteristics ''temporary, stagnant'' and ''natural''.

The original formal context can be reconstructed from the labelled diagram, as well as the formal concepts. The extent of a concept consists of those objects from which an ascending path leads to the circle representing the concept. The intent consists of those attributes to which there is an ascending path from that concept circle (in the diagram).
In this diagram the concept  immediately to the left of the label ''reservoir'' has the intent ''stagnant'' and ''natural'' and the extent ''puddle, maar, lake, pond, tarn, pool, lagoon,'' and ''sea''.

== Formal contexts and concepts ==
A formal context is a triple ''K'' = (''G'', ''M'', ''I''), where ''G'' is a set of ''objects'', ''M'' is a set of ''attributes'', and ''I'' &amp;sube; ''G'' &amp;times; ''M'' is a binary relation called ''incidence'' that expresses which objects ''have'' which attributes.&lt;ref name=&quot;GW&quot; /&gt;
. For subsets ''A'' &amp;sube; ''G'' of objects and a subsets ''B'' &amp;sube; ''M'' of attributes, one defines two ''derivation operators'' as follows:

''A''' = {''m'' &amp;isin; ''M'' | &amp;forall; ''g'' &amp;isin; ''A'', ''gIm''}, and dually

''B''' = {''g'' &amp;isin; ''G'' | &amp;forall; ''m'' &amp;isin; ''B'', ''gIm''}.

Applying either derivation operator and then the other constitutes two [[:en:closure operator|closure operators]]:

''A'' &amp;nbsp; ↦ &amp;nbsp;''A''&quot;  =  (''A''')' &amp;nbsp; for ''A'' &amp;sube; G &amp;nbsp; (extent closure), and

''B'' &amp;nbsp; ↦ &amp;nbsp;''B''&quot;  =  (''B''')' &amp;nbsp; for ''B'' &amp;sube; M &amp;nbsp; (intent closure).

The derivation operators define a  [[:en:Galois connection|Galois connection]] between sets of objects and of attributes. This is why in
French a concept lattice is sometimes called a ''trellis de Galois'' (Galois lattice).

With these derivation operators, it is possible to restate the definition of the term &quot;formal concept&quot; more rigorously:
a pair (''A'',''B'') is a ''formal concept'' of a context (''G'', ''M'', ''I'') provided that:

''A'' &amp;sube; ''G'', &amp;nbsp;  ''B'' &amp;sube; ''M'', &amp;nbsp;  ''A''&amp;prime; = ''B'', &amp;nbsp;  and  &amp;nbsp;''B''&amp;prime; = ''A''.

Equivalently and more intuitively, (''A'',''B'') is a formal concept precisely when:
* every object in ''A'' has every attribute in ''B'',
* for every object in ''G'' that is not in ''A'', there is some attribute in ''B'' that the object does not have,
* for every attribute in ''M'' that is not in ''B'', there is some object in ''A'' that does not have that attribute.

For computing purposes, a formal context may be naturally represented as a 0-1-[[:en:matrix (math)|matrix]] ''K'' in which the rows correspond to the objects, the columns correspond to the attributes, and each entry ''k''&lt;sub&gt;''i'',''j''&lt;/sub&gt; equals to 1 if &quot;object ''i'' has attribute ''j''.&quot; In this matrix representation, each formal concept corresponds to a [[:en:maximal element|maximal]] submatrix (not necessarily contiguous) all of whose elements equal 1. It is however misleading to consider a formal context as ''boolean'', because the negated incidence (&quot;object ''g'' does '''not''' have attribute ''m''&quot;) is not concept forming in the same way as defined above. For this reason, the values TRUE and FALSE are usually avoided when representing formal contexts, and a symbol like &lt;math&gt;\times&lt;/math&gt; is used to express incidence.

== Concept lattice of a formal context ==
The concepts (''A''&lt;sub&gt;''i''&lt;/sub&gt;, ''B''&lt;sub&gt;''i''&lt;/sub&gt;) of a context ''K'' can be [[:en:Partial order|(partially) ordered]] by the inclusion of extents, or, equivalently, by the dual inclusion of intents. An order ≤ on the concepts is defined as follows: for any two concepts (''A''&lt;sub&gt;''1''&lt;/sub&gt;, ''B''&lt;sub&gt;''1''&lt;/sub&gt;) and (''A''&lt;sub&gt;''2''&lt;/sub&gt;, ''B''&lt;sub&gt;''2''&lt;/sub&gt;) of ''K'', we say that (''A''&lt;sub&gt;''1''&lt;/sub&gt;, ''B''&lt;sub&gt;''1''&lt;/sub&gt;) ≤ (''A''&lt;sub&gt;''2''&lt;/sub&gt;, ''B''&lt;sub&gt;''2''&lt;/sub&gt;) precisely when ''A''&lt;sub&gt;''1''&lt;/sub&gt; ⊆ ''A''&lt;sub&gt;''2''&lt;/sub&gt;. Equivalently, (''A''&lt;sub&gt;''1''&lt;/sub&gt;, ''B''&lt;sub&gt;''1''&lt;/sub&gt;) ≤ (''A''&lt;sub&gt;''2''&lt;/sub&gt;, ''B''&lt;sub&gt;''2''&lt;/sub&gt;) whenever ''B''&lt;sub&gt;''1''&lt;/sub&gt; ⊇ ''B''&lt;sub&gt;''2''&lt;/sub&gt;.

In this order, every set of formal concepts has a [[:en:join and meet|greatest common subconcept]], or meet. Its extent consists of those objects that are common to all extents of the set. [[:en:dual (math)|Dually]], every set of formal concepts has a ''least common superconcept'', the intent of which comprises all attributes which all objects of that set of concepts have.

These meet and join operations satisfy the axioms defining a [[:en:Lattice (order)|lattice]], in fact a [[:en:complete lattice|complete lattice]]. Conversely, it can be shown that every complete lattice is the concept lattice of some formal context (up to isomorphism).

== Attribute values and negation ==
Real-world data is often given in the form of an object-attribute table, where the attributes have &quot;values&quot;. Formal concept analysis handles such data by transforming them into the basic type of a (&quot;one-valued&quot;) formal context. The method is called ''conceptual scaling''.

The negation of an attribute ''m'' is an attribute &amp;not;''m'', the extent of which is just the complement of the extent of ''m'', i.e., with (&amp;not;''m'')' = G&amp;nbsp;\&amp;nbsp;m'. It is in general ''not'' assumed that negated attributes are available for concept formation. But pairs of attributes which are negations of each other often naturally occur, for example in contexts derived from conceptual scaling.

For possible negations of formal concepts see the section [[#concept algebra|concept algebras]] below.

== Implications ==
An '''[[Implication (information science)|implication]]''' ''A &amp;rarr; B'' relates two sets ''A'' and ''B'' of attributes and expresses that every object possessing each attribute from ''A'' also has each attribute from ''B''.
When  (''G'',''M'',''I'') is a formal context and  ''A'', ''B'' are subsets  of the set ''M'' of attributes (i.e., ''A,B &amp;sube; M''), then the implication ''A &amp;rarr; B''  holds if ''A&amp;prime; &amp;sube; B&amp;prime;''. For each finite formal context, the set of all valid implications has a ''canonical basis'',&lt;ref&gt;Guigues, J.L. and Duquenne, V. ''Familles minimales d'implications informatives résultant d'un tableau de données binaires.'' Mathématiques et Sciences Humaines 95 (1986): 5-18.&lt;/ref&gt; an irredundant set of implications from which all valid implications can be derived by the natural inference ([[:en:Armstrong axioms|Armstrong rules]]).
This is used in ''Attribute Exploration'', a knowledge acquisition method based on implications.&lt;ref name=&quot;GanterObiedkov&quot;&gt;
Ganter, Bernhard and Obiedkov, Sergei (2016) ''Conceptual Exploration''. Springer, {{ISBN|978-3-662-49290-1}}&lt;/ref&gt;

== Arrow relations ==
Formal concept analysis has elaborate mathematical foundations&lt;ref name=&quot;GW&quot; /&gt;, making the field versatile. As a basic example we mention the '''arrow relations''', which are simple and easy to compute, but very useful. They are defined as follows: For ''g'' &amp;isin; ''G'' and ''m'' &amp;isin; ''M'' let

''g'' ↗ ''m'' &amp;nbsp;⇔&amp;nbsp; &amp;not;''(gIm)'' and if ''m'&amp;sube;n' '' and ''m' ≠ n' '', then ''gIn'',

and dually

''g'' ↙ ''m'' &amp;nbsp;⇔&amp;nbsp; &amp;not;''(gIm)'' and if ''g'&amp;sube;h' '' and ''g' ≠ h' '', then ''hIm''.

Since only non-incident object-attribute pairs can be related, these relations can conveniently be recorded in the table representing a formal context. Many lattice properties can be read off from the arrow relations, including distributivity and several of its generalizations. They also reveal structural information and can be used for determining, e.g., the congruence relations of the lattice.

== Extensions of the theory ==
* '''Triadic concept analysis''' replaces the binary incidence relation between objects and attributes by a ternary relation between objects, attributes, and conditions. An incidence ''(g,m,c)'' then expresses that ''the object g has the attribute m under the condition c''. Although ''triadic concepts'' can be defined in analogy to the formal concepts above, the theory of the ''trilattices'' formed by them is much less developed than that of concept lattices, and seems to be difficult.&lt;ref&gt;Wille R. ''The basic theorem of triadic concept analysis''. Order 12, 149-158., 1995&lt;/ref&gt; Voutsadakis has studied the ''n''-ary case.&lt;ref name=&quot;Voutsadakis&quot;&gt;Voutsadakis G. ''Polyadic Concept Analysis''. Order, 19 (3), 295-304., 2002&lt;/ref&gt;
* '''Fuzzy concept analysis:''' Extensive work has been done on a fuzzy version of formal concept analysis.&lt;ref&gt;See http://www.glc.us.es/cla2010/slides/tutorialI_Belohlavek.pdf for a tutorial.&lt;/ref&gt;
* &lt;span id=&quot;concept algebra&quot;&gt;'''Concept  algebras:'''&lt;/span&gt; Modelling negation of formal concepts is somewhat problematic because the complement (''G''&amp;nbsp;\&amp;nbsp;''A'', ''M''&amp;nbsp;\&amp;nbsp;''B'') of a formal concept (''A'', ''B'') is in general not a concept. However, since the concept lattice is complete one can consider the join (''A'', ''B'')&lt;sup&gt;Δ&lt;/sup&gt; of all concepts (''C'', ''D'') that satisfy ''C''&amp;nbsp;⊆&amp;nbsp;''G''&amp;nbsp;\&amp;nbsp;''A''; or dually the meet (''A'', ''B'')&lt;sup&gt;𝛁&lt;/sup&gt; of all concepts satisfying ''D''&amp;nbsp;⊆&amp;nbsp;''M''&amp;nbsp;\&amp;nbsp;''B''. These two operations are known as ''weak negation'' and ''weak opposition'', respectively. This can be expressed in terms of the ''derivation operators''.  Weak negation can be written as (''A'', ''B'')&lt;sup&gt;Δ&lt;/sup&gt; = ((''G''&amp;nbsp;\&amp;nbsp;''A'')&lt;nowiki&gt;''&lt;/nowiki&gt;, (''G''&amp;nbsp;\&amp;nbsp;''A'')'), and weak opposition can be written as (''A'', ''B'')&lt;sup&gt;𝛁&lt;/sup&gt; = ((''M''&amp;nbsp;\&amp;nbsp;''B'')', (''M''&amp;nbsp;\&amp;nbsp;''B'')&lt;nowiki&gt;''&lt;/nowiki&gt;). The concept lattice equipped with the two additional operations Δ and 𝛁 is known as the ''concept algebra'' of a context. Concept algebras are a generalization of [[:en:power set|power set]]s. Weak negation on a concept lattice ''L'' is a ''weak complementation'', i.e. an [[:en:order-reversing|order-reversing]] map Δ:&amp;nbsp;''L''&amp;nbsp;→&amp;nbsp;''L'' which satisfies the axioms ''x''&lt;sup&gt;ΔΔ&lt;/sup&gt;&amp;nbsp;≤&amp;nbsp;''x'' and (''x''⋀''y'')&amp;nbsp;⋁&amp;nbsp;(''x''⋀''y''&lt;sup&gt;Δ&lt;/sup&gt;)&amp;nbsp;=&amp;nbsp;''x''. Weak composition is a dual weak complementation. A (bounded) lattice such as a concept algebra, which is equipped with a weak complementation and a dual weak complementation, is called a ''weakly dicomplemented lattice''. Weakly dicomplemented lattices generalize distributive [[:en:orthocomplemented lattice|orthocomplemented lattice]]s, i.e. [[:en:Boolean algebra (structure)|Boolean algebras]].&lt;ref&gt;{{Citation
|last=Wille
|first=Rudolf
|year=2000
|contribution=Boolean Concept Logic
|editor1-last=Ganter
|editor1-first=B.
|editor2-last=Mineau
|editor2-first=G. W.
|title=ICCS 2000 Conceptual Structures: Logical, Linguistic and Computational Issues
|publisher=Springer
|pages=317–331
|isbn=978-3-540-67859-5
|series=LNAI 1867}}.&lt;/ref&gt;&lt;ref name=kwuida2004&gt;{{Citation
|last=Kwuida
|first=Léonard
|title=Dicomplemented Lattices. A contextual generalization of Boolean algebras
|year=2004
|publisher=[[Shaker Verlag]]
|isbn=978-3-8322-3350-1
|url=http://hsss.slub-dresden.de/documents/1101148726640-2926/1101148726640-2926.pdf
}}&lt;/ref&gt;

== Algorithms and tools==
There is a number of simple and fast algorithms for generating formal concepts and for constructing and navigating concept lattices. For a survey, see Kuznetsov and Obiedkov&lt;ref name=&quot;AlgSurvey&quot;&gt;Kuznetsov S., Obiedkov S. ''Comparing Performance of Algorithms for Generating Concept Lattices'', 14, [[Journal of Experimental and Theoretical Artificial Intelligence]], Taylor &amp; Francis, {{ISSN|0952-813X}} (print) {{ISSN|1362-3079}} (online), pp.189–216, 2002&lt;/ref&gt; or the book by Ganter and Obiedkov,&lt;ref name=&quot;GanterObiedkov&quot;/&gt; where also some pseudo-code can be found. Since the number of formal concepts may be exponential in the size of the formal context, the complexity of the algorithms usually is given with respect to the output size. Concept lattices with a few million elements can be handled without problems.

Many FCA software applications are available today.&lt;ref name=&quot;fcahome.org.uk&quot;&gt;One can find a non exhaustive list of FCA tools in the FCA software website: http://www.fcahome.org.uk/fcasoftware.html&lt;/ref&gt; The main purpose of these tools varies from formal context creation to formal [[:en:concept mining|concept mining]] and generating the concepts lattice of a given formal context and the corresponding implications and [[:en:association rules|association rules]]. Most of these tools are academic open-source applications, such as:
* ConExp&lt;ref name=&quot;conexp.sourceforge.net&quot;&gt;http://conexp.sourceforge.net&lt;/ref&gt;
* ToscanaJ&lt;ref name=&quot;toscanaj.sourceforge.net&quot;&gt;http://toscanaj.sourceforge.net&lt;/ref&gt;
* [[Lattice Miner]]&lt;ref name=&quot;ReferenceB&quot;&gt;Boumedjout Lahcen and Leonard Kwuida. ''Lattice Miner: A Tool for Concept Lattice Construction and Exploration.'' In: Supplementary Proceeding of International Conference on Formal concept analysis (ICFCA'10), 2010&lt;/ref&gt;
* Coron&lt;ref name=&quot;coron.loria.fr&quot;&gt;http://coron.loria.fr&lt;/ref&gt;
* FcaBedrock&lt;ref name=&quot;sourceforge.net&quot;&gt;http://sourceforge.net/projects/fcabedrock&lt;/ref&gt;

== Related topics ==
* '''Bicliques:'''
: A formal context can naturally be interpreted as a [[bipartite graph]]. The formal concepts then correspond to the maximal [[biclique|bicliques]] in that graph. The mathematical and algorithmic results of formal concept analysis may thus be used for the theory of maximal bicliques. The notion of [[bipartite dimension]] (of the complemented bipartite graph) translates&lt;ref name=&quot;GW&quot; /&gt; to that of ''Ferrers dimension'' (of the formal context) and of [[order dimension]] (of the concept lattice) and has applications e.g. for Boolean matrix factorization.&lt;ref&gt;Belohlavek, Radim, and Vychodil, Vilem. ''Discovery of optimal factors in binary data via a novel method of matrix decomposition.'' Journal of Computer and System Sciences 76.1 (2010): 3-20.&lt;/ref&gt;
* '''Biclustering and multidimensional clustering:'''
: Given an object-attribute numerical data-table, the goal of [[biclustering]] is to group together some objects having similar values of some attributes. For example, in gene expression data, it is known that genes (objects) may share a common behavior for a subset of biological situations (attributes) only: one should accordingly produce local patterns to characterize biological processes, the latter should possibly overlap, since a gene may be involved in several processes. The same remark applies for recommender systems where one is interested in local patterns characterizing groups of users that strongly share almost the same tastes for a subset of items.&lt;ref name=&quot;AdomTuzh&quot;&gt;Adomavicius C., Tuzhilin A. ''Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions''. Knowledge and Data Engineering, IEEE Transac-tions on, 17(6): 734 -749, 2005.&lt;/ref&gt;

: A bicluster in a binary object-attribute data-table is a pair ''(A,B)'' consisting of an inclusion-maximal set of objects ''A'' and an inclusion-maximal set of attributes ''B'' such that almost all objects from ''A'' have almost all attributes from ''B'' and vice versa.

: Of course, formal concepts can be considered as &quot;rigid&quot; biclusters where all objects have all attributes and vice versa. Hence, it is not surprising that some bicluster definitions coming from practice&lt;ref&gt;Prelic, S. Bleuler, P. Zimmermann, A. Wille, P. Buhlmann, W. Gruissem, L. Hennig, L. Thiele, and E. Zitzler. ''A Systematic Comparison and Evaluation of Biclustering
Methods for Gene Expression Data''. Bioinformatics, 22(9):1122-1129, 2006&lt;/ref&gt; are just definitions of a formal concept.&lt;ref name=&quot;KayMinBicl&quot;&gt;Kaytoue M., Kuznetsov S., Macko J., Wagner Meira Jr., Napoli A. ''Mining Biclusters of Similar Values with Triadic Concept Analysis''. CLA : 175-190, 2011&lt;/ref&gt;

: A bicluster of similar values in a numerical object-attribute data-table is usually defined&lt;ref&gt;R. G. Pensa, C. Leschi, J. Besson, J.-F. Boulicaut. ''Assessment of discretization techniques for relevant pattern discovery from gene expression data''. In M. J. Zaki, S. Morishita, and I. Rigoutsos, editors, Proceedings of the 4th ACM SIGKDD Workshop on Data Mining in Bioinformatics (BIOKDD 2004), 24-30, 2004.&lt;/ref&gt;&lt;ref&gt;Besson J., Robardet C. Raedt L.D., Boulicaut, J.-F. ''Mining bi-sets in numerical data''. In S. Dzeroski and J. Struyf, editors, KDID, LNCS 4747, p.11-23. Springer, 2007.&lt;/ref&gt;&lt;ref name=&quot;CerfCloPat&quot;&gt;Cerf L., Besson J., Robardet C., Boulicaut J.-F. ''Closed patterns meet n-ary relations''. TKDD, 3(1), 2009&lt;/ref&gt; as a pair consisting of an inclusion-maximal set of objects and an inclusion-maximal set of attributes having similar values for the objects. Such a pair can be represented as an inclusion-maximal rectangle in the numerical table, modulo rows and columns permutations. In&lt;ref name=&quot;KayMinBicl&quot; /&gt; it was shown that biclusters of similar values correspond to triconcepts of a triadic context where the third dimension is given by a scale that represents numerical attribute values by binary attributes.

: This fact can be generalized to ''n''-dimensional case, where ''n''-dimensional clusters of similar values in ''n''-dimensional data are represented by ''n+1''-dimensional concepts. This reduction allows one to use standard definitions and algorithms from multidimensional concept analysis&lt;ref name=&quot;CerfCloPat&quot;/&gt;&lt;ref name=&quot;Voutsadakis&quot; /&gt; for computing multidimensional clusters.
* '''Knowledge spaces:'''
: In the theory of [[:en:Knowledge space|knowledge spaces]] it is assumed that in any knowledge space the family of ''knowledge states'' is union-closed. The complements of knowledge states therefore form a [[:en:closure operator|closure  system]] and may be represented as the extents of some formal context.

== Hands-on experience with formal concept analysis ==
The formal concept analysis can be used as a qualitative method for data analysis. Since the early beginnings of FBA in the early 1980s, the FBA research group at TU Darmstadt has gained experience from more than 200 projects using the FBA (as of 2005).&lt;ref name=&quot;FCAFaA&quot;&gt;{{citation/core| EditorSurname1=Bernhard Ganter, Gerd Stumme, Rudolf Wille|Title=Formal Concept Analysis. Foundations and Applications| Publisher=Springer Science &amp; Business Media| PublicationPlace=Berlin Heidelberg| ISBN=3-540-27891-5| Date=2005| DOI=10.1007/978-3-540-31881-1| URL=[http://books.google.de/books?id=nEh4D4e88NwC&amp;printsec=frontcover&amp;hl=de&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false books.google.de]| AccessDate=2015-11-14}}&lt;/ref&gt; Including the fields of: [[medicine]] and [[cell biology]],&lt;ref&gt;{{citation/core| Surname1=Susanne Motameny, Beatrix Versmold, Rita Schmutzler| EditorSurname1=Raoul Medina, Sergei Obiedkov| Periodical=ICFCA 2008| Title=Formal Concept Analysis for the Identification of Combinatorial Biomarkers in Breast Cancer| Series=LNAI| Volume=4933| Publisher=Springer| PublicationPlace=Berlin Heidelberg| At=pp.&amp;nbsp;229–240| ISBN=978-3-540-78136-3| Date=2008| URL=[http://www.springer.com/us/book/9783540781363 springer.com]| AccessDate=2016-01-29}}&lt;/ref&gt;&lt;ref&gt;{{citation/core| Surname1=Dominik Endres, Ruth Adam, Martin A. Giese, Uta Noppeney| EditorSurname1=Florent Domenach, Dmitry I. Ignatov, Jonas Poelmans| Periodical=ICFCA 2012| Title=Understanding the Semantic Structure of Human fMRI Brain Recordings with Formal Concept Analysis| Series=LNCS| Volume=7278| Publisher=Springer| PublicationPlace=Berlin Heidelberg| At=pp.&amp;nbsp;96–111| ISBN=978-3-642-29891-2| ISSN=0302-9743|Date=2012| DOI=10.1007/978-3-642-29892-9}}&lt;/ref&gt; [[genetics]],&lt;ref&gt;{{citation/core| Surname1=Denis Ponomaryov, Nadezhda Omelianchuk, Victoria Mironova, Eugene Zalevsky, Nikolay Podkolodny, Eric Mjolsness, Nikolay Kolchanov| EditorSurname1= Karl Erich Wolff, Dmitry E. Palchunov, Nikolay G. Zagoruiko, Urs Andelfinger| Periodical=KONT 2007, KPP 2007| Title=From Published Expression and Phenotype Data to Structured Knowledge: The Arabidopsis Gene Net Supplementary Database and Its Applications| Series=LNCS|Volume=6581| Publisher=Springer| PublicationPlace=Heidelberg New York|At=pp.&amp;nbsp;101–120| ISBN=978-3-642-22139-2|ISSN=0302-9743| Date=2011| DOI=10.1007/978-3-642-22140-8}}&lt;/ref&gt;&lt;ref&gt;{{citation/core| Surname1=Mehdi Kaytoue, Sergei Kuznetsov, Amedeo Napoli, Sébastien Duplessis| Periodical=Information Sciences| Title=Mining gene expression data with pattern structures in formal concept analysis| Volume=181| Issue=10| Publisher=Elsevier| At=pp.&amp;nbsp;1989–2001| Date=2011| DOI=10.1016/j.ins.2010.07.007| URL=[https://www.hse.ru/data/2010/11/01/1223500185/InformationSciences.pdf hse.ru]|AccessDate=2016-02-13}}, Format: PDF&lt;/ref&gt; [[ecology]],&lt;ref&gt;{{citation/core| Surname1=Aurélie Bertaux, Florence Le Ber, Agnès Braud, Michèle Trémolières| EditorSurname1=Sébastien Ferré, Sebastian Rudolph| Periodical=ICFCA 2009| Title=Identifying Ecological Traits: A Concrete FCA-Based Approach| Series=LNAI| Volume=5548| Publisher=Springer-Verlag| PublicationPlace=Berlin Heidelberg| At=pp.&amp;nbsp;224–236| ISBN=978-3-642-01814-5| Date=2009|DOI=10.1007/978-3-642-01815-2}}&lt;/ref&gt; [[software engineering]],&lt;ref&gt;{{citation/core| Surname1=Gregor Snelting, Frank Tip|Periodical=Proceeding. SIGSOFT ’98/FSE-6| Title=Reengineering class hierarchies using concept analysis| Volume=23| Issue=6| Publisher=ACM| PublicationPlace=New York|At=pp.&amp;nbsp;99–110| ISBN=1-58113-108-9|Date=1998| DOI=10.1145/291252.288273| URL=[http://dl.acm.org/citation.cfm?doid=288195.288273 dl.acm.org]| AccessDate=2016-02-04}}&lt;/ref&gt; [[ontology (information science)|ontology]],&lt;ref&gt;{{citation/core| Surname1=Gerd Stumme, Alexander Maedche| EditorSurname1= [[:de:Universität Leipzig|Uni Leipzig]]| Periodical=IJCAI| Title=FCA-Merge: Bottom-up merging of ontologies|PublicationPlace=Leipzig|At=pp.&amp;nbsp;225–230|Date=2001| URL=[http://se-pubs.dbs.uni-leipzig.de/files/Stumme2001FCAMergeBottomupmergingofontologies.pdf se-pubs.dbs.uni-leipzig.de]| AccessDate=2016-02-13}}, Format: PDF&lt;/ref&gt; [[information management|information]] and [[library science]]s,&lt;ref&gt;{{citation/core| Surname1=Uta Priss| EditorSurname1=American Documentation Institute|Periodical=Annual Review of Information Science and Technology| Title=Formal Concept Analysis in Information Science| Volume=40|Issue=1| Publisher=Information Today Inc.| PublicationPlace=Medford, NJ 09855|At=pp.&amp;nbsp;521–543| ISSN=0066-4200| Date=2006| DOI=10.1002/aris.1440400120| URL=[http://www.upriss.org.uk/papers/arist.pdf upriss.org.uk]| AccessDate=2016-02-04}}, Format: PDF&lt;/ref&gt;&lt;ref&gt;{{citation/core| Surname1=Jens Illig, Andreas Hotho, Robert Jäschke, Gerd Stumme| EditorSurname1=Karl Erich Wolff, Dmitry E. Palchunov, Nikolay G. Zagoruiko, Urs Andelfinger| Periodical=KONT 2007, KPP 2007| Title=A Comparison of Content-Based Tag Recommendations in Folksonomy Systems|Series=LNCS| Volume=6581| Publisher=Springer| PublicationPlace=Heidelberg New York| At=pp.&amp;nbsp;136–149| ISBN=978-3-642-22139-2| ISSN=0302-9743| Date=2011| DOI=10.1007/978-3-642-22140-8}}&lt;/ref&gt;&lt;ref&gt;{{citation/core| EditorSurname1=Claudio Carpineto, Giovanni Romano| Title=Concept Data Analysis: Theory and Applications| Publisher=John Wiley &amp; Sons| ISBN=0-470-85055-8| Date=2004| URL=[http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470850558.html eu.wiley.com]| AccessDate=2016-02-04}}&lt;/ref&gt; [[office administration]],&lt;ref&gt;{{citation/core| Surname1=Richard Cole, Gerd Stumme| EditorSurname1=Bernhard Ganter, Guy W. Mineau| Periodical=Conceptual Structures: Logical, Linguistic, and Computational Issues| Title=CEM – A Conceptual Email Manager| Series=LNAI| Volume=1867| Publisher=Springer-Verlag| PublicationPlace=Berlin Heidelberg| At=pp.&amp;nbsp;438–452| ISBN=3-540-67859-X| Date=2000| DOI=10.1007/10722280}}&lt;/ref&gt; [[law]],&lt;ref&gt;{{citation/core| Surname1=Dieter Eschenfelder, Wolfgang Kollewe, Martin Skorsky, Rudolf Wille| EditorSurname1= Gerd Stumme, Rudolf Wille| Periodical=Begriffliche Wissensverarbeitung – Methoden und Anwendungen| Title=Ein Erkundungssystem zum Baurecht: Methoden der Entwicklung eines TOSCANA-Systems| Publisher=Springer| PublicationPlace=Berlin Heidelberg| At=pp.&amp;nbsp;254–272| ISBN=3-540-66391-6| Date=2000| language=German| DOI=10.1007/978-3-642-57217-3_12}}&lt;/ref&gt;&lt;ref&gt;{{citation/core| Surname1=Nada Mimouni, Adeline Nazarenko; Sylvie Salotti| EditorSurname1=Jaume Baixeries, Christian Sacarea, Manuel Ojeda-Aciego| Periodical=ICFCA 2015| Title=A Conceptual Approach for Relational IR: Application to Legal Collections| Series=LNAI| Volume=9113| Publisher=Springer| PublicationPlace=Heidelberg New York| At=pp.&amp;nbsp;303–318| ISBN=978-3-319-19544-5| ISSN=0302-9743| Date=2015| DOI=10.1007/978-3-319-19545-2_19}}&lt;/ref&gt; [[linguistics]],&lt;ref&gt;{{citation/core| Surname1=Uta Priss| EditorSurname1= [[:de:Bernhard Ganter]], Gerd Stumme, [[:de:Rudolf Wille (Mathematiker)|Rudolf Wille]]| Periodical=Formal Concept Analysis – Foundations and Applications|Title=Linguistic Applications of Formal Concept Analysis| Series=LNCS| Volume=3626| Publisher=Springer| PublicationPlace=Berlin Heidelberg| At=pp.&amp;nbsp;149–160| ISBN=3-540-27891-5| ISSN=0302-9743| Date=2005|DOI=10.1007/978-3-540-31881-1}}&lt;/ref&gt; [[political science]].&lt;ref&gt;{{citation/core| Surname1=Beate Kohler-Koch, Frank Vogt, Gerhard Stumme, [[:de:Rudolf Wille (Mathematiker)|Rudolf Wille]]| Periodical=Begriffliche Wissenverarbeitung – Methoden und Anwendungen| Title=Normen- und Regelgeleitete internationale Kooperationen: &quot;Quoted in: Peter Becker et al. The ToscanaJ Suite for Implementing Conceptual Information Systems&quot;| Publisher=Springer| PublicationPlace=Berlin, Heidelberg, New York|At=pp.&amp;nbsp;325–340| ISBN=978-3-540-66391-1| Date=2000| language=German}}&lt;/ref&gt;

Many more examples are e.g. described in: ''Formal Concept Analysis. Foundations and Applications'',&lt;ref name=&quot;FCAFaA&quot; /&gt; conference papers at regular conferences such as: ''International Conference on Formal Concept Analysis'' (ICFCA),&lt;ref&gt;{{cite web| title=International Conference on Formal Concept Analysis| periodical=| publisher=[[Digital Bibliography &amp; Library Project|dblp]]| url=http://dblp.uni-trier.de/db/conf/icfca/index| accessdate=2016-02-14}}&lt;/ref&gt; ''Concept Lattices and their Applications'' (CLA),&lt;ref&gt;{{cite web| title=CLA: Concept Lattices and Their Applications| publisher=CLA| url=http://cla.inf.upol.cz/papers.html| accessdate=2015-11-14}}&lt;/ref&gt; or ''International Conference on Conceptual Structures'' (ICCS).&lt;ref&gt;{{cite web| title=International Conferences On Conceptual Structures – Conferences and Workshops| publisher=New Mexico State University|url=http://conceptualstructures.org/confs.htm| accessdate=2016-02-14}}&lt;/ref&gt;

== See also ==
{{Div col|colwidth=22em}}
* [[Association rule learning]]
* [[Cluster analysis]]
* [[Commonsense reasoning]]
* [[Conceptual clustering]]
* [[Concept learning]]
* [[Correspondence analysis]]
* [[Description logic]]
* [[Factor analysis]]
* [[Graphical model]]
* [[Grounded theory]]
* [[Inductive logic programming]]
* [[Pattern theory]]
* [[Statistical relational learning]]
* [[Schema (genetic algorithms)]]
{{Div col end}}

== Notes ==
{{reflist|colwidth=30em}}

== References ==
{{refbegin}}
* {{citation
  | editor1-last = Ganter | editor1-first = Bernhard
  | editor2-last = Stumme | editor2-first = Gerd
  | editor3-last = Wille | editor3-first = Rudolf
  | title = Formal Concept Analysis: Foundations and Applications
  | publisher = Lecture Notes in Artificial Intelligence, no. 3626, Springer-Verlag
  | year = 2005
  | isbn = 3-540-27891-5}}.
* {{citation
  | last1 = Ganter | first1 = Bernhard | last2 = Wille | first2 = Rudolf
  | title = Formal Concept Analysis: Mathematical Foundations
  | publisher = Springer-Verlag, Berlin
  | year = 1998
  | isbn = 3-540-62771-5}}. Translated by C. Franzke.
* {{citation
  | last1 = Carpineto | first1 = Claudio | last2 = Romano | first2 = Giovanni
  | title = Concept Data Analysis: Theory and Applications
  | publisher = Wiley
  | year = 2004
  | isbn = 978-0-470-85055-8}}.
* {{citation
 |first       = Karl Erich
 |last        = Wolff
 |title       = A first course in Formal Concept Analysis
 |editor      = F. Faulbaum in StatSoft 1993
 |publisher   = Gustav Fischer Verlag
 |url         = http://fbmn.fh-darmstadt.de/home/wolff/Publikationen/A_First_Course_in_Formal_Concept_Analysis.pdf
 |pages       = 429–438
 |year        = 1994
 |deadurl     = bot: unknown
 |archiveurl  = https://web.archive.org/web/20060323075347/http://fbmn.fh-darmstadt.de/home/wolff/Publikationen/A_First_Course_in_Formal_Concept_Analysis.pdf
 |archivedate = 2006-03-23
 |df          =
}}.
* {{citation
  | last1=Davey | first1=B.A.
  | last2=Priestley | first2=H. A.
  | title=Introduction to Lattices and Order, chapter 3. Formal Concept Analysis
  | publisher=[[Cambridge University Press]]
  | isbn=978-0-521-78451-1
  | year=2002}}.
{{refend}}

== External links ==
* [http://www.upriss.org.uk/fca/fca.html A Formal Concept Analysis Homepage]
* [http://www.ketlab.org.uk/scripts/context Demo]
* [http://www.math.tu-dresden.de/icfca13/ 11th International Conference on Formal Concept Analysis. ICFCA 2013 - Dresden, Germany - May 21-24, 2013]

{{DEFAULTSORT:Formal Concept Analysis}}



[[Category:Ontology (information science)]]</text>
      <sha1>evik5djkedidl10q5e47emdj7r7j2xi</sha1>
    </revision>
  </page>
  <page>
    <title>Deeplearning4j</title>
    <ns>0</ns>
    <id>43169442</id>
    <revision>
      <id>811774664</id>
      <parentid>810962297</parentid>
      <timestamp>2017-11-23T21:38:25Z</timestamp>
      <contributor>
        <username>Premeditated Chaos</username>
        <id>31530</id>
      </contributor>
      <minor/>
      <comment>Removing link(s): [[Wikipedia:Articles for deletion/Skymind]] closed as delete ([[WP:XFDC|XFDcloser]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13363">{{news release|1=article|date=November 2017}}
{{Infobox software
| name                   = Deeplearning4j
| logo                   =
| screenshot             =
| caption                =
| collapsible            =
| author                 = [[Adam Gibson (computer scientist)|Adam Gibson]], [[Chris Nicholson (entrepreneur)|Chris Nicholson]], Josh Patterson
| developer              = [https://github.com/SkymindIO/deeplearning4j/graphs/contributors Various]
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes}} --&gt;
| latest release version = 0.9.1
| latest release date    = {{Start date and age|2017|8|13|df=yes}}
| latest preview version =
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], [[CUDA]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]], [[Clojure]]
| operating system       = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]], [[Android (operating system)|Android]]
| platform               = [[Cross-platform]]
| size                   =
| language               = English
| status                 = Active
| genre                  = [[Natural language processing]], [[deep learning]], [[machine vision]], [[artificial intelligence]]
| license                = [[Apache License|Apache]] 2.0
| website                = {{URL|deeplearning4j.org}}
}}

{{machine learning bar}}
Eclipse '''Deeplearning4j''' is a [[deep learning]] programming [[Library (computing)|library]] written for [[Java (programming language)|Java]] and the [[Java virtual machine]] (JVM)&lt;ref name=&quot;wired&quot;&gt;{{cite web|first=Cade|last=Metz|title=The Mission to Bring Google's AI to the Rest of the World|work=[[Wired.com]]|date=2014-06-02|url=https://www.wired.com/2014/06/skymind-deep-learning/|accessdate=2014-06-28}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.businessweek.com/articles/2014-06-03/teaching-smaller-companies-how-to-probe-deep-learning-on-their-own|title=Deep Learning for (Some of) the People|last=Vance|first=Ashlee|work=[[Bloomberg Businessweek]]|date=2014-06-03|accessdate=2014-06-28}}&lt;/ref&gt; and a [[computing]] framework with wide support for [[deep learning]] algorithms.&lt;ref&gt;{{cite web|url=https://venturebeat.com/2015/11/14/deep-learning-frameworks/|title=Want an open-source deep learning framework? Take your pick|last=Novet|first=Jordan|work=[[VentureBeat]]|date=2015-11-14|accessdate=2015-11-24}}&lt;/ref&gt; Deeplearning4j includes implementations of the [[restricted Boltzmann machine]], [[deep belief net]], deep autoencoder, stacked denoising autoencoder and [[Recursive neural network#Tensor|recursive neural tensor network]], [[word2vec]], doc2vec, and [[GloVe (machine learning)|GloVe]]. These algorithms all include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions that integrate with [[Apache Hadoop]] and [[Apache Spark|Spark]].&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=LCsc1hFuNac&amp;feature=youtu.be|title=Adam Gibson, DeepLearning4j on Spark and Data Science on JVM with nd4j, SF Spark @Galvanize 20150212 |last=TV|first=Functional|work=SF Spark Meetup|date=2015-02-12|accessdate=2015-03-01}}&lt;/ref&gt;

Deeplearning4j is [[open-source software]] released under [[Apache License]] 2.0,&lt;ref&gt;{{cite web|title=Github Repository|url=https://github.com/agibsonccc/java-deeplearning}}&lt;/ref&gt; developed mainly by a [[machine learning]] group headquartered in [[San Francisco]] and [[Tokyo]] and led by Adam Gibson.&lt;ref name=&quot;deeplearning4j.org&quot;&gt;{{cite web|url=http://deeplearning4j.org/|title=deeplearning4j.org}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Crunchbase Profile|url=http://www.crunchbase.com/person/adam-gibson}}&lt;/ref&gt; It is supported commercially by the startup Skymind, which bundles DL4J, [[Tensorflow]], [[Keras]] and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer.&lt;ref&gt;{{cite web|title=Skymind Intelligence Layer Community Edition|url=https://skymind.ai/quickstart}}&lt;/ref&gt; Deeplearning4j was contributed to the [[Eclipse Foundation]] in October 2017.&lt;ref&gt;{{cite web|title=Eclipse Deeplearning4j Project Page|url=https://projects.eclipse.org/proposals/deeplearning4j}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Skymind’s Deeplearning4j, the Eclipse Foundation, and scientific computing in the JVM|url=https://jaxenter.com/skymind-deeplearning4j-eclipse-138872.html|work=Jaxenter|accessdate=2017-11-15}}&lt;/ref&gt;

==Introduction==
Deeplearning4j relies on the widely used programming language, [[Java (programming language)|Java]], though it is compatible with [[Clojure]] and includes a [[Scala (programming language)|Scala]] [[application programming interface]] (API). It is powered by its own open-source numerical computing library, [[ND4J (software)|ND4J]], and works with both [[central processing unit]]s (CPUs) and [[graphics processing unit]]s (GPUs).&lt;ref name=&quot;om&quot;&gt;{{cite web|first=Derrick|last=Harris|title=A startup called Skymind launches, pushing open source deep learning|work=[[GigaOM.com]]|date=2014-06-02|url=http://gigaom.com/2014/06/02/a-startup-called-skymind-launches-pushing-open-source-deep-learning/|accessdate=2014-06-29}}&lt;/ref&gt;&lt;ref name=&quot;vb&quot;&gt;{{cite web|first=Jordan|last=Novet|title=Skymind launches with open-source, plug-and-play deep learning features for your app|date=2014-06-02|url=https://venturebeat.com/2014/06/02/skymind-launches-with-open-source-plug-and-play-deep-learning-features-for-your-app//|accessdate=2014-06-29}}&lt;/ref&gt;

Deeplearning4j has been used in several commercial and academic applications. The code is hosted on [[GitHub]].&lt;ref&gt;[https://github.com/deeplearning4j/deeplearning4j Deeplearning4j source code]&lt;/ref&gt; A support forum is maintained on [[Gitter]].&lt;ref&gt;[https://gitter.im/deeplearning4j/deeplearning4j Deeplearning4j Gitter Support Channel]&lt;/ref&gt;

The framework is composable, meaning shallow neural nets such as restricted Boltzmann machines, convolutional nets, autoencoders, and recurrent nets can be added to one another to create deep nets of varying types. It also has extensive visualization tools,&lt;ref&gt;[http://deeplearning4j.org/visualization Deeplearning4j Visualization Tools]&lt;/ref&gt; and a computation graph.&lt;ref&gt;[http://deeplearning4j.org/compgraph Deeplearning4j Computation Graph]&lt;/ref&gt;

==Distributed==
Training with Deeplearning4j occurs in a cluster. Neural nets are trained in parallel via iterative reduce, which works on [[Hadoop]]-YARN and on [[Apache Spark|Spark]].&lt;ref name=&quot;deeplearning4j.org&quot;/&gt;&lt;ref&gt;{{cite web|url=https://github.com/emsixteeen/IterativeReduce|title=Iterative reduce}}&lt;/ref&gt; Deeplearning4j also integrates with CUDA kernels to conduct pure GPU operations, and works with distributed GPUs.

==Scientific computing for the JVM==
Deeplearning4j includes an n-dimensional array class using [[ND4J (software)|ND4J]] that allows scientific computing in Java and Scala, similar to the functions that [[NumPy]] provides to [[Python (programming language)|Python]]. It's effectively based on a library for [[linear algebra]] and [[Matrix (mathematics)|matrix]] manipulation in a production environment.

==DataVec vectorization library for machine-learning==
DataVec vectorizes various file formats and data types using an [[input/output]] format system similar to Hadoop's use of MapReduce; that is, it turns various data types into columns of scalars termed [[Vector (mathematics and physics)|vectors]]. DataVec is designed to vectorize CSVs, images, sound, text, video, and time series.&lt;ref&gt;[http://deeplearning4j.org/datavec DataVec ETL for Machine Learning]&lt;/ref&gt;&lt;ref&gt;[https://www.infoq.com/articles/deep-learning-time-series-anomaly-detection Anomaly Detection for Time Series Data with Deep Learning]&lt;/ref&gt;

==Text and NLP==
Deeplearning4j includes a [[vector space model]]ing and [[topic model]]ing toolkit, implemented in Java and integrating with parallel GPUs for performance. It is designed to handle large text sets.

Deeplearning4j includes implementations of term frequency–inverse document frequency ([[tf–idf]]), [[deep learning]], and Mikolov's word2vec algorithm,&lt;ref&gt;[https://code.google.com/p/word2vec/ word2vec]&lt;/ref&gt; doc2vec, and GloVe, reimplemented and optimized in Java. It relies on [[t-distributed stochastic neighbor embedding]] (t-SNE) for word-cloud visualizations.

==Real-world use cases and integrations==
Real-world use cases for Deeplearning4j include network intrusion detection and cybersecurity, fraud detection for the financial sector,&lt;ref&gt;http://www.skymind.io/finance/&lt;/ref&gt;&lt;ref&gt;https://skymind.ai/bsa-aml&lt;/ref&gt; anomaly detection in industries such as manufacturing, recommender systems in e-commerce and advertising,&lt;ref&gt;{{cite web |url=http://www.skymind.io/commerce/ |title=Archived copy |accessdate=2016-02-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20160310082156/http://www.skymind.io/commerce/ |archivedate=2016-03-10 |df= }}&lt;/ref&gt; and image recognition.&lt;ref&gt;https://skymind.ai/image&lt;/ref&gt; Deeplearning4j has integrated with other machine-learning platforms such as RapidMiner, Prediction.io,&lt;ref&gt;https://www.rapidminerchina.com/en/products/shop/product/deeplearning4j/&lt;/ref&gt; and [[Weka (machine learning)|Weka]].&lt;ref&gt;https://deeplearning.cms.waikato.ac.nz/&lt;/ref&gt;

==Machine Learning Model Server==

Deeplearning4j serves machine-learning models for inference in production using the free developer edition of SKIL, the Skymind Intelligence Layer.&lt;ref&gt;https://skymind.ai/products&lt;/ref&gt;&lt;ref&gt;https://deeplearning4j.org/modelserver&lt;/ref&gt; A model server serves the parametric machine-learning models that makes decisions about data. It is used for the inference stage of a machine-learning workflow, after data pipelines and model training. A model server is the tool that allows data science research to be deployed in a real-world production environment.

What a Web server is to the Internet, a model server is to AI. Where a Web server receives an HTTP request and returns data about a Web site, a model server receives data, and returns a decision or prediction about that data: e.g. sent an image, a model server might return a label for that image, identifying faces or animals in photographs.

The SKIL model server is able to import models from Python frameworks such as Tensorflow, Keras, Theano and CNTK, overcoming a major barrier in deploying deep learning models.

==Benchmarks==
Deeplearning4j is as fast as Caffe for non-trivial image recognition tasks using multiple GPUs.&lt;ref&gt;https://github.com/deeplearning4j/dl4j-benchmark&lt;/ref&gt; For programmers unfamiliar with HPC on the JVM, there are several parameters that must be adjusted to optimize neural network training time. These include setting the heap space, the garbage collection algorithm, employing off-heap memory and pre-saving data (pickling) for faster ETL.&lt;ref&gt;https://deeplearning4j.org/benchmark&lt;/ref&gt; Together, these optimizations can lead to a 10x acceleration in performance with Deeplearning4j.

==API Languages: Java, Scala, Python &amp; Clojure==
Deeplearning4j can be used via multiple API languages including Java, Scala, Python and Clojure. Its Scala API is called ScalNet.&lt;ref&gt;https://deeplearning4j.org/scala&lt;/ref&gt;  Keras serves as its Python API.&lt;ref&gt;https://deeplearning4j.org/keras&lt;/ref&gt;  And its Clojure wrapper is known as DL4CLJ.&lt;ref&gt;https://deeplearning4j.org/clojure&lt;/ref&gt; The core languages performing the large-scale mathematical operations necessary for deep learning are C, C++ and CUDA C.

==Tensorflow, Keras &amp; Deeplearning4j==

Tensorflow, Keras and Deeplearning4j work together. Deeplearning4j can import models from Tensorflow and other Python frameworks if they have been created with Keras.&lt;ref&gt;https://deeplearning4j.org/tensorflow&lt;/ref&gt;

==See also==
{{Portal|Free software|Java}}
* [[Comparison of deep learning software]]
* [[Artificial intelligence]]
* [[Machine learning]]
* [[Deep learning]]

==References==
{{Reflist|30em}}

==External links==
* {{official website|www.deeplearning4j.org}}


{{Computer vision footer}}
{{Deep Learning Software}}




[[Category:Data mining and machine learning software]]


[[Category:Data mining and machine learning software]]


[[Category:Free software programmed in Java (programming language)]]
[[Category:Software programmed in Java (programming language)]]
[[Category:Free software programmed in Scala]]



[[Category:Information technology companies of the United States]]
[[Category:Java (programming language) libraries]]






[[Category:Open-source artificial intelligence]]
[[Category:Scala (programming language)]]
[[Category:Software using the Apache license]]
[[Category:Technology companies based in the San Francisco Bay Area]]</text>
      <sha1>09pkskpyt5k3ur2xomd95jhfqpv8dq3</sha1>
    </revision>
  </page>
  <page>
    <title>Local case-control sampling</title>
    <ns>0</ns>
    <id>46963137</id>
    <revision>
      <id>750290495</id>
      <parentid>750277070</parentid>
      <timestamp>2016-11-18T21:49:10Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed ; added  using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5745">In [[machine learning]], '''local case-control sampling''' &lt;ref name=&quot;LCC&quot;&gt;{{cite journal|last1=Fithian|first1=William|last2=Hastie|first2=Trevor|title=Local case-control sampling: Efficient subsampling in imbalanced data sets|journal=The Annals of Statistics|date=2014|volume=42|issue=5|pages=1693–1724|ref=http://arxiv.org/abs/1306.3706|doi=10.1214/14-aos1220}}&lt;/ref&gt; is an [[algorithm]] used to reduce the complexity of training a [[logistic regression]] classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most &quot;surprising&quot; samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as [[Logistic regression#Case-control sampling|case control sampling]] and weighted case control sampling.

== Imbalanced datasets ==
In [[Statistical classification|classification]], a dataset is a set of ''N'' data points &lt;math&gt; (x_i, y_i)_{i=1}^N &lt;/math&gt;, where &lt;math&gt; x_i \in\mathbb R^d &lt;/math&gt; is a feature vector, &lt;math&gt; y_i \in \{0,1\} &lt;/math&gt; is a label. Intuitively, a dataset is imbalanced when certain important statistical patterns are rare. The lack of observations of certain patterns does not always imply their irrelevance. For example, in medical studies of rare diseases, the small number of infected patients (cases) conveys the most valuable information for diagnosis and treatments.

Formally, an imbalanced dataset exhibits one or more of the following properties:
* ''Marginal Imbalance''. A dataset is marginally imbalanced if one class is rare compared to the other class. In other words, &lt;math&gt; \mathbb{P}(Y=1) \approx 0 &lt;/math&gt;.
* ''Conditional Imbalance''. A dataset is conditionally imbalanced when it is easy to predict the correct labels in most cases. For example, if &lt;math&gt; X \in \{0,1\} &lt;/math&gt;, the dataset is conditionally imbalanced if &lt;math&gt; \mathbb{P}(Y=1\mid X=0) \approx 0 &lt;/math&gt; and &lt;math&gt; \mathbb{P}(Y=1\mid X=1) \approx 1 &lt;/math&gt;.

== Algorithm outline ==
In logistic regression, given the model &lt;math&gt; \theta = (\alpha, \beta) &lt;/math&gt;, the prediction is made according to &lt;math&gt; \mathbb{P}(Y=1\mid X; \theta) =  \tilde{p}_{\theta}(x) = \frac{\exp(\alpha+\beta^T x)}{1+\exp(\alpha+\beta^T x)} &lt;/math&gt;. The local-case control sampling algorithm assumes the availability of a pilot model &lt;math&gt;\tilde{\theta} =  (\tilde{\alpha}, \tilde{\beta}) &lt;/math&gt;. Given the pilot model, the algorithm performs a single pass over the entire dataset to select the subset of samples to include in training the logistic regression model. For a sample &lt;math&gt; (x,y) &lt;/math&gt;, define the acceptance probability as &lt;math&gt; a(x,y) = |y-\tilde{p}_{\tilde{\theta}}(x)| &lt;/math&gt;. The algorithm proceeds as follows:

# Generate independent &lt;math&gt; z_i \sim \text{Bernoulli}(a(x_i,y_i)) &lt;/math&gt; for &lt;math&gt; i \in \{1, \ldots, N\} &lt;/math&gt;.
# Fit a logistic regression model to the subsample &lt;math&gt; S = \{(x_i, y_i) : z_i =1 \} &lt;/math&gt;, obtaining the unadjusted estimates &lt;math&gt; \hat{\theta}_S = (\hat{\alpha}_S, \hat{\beta}_S) &lt;/math&gt;.
# The output model is &lt;math&gt; \hat{\theta} = (\hat{\alpha}, \hat{\beta}) &lt;/math&gt;, where &lt;math&gt;\hat{\alpha} \leftarrow \hat{\alpha}_S + \tilde{\alpha} &lt;/math&gt; and &lt;math&gt;\hat{\beta} \leftarrow \hat{\beta}_S + \tilde{\beta} &lt;/math&gt;.

The algorithm can be understood as selecting samples that surprises the pilot model. Intuitively these samples are closer to the [[decision boundary]] of the classifier and is thus more informative.

=== Obtaining the pilot model ===
In practice, for cases where a pilot model is naturally available, the algorithm can be applied directly to reduce the complexity of training. In cases where a natural pilot is nonexistent, an estimate using a subsample selected through another sampling technique can be used instead. In the original paper describing the algorithm, the authors propose to use weighted case-control sampling with half the assigned sampling budget. For example, if the objective is to use a subsample with size &lt;math&gt; N=1000 &lt;/math&gt;, first estimate a model &lt;math&gt;\tilde{\theta} &lt;/math&gt; using &lt;math&gt; N_h = 500 &lt;/math&gt; samples from weighted case control sampling, then collect another &lt;math&gt; N_h = 500 &lt;/math&gt; samples using local case-control sampling.

=== Larger or smaller sample size ===
It is possible to control the sample size by multiplying the acceptance probability with a constant &lt;math&gt; c &lt;/math&gt;. For a larger sample size, pick &lt;math&gt; c&gt;1 &lt;/math&gt; and adjust the acceptance probability to &lt;math&gt; \min(ca(x_i, y_i), 1) &lt;/math&gt;. For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly downsample from a larger subsample selected by local case-control sampling.

== Properties ==
The algorithm has the following properties. When the pilot is [[Consistency (statistics)|consistent]], the estimates using the samples from local case-control sampling is consistent even under [[Specification (regression)|model misspecification]]. If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set. For a larger sample size with &lt;math&gt; c&gt;1 &lt;/math&gt;, the factor 2 is improved to &lt;math&gt; 1+\frac{1}{c} &lt;/math&gt;.

== References ==
{{Reflist}}


</text>
      <sha1>mgql283k148dkcrv88c6pvfzcqc5z8k</sha1>
    </revision>
  </page>
  <page>
    <title>Machine learning</title>
    <ns>0</ns>
    <id>233488</id>
    <revision>
      <id>816206562</id>
      <parentid>816205676</parentid>
      <timestamp>2017-12-19T23:12:30Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/76.14.39.29|76.14.39.29]] ([[User talk:76.14.39.29|talk]]) to last revision by Lamb99. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="47577">{{for|the journal|Machine Learning (journal)}}
{{Machine learning bar}}

'''Machine learning''' is a field of [[computer science]] that gives [[computer]]s the ability to learn without being explicitly programmed.&lt;ref&gt;Supposedly [[Paraphrase|paraphrased]] from: {{Cite journal|last=Samuel|first=Arthur|date=1959|title=Some Studies in Machine Learning Using the Game of Checkers|url=https://doi.org/10.1147/rd.33.0210|journal=IBM Journal of Research and Development|volume=3|issue=3|pages=|doi=10.1147/rd.33.0210|via=}}.&lt;br /&gt;Confer {{Cite conference|url=https://link.springer.com/chapter/10.1007/978-94-009-0279-4_9|title=Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming|conference=Artificial Intelligence in Design ’96|last=Koza|first=John R.|last2=Bennett|first2=Forrest H.|last3=Andre|first3=David|last4=Keane|first4=Martin A.|date=1996|publisher=Springer, Dordrecht|pages=151–170|language=en|doi=10.1007/978-94-009-0279-4_9|quote=Paraphrasing Arthur Samuel (1959), the question is: How can computers learn to solve problems without being explicitly programmed?}}&lt;/ref&gt;

[[Arthur Samuel]], an American pioneer in the field of [[Computer Gaming|computer gaming]] and [[artificial intelligence]], coined the term &quot;Machine Learning&quot; in 1959 while at [[IBM]]&lt;ref&gt;R. Kohavi and F. Provost, \Glossary of terms,&quot; Machine Learning, vol. 30, no. 2-3, pp. 271-274, 1998.&lt;/ref&gt;. Evolved from the study of [[pattern recognition]] and [[computational learning theory]] in artificial intelligence,&lt;ref name=Britannica&gt;http://www.britannica.com/EBchecked/topic/1116194/machine-learning {{tertiary}}&lt;/ref&gt; machine learning explores the study and construction of [[algorithm]]s that can learn from and make predictions on [[data]]&lt;ref&gt;{{cite journal |title=Glossary of terms |author1=Ron Kohavi |author2=Foster Provost |journal=[[Machine Learning (journal)|Machine Learning]] |volume=30 |pages=271–274 |year=1998 |url=http://ai.stanford.edu/~ronnyk/glossary.html}}&lt;/ref&gt; – such algorithms overcome following strictly static [[computer program|program instruction]]s by making data-driven predictions or decisions,&lt;ref name=&quot;bishop&quot; /&gt;{{rp|2}} through building a [[Mathematical model|model]] from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include [[email filtering]], detection of network intruders or malicious insiders working towards a [[data breach]],&lt;ref&gt;{{Cite web|url=https://techcrunch.com/2016/07/01/exploiting-machine-learning-in-cybersecurity/|title=Exploiting machine learning in cybersecurity|last=Dickson|first=Ben|website=TechCrunch|access-date=2017-05-23}}&lt;/ref&gt; [[optical character recognition]] (OCR),&lt;ref name=Wernick-Signal-Proc-July-2010&gt;Wernick, Yang, Brankov, Yourganov and Strother, Machine Learning in Medical Imaging, ''[[IEEE Signal Processing Society|IEEE Signal Processing Magazine]]'', vol. 27, no. 4, July 2010, pp. 25–38&lt;/ref&gt; [[learning to rank]], and [[computer vision]].

Machine learning is closely related to (and often overlaps with) [[computational statistics]], which also focuses on prediction-making through the use of computers. It has strong ties to [[mathematical optimization]], which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with [[data mining]],&lt;ref&gt;{{cite conference |last=Mannila |first=Heikki |title=Data mining: machine learning, statistics, and databases |conference=Int'l Conf. Scientific and Statistical Database Management |publisher=IEEE Computer Society |year=1996}}&lt;/ref&gt; where the latter subfield focuses more on exploratory data analysis and is known as [[unsupervised learning]].&lt;ref name=&quot;bishop&quot;&gt;Machine learning and pattern recognition &quot;can be viewed as two facets of the same field.&quot;&lt;/ref&gt;{{rp|vii}}&lt;ref&gt;{{cite journal |last=Friedman |first=Jerome H. |authorlink = Jerome H. Friedman|title=Data Mining and Statistics: What's the connection? |journal=Computing Science and Statistics |volume=29 |issue=1 |year=1998 |pages=3–9}}&lt;/ref&gt; Machine learning can also be unsupervised&lt;ref&gt;{{Cite web|url=http://www.darkreading.com/threat-intelligence/3-flavors-of-machine-learning--who-what-and-where/a/d-id/1324278|title=Dark Reading|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; and be used to learn and establish baseline behavioral profiles for various entities&lt;ref&gt;{{Cite web|url=http://aibusiness.org/lightcybers-jason-matlof-explains-how-magna-detects-hackers-before-they-attack/|title=AI Business|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; and then used to find meaningful anomalies.

Within the field of [[data analytics]], machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as [[predictive analytics]]. These analytical models allow researchers, [[data science|data scientists]], engineers, and analysts to &quot;produce reliable, repeatable decisions and results&quot; and uncover &quot;hidden insights&quot; through learning from historical relationships and trends in the data.&lt;ref&gt;{{Cite web|url=http://www.sas.com/it_it/insights/analytics/machine-learning.html|title=Machine Learning: What it is and why it matters|website=www.sas.com|access-date=2016-03-29}}&lt;/ref&gt;

According to the Gartner [[hype cycle]] of 2016, machine learning is at its peak of inflated expectations.&lt;ref&gt;{{Cite news|url=http://www.gartner.com/newsroom/id/3412017|title=Gartner's 2016 Hype Cycle for Emerging Technologies Identifies Three Key Trends That Organizations Must Track to Gain Competitive Advantage|access-date=2017-04-10|language=en}}&lt;/ref&gt; Effective machine learning is difficult because finding patterns is hard and often not enough training data is available; as a result, machine-learning programs often fail to deliver.&lt;ref&gt;{{Cite news|url=https://www.bloomberg.com/news/articles/2016-11-10/why-machine-learning-models-often-fail-to-learn-quicktake-q-a|title=Why Machine Learning Models Often Fail to Learn: QuickTake Q&amp;A|date=2016-11-10|work=Bloomberg.com|access-date=2017-04-10}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://www.technologyreview.com/s/603944/microsoft-ai-isnt-yet-adaptable-enough-to-help-businesses/|title=Microsoft says its racist chatbot illustrates how AI isn’t adaptable enough to help most businesses|last=Simonite|first=Tom|work=MIT Technology Review|access-date=2017-04-10|language=en}}&lt;/ref&gt;

== Overview ==
[[Tom M. Mitchell]] provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: &quot;A computer program is said to learn from experience ''E'' with respect to some class of tasks ''T'' and performance measure ''P'' if its performance at tasks in ''T'', as measured by ''P'', improves with experience ''E''.&quot;&lt;ref&gt;{{cite book
|author=Mitchell, T.
|title=Machine Learning
|publisher=McGraw Hill
|isbn= 0-07-042807-7
|pages=2
|year=1997}}&lt;/ref&gt; This definition of the tasks in which machine learning is concerned offers a fundamentally [[Operational definition|operational definition]] rather than defining the field in cognitive terms. This follows [[Alan Turing]]'s proposal in his paper &quot;[[Computing Machinery and Intelligence]]&quot;, in which the question &quot;Can machines think?&quot; is replaced with the question &quot;Can machines do what we (as thinking entities) can do?&quot;.&lt;ref&gt;{{Citation |chapterurl=http://eprints.ecs.soton.ac.uk/12954/ |author = [[Stevan Harnad]] |year=2008 |chapter=The Annotation Game: On Turing (1950) on Computing, Machinery, and Intelligence |editor1-last=Epstein |editor1-first=Robert |editor2-last=Peters |editor2-first=Grace |title=The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer |location= |publisher=Kluwer |isbn= }}&lt;/ref&gt; In Turing's proposal the various characteristics that could be possessed by a ''thinking machine'' and the various implications in constructing one are exposed.

=== Types of problems and tasks ===
{{Anchor|Algorithm types}}

Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning &quot;signal&quot; or &quot;feedback&quot; available to a learning system:
* [[Supervised learning]]:  The computer is presented with example inputs and their desired outputs, given by a &quot;teacher&quot;, and the goal is to learn a general rule that [[Map (mathematics)|maps]] inputs to outputs. As special cases, the input signal can be only partially available, or restricted to special feedback:
** [[Semi-supervised learning]]: the computer is given only an incomplete training signal: a training set with some (often many) of the target outputs missing.
** [[Active learning (machine learning)|Active learning]]: the computer can only obtain training labels for a limited set of instances (based on a budget), and also has to optimize its choice of objects to acquire labels for. When used interactively, these can be presented to the user for labeling.
** [[Reinforcement learning]]: training data (in form of rewards and punishments) is given only as feedback to the program's actions in a dynamic environment, such as [[Autonomous car|driving a vehicle]] or playing a game against an opponent.&lt;ref name=&quot;bishop&quot; /&gt;{{rp|3}}
* [[Unsupervised learning]]:  No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end ([[feature learning]]).

[[File:Svm max sep hyperplane with margin.png|thumb|A [[support vector machine]] is a classifier that divides its input space into two regions, separated by a [[linear classifier|linear boundary]]. Here, it has learned to distinguish black and white circles.]]

Another categorization of machine learning tasks arises when one considers the desired ''output'' of a machine-learned system:&lt;ref name=&quot;bishop&quot; /&gt;{{rp|3}}
* In [[Statistical classification|classification]], inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more ([[multi-label classification]]) of these classes. This is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are &quot;spam&quot; and &quot;not spam&quot;.
* In [[regression analysis|regression]], also a supervised problem, the outputs are continuous rather than discrete.
* In [[Cluster analysis|clustering]], a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.
* [[Density estimation]] finds the [[Probability distribution|distribution]] of inputs in some space.
* [[Dimensionality reduction]] simplifies inputs by mapping them into a lower-dimensional space. [[Topic modeling]] is a related problem, where a program is given a list of [[natural language|human language]] documents and is tasked to find out which documents cover similar topics.
Among other categories of machine learning problems, [[Meta learning (computer science)|learning to learn]] learns its own [[inductive bias]] based on previous experience. [[Developmental robotics|Developmental learning]], elaborated for [[robot learning]], generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.

== History and relationships to other fields ==
{{see also|Timeline of machine learning}}
As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed &quot;[[neural network]]s&quot;; these were mostly [[perceptron]]s and [[ADALINE|other models]] that were later found to be reinventions of the [[generalized linear model]]s of statistics.&lt;ref&gt;{{cite journal|last1=Sarle|first1=Warren|title=Neural Networks and statistical models|journal=CiteseerX|citeseerx=10.1.1.27.699}}&lt;/ref&gt; [[Probability theory|Probabilistic]] reasoning was also employed, especially in automated medical diagnosis.&lt;ref name=&quot;aima&quot;&gt;{{cite AIMA|edition=2}}&lt;/ref&gt;{{rp|488}}

However, an increasing emphasis on the [[GOFAI|logical, knowledge-based approach]] caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.&lt;ref name=&quot;aima&quot; /&gt;{{rp|488}} By 1980, [[expert system]]s had come to dominate AI, and statistics was out of favor.&lt;ref name=&quot;changing&quot;&gt;{{Cite journal | last1 = Langley | first1 = Pat| title = The changing science of machine learning | doi = 10.1007/s10994-011-5242-y | journal = [[Machine Learning (journal)|Machine Learning]]| volume = 82 | issue = 3 | pages = 275–279 | year = 2011 | pmid =  | pmc = }}&lt;/ref&gt; Work on symbolic/knowledge-based learning did continue within AI, leading to [[inductive logic programming]], but the more statistical line of research was now outside the field of AI proper, in [[pattern recognition]] and [[information retrieval]].&lt;ref name=&quot;aima&quot; /&gt;{{rp|708–710; 755}} Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as &quot;[[connectionism]]&quot;, by researchers from other disciplines including [[John Hopfield|Hopfield]], [[David Rumelhart|Rumelhart]] and [[Geoff Hinton|Hinton]]. Their main success came in the mid-1980s with the reinvention of [[backpropagation]].&lt;ref name=&quot;aima&quot; /&gt;{{rp|25}}

Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and [[probability theory]].&lt;ref name=&quot;changing&quot; /&gt; It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the [[Internet]].

Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on ''known'' properties learned from the training data, [[data mining]] focuses on the [[discovery (observation)|discovery]] of (previously) ''unknown'' properties in the data (this is the analysis step of [[knowledge discovery]] in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as &quot;unsupervised learning&quot; or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, [[ECML PKDD]] being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to ''reproduce known'' knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously ''unknown'' knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some [[loss function]] on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.&lt;ref&gt;{{cite encyclopedia |last1=Le Roux |first1=Nicolas |first2=Yoshua |last2=Bengio |first3=Andrew |last3=Fitzgibbon |title=Improving First and Second-Order Methods by Modeling Uncertainty |encyclopedia=Optimization for Machine Learning |year=2012 |page=404 |editor-last1=Sra |editor-first1=Suvrit |editor-first2=Sebastian |editor-last2=Nowozin |editor-first3=Stephen J. |editor-last3=Wright |publisher=MIT Press}}&lt;/ref&gt;

=== Relation to statistics ===
Machine learning and [[statistics]] are closely related fields. According to [[Michael I. Jordan]], the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.&lt;ref name=&quot;mi jordan ama&quot;&gt;{{cite web|url=https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ckelmtt?context=3 |title=statistics and machine learning|publisher=reddit|date=2014-09-10|accessdate=2014-10-01|language=|author = [[Michael I. Jordan]]}}&lt;/ref&gt; He also suggested the term [[data science]] as a placeholder to call the overall field.&lt;ref name=&quot;mi jordan ama&quot; /&gt;

[[Leo Breiman]] distinguished two statistical modelling paradigms: data model and algorithmic model,&lt;ref&gt;{{cite web|url=http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726|title=Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)|author=Cornell University Library|publisher=|accessdate=8 August 2015}}&lt;/ref&gt; wherein &quot;algorithmic model&quot; means more or less the machine learning algorithms like [[Random forest]].

Some statisticians have adopted methods from machine learning, leading to a combined field that they call ''statistical learning''.&lt;ref name=&quot;islr&quot;&gt;{{cite book |author1=Gareth James |author2=Daniela Witten |author3=Trevor Hastie |author4=Robert Tibshirani |title=An Introduction to Statistical Learning |publisher=Springer |year=2013 |url=http://www-bcf.usc.edu/~gareth/ISL/ |page=vii}}&lt;/ref&gt;

== {{anchor|Generalization}} Theory ==
{{Main article|Computational learning theory}}
A core objective of a learner is to generalize from its experience.&lt;ref name=&quot;bishop2006&quot;&gt;{{citation|first= C. M. |last= Bishop |authorlink=Christopher M. Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |isbn=0-387-31073-8}}&lt;/ref&gt;&lt;ref&gt;{{Cite Mehryar Afshin Ameet 2012}}&lt;/ref&gt; Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.

The computational analysis of machine learning algorithms and their performance is a branch of [[theoretical computer science]] known as [[computational learning theory]]. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The [[bias–variance decomposition]] is one way to quantify generalization [[Errors and residuals|error]].

For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to [[overfitting]] and generalization will be poorer.&lt;ref name=&quot;alpaydin&quot;&gt;{{Cite book|author=Alpaydin, Ethem|title=Introduction to Machine Learning|url=https://mitpress.mit.edu/books/introduction-machine-learning |year=2010 |publisher=The MIT Press |place=London|isbn= 978-0-262-01243-0 |access-date=4 February 2017 }}&lt;/ref&gt;

In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in [[Time complexity#Polynomial time|polynomial time]]. There are two kinds of [[time complexity]] results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.

== Approaches ==
{{Main article|List of machine learning algorithms}}

=== Decision tree learning ===
{{Main article|Decision tree learning}}

Decision tree learning uses a [[decision tree]] as a [[predictive modelling|predictive model]], which maps observations about an item to conclusions about the item's target value.

=== Association rule learning ===
{{Main article|Association rule learning}}

Association rule learning is a method for discovering interesting relations between variables in large databases.

=== Artificial neural networks ===
{{Main article|Artificial neural network}}
An [[artificial neural network]] (ANN) learning algorithm, usually called &quot;neural network&quot; (NN), is a learning algorithm that is inspired by the structure and functional aspects of [[biological neural networks]]. Computations are structured in terms of an interconnected group of [[artificial neuron]]s, processing information using a [[connectionism|connectionist]] approach to [[computation]]. Modern neural networks are [[non-linear]] [[statistical]] [[data modeling]] tools. They are usually used to model complex relationships between inputs and outputs, to [[pattern recognition|find patterns]] in data, or to capture the statistical structure in an unknown [[joint probability distribution]] between observed variables.

=== Deep learning ===
{{Main article|Deep learning}}
Falling hardware prices and the development of [[GPU]]s for personal use in the last few years have contributed to the development of the concept of [[deep learning]] which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are [[computer vision]] and [[speech recognition]].&lt;ref&gt;Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng. &quot;[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;rep=rep1&amp;type=pdf Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations]&quot; Proceedings of the 26th Annual International Conference on Machine Learning, 2009.&lt;/ref&gt;

=== Inductive logic programming ===
{{Main article|Inductive logic programming}}

Inductive logic programming (ILP) is an approach to rule learning using [[logic programming]] as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that [[Entailment|entails]] all positive and no negative examples. [[Inductive programming]] is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as [[Functional programming|functional programs]].

=== Support vector machines ===
{{Main article|Support vector machines}}

Support vector machines (SVMs) are a set of related [[supervised learning]] methods used for [[statistical classification|classification]] and [[regression analysis|regression]]. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.

=== Clustering ===
{{Main article|Cluster analysis}}
Cluster analysis is the assignment of a set of observations into subsets (called ''clusters'') so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some ''similarity metric'' and evaluated for example by ''internal compactness'' (similarity between members of the same cluster) and ''separation'' between different clusters. Other methods are based on ''estimated density'' and ''graph connectivity''.
Clustering is a method of [[unsupervised learning]], and a common technique for [[statistics|statistical]] [[data analysis]].

=== Bayesian networks ===
{{Main article|Bayesian network}}

A Bayesian network, belief network or directed acyclic graphical model is a [[graphical model|probabilistic graphical model]] that represents a set of [[random variables]] and their [[conditional independence|conditional independencies]] via a [[directed acyclic graph]] (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform [[inference]] and learning.

=== Reinforcement learning ===
{{Main article|Reinforcement learning}}

Reinforcement learning is concerned with how an ''agent'' ought to take ''actions'' in an ''environment'' so as to maximize some notion of long-term ''reward''. Reinforcement learning algorithms attempt to find a ''policy'' that maps ''states'' of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the [[supervised learning]] problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.

=== Representation learning ===
{{Main article|Representation learning}}

Several learning algorithms, mostly [[unsupervised learning]] algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include [[principal components analysis]] and [[cluster analysis]]. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.

[[Manifold learning]] algorithms attempt to do so under the constraint that the learned representation is low-dimensional. [[Sparse coding]] algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). [[Multilinear subspace learning]] algorithms aim to learn low-dimensional representations directly from [[tensor]] representations for multidimensional data, without reshaping them into (high-dimensional) vectors.&lt;ref&gt;{{cite journal |first1=Haiping |last1=Lu |first2=K.N. |last2=Plataniotis |first3=A.N. |last3=Venetsanopoulos |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf |title=A Survey of Multilinear Subspace Learning for Tensor Data |journal=Pattern Recognition |volume=44 |number=7 |pages=1540–1551 |year=2011 |doi=10.1016/j.patcog.2011.01.004}}&lt;/ref&gt; [[Deep learning]] algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.&lt;ref&gt;{{cite book
 | title = Learning Deep Architectures for AI
 | author = [[Yoshua Bengio]]
 | publisher = Now Publishers Inc.
 | year = 2009
 | isbn = 978-1-60198-294-0
 | pages = 1–3
 | url = https://books.google.com/books?id=cq5ewg7FniMC&amp;pg=PA3
 }}&lt;/ref&gt;

=== Similarity and metric learning ===
{{Main article|Similarity learning}}

In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in [[Recommendation systems]].

=== Sparse dictionary learning ===
{{Main article|Sparse dictionary learning}}

In this method, a datum is represented as a linear combination of [[basis function]]s, and the coefficients are assumed to be sparse. Let ''x'' be a ''d''-dimensional datum, ''D'' be a ''d'' by ''n'' matrix, where each column of ''D'' represents a basis function. ''r'' is the coefficient to represent ''x'' using ''D''. Mathematically, sparse dictionary learning means solving &lt;math&gt;x \approx D r&lt;/math&gt; where ''r'' is sparse. Generally speaking, ''n'' is assumed to be larger than ''d'' to allow the freedom for a sparse representation.

Learning a dictionary along with sparse representations is [[strongly NP-hard]] and also difficult to solve approximately.&lt;ref&gt;A. M. Tillmann, &quot;[https://dx.doi.org/10.1109/LSP.2014.2345761 On the Computational Intractability of Exact and Approximate Dictionary Learning]&quot;,
IEEE Signal Processing Letters 22(1), 2015: 45–49.&lt;/ref&gt; A popular heuristic method for sparse dictionary learning is [[K-SVD]].

Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in [[image de-noising]]. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.&lt;ref&gt;Aharon, M, M Elad, and A Bruckstein. 2006. &quot;K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.&quot; Signal Processing, IEEE Transactions on 54 (11): 4311–4322&lt;/ref&gt;

=== Genetic algorithms ===
{{Main article|Genetic algorithm}}

A genetic algorithm (GA) is a [[Search algorithm|search]] [[Heuristic (computer science)|heuristic]] that mimics the process of [[natural selection]], and uses methods such as [[Mutation (genetic algorithm)|mutation]] and [[Crossover (genetic algorithm)|crossover]] to generate new [[Chromosome (genetic algorithm)|genotype]] in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.&lt;ref&gt;{{cite journal |last1=Goldberg |first1=David E. |first2=John H. |last2=Holland |title=Genetic algorithms and machine learning |journal=[[Machine Learning (journal)|Machine Learning]] |volume=3 |issue=2 |year=1988 |pages=95–99 |doi=10.1007/bf00113892}}&lt;/ref&gt;&lt;ref&gt;{{cite book |title=Machine Learning, Neural and Statistical Classification |first1=D. |last1=Michie |first2=D. J. |last2=Spiegelhalter |first3=C. C. |last3=Taylor |year=1994 |publisher=Ellis Horwood}}&lt;/ref&gt; Conversely, machine learning techniques have been used to improve the performance of genetic and [[evolutionary algorithm]]s.&lt;ref&gt;{{cite journal |last1=Zhang |first1=Jun |last2=Zhan |first2=Zhi-hui |last3=Lin |first3=Ying |last4=Chen |first4=Ni |last5=Gong |first5=Yue-jiao |last6=Zhong |first6=Jing-hui |last7=Chung |first7=Henry S.H. |last8=Li |first8=Yun |last9=Shi |first9=Yu-hui |title=Evolutionary Computation Meets Machine Learning: A Survey |journal=Computational Intelligence Magazine |publisher=IEEE |year=2011 |volume=6 |issue=4 |pages=68–75 |url=http://ieeexplore.ieee.org/iel5/10207/6052357/06052374.pdf?arnumber=6052374 |doi=10.1109/mci.2011.942584}}&lt;/ref&gt;

=== Rule-based machine learning ===
[[Rule-based machine learning]] is a general term for any machine learning method that identifies, learns, or evolves `rules’ to store, manipulate or apply, knowledge.  The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system.  This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.&lt;ref&gt;{{Cite journal|last=Bassel|first=George W.|last2=Glaab|first2=Enrico|last3=Marquez|first3=Julietta|last4=Holdsworth|first4=Michael J.|last5=Bacardit|first5=Jaume|date=2011-09-01|title=Functional Network Construction in Arabidopsis Using Rule-Based Machine Learning on Large-Scale Data Sets|url=http://www.plantcell.org/content/23/9/3101|journal=The Plant Cell|language=en|volume=23|issue=9|pages=3101–3116|doi=10.1105/tpc.111.088153|issn=1532-298X|pmc=3203449|pmid=21896882}}&lt;/ref&gt;  Rule-based machine learning approaches include [[learning classifier system]]s, [[association rule learning]], and [[artificial immune system]]s.

==== Learning classifier systems ====
{{Main article|Learning classifier system}}

Learning classifier systems (LCS) are a family of [[rule-based machine learning]] algorithms that combine a discovery component  (e.g. typically a [[genetic algorithm]]) with a learning component (performing either [[supervised learning]], [[reinforcement learning]], or [[unsupervised learning]]). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a [[piecewise]] manner in order to make predictions.&lt;ref&gt;{{Cite journal|last=Urbanowicz|first=Ryan J.|last2=Moore|first2=Jason H.|date=2009-09-22|title=Learning Classifier Systems: A Complete Introduction, Review, and Roadmap|url=http://www.hindawi.com/archive/2009/736398/|journal=Journal of Artificial Evolution and Applications|language=en|volume=2009|pages=1–25|doi=10.1155/2009/736398|issn=1687-6229}}&lt;/ref&gt;

== Applications ==
Applications for machine learning include:

{{div col}}
* [[Automated theorem proving]]&lt;ref&gt;Bridge, James P., Sean B. Holden, and Lawrence C. Paulson. &quot;[https://www.cl.cam.ac.uk/~lp15/papers/Reports/Bridge-ml.pdf Machine learning for first-order theorem proving].&quot; Journal of automated reasoning 53.2 (2014): 141-172.&lt;/ref&gt;&lt;ref&gt;Loos, Sarah, et al. &quot;[https://arxiv.org/pdf/1701.06972 Deep Network Guided Proof Search].&quot; arXiv preprint arXiv:1701.06972 (2017).&lt;/ref&gt;
* [[Adaptive website]]s{{citation needed|date=August 2017}}
* [[Affective computing]]
* [[Bioinformatics]]
* [[Brain–machine interface]]s
* [[Cheminformatics]]
* Classifying [[DNA sequence]]s
* [[Computational anatomy]]
* [[Network_simulation|Computer Networks]]
* [[Computer vision]], including [[object recognition]]
* Detecting [[credit-card fraud]]
* [[General game playing]]&lt;ref&gt;Finnsson, Hilmar, and Yngvi Björnsson. &quot;[https://vvvvw.aaai.org/Papers/AAAI/2008/AAAI08-041.pdf Simulation-Based Approach to General Game Playing].&quot; AAAI. Vol. 8. 2008.&lt;/ref&gt;
* [[Information retrieval]]
* [[Internet fraud]] detection&lt;ref name=&quot;alpaydin&quot;/&gt;
* [[Linguistics]]
* [[Marketing]]
* [[Machine learning control]]
* [[Machine perception]]
* [[Diagnosis (artificial intelligence)|Medical diagnosis]]
* [[Economics]]
* [[Insurance]]
* [[Natural language processing]]
* [[Natural language understanding]]&lt;ref&gt;Sarikaya, Ruhi, Geoffrey E. Hinton, and Anoop Deoras. &quot;[http://www.cs.utoronto.ca/~hinton/absps/ruhijournal.pdf Application of deep belief networks for natural language understanding].&quot; IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 22.4 (2014): 778-784.&lt;/ref&gt;
* [[Mathematical optimization|Optimization]] and [[metaheuristic]]
* [[Online advertising]]
* [[Recommender system]]s
* [[Robot locomotion]]
* [[Search engines]]
* [[Sentiment analysis]] (or opinion mining)
* [[Sequence mining]]
* [[Software engineering]]
* [[Speech recognition|Speech]] and [[handwriting recognition]]
* [[Financial market]] analysis
* [[Structural health monitoring]]
* [[Syntactic pattern recognition]]
* [[Time series|Time series forecasting]]
* [[User behavior analytics]]
* [[Translation]]&lt;ref&gt;{{cite web|url=http://english.yonhapnews.co.kr/news/2017/01/10/0200000000AEN20170110009700320.html?did=2106m|title=AI-based translation to soon reach human levels: industry officials|publisher=Yonhap news agency|accessdate=4 Mar 2017}}&lt;/ref&gt;{{div col end}}

In 2006, the online movie company [[Netflix]] held the first &quot;[[Netflix Prize]]&quot; competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from [[AT&amp;T Labs]]-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an [[Ensemble Averaging|ensemble model]] to win the Grand Prize in 2009 for $1 million.&lt;ref&gt;[http://www2.research.att.com/~volinsky/netflix/ &quot;BelKor Home Page&quot;] research.att.com&lt;/ref&gt; Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (&quot;everything is a recommendation&quot;) and they changed their recommendation engine accordingly.&lt;ref&gt;{{cite web|url=http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html|title=The Netflix Tech Blog: Netflix Recommendations: Beyond the 5 stars (Part 1)|publisher=|accessdate=8 August 2015}}&lt;/ref&gt;

In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis.
&lt;ref&gt;[https://www.wsj.com/articles/SB10001424052748703834604575365310813948080]&lt;/ref&gt;

In 2012, co-founder of [[Sun Microsystems]] [[Vinod Khosla]] predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.&lt;ref&gt;{{cite web|url=https://techcrunch.com/2012/01/10/doctors-or-algorithms/|author=Vonod Khosla|publisher=Tech Crunch|title=Do We Need Doctors or Algorithms?|date=January 10, 2012}}&lt;/ref&gt;

In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.&lt;ref&gt;[https://medium.com/the-physics-arxiv-blog/when-a-machine-learning-algorithm-studied-fine-art-paintings-it-saw-things-art-historians-had-never-b8e4e7bf7d3e When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed], ''The Physics at [[ArXiv]] blog''&lt;/ref&gt;

== Model assessments ==
Classification machine learning models can be validated by accuracy estimation techniques like the [[Test set|Holdout]] method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-[[Cross-validation (statistics)|cross-validation]] method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, [[bootstrap]], which samples n instances with replacement from the dataset, can be used to assess model accuracy.&lt;ref&gt;{{cite journal|last1=Kohavi|first1=Ron|title=A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection|journal=International Joint Conference on Artificial Intelligence|date=1995|url=http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf}}&lt;/ref&gt;

In addition to overall accuracy, investigators frequently report [[sensitivity and specificity]]  meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the [[False positive rate|False Positive Rate]] (FPR) as well as the [[False negative rate|False Negative Rate]] (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model’s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used [[Receiver operating characteristic]] (ROC) and ROC’s associated Area Under the Curve (AUC).

== Ethics ==
Machine learning poses a host of [[Machine ethics|ethical questions]]. Systems which are trained on datasets collected with biases may exhibit these biases upon use ([[algorithmic bias]]), thus digitizing cultural prejudices.&lt;ref&gt;{{Cite web|url=http://www.nickbostrom.com/ethics/artificial-intelligence.pdf|title=The Ethics of Artificial Intelligence|last=Bostrom|first=Nick|date=2011|website=|publisher=|access-date=11 April 2016 }}&lt;/ref&gt; For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.&lt;ref name=&quot;Edionwe Outline&quot;&gt;{{cite web|last1=Edionwe|first1=Tolulope|title=The fight against racist algorithms|url=https://theoutline.com/post/1571/the-fight-against-racist-algorithms|website=The Outline|accessdate=17 November 2017}}&lt;/ref&gt;&lt;ref name=&quot;Jeffries Outline&quot;&gt;{{cite web|last1=Jeffries|first1=Adrianne|title=Machine learning is racist because the internet is racist|url=https://theoutline.com/post/1439/machine-learning-is-racist-because-the-internet-is-racist|website=The Outline|accessdate=17 November 2017}}&lt;/ref&gt; Responsible [[Data collection|collection of data]] and documentation of algorithmic rules used by a system thus is a critical part of machine learning.

Because language contains biases, machines trained on language ''[[Text corpus|corpora]]'' will necessarily also learn bias.&lt;ref&gt;[https://freedom-to-tinker.com/2016/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/]&lt;/ref&gt;

== Software ==
[[Software suite]]s containing a variety of machine learning algorithms include the following :

=== Free and open-source software ===
{{Div col||15em}}
* [[Microsoft_Cognitive_Toolkit|CNTK]]
* [[Deeplearning4j]]
* [[dlib]]
* [[ELKI]]
* [[GNU Octave]]
* [[H2o (Analytics tool)|H2O]]
* [[Apache Mahout|Mahout]]
* [[Mallet (software project)|Mallet]]
* [[Multi expression programming|MEPX]]
* [[mlpy]]
* [[MLPACK (C++ library)|MLPACK]]
* [[MOA (Massive Online Analysis)]]
* [[MXNet]]
* [[ND4J (software)|ND4J: ND arrays for Java]]
* [[Numenta#The NuPIC Open Source Project|NuPIC]]
* [[OpenAI#OpenAI Gym|OpenAI Gym]]
* [[OpenAI#Universe|OpenAI Universe]]
* [[OpenNN]]
* [[Orange (software)|Orange]]
* [[R (programming language)|R]]
* [[scikit-learn]]
* [[Shogun (toolbox)|Shogun]]
* [[TensorFlow]]
* [[Torch (machine learning)|Torch]]
* [[Yooreeka]]
* [[Weka (machine learning)|Weka]]
{{Div col end}}

=== Proprietary software with free and open-source editions ===
{{Div col||15em}}
* [[KNIME]]
* [[RapidMiner]]
{{Div col end}}

=== Proprietary software ===
{{Div col||15em}}
* [[Amazon Machine Learning]]
* [[Angoss]] KnowledgeSTUDIO
* [[Ayasdi]]
* [[IBM Data Science Experience]]
* [[Google APIs|Google Prediction API]]
* [[SPSS Modeler|IBM SPSS Modeler]]
* [[KXEN Inc.|KXEN Modeler]]
* [[LIONsolver]]
* [[Mathematica]]
* [[MATLAB]]
* [[Azure machine learning studio|Microsoft Azure Machine Learning]]
* [[Neural Designer]]
* [[NeuroSolutions]]
* [[Oracle Data Mining]]
* [[Oracle_Corporation#Services|Oracle AI Platform Cloud Service]]
* [[RCASE]]
* [[SAP Leonardo]]
* [[SAS (software)#Components|SAS Enterprise Miner]]
* [[SequenceL]]
* [[Splunk]]
* [[STATISTICA]] Data Miner
{{Div col end}}

== Journals ==
* ''[[Journal of Machine Learning Research]]''
* [[Machine Learning (journal)|''Machine Learning'']]
* [[Neural Computation (journal)|''Neural Computation'']]

== Conferences ==
* [[Conference on Neural Information Processing Systems]]
* [[International Conference on Machine Learning]]
* International Conference on Learning Representations

== See also ==
{{Portal|Artificial intelligence|Machine learning}}
{{columns-list|2|
* [[Artificial intelligence]]
* [[Automatic reasoning]]
* [[Big data]]
* [[Computational intelligence]]
* [[Computational neuroscience]]
* [[Data science]]
* [[Deep learning]]
* [[Ethics of artificial intelligence]]
* [[Existential risk from advanced artificial intelligence]]
* [[Explanation-based learning]]
* [[Quantum machine learning]]
* [[List of important publications in computer science#Machine learning|Important publications in machine learning]]
* [[List of machine learning algorithms]]
* [[List of datasets for machine learning research]]
* [[Similarity learning]]
*[[Machine learning in bioinformatics|Machine-learning applications in bioinformatics]]
}}

== References ==
{{Reflist|30em}}

== Further reading ==
{{Refbegin|2}}
*  Nils J. Nilsson, ''[http://ai.stanford.edu/people/nilsson/mlbook.html Introduction to Machine Learning]''.
* [[Trevor Hastie]], [[Robert Tibshirani]] and [[Jerome H. Friedman]] (2001). ''[http://www-stat.stanford.edu/~tibs/ElemStatLearn/ The Elements of Statistical Learning]'', Springer. {{ISBN|0-387-95284-5}}.
* [[Pedro Domingos]] (September 2015), [[The Master Algorithm]], Basic Books, {{ISBN|978-0-465-06570-7}}
* Ian H. Witten and Eibe Frank (2011). ''Data Mining: Practical machine learning tools and techniques'' Morgan Kaufmann, 664pp., {{ISBN|978-0-12-374856-0}}.
* Ethem Alpaydin (2004). Introduction to Machine Learning, MIT Press, {{ISBN|978-0-262-01243-0}}.
* [[David J. C. MacKay]]. ''[http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. {{ISBN|0-521-64298-1}}
* [[Richard O. Duda]], [[Peter E. Hart]], David G. Stork (2001) ''Pattern classification'' (2nd edition), Wiley, New York, {{ISBN|0-471-05669-3}}.
* [[Christopher Bishop]] (1995). ''Neural Networks for Pattern Recognition'', Oxford University Press. {{ISBN|0-19-853864-2}}.
* Stuart Russell &amp; Peter Norvig, (2002). ''Artificial Intelligence - A Modern Approach''. Prentice Hall, {{ISBN|0-136-04259-7}}.
* [[Ray Solomonoff]], ''An Inductive Inference Machine'', IRE Convention Record, Section on Information Theory, Part 2, pp., 56-62, 1957.
* [[Ray Solomonoff]], &quot;[http://world.std.com/~rjs/indinf56.pdf An Inductive Inference Machine]&quot; A privately circulated report from the 1956 [[Dartmouth workshop|Dartmouth Summer Research Conference on AI]].
{{Refend}}

== External links ==
* [http://machinelearning.org/ International Machine Learning Society]
* Popular online course by [[Andrew Ng]], at [https://www.coursera.org/learn/machine-learning Coursera]. It uses [[GNU Octave]]. The course is a free version of [[Stanford University]]'s actual course taught by Ng, whose lectures are also [https://see.stanford.edu/Course/CS229 available for free].
* [https://mloss.org/ mloss] is an academic database of open-source machine learning software.

[[Category:Machine learning| ]]

</text>
      <sha1>405hiukcnza116nebkj8yctg6y02x03</sha1>
    </revision>
  </page>
  <page>
    <title>OpenNN</title>
    <ns>0</ns>
    <id>42129549</id>
    <revision>
      <id>808477955</id>
      <parentid>804552073</parentid>
      <timestamp>2017-11-03T02:57:48Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v475)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5994">{{Infobox software
| name                   = Open Neural Networks Library
| title                  = OpenNN
| logo                   =
| screenshot             =
| caption                =
| developer              = [http://www.artelnics.com Artelnics]
| frequently updated     =
| operating system       = [[Cross-platform]]
| platform               =
| genre                  = [[Neural networks]]
| license                = [[GNU Lesser General Public License|LGPL]]
| website                = {{URL|http://www.opennn.net}}
}}

'''OpenNN''' (Open Neural Networks Library) is a [[software library]] written in the [[C++]] [[programming language]] which implements [[neural networks]],&lt;ref&gt;{{cite web|url=http://www.kdnuggets.com/2014/06/opennn-open-source-library-neural-networks.html|title=OpenNN, An Open Source Library For Neural Networks | publisher=KDNuggets | date= June 2014}}&lt;/ref&gt; a main area of [[deep learning]] research. The library is [[open source]], licensed under the [[GNU Lesser General Public License]].

==Characteristics==

The software implements any number of layers of non-linear processing units for [[supervised learning]]. This deep architecture allows the design of neural networks with [[Universal approximation theorem | universal approximation]] properties. Additionally, it allows [[multiprocessing]] programming by means of [[OpenMP]], in order to increase [[computer performance]].

OpenNN contains [[data mining]] algorithms as a bundle of functions. These can be embedded in other software tools, using an [[application programming interface]], for the integration of the [[predictive analytics]] tasks. In this regard, a graphical user interface is missing but some functions can be supported by specific visualization tools.&lt;ref&gt;{{cite journal | url=https://www.academia.edu/6491835/Categorization_of_Data_Mining_Tools_Based_on_Their_Types_ | title=Categorization of Data Mining Tools Based on Their Types | author=J. Mary Dallfin Bruxella| journal = International Journal of Computer Science and Mobile Computing | volume = 3 | issue = 3 | pages = 445-452 | year = 2014 |display-authors=etal}}&lt;/ref&gt;

==History==

The development started in 2003 at the [[International Center for Numerical Methods in Engineering | International Center for Numerical Methods in Engineering (CIMNE)]], within the research project funded by the [[European Union]] called RAMFLOOD (Risk Assessment and Management of FLOODs).&lt;ref&gt;{{cite web|url=http://cordis.europa.eu/projects/rcn/67049_en.html |title=CORDIS - EU Research Project RAMFLOOD |publisher=European Commission | date=December 2004}}&lt;/ref&gt; Then it continued as part of similar projects.
At present, OpenNN is being developed by the [[startup company]] Artelnics.&lt;ref&gt;{{cite web | url=http://www.artelnics.com |title=Artelnics home page}}&lt;/ref&gt;

In 2014, ''Big Data Analytics Today'' rated OpenNN as the #1 brain inspired [[artificial intelligence]] project.&lt;ref&gt;{{cite web | url=http://www.bigdataanalyticstoday.com/top-brain-inspired-artificial-intelligence-projects | title = Top 12 Brain Inspired Artificial Intelligence Projects | publisher= Big Data Analytics Today | date= October 2014}}&lt;/ref&gt;
Also, during the same year, ''ToppersWorld'' selected OpenNN among the top 5 open source [[data mining]] tools.&lt;ref&gt;{{cite web | url=http://toppersworld.com/top-5-open-source-data-mining-tools/|title=Top 5 Open Source Data Mining Tools|publisher=ToppersWorld | date = November 2014}}&lt;/ref&gt;

==Applications==

OpenNN is a general purpose [[artificial intelligence]] software package.&lt;ref&gt;{{cite web|url= http://www.efytimes.com/e1/fullnews.asp?edid=142005|title=Here Are 7 Thought-Provoking AI Software Packages For Your Info|publisher=Saurabh Singh|accessdate=25 June 2014}}&lt;/ref&gt; It uses [[machine learning]] techniques for solving [[data mining]] and [[predictive analytics]] tasks in different fields. For instance, the library has been applied in the engineering,&lt;ref&gt;{{cite journal|url= http://onlinelibrary.wiley.com/doi/10.1002/nme.2304/abstract|title= Neural Networks for Variational Problems in Engineering | author = R. Lopez| journal = International Journal for Numerical Methods in Engineering | volume = 75 | issue = 11 | pages =  1341–1360 | year = 2008 | doi=10.1002/nme.2304|display-authors=etal}}&lt;/ref&gt; energy,&lt;ref&gt;{{cite journal | url=https://link.springer.com/chapter/10.1007/978-3-642-20282-7_20 | title=Optimisation of Concentrating Solar Thermal Power Plants with Neural Networks | journal = Lecture Notes in Computer Science | author = P. Richter| volume = 6593 | pages = 190–199 | year = 2011 | doi=10.1007/978-3-642-20282-7_20|display-authors=etal}}&lt;/ref&gt; or chemistry&lt;ref&gt;{{cite journal | url= https://link.springer.com/article/10.1007/s00216-014-8317-3 | title = Artificial Neural Network Prediction of Multilinear Gradient Retention in Reversed-Phase HPLC | journal = Analytical and Bioanalytical Chemistry |  author= A.A. D’Archivio| pages = 1-10 | year= 2014 | doi=10.1007/s00216-014-8317-3 | volume=407|display-authors=etal}}&lt;/ref&gt; sectors.

==See also==

{{Portal|Free software|Artificial intelligence}}
* [[Comparison of deep learning software]]
* [[Neural Designer]], also developed by Artelnics
* [[Artificial intelligence]]
* [[Machine learning]]
* [[Deep learning]]
* [[Artificial neural network]]

==References==
{{Reflist}}

==External links==
* [https://github.com/Artelnics/OpenNN OpenNN project at GitHub]
* [http://www.sourceforge.net/projects/opennn OpenNN project at SourceForge]

{{Deep Learning Software}}




[[Category:C++ libraries]]
[[Category:Data mining and machine learning software]]

[[Category:Free software programmed in C++]]


[[Category:Open-source artificial intelligence]]
[[Category:Software using the LGPL license]]</text>
      <sha1>1kuuj43vyq6p0ogqv8nng68l6iq7n5f</sha1>
    </revision>
  </page>
  <page>
    <title>Ball tree</title>
    <ns>0</ns>
    <id>31877832</id>
    <revision>
      <id>814701455</id>
      <parentid>814700819</parentid>
      <timestamp>2017-12-10T11:18:37Z</timestamp>
      <contributor>
        <ip>77.177.132.161</ip>
      </contributor>
      <comment>Earliest reference we have so far.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10011">{{About|the binary tree variant|a metric-tree involving multi-way splits|M-tree}}

In [[computer science]], a '''ball tree''', '''balltree'''&lt;ref name=r1 /&gt; or '''metric tree''', is a [[space partitioning]] [[data structure]] for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as &quot;balls&quot;. The resulting data structure has characteristics that make it useful for a number of applications, most notably [[nearest neighbor search]].

== Informal description ==
A ball tree is a [[binary tree]] in which every node defines a D-dimensional [[hypersphere]], or ball, containing a subset of the points to be searched. Each internal node of the tree partitions the data points into two disjoint sets which are associated with different balls. While the balls themselves may intersect, each point is assigned to one or the other ball in the partition according to its distance from the ball's center. Each leaf node in the tree defines a ball and enumerates all data points inside that ball.

Each node in the tree defines the smallest ball that contains all data points in its subtree. This gives rise to the useful property that, for a given test point {{mvar|t}}, the distance to any point in a ball {{mvar|B}} in the tree is greater than or equal to the distance from {{mvar|t}} to the ball. &lt;!-- an explanation with a graphic might be nice here --&gt;  Formally:
&lt;ref name=&quot;liu&quot;&gt;{{cite journal|author1=Liu, T. |author2=Moore, A. |author3=Gray, A.  |lastauthoramp=yes |url=http://people.ee.duke.edu/~lcarin/liu06a.pdf |title=New Algorithms for Efficient High-Dimensional Nonparametric Classification|journal=Journal of Machine Learning Research|volume=7|pages=1135–1158|year= 2006}}&lt;/ref&gt;
: &lt;math&gt;
    D^{B}(t) =
        \begin{cases}
            \max(|t - B.pivot| - B.radius, D^{B.parent}),
                &amp; \text{if }B \neq Root \\
            \max(|t - B.pivot| - B.radius, 0),
                &amp; \text{if }B = Root \\
        \end{cases}
&lt;/math&gt;
Where &lt;math&gt;D^{B}(t)&lt;/math&gt; is the minimum possible distance from any point in the ball {{mvar|B}} to some point {{mvar|t}}.

Ball-trees are related to the [[M-tree]], but only support binary splits, whereas in the M-tree each level splits &lt;math&gt;m&lt;/math&gt; to &lt;math&gt;2m&lt;/math&gt; fold, thus leading to a shallower tree structure, therefore need fewer distance computations, which usually yields faster queries. Furthermore, M-trees can better be stored [[Computer_data_storage#Secondary_storage|on disk]], which is organized in [[Page (computer memory)|pages]]. The M-tree also keeps the distances from the parent node precomputed to speed up queries.

[[Vantage-point tree]]s are also similar, but they binary split into one ball, and the remaining data, instead of using two balls.

== Construction ==
A number of ball tree construction algorithms are available.&lt;ref name=r1&gt;Omohundro, Stephen M. (1989) [ftp://ftp.icsi.berkeley.edu/pub/techreports/1989/tr-89-063.pdf &quot;Five Balltree Construction Algorithms&quot;]&lt;/ref&gt; The goal of such an algorithm is to produce a tree that will efficiently support queries of the desired type (e.g. nearest-neighbor) efficiently in the average case. The specific criteria of an ideal tree will depend on the type of question being answered and the distribution of the underlying data. However, a generally applicable measure of an efficient tree is one that minimizes the total volume of its internal nodes. Given the varied distributions of real-world data sets, this is a difficult task, but there are several heuristics that partition the data well in practice. In general, there is a tradeoff between the cost of constructing a tree and the efficiency achieved by this metric.
&lt;ref name=&quot;liu&quot; /&gt;

This section briefly describes the simplest of these algorithms. A more in-depth discussion of five algorithms was given by Stephen Omohundro.&lt;ref name=r1/&gt;

=== k-d Construction Algorithm ===
The simplest such procedure is termed the &quot;k-d Construction Algorithm&quot;, by analogy with the process used to construct [[k-d tree]]s. This is an [[off-line algorithm]], that is, an algorithm that operates on the entire data set at once. The tree is built top-down by recursively splitting the data points into two sets. Splits are chosen along the single dimension with the greatest spread of points, with the sets partitioned by the median value of all points along that dimension. Finding the split for each internal node requires linear time in the number of samples contained in that node, yielding an algorithm with [[time complexity]] &lt;math&gt;O(n\, \log\, n)&lt;/math&gt;, where ''n'' is the number of data points.

==== Pseudocode ====
    '''function''' construct_balltree '''is'''
        '''input:'''
            D, an array of data points
        '''output:'''
            B, the root of a constructed ball tree
        '''if''' a single point remains '''then'''
            create a leaf B containing the single point in D
            return B
        '''else'''
            let c be the dimension of greatest spread
            let p be the central point selected considering c
            let L,R be the sets of points lying to the left and right of the median along dimension c
            create B with two children:
                B.pivot = p
                B.child1 = construct_balltree(L),
                B.child2 = construct_balltree(R),
                let B.radius be maximum distance from p among children
            return B
        '''end if'''
    '''end function'''

== Nearest-neighbor search ==

An important application of ball trees is expediting [[nearest neighbor search]] queries, in which the objective is to find the k points in the tree that are closest to a given test point by some distance metric (e.g. [[Euclidean distance]]). A simple search algorithm, sometimes called KNS1, exploits the distance property of the ball tree. In particular, if the algorithm is searching the data structure with a test point ''t'', and has already seen some point ''p'' that is closest to ''t'' among the points encountered so far, then any subtree whose ball is further from ''t'' than ''p'' can be ignored for the rest of the search.

=== Description ===

The ball tree nearest-neighbor algorithm examines nodes in depth-first order, starting at the root. During the search, the algorithm
maintains a max-first [[priority queue]] (often implemented with a [[Heap (data structure)|heap]]), denoted ''Q'' here, of the k nearest points encountered so far. At each node ''B'', it may perform one of three operations, before finally returning an updated version of the priority queue:

# If the distance from the test point ''t'' to the current node ''B'' is greater than the furthest point in ''Q'', ignore ''B'' and return ''Q''.
# If ''B'' is a leaf node, scan through every point enumerated in ''B'' and update the nearest-neighbor queue appropriately. Return the updated queue.
# If ''B'' is an internal node, call the algorithm recursively on ''B'''s two children, searching the child whose center is closer to ''t'' first. Return the queue after each of these calls has updated it in turn.

Performing the recursive search in the order described in point 3 above increases likelihood that the further child will be pruned
entirely during the search.

=== Pseudocode ===
    '''function''' knn_search '''is'''
        '''input:'''
            t, the target point for the query
            k, the number of nearest neighbors of t to search for
            Q, max-first priority queue containing at most k points
            B, a node, or ball, in the tree
        '''output:'''
            Q, containing the k nearest neighbors from within B
        '''if''' distance(t, B.pivot) - B.radius ≥ distance(t, Q.first) '''then'''
            '''return''' Q unchanged
        '''else if''' B is a leaf node '''then'''
            '''for each''' point p in B '''do'''
                '''if''' distance(t, p) &lt; distance(t, Q.first) '''then'''
                    add p to Q
                    '''if''' size(Q) &gt; k '''then'''
                        remove the furthest neighbor from Q
                    '''end if'''
                '''end if'''
            '''repeat'''
        '''else'''
            let child1 be the child node closest to t
            let child2 be the child node furthest from t
            knn_search(t, k, Q, child1)
            knn_search(t, k, Q, child2)
        '''end if'''
    '''end function'''&lt;ref name=&quot;liu&quot; /&gt;

=== Performance ===
In comparison with several other data structures, ball trees have been shown to perform fairly well on
the nearest-neighbor search problem, particularly as their number of dimensions grows.&lt;ref name=&quot;kumar&quot;&gt;{{Cite book | doi = 10.1007/978-3-540-88688-4_27| chapter = What is a Good Nearest Neighbors Algorithm for Finding Similar Patches in Images?| title = Computer Vision – ECCV 2008| volume = 5303| pages = 364| series = Lecture Notes in Computer Science| year = 2008| last1 = Kumar | first1 = N. | last2 = Zhang | first2 = L. | last3 = Nayar | first3 = S. | isbn = 978-3-540-88685-3|url=http://www1.cs.columbia.edu/CAVE/publications/pdfs/Kumar_ECCV08_2.pdf}}&lt;/ref&gt;&lt;ref name=&quot;kibriya&quot;&gt;{{Cite book | doi = 10.1007/978-3-540-74976-9_16| chapter = An Empirical Comparison of Exact Nearest Neighbour Algorithms| title = Knowledge Discovery in Databases: PKDD 2007| volume = 4702| pages = 140| series = Lecture Notes in Computer Science| year = 2007| last1 = Kibriya | first1 = A. M. | last2 = Frank | first2 = E. | isbn = 978-3-540-74975-2|url=http://www.cs.waikato.ac.nz/~ml/publications/2007/KibriyaAndFrankPKDD07.pdf}}&lt;/ref&gt;
However, the best nearest-neighbor data structure for a given application will depend on the dimensionality, number of data points, and underlying structure of the data.

==References==

{{reflist}}

[[Category:Trees (data structures)]]

</text>
      <sha1>kg6yirzql2w6m90qrnb2q4jwd9ulfg5</sha1>
    </revision>
  </page>
  <page>
    <title>User behavior analytics</title>
    <ns>0</ns>
    <id>47228422</id>
    <revision>
      <id>813856443</id>
      <parentid>801515008</parentid>
      <timestamp>2017-12-05T16:22:03Z</timestamp>
      <contributor>
        <ip>179.208.120.116</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6137">'''User behavior analytics''' (&quot;'''UBA'''&quot;) as defined by [[Gartner]] or '''Análise de comportamento de Usuário''' ('''''ACU''''') in Portuguese, is a [[cybersecurity]] process about [[threat detection|detection of insider threats]], targeted attacks, and financial fraud. UBA solutions look at patterns of [[human behavior]], and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats.&lt;ref&gt;[https://www.gartner.com/doc/2831117/market-guide-user-behavior-analytics Market Guide for User Behavior Analytics&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;  Instead of tracking devices or security events, UBA tracks a system's users.&lt;ref&gt;[http://searchsecurity.techtarget.com/feature/The-hunt-for-data-analytics-Is-your-SIEM-on-the-endangered-list The hunt for data analytics: Is your SIEM on the endangered list?&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; [[Big data]] platforms like [[Apache Hadoop]] are increasing UBA functionality by allowing them to analyze [[petabyte]]s worth of data to detect [[insider threat]]s and [[advanced persistent threat]]s.&lt;ref&gt;{{Cite journal|last=Ahlm|first=Eric|last2=Litan|first2=Avivah|date=26 April 2016|title=Market Trends: User and Entity Behavior Analytics Expand Their Market Reach|url=https://www.gartner.com/doc/reprints?id=1-370BP2V&amp;ct=160518&amp;st=sb|journal=Gartner|volume=|issue=|doi=|pmid=|access-date=15 July 2016|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.cloudera.com/solutions/cybersecurity.html|title=Cybersecurity at petabyte scale|last=|first=|date=|website=|publisher=|access-date=15 July 2016}}&lt;/ref&gt;

== Purpose ==
The problem UBA responds to, as described by [[Nemertes Research]] CEO Johna Till Johnson, is that &quot;[[Security system]]s provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that [[SIEM]], [[Intrusion detection system|IDS]]/IPS, [[system log]]s, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-[[buying pattern]]s. But as it turns out, UBA can be extraordinarily useful in the security context too.&quot; &lt;ref&gt;[http://searchsecurity.techtarget.com/feature/User-behavioral-analytics-tools-can-thwart-security-attacks User behavioral analytics tools can thwart security attacks&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

== Market developments ==
Developments in UBA technology led Gartner to evolve the category to '''user and entity behavior analytics''' (&quot;'''UEBA'''&quot;). In September 2015, Gartner published the Market Guide for User and Entity Analytics by Vice President and Distinguished Analyst, Avivah Litan, that provided a thorough definition and explanation. UEBA was referred to in earlier Gartner reports but not in much depth. Expanding the definition from UBA includes devices, applications, [[server (computing)|server]]s, [[data]], or anything with an [[IP address]]. It moves beyond the fraud-oriented UBA focus to a broader one encompassing &quot;malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems, such as SIEM and DLP.&quot;&lt;ref&gt;{{Cite web|url=https://www.gartner.com/doc/3134524/market-guide-user-entity-behavior|title=Market Guide for User and Entity Behavior Analytics|website=www.gartner.com|access-date=2016-11-10}}&lt;/ref&gt; The addition of &quot;entity&quot; reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity. &quot;When end users have been compromised, [[malware]] can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats.&quot;&lt;ref&gt;{{Cite web|url=http://www.csoonline.com/article/2998174/security-awareness/user-entity-behavior-analytics-next-step-in-security-visibilty.html|title=User entity behavior analytics, next step in security {{sic|hide=y|nolink=y|reason=typo in source|visibil|ty}}|last=Zurkus|first=Kacy|website=CSO Online|access-date=2016-06-06}}&lt;/ref&gt;

Particularly in the [[computer security]] market, there are many vendors for UEBA applications. They can be &quot;differentiated by whether they are designed to monitor on-premises or [[Cloud computing|cloud]]-based [[software as a service]] (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based).&quot;&lt;ref&gt;{{Cite web|url=http://www.gartner.com/smarterwithgartner/detect-security-breaches-early-by-analyzing-behavior/|title=Detect Security Breaches Early by Analyzing Behavior - Smarter With Gartner|date=2015-06-04|website=Smarter With Gartner|language=en-US|access-date=2016-06-06}}&lt;/ref&gt;
According to the 2015 market guide released by Gartner, &quot;the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased.&quot;&lt;ref name=&quot;:0&quot;&gt;{{Cite web|url=https://www.gartner.com/doc/reprints?id=1-2NK6M1R&amp;ct=150922&amp;st=sb|title=Market Guide for User and Entity Behavior Analytics|last=|first=|date=September 22, 2015|website=|publisher=Gartner, Inc.|access-date=June 6, 2016}}&lt;/ref&gt; The report further projected, &quot;Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems.&quot;&lt;ref name=&quot;:0&quot; /&gt;

==See also==
* [[Network Behavior Anomaly Detection]]
* [[Behavioral analytics]]

==References==
{{Reflist}}




</text>
      <sha1>kdc8ozzxq49x3fpat53qyxkmhv8wgjq</sha1>
    </revision>
  </page>
  <page>
    <title>Word2vec</title>
    <ns>0</ns>
    <id>47527969</id>
    <revision>
      <id>813095384</id>
      <parentid>806543493</parentid>
      <timestamp>2017-12-01T18:57:26Z</timestamp>
      <contributor>
        <ip>171.66.213.131</ip>
      </contributor>
      <comment>Added an extended version of Word2vec in the &quot;Extensions&quot; section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14800">{{machine learning bar}}

'''Word2vec''' is a group of related models that are used to produce [[word embedding]]s. These models are shallow, two-layer [[neural network]]s that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a [[vector space]], typically of several hundred [[dimensions]], with each unique word in the [[Corpus linguistics|corpus]] being assigned a corresponding vector in the space. [[Word vectors]] are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.&lt;ref name=&quot;mikolov&quot;&gt;{{cite arXiv|first=Tomas |last=Mikolov |title=Efficient Estimation of Word Representations in Vector Space|arxiv=1301.3781|display-authors=etal}}&lt;/ref&gt;

Word2vec was created by a team of researchers led by Tomas Mikolov at [[Google]]. The algorithm has been subsequently analysed and explained by other researchers.&lt;ref name=&quot;explain&quot;&gt;{{cite arXiv |first1=Yoav |last1=Goldberg |first2=Omer |last2=Levy |title=word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method|arxiv=1402.3722}}&lt;/ref&gt;&lt;ref name=&quot;extensions&quot;&gt;{{cite AV media
|first=Radim|last=Řehůřek|title=Word2vec and friends|medium=Youtube video|url=https://www.youtube.com/watch?v=wTp3P2UnTfQ|accessdate=2015-08-14}}&lt;/ref&gt; Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms&lt;ref name=&quot;mikolov&quot; /&gt; such as [[latent semantic analysis]].

==CBOW and skip grams==
Word2vec can utilize either of two model architectures to produce a [[distributed representation]] of words: [[continuous bag-of-words]] (CBOW) or continuous [[skip-gram]]. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction ([[bag-of-words]] assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.  The skip-gram architecture weighs nearby context words more heavily than more distant context words.&lt;ref name=&quot;mikolov&quot;/&gt;&lt;ref name=&quot;mikolov-nips&quot;&gt;{{cite conference |first1=Tomas |last1=Mikolov |first2=Ilya |last2=Sutskever |first3=Kai |last3=Chen |first4=Greg S. |last4=Corrado |first5=Jeff |last5=Dean |title=Distributed representations of words and phrases and their compositionality |conference=[[Advances in Neural Information Processing Systems]] |year=2013|arxiv=1310.4546}}&lt;/ref&gt; According to the authors' note,&lt;ref name=&quot;:1&quot;&gt;{{Cite web|url=https://code.google.com/archive/p/word2vec/|title=Google Code Archive - Long-term storage for Google Code Project Hosting.|website=code.google.com|access-date=2016-06-13}}&lt;/ref&gt; CBOW is faster while skip-gram is slower but does a better job for infrequent words.

== Parametrization ==
Results of word2vec training can be sensitive to [[parametrization]]. The following are some important parameters in word2vec training.

=== Training algorithm ===
A Word2vec model can be trained with [[hierarchical softmax]] and/or [[negative sampling]]. To approximate the [[conditional log-likelihood]] a model seeks to maximize, the hierarchical softmax method uses a [[Huffman coding|Huffman tree]] to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the [[log-likelihood]] of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.&lt;ref name=&quot;:1&quot; /&gt; As training epochs increase, hierarchical softmax stops being useful.&lt;ref&gt;{{Cite web|url=https://groups.google.com/forum/#!msg/word2vec-toolkit/WUWad9fL0jU/LdbWy1jQjUIJ|title=Parameter (hs &amp; negative)|last=|first=|date=|website=Google Groups|publisher=|access-date=2016-06-13}}&lt;/ref&gt;

=== Sub-sampling ===
High frequency words often provide little information. Words with frequency above a certain threshold may be subsampled to increase training speed.&lt;ref&gt;{{Cite web|url=http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf|title=Visualizing Data using t-SNE|last=|first=|date=|website= Journal of Machine Learning Research, 2008. Vol. 9, pg. 2595|publisher=|access-date=2017-03-18}}&lt;/ref&gt;

=== Dimensionality ===
Quality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain will diminish.&lt;ref name=&quot;mikolov&quot; /&gt; Typically, the dimensionality of the vectors is set to be between 100 and 1,000.

=== Context window ===
The size of the context window determines how many words before and after a given word would be included as context words of the given word. According to the authors' note, the recommended value is 10 for skip-gram and 5 for CBOW.&lt;ref name=&quot;:1&quot; /&gt;

==Extensions==
An extension of word2vec to construct embeddings from entire documents (rather than the individual words) has been proposed.&lt;ref name=&quot;doc2vec&quot;&gt;{{cite arXiv
|first=Quoc|last=Le|title=Distributed Representations of Sentences and Documents. |arxiv=1405.4053|display-authors=etal}}&lt;/ref&gt; This extension is called paragraph2vec or doc2vec and has been implemented in the C, Python&lt;ref name=&quot;doc2vec_python&quot;&gt;{{cite web
|title=Doc2Vec tutorial using Gensim|url=https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1|accessdate=2015-08-02|display-authors=etal}}&lt;/ref&gt;&lt;ref name=&quot;doc2vec_imdb&quot;&gt;{{cite web
|title=Doc2vec for IMDB sentiment analysis|url=https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb|accessdate=2016-02-18|display-authors=etal}}&lt;/ref&gt; and Java/Scala&lt;ref name=&quot;doc2vec_java&quot;&gt;{{cite web
|title=Doc2Vec and Paragraph Vectors for Classification|url=http://deeplearning4j.org/doc2vec.html|accessdate=2016-01-13|display-authors=etal}}&lt;/ref&gt; tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.

==Word vectors for bioinformatics: BioVectors==
An extension of word vectors for n-grams in [[biological]] sequences (e.g. [[DNA]], [[RNA]], and [[Protein]]s) for [[bioinformatic]]s applications have been proposed by Asgari and Mofrad.&lt;ref name=&quot;:0&quot;&gt;{{cite journal|last1=Asgari|first1=Ehsaneddin|last2=Mofrad|first2=Mohammad R.K.|title=Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics|journal=PLOS ONE|date=2015|volume=10|issue=11|page=e0141287|url=http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287|doi=10.1371/journal.pone.0141287}}&lt;/ref&gt; Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.&lt;ref name=&quot;:0&quot;/&gt;

== Word vectors for Radiology: Intelligent Word Embedding (IWE) ==
An extension of word vectors for creating a dense vector representation of unstructured radiology reports has been proposed by Banerjee et. al.&lt;ref&gt;{{Cite journal|last=Banerjee|first=Imon|last2=Chen|first2=Matthew C.|last3=Lungren|first3=Matthew P.|last4=Rubin|first4=Daniel L.|title=Radiology report annotation using intelligent word embeddings: Applied to multi-institutional chest CT cohort|url=https://doi.org/10.1016/j.jbi.2017.11.012|journal=Journal of Biomedical Informatics|volume=77|pages=11–20|doi=10.1016/j.jbi.2017.11.012}}&lt;/ref&gt; One of the biggest challenges with Word2Vec is how to handle unknown or out-of-vocabulary (OOV) words and morphologically similar words. This can particularly be an issue in domains like medicine where synonyms and related words can be used depending on the preferred style of radiologist, and words may have been used infrequently in a large corpus. If the word2vec model has not encountered a particular word before, it will be forced to use a random vector, which is generally far from its ideal representation.

IWE combines Word2vec with a semantic dictionary mapping technique to tackle the major challenges of information extraction from clinical texts, which include ambiguity of free text narrative style, lexical variations, use of ungrammatical and telegraphic phases, arbitrary ordering of words, and frequent appearance of abbreviations and acronyms.  Of particular interest, the IWE model (trained on the one institutional dataset) successfully translated to a different institutional dataset which demonstrates good generalizability of the approach across institutions.

==Analysis==
The reasons for successful [[word embedding]] learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by [[cosine similarity]]) and note that this is in line with J. R. Firth's [[Distributional semantics|distributional hypothesis]]. However, they note that this explanation is &quot;very hand-wavy&quot; and argue that a more formal explanation would be preferable.&lt;ref name=&quot;explain&quot; /&gt;

Levy et al. (2015)&lt;ref&gt;{{cite book|last1=Levy|first1=Omer|last2=Goldberg|first2=Yoav|last3=Dagan|first3=Ido|title=Improving Distributional Similarity with Lessons Learned from Word Embeddings|date=2015|location=Transactions of the Association for Computational Linguistics|url=http://www.aclweb.org/anthology/Q15-1016}}&lt;/ref&gt; show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more 'traditional' approaches yields similar performances in downstream tasks.

== Preservation of semantic and syntactic relationships ==
The word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al. (2013)&lt;ref&gt;{{Cite journal|last=Mikolov|first=Tomas|last2=Yih|first2=Wen-tau|last3=Zweig|first3=Geoffrey|date=2013|title=Linguistic Regularities in Continuous Space Word Representations.|url=|journal=HLT-NAACL|page=pp. 746–751|doi=|pmid=}}&lt;/ref&gt; found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Man is to Woman as Brother is to Sister” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “Brother” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Sister” in the model. Such relationships can be generated for a range of semantic relations (such as Country—Capital) as well as syntactic relations (e.g. present tense—past tense)

== Assessing the quality of a model ==
Mikolov et al. (2013)&lt;ref name=&quot;mikolov&quot; /&gt; develop an approach to assessing the quality of a word2vec model which draws on the semantic and syntactic patterns discussed above. They developed a set of 8,869 semantic relations and 10,675 syntactic relations which they use as a benchmark to test the accuracy of a model. When assessing the quality of a vector model, a user may draw on this accuracy test which is implemented in word2vec,&lt;ref&gt;{{Cite web|url=https://radimrehurek.com/gensim/models/word2vec.html|title=Gensim - Deep learning with word2vec|last=|first=|date=|website=|publisher=|access-date=10 June 2016}}&lt;/ref&gt; or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible.&lt;ref name=&quot;mikolov&quot; /&gt;

=== Parameters and model quality ===
The use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.&lt;ref name=&quot;mikolov&quot; /&gt;

In models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.&lt;ref name=&quot;mikolov&quot; /&gt;

Accuracy increases overall as the number of words used increases, and as the number of dimensions increases. Mikolov et al.&lt;ref name=&quot;mikolov&quot; /&gt; report that doubling the amount of training data results in an increase in computational complexity equivalent to doubling the number of vector dimensions.

Altszyler et al. (2016) &lt;ref name=&quot;Altszyler&quot;&gt;{{cite arXiv|author1=Altszyler, E. |author2=Ribeiro, S. | author3= Sigman, M.|author4=Fernández Slezak, D. ||title=Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database|arxiv=1610.01520}}&lt;/ref&gt; studied Word2vec performance in two semantic tests for different corpus size. they found that Word2vec has a steep learning curve, outperforming other word-embedding technique (LSA) when it is trained with medium to large corpus size (more than 10 million words). However, with small training corpus LSA showed better performances. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, 50 dimensions, window size of 15 and 10 negative samples seems to be a good parameter setting.

==Implementations==
* [https://github.com/dav/word2vec/ C]
* [https://github.com/deeplearning4j/deeplearning4j Java/Scala]
* [https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py Python]
* [http://radimrehurek.com/gensim/models/word2vec.html Python]

== See also ==
* [[Autoencoder]]
* [[Document-term matrix]]
* [[Feature extraction]]
* [[Feature learning]]
* [[Language model#Neural_language_models]]
* [[Vector space model]]
* [[Thought vector]]

==References==
{{Reflist}}

{{Natural Language Processing}}
{{Use dmy dates|date=April 2017}}




</text>
      <sha1>hrzbuhnb1lrmwbgkkcifotmuyw1gcwg</sha1>
    </revision>
  </page>
  <page>
    <title>Trax Image Recognition</title>
    <ns>0</ns>
    <id>47577902</id>
    <revision>
      <id>809121341</id>
      <parentid>807013461</parentid>
      <timestamp>2017-11-07T06:57:29Z</timestamp>
      <contributor>
        <username>NihlusBOT</username>
        <id>31996569</id>
      </contributor>
      <minor/>
      <comment>Bot: removed invalid image syntax from file parameters ([[Wikipedia:Bots/Requests for approval/NihlusBOT 7|Task 7]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5613">{{Orphan|date=December 2016}}

{{Infobox dot-com company
| name = Trax
| logo = TraxLogo250.png
| collapsible = yes
| collapsetext = Website Screenshot
| screenshot =
| company_type = Private
| industry = Software for [[Retail]] &amp; [[CPG]]{{dn|date=October 2017}}
| foundation = {{Start date|2010}}
| location = [[Singapore]]
| area_served = Worldwide
| founders = Joel Bar-El, Dror Feldheim
| key_people = Joel Bar-El&lt;br /&gt;([[CEO]]),
Dror Feldheim&lt;br/&gt;([[CCO and Co-Founder]])
| url = {{URL|http://www.traxretail.com/}}
| current_status = Active
}}

'''Trax''' is a technology company headquartered in [[Singapore]], with offices throughout APAC, Europe, Middle East, North America and South America. Its [[computer vision]] technology is used by [[Fast-moving consumer goods|FMCG]] companies such as [[Coca-Cola]]&lt;ref name=&quot;O'Donoghue&quot;&gt;{{cite news|author=O'Donoghue, Jasmine|date=November 10, 2014|title=Three most innovative manufacturers named|url=http://www.foodmag.com.au/News/Top-3-most-innovative-manufacturers-named|work=FOOD Magazine|accessdate=August 20, 2015}}&lt;/ref&gt; and Retailers to collect, measure and analyse what is happening on physical shelves.

==History==
Founded in 2010, Trax has over 150 customers in the [[Retail|Retail]] and  [[Fast-moving consumer goods|FMCG]] industries, including beverage giant [[Coca-Cola]] and brewer [[Anheuser-Busch InBev]]. Its service is available in 45 markets and the company's development centre is located in [[Tel Aviv]].&lt;ref name=&quot;Chng&quot;/&gt; Trax closed its first round of funding for US$1.1 million, in June, 2011. They opened their Tel-Aviv office in July, 2012, and closed their second round of funding for US$6.4 million in December, 2012. Their third round of funding for US$15.7 million closed in February, 2014. In December 2014 Trax announced its fourth round of investment of US$15 million.&lt;ref name=Shahi&gt;{{cite news|author=Shahi, Twishy|date=December 16, 2014|title=Singapore’s Trax Image Recognition raises US$15M funding|url=http://e27.co/singapores-trax-image-recognition-raises-us15m-funding-20141216/|work=e27|accessdate=August 20, 2015}}&lt;/ref&gt;

In 2015, Trax opened their first two regional offices, London in January, and Brazil in April. In March 2016, Trax established their LATAM headquarters in Atlanta, Georgia. Trax announced a 5th round of funding for US$40 million on June 8, 2016.&lt;ref name=june16funding&gt;{{cite web|url=https://www.reuters.com/article/tech-traximage-fundraising-idUSL8N1901P1|title=Trax Image Recognition raises $40 mln in funding round|author=Trax Press Release|date=June 8, 2016}}&lt;/ref&gt; Two new regional offices were opened in Shanghai and Mexico City, in June and September 2016 respectively. On February 8, 2017, Trax closed their sixth round of funding for US$19.5 million.&lt;ref name=investec&gt;{{cite web|url=http://www.finsmes.com/2017/02/trax-image-recognition-raises-19-5m-in-funding.html|title=Trax Image Recognition Raises $19.5M in Funding|author=Trax Press Release|date=February 8, 2017}}&lt;/ref&gt;  On June  30, 2017 Trax announced its most recent funding round of US$64 million lead by global private equity giant Warburg Pincus.&lt;ref name=warburg&gt;{{cite web|url=https://www.chinamoneynetwork.com/2017/06/30/warburg-pincus-leads-64m-round-in-asian-retail-computer-vision-firm-trax|title=Warburg Pincus Leads $64M Round In Asian Retail Computer Vision Firm Trax|author=Trax Press Release|date=June 30, 2017}}&lt;/ref&gt;

===Mergers and Acquisitions===
On July 12, 2017, Trax announced that they had acquired Nielsen Store Observation (NSO) assets in the USA from [[Nielsen Corporation]].&lt;ref name=nielsennso&gt;{{cite web|url=http://www.prnewswire.com/news-releases/trax-acquires-nielsen-store-observation-nso-assets-in-usa-634032293.html|title=Trax Acquires Nielsen Store Observation (NSO) Assets in USA|author=Trax Press Release|date=July 12, 2017}}&lt;/ref&gt;

==Software and Services==
Trax reduces the time an employee needs to spend on audits to check inventory, shelf display and product promotions. It is also gathers more extensive data such as product assortment, shelf space, pricing, promotions, shelf location and arrangement of products on display. This market intelligence is valuable to [[Retailers|Retail]] and [[Fast-moving consumer goods|FMCG]] manufacturers because they pay large sums for space in supermarkets and stores. For example, in the US companies pay approximately $18 billion for shelf space.

==Technology==
The [[computer vision]] technology uses [[Artificial Intelligence]], fine-grained image recognition, and [[machine learning]] engines to convert store images into shelf insights. Trax is able to recognise products that are similar or identical such as branded drinks or shampoo bottles whilst also being able to differentiate between them based on variety and size. It piloted its [[machine learning algorithms]] with initial customers, allowing its algorithm to learn about different products. As the company processes more images, the better it gets at recognising the same products in different shapes and sizes.&lt;ref name=Chng&gt;{{cite news|author=Chng, Grace|date=September 3, 2015|title=The 'eyes' that help a store track its goods|url=http://www.straitstimes.com/business/the-eyes-that-help-a-store-track-its-goods|work=The Straits Times|accessdate=September 6, 2015}}&lt;/ref&gt;.  To date, Trax has recognized more than 8 billion images, and recognizes approximately 400,000 million new products per month.

== References ==
{{reflist}}

==External Links==
* {{Official website|1=http://www.traxretail.com}}


</text>
      <sha1>b1ds65mvexdvs313q835qt7yxa21hsx</sha1>
    </revision>
  </page>
  <page>
    <title>Stochastic block model</title>
    <ns>0</ns>
    <id>47845063</id>
    <revision>
      <id>809879392</id>
      <parentid>797716875</parentid>
      <timestamp>2017-11-12T01:42:41Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Elizaveta Levina]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10122">The '''stochastic block model''' is a [[generative model]] for random [[Graph (discrete mathematics)|graphs]]. This model tends to produce graphs containing ''communities'', subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in [[statistics]], [[machine learning]], and [[network science]], where it serves as a useful benchmark for the task of recovering [[community structure]] in graph data.

== Definition ==
The stochastic block model takes the following parameters:
* The number &lt;math&gt;n&lt;/math&gt; of vertices;
* a partition of the vertex set &lt;math&gt;\{1,\ldots,n\}&lt;/math&gt; into disjoint subsets &lt;math&gt;C_1,\ldots,C_r&lt;/math&gt;, called ''communities'';
* a symmetric &lt;math&gt;r \times r&lt;/math&gt; matrix &lt;math&gt;P&lt;/math&gt; of edge probabilities.
The edge set is then sampled at random as follows: any two vertices &lt;math&gt;u \in C_i&lt;/math&gt; and &lt;math&gt;v \in C_j&lt;/math&gt; are connected by an edge with probability &lt;math&gt;P_{ij}&lt;/math&gt;.

== Special cases ==
If the probability matrix is a constant, in the sense that &lt;math&gt;P_{ij} = p&lt;/math&gt; for all &lt;math&gt;i,j&lt;/math&gt;, then the result is the [[Erdős–Rényi model]] &lt;math&gt;G(n,p)&lt;/math&gt;. This case is degenerate—the partition into communities becomes irrelevant—but it illustrates a close relationship to the Erdős–Rényi model.

The ''planted partition model'' is the special case that the values of the probability matrix &lt;math&gt;P&lt;/math&gt; are a constant &lt;math&gt;p&lt;/math&gt; on the diagonal and another constant &lt;math&gt;q&lt;/math&gt; off the diagonal. Thus two vertices within the same community share an edge with probability &lt;math&gt;p&lt;/math&gt;, while two vertices in different communities share an edge with probability &lt;math&gt;q&lt;/math&gt;. Sometimes it is this restricted model that is called the stochastic block model. The case where &lt;math&gt;p &gt; q&lt;/math&gt; is called an ''assortative'' model, while the case &lt;math&gt;p &lt; q&lt;/math&gt; is called ''dissortative''.

Returning to the general stochastic block model, a model is called ''strongly assortative'' if &lt;math&gt;P_{ii} &gt; P_{jk}&lt;/math&gt; whenever &lt;math&gt;j \neq k&lt;/math&gt;: all diagonal entries dominate all off-diagonal entries. A model is called ''weakly assortative'' if &lt;math&gt;P_{ii} &gt; P_{ij}&lt;/math&gt; whenever &lt;math&gt;i \neq j&lt;/math&gt;: each diagonal entry is only required to dominate the rest of its own row and column.&lt;ref name=&quot;al14&quot; /&gt; ''Dissortative'' forms of this terminology exist, by reversing all inequalities. Algorithmic recovery is often easier against block models with assortative or dissortative conditions of this form.&lt;ref name=&quot;al14&quot; /&gt;

== Typical statistical tasks ==
Much of the literature on algorithmic community detection addresses three statistical tasks: detection, partial recovery, and exact recovery.

=== Detection ===
The goal of detection algorithms is simply to determine, given a sampled graph, whether the graph has latent community structure. More precisely, a graph might be generated, with some known prior probability, from a known stochastic block model, and otherwise from a similar [[Erdos-Renyi model]]. The algorithmic task is to correctly identify which of these two underlying models generated the graph.&lt;ref name=&quot;mns12&quot; /&gt;

=== Partial recovery ===
In partial recovery, the goal is to approximately determine the latent partition into communities, in the sense of finding a partition that is correlated with the true partition significantly better than a random guess.&lt;ref name=&quot;mas13&quot; /&gt;

=== Exact recovery ===
In exact recovery, the goal is to recover the latent partition into communities exactly. The community sizes and probability matrix may be known&lt;ref name=&quot;as15a&quot; /&gt; or unknown.&lt;ref name=&quot;as15b&quot; /&gt;

== Statistical lower bounds and threshold behavior ==
Stochastic block models exhibit a sharp [[threshold effect (disambiguation)|threshold effect]] reminiscent of [[percolation threshold]]s.&lt;ref name=&quot;decelle11&quot;/&gt;&lt;ref name=&quot;mns12&quot; /&gt;&lt;ref name=&quot;abh14&quot; /&gt; Suppose that we allow the size &lt;math&gt;n&lt;/math&gt; of the graph to grow, keeping the community sizes in fixed proportions. If the probability matrix remains fixed, tasks such as partial and exact recovery become feasible for all non-degenerate parameter settings. However, if we scale down the probability matrix at a suitable rate as &lt;math&gt;n&lt;/math&gt; increases, we observe a sharp phase transition: for certain settings of the parameters, it will become possible to achieve recovery with probability tending to 1, whereas on the opposite side of the parameter threshold, the probability of recovery tends to 0 no matter what algorithm is used.

For partial recovery, the appropriate scaling is to take &lt;math&gt;P_{ij} = \tilde P_{ij} / n&lt;/math&gt; for fixed &lt;math&gt;\tilde P&lt;/math&gt;, resulting in graphs of constant average degree. In the case of two equal-sized communities, in the assortative planted partition model with probability matrix
&lt;math display=&quot;block&quot;&gt; P = \left(\begin{array}{cc} \tilde p/n &amp; \tilde q/n \\ \tilde q/n &amp; \tilde p/n \end{array} \right), &lt;/math&gt;
partial recovery is feasible&lt;ref name=&quot;mas13&quot; /&gt; with probability &lt;math&gt;1 - o(1)&lt;/math&gt; whenever &lt;math&gt;(\tilde p - \tilde q)^2 &gt; 2(\tilde p + \tilde q)&lt;/math&gt;, whereas any [[estimator]] fails&lt;ref name=&quot;mns12&quot; /&gt; partial recovery with probability &lt;math&gt;1-o(1)&lt;/math&gt; whenever &lt;math&gt;(\tilde p - \tilde q)^2 &lt; 2(\tilde p + \tilde q)&lt;/math&gt;.

For exact recovery, the appropriate scaling is to take &lt;math&gt;P_{ij} = \tilde P_{ij} \log n / n&lt;/math&gt;, resulting in graphs of logarithmic average degree. Here a similar threshold exists: for the assortative planted partition model with &lt;math&gt;r&lt;/math&gt; equal-sized communities, the threshold lies at &lt;math&gt;\sqrt{\tilde p} - \sqrt{\tilde q} = \sqrt{r}&lt;/math&gt;. In fact, the exact recovery threshold is known for the fully general stochastic block model.&lt;ref name=&quot;as15a&quot; /&gt;
&lt;!---
it'd be great to have plots of these thresholds ---&gt;

== Algorithms ==
In principle, exact recovery can be solved in its feasible range using [[maximum likelihood]], but this amounts to solving a constrained or [[Regularization (mathematics)|regularized]] cut problem such as minimum bisection that is typically [[NP-complete]]. Hence, no known efficient algorithms will correctly compute the maximum-likelihood estimate in the worst case.

However, a wide variety of algorithms perform well in the average case, and many high-probability performance guarantees have been proven for algorithms in both the partial and exact recovery settings. Successful algorithms include [[spectral clustering]] of the vertices,&lt;ref name=&quot;krzakala-pnas&quot;/&gt;&lt;ref name=&quot;mas13&quot; /&gt;&lt;ref name=&quot;as15a&quot; /&gt;&lt;ref name=&quot;lr15&quot; /&gt; [[semidefinite programming]],&lt;ref name=&quot;al14&quot; /&gt;&lt;ref name=&quot;abh14&quot; /&gt; and forms of [[belief propagation]],&lt;ref name=&quot;decelle11&quot;/&gt;&lt;ref name=&quot;mns13&quot; /&gt; among others.

== Variants ==
Several variants of the model exist. One minor tweak allocates vertices to communities randomly, according to a [[categorical distribution]], rather than in a fixed partition.&lt;ref name=&quot;as15a&quot; /&gt; More significant variants include the censored block model and the mixed-membership block model.

==References==
{{reflist|refs=
&lt;ref name=&quot;decelle11&quot;&gt;{{cite journal|
last1 = Decelle| first1 = Aurelien |
last2 = Krzakala| first2 = Florent |
last3 = Moore| first3 = Cristopher |
last4 = Zdeborova| first4 = Lenka |
title = Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications |
arxiv = 1109.3041 |
date = September 2011 | doi=10.1103/PhysRevE.84.066106 | volume=84 | journal=Physical Review E}}&lt;/ref&gt;
&lt;ref name=&quot;abh14&quot;&gt;{{cite arXiv| last1 = Abbe| first1 = Emmanuel| last2 = Bandeira| first2 = Afonso S.| last3 = Hall| first3 = Georgina| title = Exact Recovery in the Stochastic Block Model|arxiv= 1405.3267| date = May 2014  }}&lt;/ref&gt;
&lt;ref name=&quot;as15a&quot;&gt;{{cite arXiv| last1 = Abbe| first1 = Emmanuel| last2 = Sandon| first2 = Colin| title = Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms|arxiv= 1503.00609| date = March 2015  }}&lt;/ref&gt;
&lt;ref name=&quot;as15b&quot;&gt;{{cite arXiv| last1 = Abbe| first1 = Emmanuel| last2 = Sandon| first2 = Colin| title = Recovering communities in the general stochastic block model without knowing the parameters|arxiv= 1506.03729|  date = June 2015  }}&lt;/ref&gt;
&lt;ref name=&quot;lr15&quot;&gt;{{Cite journal| doi = 10.1214/14-AOS1274| issn = 0090-5364| volume = 43| issue = 1| pages = 215–237| last1 = Lei| first1 = Jing| last2 = Rinaldo| first2 = Alessandro| title = Consistency of spectral clustering in stochastic block models| journal = The Annals of Statistics| date = February 2015 | arxiv = 1312.2050}}&lt;/ref&gt;
&lt;ref name=&quot;krzakala-pnas&quot;&gt;{{cite journal|
last1 = Krzakala| first1 = Florent|
last2 = Moore| first2 = Cristopher|
last3 = Mossel| first3 = Elchanan|
last4 = Neeman| first4 = Joe|
last5 = Sly| first5 = Allan|
last6 = Lenka| first6 = Lenka|
last7 = Zhang| first7 = Pan|
title = Spectral redemption in clustering sparse networks | journal = Proceedings of the National Academy of Sciences| date = October 2013 }}&lt;/ref&gt;
&lt;ref name=&quot;mns12&quot;&gt;{{cite arXiv| last1 = Mossel| first1 = Elchanan| last2 = Neeman| first2 = Joe| last3 = Sly| first3 = Allan| title = Stochastic Block Models and Reconstruction|arxiv= 1202.1499|  date = February 2012  }}&lt;/ref&gt;
&lt;ref name=&quot;mns13&quot;&gt;{{cite arXiv| last1 = Mossel| first1 = Elchanan| last2 = Neeman| first2 = Joe| last3 = Sly| first3 = Allan| title = Belief Propagation, Robust Reconstruction, and Optimal Recovery of Block Models|arxiv= 1309.1380| date = September 2013 }}&lt;/ref&gt;
&lt;ref name=&quot;mas13&quot;&gt;{{cite arXiv| last = Massoulie| first = Laurent| title = Community detection thresholds and the weak Ramanujan property|arxiv= 1311.3085| date = November 2013 }}&lt;/ref&gt;
&lt;ref name=&quot;al14&quot;&gt;{{cite arXiv| last1 = Amini| first1 = Arash A.| last2 = Levina| first2 = Elizaveta|author2-link= Elizaveta Levina| title = On semidefinite relaxations for the block model|arxiv= 1406.5647|  date = June 2014 }}&lt;/ref&gt;
}}



</text>
      <sha1>jjki11tvxv8l78aqlir1lkbfhd4ugk4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Machine learning task</title>
    <ns>14</ns>
    <id>47991509</id>
    <revision>
      <id>683599642</id>
      <parentid>683599561</parentid>
      <timestamp>2015-10-01T08:56:05Z</timestamp>
      <contributor>
        <username>Golopotw</username>
        <id>19519761</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="65">Category for machine learning tasks
</text>
      <sha1>gvjayn6f7231opsg76xumnrqwtcmgfu</sha1>
    </revision>
  </page>
  <page>
    <title>Cleverbot</title>
    <ns>0</ns>
    <id>28650287</id>
    <revision>
      <id>812759426</id>
      <parentid>812759329</parentid>
      <timestamp>2017-11-29T18:12:10Z</timestamp>
      <contributor>
        <username>72</username>
        <id>28970402</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/2A02:2F0A:B060:87E:19E9:ED4:3984:6030|2A02:2F0A:B060:87E:19E9:ED4:3984:6030]] ([[User talk:2A02:2F0A:B060:87E:19E9:ED4:3984:6030|talk]]) to last version by Atvelonis</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7365">{{Infobox Website
| name = Cleverbot
| url = {{URL|https://www.cleverbot.com/}}
| type = [[Chatterbot]]
| registration = None
| author = [[Rollo Carpenter]]
| screenshot = [[Image:Cleverbot website.png|border|240px]]
| caption =
| collapsible = very
| alexa = {{IncreaseNegative}} 43,663 ({{as of|2017|8|11|alt=August 2017}})&lt;ref name=&quot;alexa&quot;&gt;{{cite web|url= http://www.alexa.com/siteinfo/cleverbot.com |title= Cleverbot.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}&lt;/ref&gt;&lt;!--Updated monthly by OKBot.--&gt;
| current status = Active
}}

'''Cleverbot''' is a [[chatterbot]] [[web application]] that uses an [[artificial intelligence]] (AI) [[algorithm]] to have conversations with humans. It was created by British AI scientist [[Rollo Carpenter]]. It was preceded by [[Jabberwacky]], a chatbot project that began in 1988 and went online in 1997.&lt;ref&gt;{{Cite web|url=http://www.jabberwacky.com/j2about|title=About the Jabberwacky AI|last=|first=|date=|website=www.jabberwacky.com|publisher=|archive-url=https://web.archive.org/web/20170107185820/http://www.jabberwacky.com/j2about|archive-date=2017-01-07|access-date=2017-01-07|quote=The whole thing started way back in 1988, and went on the web in 1997.}}&lt;/ref&gt; In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web, the number of conversations held has exceeded 200 million. Besides the web application, Cleverbot is also available as an [[iOS]], [[Android (operating system)|Android]], and [[Windows Phone]] app.&lt;ref&gt;{{cite web|url=http://www.cleverbot.com/app|title=Cleverbot|publisher=Cleverbot.com|accessdate=14 January 2013}}&lt;/ref&gt;

== Operation ==
Unlike some other chatterbots, Cleverbot's responses are not pre-programmed. Instead, it learns from human input: Humans type into the box below the Cleverbot logo and the system finds all keywords or an exact phrase matching the input. After searching through its saved conversations, it responds to the input by finding how a human responded to that input when it was asked, in part or in full, by Cleverbot.&lt;ref name=&quot;sh&quot;&gt;{{cite web|last=Saenz|first=Aaron|title=Cleverbot Chat Engine Is Learning From The Internet To Talk Like A Human|url=http://singularityhub.com/2010/01/13/cleverbot-chat-engine-is-learning-from-the-internet-to-talk-like-a-human/|work=Singularity Hub|accessdate=2011-06-06|date=2010-01-13}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Rollo Carpenter|url=http://www.techniche.org/techniche11/lectures/287.html|work=Technische|publisher=Indian Institute of Technology Guwahati|accessdate=13 November 2011}}&lt;/ref&gt;

Cleverbot participated in a formal [[Turing test]] at the 2011 [[Techniche]] festival at the [[Indian Institute of Technology Guwahati]] on September 3, 2011. Out of the 334 votes cast, Cleverbot was judged to be 59.3% human, compared to the rating of 63.3% human achieved by human participants. A score of 50.05% or higher is often considered to be a passing grade.&lt;ref&gt;{{cite news|last=Aron|first=Jacob|title=Software tricks people into thinking it is human|url=https://www.newscientist.com/article/dn20865-software-tricks-people-ino-thinking-it-is-human.html|accessdate=13 November 2011|newspaper=New Scientist|date=6 September 2011}}&lt;/ref&gt; The software running for the event had to handle just 1 or 2 simultaneous requests, whereas online Cleverbot is usually talking to around 80,000 people at once.

== Developments ==
Cleverbot is constantly learning, growing in data size at a rate of 4 to 7 million interactions per second. Updates to the software have been mostly behind the scenes. In 2014, Cleverbot was upgraded to use [[GPU]] serving techniques.&lt;ref&gt;{{cite web|url=http://www.existor.com/ai-parallel |title=Parallel Processing on Graphics Cards - Existor.com - Cleverbot |publisher=Existor.com |date=2014-02-05 |accessdate=2014-06-09}}&lt;/ref&gt; The program chooses how to respond to users fuzzily, the whole of the conversation being compared to the millions that have taken place before. Cleverbot now uses over 279 million interactions, about 3-4% of the data it has already accumulated. The developers of Cleverbot are attempting to build a new version using machine learning techniques.&lt;ref&gt;{{Cite web|url=https://www.existor.com/products/cleverbot-data-for-machine-learning/|title=Cleverbot Data for Machine Learning – Existor|website=www.existor.com|access-date=2016-11-30}}&lt;/ref&gt;

A significant part of the engine behind Cleverbot and an API for accessing it has been made available to developers in the form of Cleverscript. A service for directly accessing Cleverbot has been made available to developers in the form of Cleverbot.io.&lt;ref&gt;{{Cite web|url=https://cleverbot.io|title=Cleverbot.io - Cloud based Cleverbot|website=cleverbot.io|access-date=2017-01-07}}&lt;/ref&gt;

An app that uses the Cleverscript engine to play a game of 20 Questions, has been launched under the name ''Clevernator''. Unlike other such games, the player asks the questions and it is the role of the AI to understand, and answer factually. An app that allows owners to create and talk to their own small Cleverbot-like AI has been launched, called ''Cleverme!'' for Apple products.&lt;ref&gt;{{cite web|url=https://itunes.apple.com/us/app/cleverme!/id685281893?mt=8|title=Cleverme! on the App Store on iTunes|work=iTunes|accessdate=24 March 2014}}&lt;/ref&gt;

In early 2017, a [[Twitch.tv|Twitch]] stream of two [[Google Home]] devices modified to talk to each other using Cleverbot.io garnered over 700,000 visitors and over 30,000 peak concurrent viewers.&lt;ref&gt;{{Cite web|url=https://www.engadget.com/2017/01/07/two-google-home-bots-engage-in-a-duel-of-words/|title=Two Google Home bots engage in a duel of words|last=Moon|first=Mariella|date=2017-01-07|website=Engadget|publisher=|access-date=2017-01-07}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://www.twitch.tv/seebotschat|title=seebotschat - Twitch|last=|first=|date=|website=|publisher=|access-date=2017-01-07|quote=dependency.list(); &gt;&gt; cleverbot.com}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=http://nerdist.com/two-google-homes-are-having-a-bizarre-and-fascinating-conversation-on-twitch/|title=Two Google Homes are Having a Bizarre and Fascinating Conversation on Twitch|last=Rossignol|first=Derrick|date=2017-01-06|work=Nerdist|newspaper=|quote=Twitch user seebotschat got Cleverbot, an artificial intelligence chatbot that’s been online since 1996, running on two units...|access-date=2017-01-07|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=http://www.androidcentral.com/watching-two-google-homes-trying-have-conversation-best-thing-youll-see-today|title=Watching two Google Homes trying to have a conversation is the best thing you'll see today|last=Bader|first=Daniel|date=2017-01-06|work=Android Central|quote=Twitch channel seebotschat have managed to whip together a Cleverbot API hook that keeps the units speaking...|access-date=2017-01-07|via=}}&lt;/ref&gt;

== See also ==
* [[List of chatterbots]]
* [[Omegle]]

==References==
{{Reflist|2}}

== External links ==
* {{Official website|https://www.cleverbot.com/}}
* [http://www.cleverscript.io/ Cleverscript website]
* [https://cleverbot.io Cleverbot.io website]
* [https://www.twitch.tv/seebotschat Livestream of 2 cleverbots chatting with each other] on [[Twitch.tv]]



</text>
      <sha1>d7pdm2bwkag39wo16ep26y09ty0bih5</sha1>
    </revision>
  </page>
  <page>
    <title>Relational data mining</title>
    <ns>0</ns>
    <id>926722</id>
    <revision>
      <id>802430197</id>
      <parentid>799912714</parentid>
      <timestamp>2017-09-26T03:26:58Z</timestamp>
      <contributor>
        <username>Czar</username>
        <id>244263</id>
      </contributor>
      <minor/>
      <comment>Removing link(s): [[Wikipedia:Articles for deletion/Deep feature synthesis]] closed as delete ([[WP:XFDC|XFDcloser]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2962">'''Relational data mining''' is the [[data mining]] technique for relational
databases.&lt;ref&gt;Dzeroski, Saso, Lavrač, Nada (Eds.), Relational Data Mining, Springer 2001 [https://www.springer.com/computer/database+management+%26+information+retrieval/book/978-3-540-42289-1]&lt;/ref&gt; Unlike traditional data mining algorithms, which look for
patterns in a single table ([[propositional patterns]]),
relational data mining algorithms look for patterns among multiple tables
([[relational patterns]]). For most types of propositional
patterns, there are corresponding relational patterns. For example,
there are relational [[classification rule]]s ('''relational classification'''), relational [[Decision tree learning|regression tree]], and relational [[association rule]]s&lt;!-- minor weasel words follow but left as a prompt in case some one can add actual information:, and so on--&gt;.

There are several approaches to relational data mining:
# [[Inductive_logic_programming|Inductive Logic Programming]] (ILP)
# [[Statistical_relational_learning|Statistical Relational Learning]] (SRL)
# Graph Mining
# Propositionalization
# Multi-view learning

==Algorithms==
'''Multi-Relation Association Rules''': Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations ''live in'', ''nearby'' and ''humid'': “Those who ''live in'' a place which is ''near by'' a city with ''humid'' climate type and also are ''younger'' than 20 -&gt; their ''health condition'' is good”. Such association rules are extractable from RDBMS data or semantic web data.&lt;ref name=&quot;MRAR: Mining Multi-Relation Association Rules&quot;&gt;Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; ''MRAR: Mining Multi-Relation Association Rules'', Journal of Computing and Security, 1, no. 2 (2014)&lt;/ref&gt;

==Software==
* [http://www.kiminkii.com/safarii.html Safarii]: a Data Mining environment for analysing large relational databases based on a multi-relational data mining engine.
* [http://www.dataconda.net Dataconda]: a software, free for research and teaching purposes, that helps mining relational databases without the use of SQL.

==Datasets==
* [http://relational.fit.cvut.cz Relational dataset repository]: a collection of publicly available relational datasets.

==See also==
*[[Data mining]]
*[[Structure mining]]
*[[Database mining]]
*[[Structured data mining]]

==References==
{{Reflist}}

== External links ==
* [http://www-ai.ijs.si/SasoDzeroski/RDMBook/ Web page for a text book on relational data mining]







{{database-stub}}</text>
      <sha1>be21smo4touw1tkg0vr0706n2mnx14f</sha1>
    </revision>
  </page>
  <page>
    <title>The Master Algorithm</title>
    <ns>0</ns>
    <id>47937215</id>
    <revision>
      <id>793827243</id>
      <parentid>789219085</parentid>
      <timestamp>2017-08-04T06:22:34Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* References */Repairing broken [[Wall Street Journal]] links using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2404">{{infobox book | &lt;!-- See Wikipedia:WikiProject_Novels or Wikipedia:WikiProject_Books --&gt;
| name         = The Master Algorithm:&lt;br /&gt; How the Quest for the Ultimate Learning Machine Will Remake Our World
| image        =
| caption      = Hardcover edition
| border       = yes
| author       = [[Pedro Domingos]]
| country      = United States
| genre        = [[Philosophy]], [[popular science]]
| subject      = [[Artificial intelligence]]
| language     = English
| publisher    = [[Basic Books]]
| release_date = September 22, 2015
| media_type   = Print, e-book, audiobook
| pages        = 352 pp.
| isbn         = 978-0465065707
| preceded_by =
}}

'''''The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World''''' is a book by [[Pedro Domingos]] released in 2015. Domingos wrote it in order to generate interest from people outside the field. Towards the end of the book, while reviewing his invention of the [[Markov logic network]] &lt;ref name=&quot;Dom2015&quot;&gt;{{cite book| title=The Master Algorithm: How machine learning is reshaping how we live. |pages =246–7| date= 2015 |first= Pedro|last=Domingos}}&lt;/ref&gt; he pictures a &quot;master [[algorithm]]&quot; allowing technology to allow [[machine learning algorithm]]s to asymptotically grow to a perfect understanding of how the world and people in it work.&lt;ref name=&quot;MyUser_Slate.com_September_26_2015c&quot;&gt;{{cite web |url=http://www.slate.com/articles/technology/bitwise/2015/09/pedro_domingos_master_algorithm_how_machine_learning_is_reshaping_how_we.2.html |title=Pedro Domingos' Master Algorithm: How machine learning is reshaping how we live. |newspaper=Slate.com |date=  |author= |accessdate= September 26, 2015}}&lt;/ref&gt;

==References==
{{Reflist}}

* https://www.timeshighereducation.com/books/review-the-master-algorithm-pedro-domingos-allen-lane
* https://www.wsj.com/articles/the-sum-of-human-knowledge-1442610803
* https://www.kirkusreviews.com/book-reviews/pedro-domingos/the-master-algorithm/
* http://www.kdnuggets.com/2015/09/book-master-algorithm-pedro-domingos.html
* http://www.kdnuggets.com/2014/08/interview-pedro-domingos-master-algorithm-new-deep-learning.html (interview)

== External links ==
* {{official website|http://www.basicbooks.com/full-details?isbn{{=}}9780465065707}}

{{DEFAULTSORT:Master Algorithm}}

[[Category:2015 non-fiction books]]


{{tech-book-stub}}</text>
      <sha1>0zk6r3pydndzivghyuwk3n52hrwqj19</sha1>
    </revision>
  </page>
  <page>
    <title>Committee machine</title>
    <ns>0</ns>
    <id>9583985</id>
    <revision>
      <id>796476989</id>
      <parentid>690128654</parentid>
      <timestamp>2017-08-21T03:09:15Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>/* top */clean up spacing after commas, replaced: ,  → , using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1848">A '''committee machine''' is a type of [[artificial neural network]] using a [[Divide and rule|divide and conquer]] strategy in which the responses of multiple neural networks (experts) are combined into a single response.&lt;ref&gt;HAYKIN, S. Neural Networks - A Comprehensive Foundation. Second edition. Pearson Prentice Hall: 1999.&lt;/ref&gt;  The combined response of the committee machine is supposed to be superior to those of its constituent experts.  Compare with [[ensembles of classifiers]].

== Types ==

===Static structures===
In this class of committee machines, the responses of several predictors (experts) are combined by means of a mechanism that does not involve the input signal, hence the designation static. This category includes  the following methods:
*[[Ensemble averaging]]
In ensemble averaging, outputs of different predictors are linearly combined to produce an overall output.
*[[Boosting (meta-algorithm)|Boosting]]
In boosting, a weak algorithm is converted into one that achieves arbitrarily high accuracy.

===Dynamic structures===
In this second class of committee machines, the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output, hence the designation dynamic. There are two kinds of dynamic structures:
*[[Mixture of experts]]
In mixture of experts, the individual responses of the experts are non-linearly combined by means of a single gating network.
*[[Hierarchical mixture of experts]]
In hierarchical mixture of experts, the individual responses of the individual experts are non-linearly combined by means of several gating networks arranged in a hierarchical fashion.

== References ==
{{Reflist}}




</text>
      <sha1>sjbmytlhph13n4zyojzp6n8yvaqklnn</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix regularization</title>
    <ns>0</ns>
    <id>44628821</id>
    <revision>
      <id>799310568</id>
      <parentid>786604886</parentid>
      <timestamp>2017-09-06T22:53:15Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>adding several links using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13780">In the field of [[statistical learning theory]], '''matrix regularization''' generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, [[Tikhonov regularization]] optimizes over

: &lt;math&gt;\min_x \|Ax-y\|^2 + \lambda \|x\|^2&lt;/math&gt;

to find a vector, &lt;math&gt;x&lt;/math&gt;, that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem  can be written as

: &lt;math&gt;\min_X \|AX-Y\|^2 + \lambda \|X\|^2&lt;/math&gt;

where the vector norm enforcing a regularization penalty on &lt;math&gt;x&lt;/math&gt; has been extended to a matrix norm on &lt;math&gt;X&lt;/math&gt;.

Matrix Regularization has applications in [[matrix completion]], [[multivariate regression]], and [[multi-task learning]]. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of [[multiple kernel learning]].

== Basic definition ==

Consider a matrix &lt;math&gt;W&lt;/math&gt; to be learned from a set of examples, &lt;math&gt;S=(X_i^t,y_i^t)&lt;/math&gt;, where &lt;math&gt;i&lt;/math&gt; goes from &lt;math&gt;1&lt;/math&gt; to &lt;math&gt;n&lt;/math&gt;, and &lt;math&gt;t&lt;/math&gt; goes from &lt;math&gt;1&lt;/math&gt; to &lt;math&gt;T&lt;/math&gt;. Let each input matrix &lt;math&gt;X_i&lt;/math&gt; be &lt;math&gt;\in \mathbb{R}^{DT}&lt;/math&gt;, and let &lt;math&gt;W&lt;/math&gt; be of size &lt;math&gt;D\times T&lt;/math&gt;. A general model for the output &lt;math&gt;y&lt;/math&gt; can be posed as

: &lt;math&gt;y_i^t=\langle W,X_i^t\rangle_F&lt;/math&gt;

where the inner product is the [[Frobenius inner product]]. For different applications the matrices &lt;math&gt;X_i&lt;/math&gt; will have different forms,&lt;ref name=Notes&gt;Lorenzo Rosasco, Tomaso Poggio, &quot;A Regularization Tour of Machine Learning — MIT-9.520 Lectures Notes&quot;  Manuscript, Dec. 2014.&lt;/ref&gt; but for each of these the optimization problem to infer &lt;math&gt;W&lt;/math&gt; can be written as

: &lt;math&gt;\min_{W\in \mathcal{H}} E(W)+R(W)&lt;/math&gt;

where &lt;math&gt;E&lt;/math&gt; defines the empirical error for a given &lt;math&gt;W&lt;/math&gt;, and &lt;math&gt;R(W)&lt;/math&gt; is a matrix regularization penalty. The function &lt;math&gt;R(W)&lt;/math&gt; is typically chosen to be convex, and is often selected to enforce sparsity (using &lt;math&gt;\ell^1&lt;/math&gt;-norms) and/or smoothness (using &lt;math&gt;\ell^2&lt;/math&gt;-norms). Finally, &lt;math&gt;W&lt;/math&gt; is in the space of matrices, &lt;math&gt;\mathcal{H}&lt;/math&gt;, with Forbenius inner product,.

== General applications ==

=== Matrix completion ===

In the problem of [[matrix completion]], the matrix &lt;math&gt;X_i^t&lt;/math&gt; takes the form

: &lt;math&gt;X_i^t=e_t\otimes e_i'&lt;/math&gt;

where &lt;math&gt;(e_t)_t&lt;/math&gt; and &lt;math&gt;(e_i')_i&lt;/math&gt; are the canonical basis in &lt;math&gt;\mathbb{R}^T&lt;/math&gt; and &lt;math&gt;\mathbb{R}^D&lt;/math&gt;. In this case the role of the Frobenius inner product is to select individual elements, &lt;math&gt;w_i^t&lt;/math&gt;, from the matrix &lt;math&gt;W&lt;/math&gt;. Thus, the output, &lt;math&gt;y&lt;/math&gt;, is a sampling of entries from the matrix &lt;math&gt;W&lt;/math&gt;.

The problem of reconstructing &lt;math&gt;W&lt;/math&gt; from a small set of sampled entries is possible only under certain restrictions on the matrix, and these restrictions can be enforced by a regularization function. For example, it might be assumed that &lt;math&gt;W&lt;/math&gt; is low-rank, in which case the regularization penalty can take the form of a nuclear norm.&lt;ref name=&quot;Candès, Emmanuel J 2009 pp. 717&quot;&gt;Exact Matrix Completion via Convex Optimization by Candès, Emmanuel J. and Recht, Benjamin (2009) in Foundations of Computational Mathematics, 9 (6). pp. 717–772. {{issn|1615-3375}}&lt;/ref&gt;

: &lt;math&gt;R(W)=\lambda \|W\|_*=\lambda \sum |\sigma_i|&lt;/math&gt;

where &lt;math&gt;\sigma_i&lt;/math&gt;, with &lt;math&gt;i&lt;/math&gt; from &lt;math&gt;1&lt;/math&gt; to &lt;math&gt;\min D,T&lt;/math&gt;, are the singular values of &lt;math&gt;W&lt;/math&gt;.

=== Multivariate regression ===

Models used in [[multivariate regression]] are parameterized by a matrix of coefficients. In the Frobenius inner product above, each matrix &lt;math&gt;X&lt;/math&gt; is

: &lt;math&gt;X_i^t=e_t\otimes x_i \, &lt;/math&gt;

such that the output of the inner product is the dot product of one row of the input with one column of the coefficient matrix. The familiar form of such models is

: &lt;math&gt;Y=XW+b \, &lt;/math&gt;

Many of the vector norms used in single variable regression can be extended to the multivariate case. One example is the squared Frobenius norm, which can be viewed as an &lt;math&gt;\ell^2&lt;/math&gt;-norm acting either entrywise, or on the singular values of the matrix:

: &lt;math&gt;R(W)=\lambda \|W\|_F^2=\lambda \sum\sum |w_{ij}|^2=\lambda \operatorname{Tr}(W^*W)=\lambda \sum \sigma_i^2.&lt;/math&gt;

In the multivariate case the effect of regularizing with the Frobenius norm is the same as the vector case; very complex models will have larger norms, and, thus, will be penalized more.

=== Multi-task learning ===

The setup for multi-task learning is almost the same as the setup for multivariate regression. The primary difference is that the input variables are also indexed by task (columns of &lt;math&gt;Y&lt;/math&gt;). The representation with the Frobenius inner product is then

: &lt;math&gt;X_i^t=e_t\otimes x_i^t.&lt;/math&gt;

The role of matrix regularization in this setting can be the same as in multivariate regression, but matrix norms can also be used to couple learning problems across tasks. In particular, note that for the optimization problem

: &lt;math&gt;\min_W \|XW-Y\|_2^2 + \lambda \|W\|_2^2&lt;/math&gt;

the solutions corresponding to each column of &lt;math&gt;Y&lt;/math&gt; are decoupled. That is, the same solution can be found by solving the joint problem, or by solving an isolated regression problem for each column. The problems can be coupled by adding an additional regulatization penalty on the covariance of solutions

: &lt;math&gt;\min_{W,\Omega} \|XW-Y\|_2^2 + \lambda_1 \|W\|_2^2 + \lambda_2 \operatorname{Tr}(W^T \Omega^{-1} W)&lt;/math&gt;

where &lt;math&gt;\Omega&lt;/math&gt; models the relationship between tasks. This scheme can be used to both enforce similarity of solutions across tasks, and to learn the specific structure of task similarity by alternating between optimizations of &lt;math&gt;W&lt;/math&gt; and &lt;math&gt;\Omega&lt;/math&gt;.&lt;ref&gt;Zhang and Yeung. A Convex Formulation for Learning Task Relationships in Multi-Task Learning. Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)&lt;/ref&gt; When the relationship between tasks is known to lie on a graph, the [[Laplacian matrix]] of the graph can be used to couple the learning problems.

== Spectral regularization ==

[[Regularization by spectral filtering]] has been used to find stable solutions to problems such as those discussed above by addressing ill-posed matrix inversions (see for example  [[Regularization by spectral filtering#Filter function for Tikhonov regularization|Filter function for Tikhonov regularization]]). In many cases the regularization function acts on the input (or kernel) to ensure a bounded inverse by eliminating small singular values, but it can also be useful to have spectral norms that act on the matrix that is to be learned.

There are a number of matrix norms that act on the singular values of the matrix. Frequently used examples include the [[Schatten norm|Schatten p-norms]], with ''p''&amp;nbsp;=&amp;nbsp;1&amp;nbsp;or&amp;nbsp;2. For example, matrix regularization with a Schatten 1-norm, also called the nuclear norm, can be used to enforce sparsity in the spectrum of a matrix. This has been used in the context of matrix completion when the matrix in question is believed to have a restricted rank.&lt;ref name=&quot;Candès, Emmanuel J 2009 pp. 717&quot;/&gt; In this case the optimization problem becomes:

: &lt;math&gt;\min \|W\|_*&lt;/math&gt; subject to &lt;math&gt;W_{i,j}=Y_{ij}.&lt;/math&gt;

Spectral Regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression.&lt;ref&gt;Alan Izenman. [http://www.sciencedirect.com/science/article/pii/0047259X75900421 Reduced Rank Regression for the Multivariate Linear Model]. Journal of Multivariate Analysis 5,248-264(1975)&lt;/ref&gt; In this setting, a reduced rank coefficient matrix can be found by keeping just the top &lt;math&gt;n&lt;/math&gt; singular values, but this can be extended to keep any reduced set of singular values and vectors.

== Structured sparsity ==

Sparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables (see e.g. the  [[Lasso (statistics)|Lasso method]]). In principle, entry-wise sparsity can be enforced by penalizing the entry-wise &lt;math&gt;\ell^0&lt;/math&gt;-norm of the matrix, but the &lt;math&gt;\ell^0&lt;/math&gt;-norm is not convex. In practice this can be implemented by convex relaxation to the &lt;math&gt;\ell^1&lt;/math&gt;-norm. While entry-wise regularization with an &lt;math&gt;\ell^1&lt;/math&gt;-norm will find solutions with a small number of nonzero elements, applying an &lt;math&gt;\ell^1&lt;/math&gt;-norm to different groups of variables can enforce structure in the sparsity of solutions.&lt;ref&gt;Kakade, Shalev-Shwartz and Tewari. [http://www.jmlr.org/papers/volume13/kakade12a/kakade12a.pdf Regularization Techniques for Learning with Matrices]. Journal of Machine Learning Research 13 (2012) 1865-1890.&lt;/ref&gt;

The most straightforward example of structured sparsity uses the &lt;math&gt;\ell_{p,q}&lt;/math&gt; norm with &lt;math&gt;p=2&lt;/math&gt; and &lt;math&gt;q=1&lt;/math&gt;:

: &lt;math&gt;\|W\|_{2,1}=\sum \|w_i\|_2.&lt;/math&gt;

For example, the &lt;math&gt;\ell_{2,1}&lt;/math&gt; norm is used in multi-task learning to group features across tasks, such that all the elements in a given row of the coefficient matrix can be forced to zero as a group.&lt;ref&gt;A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008.&lt;/ref&gt; The grouping effect is achieved by taking the &lt;math&gt;\ell^2&lt;/math&gt;-norm of each row, and then taking the total penalty to be the sum of these row-wise norms. This regularization results in rows that will tend to be all zeros, or dense. The same type of regularization can be used to enforce sparsity column-wise by taking the &lt;math&gt;\ell^2&lt;/math&gt;-norms of each column.

More generally, the &lt;math&gt;\ell_{2,1}&lt;/math&gt; norm can be applied to arbitrary groups of variables:

: &lt;math&gt;R(W)=\lambda \sum_g^G \sqrt{\sum_j^{|G_g|} |w_g^j|^2}=\lambda \sum_g^G \|w_g\|_g&lt;/math&gt;

where the index &lt;math&gt;g&lt;/math&gt; is across groups of variables, and &lt;math&gt;|G_g|&lt;/math&gt; indicates the cardinality of group &lt;math&gt;g&lt;/math&gt;.

Algorithms for solving these group sparsity problems extend the more well-known Lasso and group Lasso methods by allowing overlapping groups, for example, and have been implemented via [[matching pursuit]]:&lt;ref&gt;Huang, Zhang, and Metaxas. [http://www.jmlr.org/papers/volume12/huang11b/huang11b.pdf Learning with Structured Sparsity]. Journal of Machine Learning Research 12 (2011) 3371-3412.&lt;/ref&gt; and [[proximal gradient method]]s.&lt;ref&gt;Chen et. al. [http://projecteuclid.org/download/pdfview_1/euclid.aoas/1339419614 Smoothing Proximal Gradient Method for General Structured Sparse Regression]. The Annals of Applied Statistics, 2012, Vol. 6, No. 2, 719–752 DOI: 10.1214/11-AOAS514&lt;/ref&gt; By writing the proximal gradient with respect to a given coefficient, &lt;math&gt;w_g^i&lt;/math&gt;, it can be seen that this norm enforces a group-wise soft threshold&lt;ref name=Notes /&gt;

: &lt;math&gt;\operatorname{prox}_{\lambda,R_g}(w_g)^i=\left(w_g^i-\lambda \frac{w_g^i}{\|w_g\|_g}\right)\mathbf{1}_{\|w_g\|_g\geq \lambda}.&lt;/math&gt;

where &lt;math&gt;\mathbf{1}_{\|w_g\|_g\geq \lambda}&lt;/math&gt; is the indicator function for group norms &lt;math&gt;\geq \lambda&lt;/math&gt;.

Thus, using &lt;math&gt;\ell_{2,1}&lt;/math&gt; norms it is straightforward to enforce structure in the sparsity of a matrix either row-wise, column-wise, or in arbitrary blocks. By enforcing group norms on blocks in multivariate or multi-task regression, for example, it is possible to find groups of input and output variables, such that defined subsets of output variables (columns in the matrix &lt;math&gt;Y&lt;/math&gt;) will depend on the same sparse set of input variables.

== Multiple kernel selection ==

The ideas of structured sparsity and feature selection can be extended to the nonparametric case of [[multiple kernel learning]].&lt;ref&gt;Sonnenburg, Ratsch, Schafer AND Scholkopf. [http://www.jmlr.org/papers/volume7/sonnenburg06a/sonnenburg06a.pdf Large Scale Multiple Kernel Learning]. Journal of Machine Learning Research 7 (2006) 1531–1565.&lt;/ref&gt; This can be useful when there are multiple types of input data (color and texture, for example) with different appropriate kernels for each, or when the appropriate kernel is unknown. If there are two kernels, for example, with feature maps &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; that lie in corresponding [[reproducing kernel Hilbert space]]s &lt;math&gt;\mathcal{H_A},\mathcal{H_B}&lt;/math&gt;, then a larger space, &lt;math&gt;\mathcal{H_D}&lt;/math&gt;, can be created as the sum of two spaces:

: &lt;math&gt;\mathcal{H_D}: f=h+h'; h\in \mathcal{H_A}, h'\in \mathcal{H_B}&lt;/math&gt;

assuming linear independence in &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;. In this case the &lt;math&gt;\ell_{2,1}&lt;/math&gt;-norm is again the sum of norms:

: &lt;math&gt;\|f\|_{\mathcal{H_D},1}=\|h\|_{\mathcal{H_A}}+\|h'\|_{\mathcal{H_B}}&lt;/math&gt;

Thus, by choosing a matrix regularization function as this type of norm, it is possible to find a solution that is sparse in terms of which kernels are used, but dense in the coefficient of each used kernel. Multiple kernel learning can also be used as a form of nonlinear variable selection, or as a model aggregation technique (e.g. by taking the sum of squared norms and relaxing sparsity constraints). For example, each kernel can be taken to be the Gaussian kernel with a different width.

== See also ==
* [[Regularization (mathematics)]]

== References ==
{{reflist}}



</text>
      <sha1>i0fby3xb0z2fymae9ya30mh3jocx9a1</sha1>
    </revision>
  </page>
  <page>
    <title>Manifold regularization</title>
    <ns>0</ns>
    <id>48777199</id>
    <revision>
      <id>798878835</id>
      <parentid>782256011</parentid>
      <timestamp>2017-09-04T10:06:19Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27373">[[File:Example of unlabeled data in semisupervised learning.png|thumb|250px|Manifold regularization can classify data when labeled data (black and white circles) are sparse, by taking advantage of unlabeled data (gray circles). Without many labeled data points, [[supervised learning]] algorithms can only learn very simple decision boundaries (top panel). Manifold learning can draw a decision boundary between the natural classes of the unlabeled data, under the assumption that close-together points probably belong to the same class, and so the decision boundary should avoid areas with many unlabeled points. This is one version of [[semi-supervised learning]].]]

In [[machine learning]], '''Manifold regularization''' is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a [[facial recognition system]] may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a [[manifold]], a mathematical structure with useful properties. The technique also assumes that the function to be learned is ''smooth'': data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of [[Tikhonov regularization]]. Manifold regularization algorithms can extend [[supervised learning]] algorithms in [[semi-supervised learning]] and [[Transduction (machine learning)|transductive learning]] settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.

== Manifold regularizer ==

=== Motivation ===

Manifold regularization is a type of [[Regularization (mathematics)|regularization]], a family of techniques that reduces [[overfitting]] and ensures that a problem is [[well-posed problem|well-posed]] by penalizing complex solutions. In particular, manifold regularization extends the technique of [[Tikhonov regularization]] as applied to [[Reproducing kernel Hilbert spaces]] (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function &lt;math&gt;f&lt;/math&gt; from among a hypothesis space of functions &lt;math&gt;\mathcal{H}&lt;/math&gt;. The hypothesis space is an RKHS, meaning that it is associated with a [[Kernel method|kernel]] &lt;math&gt;K&lt;/math&gt;, and so every candidate function &lt;math&gt;f&lt;/math&gt; has a [[Norm (mathematics)|norm]] &lt;math&gt;\left\| f \right\|_K&lt;/math&gt;, which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions.

Formally, given a set of labeled training data &lt;math&gt;(x_1, y_1), \ldots, (x_{\ell}, y_{\ell})&lt;/math&gt; with &lt;math&gt;x_i \in X, y_i \in Y&lt;/math&gt; and a [[loss function]] &lt;math&gt;V&lt;/math&gt;, a learning algorithm using Tikhonov regularization will attempt to solve the expression

: &lt;math&gt; \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} V(f(x_i), y_i) + \gamma \left\| f \right\|_K^2 &lt;/math&gt;

where &lt;math&gt;\gamma&lt;/math&gt; is a [[Hyperparameter optimization|hyperparameter]] that controls how much the algorithm will prefer simpler functions to functions that fit the data better.

[[File:Lle hlle swissroll.png|thumb|right|300px|A two-dimensional [[manifold]] embedded in three-dimensional space (top-left). Manifold regularization attempts to learn a function that is smooth on the unrolled manifold (top-right).]]

Manifold regularization adds a second regularization term, the ''intrinsic regularizer'', to the ''ambient regularizer'' used in standard Tikhonov regularization. Under the [[Semi-supervised learning#Manifold assumption|manifold assumption]] in machine learning, the data in question do not come from the entire input space &lt;math&gt;X&lt;/math&gt;, but instead from a nonlinear [[manifold]] &lt;math&gt;M\subset X&lt;/math&gt;. The geometry of this manifold, the intrinsic space, is used to determine the regularization norm.&lt;ref name=&quot;Belkin et al. 2006&quot;&gt;{{Cite journal| volume = 7| pages = 2399–2434| last1 = Belkin| first1 = Mikhail| last2 = Niyogi| first2 = Partha| last3 = Sindhwani| first3 = Vikas| title = Manifold regularization: A geometric framework for learning from labeled and unlabeled examples| journal = The Journal of Machine Learning Research| accessdate = 2015-12-02| date = 2006| url = http://dl.acm.org/citation.cfm?id=1248632}}&lt;/ref&gt;

=== Laplacian norm ===

There are many possible choices for &lt;math&gt;\left\| f \right\|_I&lt;/math&gt;. Many natural choices involve the [[Differential geometry|gradient on the manifold]] &lt;math&gt; \nabla_{M} &lt;/math&gt;, which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense; that is, the gradient &lt;math&gt; \nabla_{M} f(x) &lt;/math&gt; should be small where the ''marginal probability density'' &lt;math&gt;\mathcal{P}_X(x) &lt;/math&gt;, the [[probability density]] of a randomly drawn data point appearing at &lt;math&gt;x&lt;/math&gt;, is large. This gives one appropriate choice for the intrinsic regularizer:

: &lt;math&gt; \left\| f \right\|_I^2 = \int_{x \in M} \left\| \nabla_{M} f(x) \right\|^2 \, d \mathcal{P}_X(x) &lt;/math&gt;

In practice, this norm cannot be computed directly because the marginal distribution &lt;math&gt;\mathcal{P}_X&lt;/math&gt; is unknown, but it can be estimated from the provided data. In particular, if the distances between input points are interpreted as a graph, then the [[Laplacian matrix]] of the graph can help to estimate the marginal distribution. Suppose that the input data include &lt;math&gt;\ell&lt;/math&gt; labeled examples (pairs of an input &lt;math&gt;x&lt;/math&gt; and a label &lt;math&gt;y&lt;/math&gt;) and &lt;math&gt;u&lt;/math&gt; unlabeled examples (inputs without associated labels). Define &lt;math&gt;W&lt;/math&gt; to be a matrix of edge weights for a graph, where &lt;math&gt;W_{ij}&lt;/math&gt; is a distance measure between the data points &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;x_j&lt;/math&gt;. Define &lt;math&gt;D&lt;/math&gt; to be a diagonal matrix with &lt;math&gt;D_{ii} = \sum_{j=1}^{\ell + u} W_{ij}&lt;/math&gt; and &lt;math&gt;L&lt;/math&gt; to be the Laplacian matrix &lt;math&gt;D-W&lt;/math&gt;. Then, as the number of data points &lt;math&gt;\ell + u&lt;/math&gt; increases, &lt;math&gt;L&lt;/math&gt; converges to the [[Laplace-Beltrami operator]] &lt;math&gt;\Delta_{M}&lt;/math&gt;, which is the [[divergence]] of the gradient &lt;math&gt;\nabla_M&lt;/math&gt;.&lt;ref&gt;{{Cite book
| publisher = Springer
| pages = 470–485
| last1 = Hein
| first1 = Matthias
| last2 = Audibert
| first2 = Jean-Yves
| last3 = Von Luxburg
| first3 = Ulrike
| title = Learning theory
| chapter = From graphs to manifolds–weak and strong pointwise consistency of graph laplacians
| accessdate = 2015-12-14
| date = 2005
| chapterurl = https://link.springer.com/chapter/10.1007/11503415_32}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|
 publisher = Springer
| pages = 486–500
| last1 = Belkin
| first1 = Mikhail
| last2 = Niyogi
| first2 = Partha
| title = Learning theory
| chapter = Towards a theoretical foundation for Laplacian-based manifold methods
| accessdate = 2015-12-02
| date = 2005
| chapterurl = https://link.springer.com/chapter/10.1007/11503415_33}}&lt;/ref&gt; Then, if &lt;math&gt;\mathbf{f}&lt;/math&gt; is a vector of the values of &lt;math&gt;f&lt;/math&gt; at the data, &lt;math&gt;\mathbf{f} = [f(x_1), \ldots, f(x_{l+u})]^{\mathrm{T}}&lt;/math&gt;, the intrinsic norm can be estimated:

: &lt;math&gt; \left\| f \right\|_I^2 = \frac{1}{(\ell+u)^2} \mathbf{f}^{\mathrm{T}} L \mathbf{f} &lt;/math&gt;

As the number of data points &lt;math&gt;\ell + u&lt;/math&gt; increases, this empirical definition of &lt;math&gt; \left\| f \right\|_I^2&lt;/math&gt; converges to the definition when &lt;math&gt;\mathcal{P}_X&lt;/math&gt; is known.&lt;ref name=&quot;Belkin et al. 2006&quot; /&gt;

=== Solving the regularization problem ===

Using the weights &lt;math&gt;\gamma_A&lt;/math&gt; and &lt;math&gt;\gamma_I&lt;/math&gt; for the ambient and intrinsic regularizers, the final expression to be solved becomes:

: &lt;math&gt; \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} V(f(x_i), y_i) + \gamma_A \left\| f \right\|_K^2 + \frac{\gamma_I}{(\ell+u)^2} \mathbf{f}^{\mathrm{T}} L \mathbf{f} &lt;/math&gt;

As with other [[kernel methods]], &lt;math&gt;\mathcal{H}&lt;/math&gt; may be an infinite-dimensional space, so if the regularization expression cannot be solved explicitly, it is impossible to search the entire space for a solution. Instead, a [[representer theorem]] shows that under certain conditions on the choice of the norm &lt;math&gt;\left\| f \right\|_I&lt;/math&gt;, the optimal solution &lt;math&gt;f^*&lt;/math&gt; must be a linear combination of the kernel centered at each of the input points: for some weights &lt;math&gt;\alpha_i&lt;/math&gt;,

: &lt;math&gt; f^*(x) = \sum_{i=1}^{\ell + u} \alpha_i K(x_i, x) &lt;/math&gt;

Using this result, it is possible to search for the optimal solution &lt;math&gt;f^*&lt;/math&gt; by searching the finite-dimensional space defined by the possible choices of &lt;math&gt;\alpha_i&lt;/math&gt;.&lt;ref name=&quot;Belkin et al. 2006&quot; /&gt;

== Applications ==

Manifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function &lt;math&gt;V&lt;/math&gt; and hypothesis space &lt;math&gt;\mathcal{H}&lt;/math&gt;. Two commonly used examples are the families of [[support vector machines]] and [[Least squares#Regularized versions|regularized least squares]] algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and [[elastic net regularization]] can be expressed as support vector machines.&lt;ref&gt;{{cite book
|title=An Equivalence between the Lasso and Support Vector Machines
|last=Jaggi|first=Martin
|editor-last1=Suykens|editor-first1=Johan
|editor-last2=Signoretto|editor-first2=Marco
|editor-last3=Argyriou|editor-first3=Andreas
|year=2014
|publisher=Chapman and Hall/CRC}}&lt;/ref&gt;&lt;ref&gt;
{{cite conference
|last1=Zhou
|first1=Quan
|last2=Chen
|first2=Wenlin
|last3=Song
|first3=Shiji
|last4=Gardner
|first4=Jacob
|last5=Weinberger
|first5=Kilian
|last6=Chen
|first6=Yixin
|title=A Reduction of the Elastic Net to Support Vector Machines with an Application to GPU Computing
|url=https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9856
|conference=[[Association for the Advancement of Artificial Intelligence]]}}&lt;/ref&gt;) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively.&lt;ref name=&quot;Belkin et al. 2006&quot; /&gt;

=== Laplacian Regularized Least Squares (LapRLS) ===

Regularized least squares (RLS) is a family of [[Regression analysis|regression algorithms]]: algorithms that predict a value &lt;math&gt;y = f(x)&lt;/math&gt; for its inputs &lt;math&gt;x&lt;/math&gt;, with the goal that the predicted values should be close to the true labels for the data. In particular, RLS is designed to minimize the [[mean squared error]] between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS; in general, RLS is the same as ridge regression combined with the [[kernel method]].{{Citation needed|reason=Kernel ridge regression can be seen to have the same form as RLS in a general RKHS, but it is difficult to find a source that discusses the connection in detail.|date=December 2015}} The problem statement for RLS results from choosing the loss function &lt;math&gt;V&lt;/math&gt; in Tikhonov regularization to be the mean squared error:

: &lt;math&gt; f^* = \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} (f(x_i) - y_i)^2 + \gamma \left\| f \right\|_K^2 &lt;/math&gt;

Thanks to the [[representer theorem]], the solution can be written as a weighted sum of the kernel evaluated at the data points:

: &lt;math&gt; f^*(x) = \sum_{i=1}^{\ell} \alpha_i^* K(x_i, x) &lt;/math&gt;

and solving for &lt;math&gt;\alpha^*&lt;/math&gt; gives:

: &lt;math&gt; \alpha^* = (K + \gamma \ell I)^{-1} Y &lt;/math&gt;

where &lt;math&gt;K&lt;/math&gt; is defined to be the kernel matrix, with &lt;math&gt;K_{ij} = K(x_i, x_j)&lt;/math&gt;, and &lt;math&gt;Y&lt;/math&gt; is the vector of data labels.

Adding a Laplacian term for manifold regularization gives the Laplacian RLS statement:

: &lt;math&gt; f^* = \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} (f(x_i) - y_i)^2 + \gamma_A \left\| f \right\|_K^2 + \frac{\gamma_I}{(\ell+u)^2} \mathbf{f}^{\mathrm{T}} L \mathbf{f} &lt;/math&gt;

The representer theorem for manifold regularization again gives

: &lt;math&gt; f^*(x) = \sum_{i=1}^{\ell + u} \alpha_i^* K(x_i, x) &lt;/math&gt;

and this yields an expression for the vector &lt;math&gt;\alpha^*&lt;/math&gt;. Letting &lt;math&gt;K&lt;/math&gt; be the kernel matrix as above, &lt;math&gt;Y&lt;/math&gt; be the vector of data labels, and &lt;math&gt;J&lt;/math&gt; be the &lt;math&gt; (\ell + u) \times (\ell + u) &lt;/math&gt; block matrix &lt;math&gt;\begin{bmatrix} I_{\ell} &amp; 0 \\ 0 &amp; 0_u \end{bmatrix} &lt;/math&gt;:

: &lt;math&gt; \alpha^* = \underset{\alpha \in \mathbf{R}^{\ell + u}}{\arg\!\min} \frac{1}{\ell} (Y - J K \alpha)^{\mathrm{T}} (Y - J K \alpha) + \gamma_A \alpha^{\mathrm{T}} K \alpha + \frac{\gamma_I}{(\ell + u)^2} \alpha^{\mathrm{T}} K L K \alpha &lt;/math&gt;

with a solution of

: &lt;math&gt; \alpha^* = \left( JK + \gamma_A \ell I + \frac{\gamma_I \ell}{(\ell + u)^2} L K \right)^{-1} Y &lt;/math&gt;&lt;ref name=&quot;Belkin et al. 2006&quot; /&gt;

LapRLS has been applied to problems including sensor networks,&lt;ref&gt;{{Cite conference
| publisher = Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999
| volume = 21
| pages = 988
| last1 = Pan
| first1 = Jeffrey Junfeng
| last2 = Yang
| first2 = Qiang
| last3 = Chang
| first3 = Hong
| last4 = Yeung
| first4 = Dit-Yan
| title = A manifold regularization approach to calibration reduction for sensor-network based tracking
| booktitle = Proceedings of the national conference on artificial intelligence| accessdate = 2015-12-02
| date = 2006
| url = http://www.aaai.org/Papers/AAAI/2006/AAAI06-155.pdf}}&lt;/ref&gt;
[[medical imaging]],&lt;ref&gt;{{Cite conference| publisher = IEEE| pages = 1628–1631| last1 = Zhang| first1 = Daoqiang| last2 = Shen| first2 = Dinggang| title = Semi-supervised multimodal classification of Alzheimer's disease| booktitle = Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on| accessdate = 2015-12-15| date = 2011| url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5872715}}&lt;/ref&gt;&lt;ref&gt;{{Cite book| publisher = Springer| pages = 264–271| last1 = Park| first1 = Sang Hyun| last2 = Gao| first2 = Yaozong| last3 = Shi| first3 = Yinghuan| last4 = Shen| first4 = Dinggang| title = Machine Learning in Medical Imaging| chapter = Interactive Prostate Segmentation Based on Adaptive Feature Selection and Manifold Regularization| accessdate = 2015-12-15| date = 2014| chapterurl = https://link.springer.com/chapter/10.1007/978-3-319-10581-9_33}}&lt;/ref&gt;
object detection,&lt;ref&gt;{{Cite journal| last = Pillai| first = Sudeep| title = Semi-supervised Object Detector Learning from Minimal Labels| accessdate = 2015-12-15| url = http://people.csail.mit.edu/spillai/data/papers/ssl-cv-project-paper.pdf}}&lt;/ref&gt;
[[spectroscopy]],&lt;ref&gt;{{Cite journal| volume = 11| issue = 1| pages = 416–419| last1 = Wan| first1 = Songjing| last2 = Wu| first2 = Di| last3 = Liu| first3 = Kangsheng| title = Semi-Supervised Machine Learning Algorithm in Near Infrared Spectral Calibration: A Case Study on Diesel Fuels| journal = Advanced Science Letters| accessdate = 2015-12-15| date = 2012| url = http://www.ingentaconnect.com/content/asp/asl/2012/00000011/00000001/art00076| doi=10.1166/asl.2012.3044}}&lt;/ref&gt;
[[document classification]],&lt;ref&gt;{{Cite journal| volume = 8| issue = 4| pages = 1011–1018| last1 = Wang| first1 = Ziqiang| last2 = Sun| first2 = Xia| last3 = Zhang| first3 = Lijie| last4 = Qian| first4 = Xu| title = Document Classification based on Optimal Laprls| journal = Journal of Software| accessdate = 2015-12-15| date = 2013| url = http://ojs.academypublisher.com/index.php/jsw/article/view/8009| doi=10.4304/jsw.8.4.1011-1018}}&lt;/ref&gt;
drug-protein interactions,&lt;ref&gt;{{Cite journal| volume = 4| issue = Suppl 2| pages = –6| last1 = Xia| first1 = Zheng| last2 = Wu| first2 = Ling-Yun| last3 = Zhou| first3 = Xiaobo| last4 = Wong| first4 = Stephen TC| title = Semi-supervised drug-protein interaction prediction from heterogeneous biological spaces| journal = BMC systems biology| accessdate = 2015-12-15| date = 2010| url = http://www.biomedcentral.com/qc/1752-0509/4/S2/S6/}}&lt;/ref&gt;
and compressing images and videos.&lt;ref&gt;{{Cite conference| publisher = ACM| pages = 161–168| last1 = Cheng| first1 = Li| last2 = Vishwanathan| first2 = S. V. N.| title = Learning to compress images and videos| booktitle = Proceedings of the 24th international conference on Machine learning| accessdate = 2015-12-16| date = 2007| url = http://dl.acm.org/citation.cfm?id=1273517}}&lt;/ref&gt;

=== Laplacian Support Vector Machines (LapSVM) ===

[[Support vector machines]] (SVMs) are a family of algorithms often used for [[Statistical classification|classifying data]] into two or more groups, or ''classes''. Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a [[linear program]], but it is also equivalent to Tikhonov regularization with the [[hinge loss]] function, &lt;math&gt;V(f(x), y) = \max(0, 1 - yf(x))&lt;/math&gt;:

: &lt;math&gt; f^* = \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} \max(0, 1 - y_if(x_i)) + \gamma \left\| f \right\|_K^2 &lt;/math&gt;&lt;ref&gt;{{Cite journal| volume = 48| issue = 1-3| pages = 115–136| last1 = Lin| first1 = Yi| last2 = Wahba| first2 = Grace| last3 = Zhang| first3 = Hao| last4 = Lee| first4 = Yoonkyung|author4-link= Yoonkyung Lee | title = Statistical properties and adaptive tuning of support vector machines| journal = Machine Learning| accessdate = 2015-12-16| date = 2002| url = https://link.springer.com/article/10.1023/A:1013951620650| doi=10.1023/A:1013951620650}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal| volume = 6| pages = 69–87| last1 = Wahba| first1 = Grace| last2 = others| title = Support vector machines, reproducing kernel Hilbert spaces and the randomized GACV| journal = Advances in Kernel Methods-Support Vector Learning| accessdate = 2015-12-16| date = 1999| url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.2114&amp;rep=rep1&amp;type=pdf}}&lt;/ref&gt;

Adding the intrinsic regularization term to this expression gives the LapSVM problem statement:

: &lt;math&gt; f^* = \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} \max(0, 1 - y_if(x_i)) + \gamma_A \left\| f \right\|_K^2 + \frac{\gamma_I}{(\ell+u)^2} \mathbf{f}^{\mathrm{T}} L \mathbf{f} &lt;/math&gt;

Again, the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data points:

: &lt;math&gt; f^*(x) = \sum_{i=1}^{\ell + u} \alpha_i^* K(x_i, x) &lt;/math&gt;

&lt;math&gt;\alpha&lt;/math&gt; can be found by writing the problem as a linear program and solving the [[Duality (optimization)|dual problem]]. Again letting &lt;math&gt;K&lt;/math&gt; be the kernel matrix and &lt;math&gt;J&lt;/math&gt; be the block matrix &lt;math&gt;\begin{bmatrix} I_{\ell} &amp; 0 \\ 0 &amp; 0_u \end{bmatrix} &lt;/math&gt;, the solution can be shown to be

: &lt;math&gt;\alpha = \left( 2 \gamma_A I + 2 \frac{\gamma_I}{(\ell + u)^2} L K \right)^{-1} J^{\mathrm{T}} Y \beta^* &lt;/math&gt;

where &lt;math&gt;\beta^*&lt;/math&gt; is the solution to the dual problem

:&lt;math&gt; \begin{align}
&amp; &amp; \beta^* = \max_{\beta \in \mathbf{R}^{\ell}} &amp; \sum_{i=1}^{\ell} \beta_i - \frac{1}{2} \beta^{\mathrm{T}} Q \beta \\
&amp; \text{subject to} &amp;&amp; \sum_{i=1}^{\ell} \beta_i y_i = 0 \\
&amp; &amp;&amp; 0 \le \beta_i \le \frac{1}{\ell}\; i = 1, \ldots, \ell
\end{align} &lt;/math&gt;

and &lt;math&gt;Q&lt;/math&gt; is defined by

: &lt;math&gt; Q = YJK \left( 2 \gamma_A I + 2 \frac{\gamma_I}{(\ell + u)^2} L K \right)^{-1} J^{\mathrm{T}} Y &lt;/math&gt;&lt;ref name=&quot;Belkin et al. 2006&quot; /&gt;

LapSVM has been applied to problems including geographical imaging,&lt;ref&gt;{{Cite journal| volume = 48| issue = 11| pages = 4110–4121| last1 = Kim| first1 = Wonkook| last2 = Crawford| first2 = Melba M.| title = Adaptive classification for hyperspectral image data using manifold regularization kernel machines| journal = Geoscience and Remote Sensing, IEEE Transactions on| accessdate = 2015-12-02| date = 2010| url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5599864}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal| volume = 31| issue = 1| pages = 45–54| last1 = Camps-Valls| first1 = Gustavo| last2 = Tuia| first2 = Devis| last3 = Bruzzone| first3 = Lorenzo| last4 = Atli Benediktsson| first4 = Jon| title = Advances in hyperspectral image classification: Earth monitoring with statistical learning methods| journal = Signal Processing Magazine, IEEE| accessdate = 2015-12-16| date = 2014| url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6678612| doi=10.1109/msp.2013.2279179}}&lt;/ref&gt;&lt;ref&gt;{{Cite conference| publisher = IEEE| pages = 1521–1524| last1 = Gómez-Chova| first1 = Luis| last2 = Camps-Valls| first2 = Gustavo| last3 = Muñoz-Marí| first3 = Jordi| last4 = Calpe| first4 = Javier| title = Semi-supervised cloud screening with Laplacian SVM| booktitle = Geoscience and Remote Sensing Symposium, 2007. IGARSS 2007. IEEE International| accessdate = 2015-12-16| date = 2007| url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4423098}}&lt;/ref&gt;
medical imaging,&lt;ref&gt;{{Cite book| publisher = Springer| pages = 82–90| last1 = Cheng| first1 = Bo| last2 = Zhang| first2 = Daoqiang| last3 = Shen| first3 = Dinggang| title = Medical Image Computing and Computer-Assisted Intervention–MICCAI 2012| chapter = Domain transfer learning for MCI conversion prediction| accessdate = 2015-12-16| date = 2012| chapterurl = https://link.springer.com/chapter/10.1007/978-3-642-33415-3_11}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal| volume = 37| issue = 8| pages = 4155–4172| last1 = Jamieson| first1 = Andrew R.| last2 = Giger| first2 = Maryellen L.| last3 = Drukker| first3 = Karen| last4 = Pesce| first4 = Lorenzo L.| title = Enhancement of breast CADx with unlabeled dataa)| journal = Medical physics| accessdate = 2015-12-16| date = 2010| url = http://scitation.aip.org/content/aapm/journal/medphys/37/8/10.1118/1.3455704| doi=10.1118/1.3455704}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal| volume = 1| issue = 2| pages = 151–155| last1 = Wu| first1 = Jiang| last2 = Diao| first2 = Yuan-Bo| last3 = Li| first3 = Meng-Long| last4 = Fang| first4 = Ya-Ping| last5 = Ma| first5 = Dai-Chuan| title = A semi-supervised learning based method: Laplacian support vector machine used in diabetes disease diagnosis| journal = Interdisciplinary Sciences: Computational Life Sciences| accessdate = 2015-12-16| date = 2009| url = https://link.springer.com/article/10.1007/s12539-009-0016-2| doi=10.1007/s12539-009-0016-2}}&lt;/ref&gt;
face recognition,&lt;ref&gt;{{Cite journal| volume = 4| issue = 17| last1 = Wang| first1 = Ziqiang| last2 = Zhou| first2 = Zhiqiang| last3 = Sun| first3 = Xia| last4 = Qian| first4 = Xu| last5 = Sun| first5 = Lijun| title = Enhanced LapSVM Algorithm for Face Recognition.| journal = International Journal of Advancements in Computing Technology| accessdate = 2015-12-16| date = 2012| url = http://search.ebscohost.com/login.aspx?direct=true&amp;profile=ehost&amp;scope=site&amp;authtype=crawler&amp;jrnl=20058039&amp;AN=98908455&amp;h=8QzzRizi2IKxCZ4EHJjzxbGY%2FQazcifd58fcAGEG17GiFk0wZE59DrEge0xfEGhXRqsBaMwuBNyenVSP6sjwsA%3D%3D&amp;crl=c}}&lt;/ref&gt;
machine maintenance,&lt;ref&gt;{{Cite journal| volume = 38| issue = 8| pages = 10199–10204| last1 = Zhao| first1 = Xiukuan| last2 = Li| first2 = Min| last3 = Xu| first3 = Jinwu| last4 = Song| first4 = Gangbing| title = An effective procedure exploiting unlabeled data to build monitoring system| journal = Expert Systems with Applications| accessdate = 2015-12-16| date = 2011| url = http://www.sciencedirect.com/science/article/pii/S0957417411002843| doi=10.1016/j.eswa.2011.02.078}}&lt;/ref&gt;
and [[brain-computer interfaces]].&lt;ref&gt;{{Cite journal| volume = 7| issue = 1| pages = 22–26| last1 = Zhong| first1 = Ji-Ying| last2 = Lei| first2 = Xu| last3 = Yao| first3 = D.| title = Semi-supervised learning based on manifold in BCI| journal = Journal of Electronics Science and Technology of China| accessdate = 2015-12-16| date = 2009| url = http://www.journal.uestc.edu.cn/archives/2009/1/7/22-2677907.pdf}}&lt;/ref&gt;

== Limitations ==

* Manifold regularization assumes that data with different labels are not likely to be close together. This assumption is what allows the technique to draw information from unlabeled data, but it only applies to some problem domains. Depending on the structure of the data, it may be necessary to use a different semi-supervised or transductive learning algorithm.&lt;ref&gt;{{Cite journal| last = Zhu| first = Xiaojin| title = Semi-supervised learning literature survey| accessdate = 2015-12-05| date = 2005| url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681&amp;rep=rep1&amp;type=pdf}}&lt;/ref&gt;
* In some datasets, the intrinsic norm of a function &lt;math&gt;\left\| f \right\|_I&lt;/math&gt; can be very close to the ambient norm &lt;math&gt;\left\| f \right\|_K&lt;/math&gt;: for example, if the data consist of two classes that lie on perpendicular lines, the intrinsic norm will be equal to the ambient norm. In this case, unlabeled data have no effect on the solution learned by manifold regularization, even if the data fit the algorithm's assumption that the separator should be smooth. Approaches related to [[co-training]] have been proposed to address this limitation.&lt;ref&gt;{{Cite conference| publisher = ACM| pages = 976–983| last1 = Sindhwani| first1 = Vikas| last2 = Rosenberg| first2 = David S.| title = An RKHS for multi-view learning and manifold co-regularization| booktitle = Proceedings of the 25th international conference on Machine learning| accessdate = 2015-12-02| date = 2008| url = http://dl.acm.org/citation.cfm?id=1390279}}&lt;/ref&gt;
* If there are a very large number of unlabeled examples, the kernel matrix &lt;math&gt;K&lt;/math&gt; becomes very large, and a manifold regularization algorithm may become prohibitively slow to compute. Online algorithms and sparse approximations of the manifold may help in this case.&lt;ref&gt;{{Cite journal| pages = 393–407| last1 = Goldberg| first1 = Andrew| last2 = Li| first2 = Ming| last3 = Zhu| first3 = Xiaojin| title = Online manifold regularization: A new learning setting and empirical study| journal = Machine Learning and Knowledge Discovery in Databases| accessdate = 2015-12-02| date = 2008| url = http://www.springerlink.com/index/ln1805476103536p.pdf}}&lt;/ref&gt;

== Software ==
* The [http://manifold.cs.uchicago.edu/manifold_regularization/software.html ManifoldLearn library] and the [http://www.dii.unisi.it/~melacci/lapsvmp/ Primal LapSVM library] implement LapRLS and LapSVM in [[MATLAB]].
* The [http://dlib.net/ml.html Dlib library] for [[C++]] includes a linear manifold regularization function.

== See also ==
* [[Manifold learning]]
* [[Semi-supervised learning]]
* [[Transduction (machine learning)]]
* [[Spectral graph theory]]
* [[Reproducing kernel Hilbert space]]
* [[Tikhonov regularization]]
* [[Differential geometry]]

== References ==
{{Reflist}}


[[Category:Articles created via the Article Wizard]]</text>
      <sha1>jgvj5f7i9ivq1gy1qkly309f25v1wfj</sha1>
    </revision>
  </page>
  <page>
    <title>Error tolerance (PAC learning)</title>
    <ns>0</ns>
    <id>48833041</id>
    <revision>
      <id>811121674</id>
      <parentid>808819945</parentid>
      <timestamp>2017-11-19T17:27:55Z</timestamp>
      <contributor>
        <username>Mlliarm</username>
        <id>26243310</id>
      </contributor>
      <comment>/* Notation and the valiant learning model */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11216">{{Use dmy dates|date=September 2017}}
{{Orphan|date=December 2015}}
{{machine learning bar}}

==Error tolerance (PAC learning)==
In [[Probably approximately correct learning|PAC learning]], '''error tolerance''' refers to the ability of an [[algorithm]] to learn when the examples received have been corrupted in some way. In fact, this is a very common and important issue since in many applications it is not possible to access noise-free data. Noise can interfere with the learning process at different levels: the algorithm may receive data that have been occasionally mislabeled, or the inputs may have some false information, or the classification of the examples may have been maliciously adulterated.

==Notation and the Valiant learning model==
In the following, let &lt;math&gt;X&lt;/math&gt; be our &lt;math&gt;n&lt;/math&gt;-dimensional input space. Let &lt;math&gt;\mathcal{H}&lt;/math&gt; be a class of functions that we wish to use in order to learn a &lt;math&gt;\{0,1\}&lt;/math&gt;-valued target function &lt;math&gt;f&lt;/math&gt; defined over &lt;math&gt;X&lt;/math&gt;. Let &lt;math&gt;\mathcal{D}&lt;/math&gt; be the distribution of the inputs over &lt;math&gt;X&lt;/math&gt;. The goal of a learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; is to choose the best function &lt;math&gt;h \in \mathcal{H}&lt;/math&gt; such that it minimizes &lt;math&gt;error(h) = P_{x \sim \mathcal{D} }( h(x) \neq f(x))&lt;/math&gt;. Let us suppose we have a function &lt;math&gt;size(f)&lt;/math&gt; that can measure the complexity of &lt;math&gt;f&lt;/math&gt;. Let &lt;math&gt; Oracle(x)&lt;/math&gt; be an oracle that, whenever called, returns an example &lt;math&gt;x&lt;/math&gt; and its correct label &lt;math&gt;f(x)&lt;/math&gt;.

When no noise corrupts the data, we can define '''learning in the Valiant setting''':&lt;ref&gt;Valiant, L. G. (August 1985). ''[http://www.ijcai.org/Past%20Proceedings/IJCAI-85-VOL1/PDF/107.pdf Learning Disjunction of Conjunctions]''. In IJCAI (pp. 560–566).&lt;/ref&gt;&lt;ref&gt;Valiant, Leslie G. &quot;A theory of the learnable.&quot; Communications of the ACM 27.11 (1984): 1134–1142.&lt;/ref&gt;

'''Definition:'''
We say that &lt;math&gt;f&lt;/math&gt; is efficiently learnable using &lt;math&gt;\mathcal{H}&lt;/math&gt; in the [[Leslie Valiant|Valiant]] setting if there exists a learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; that has access to &lt;math&gt; Oracle(x)&lt;/math&gt; and a polynomial &lt;math&gt;p(\cdot,\cdot,\cdot,\cdot)&lt;/math&gt; such that for any &lt;math&gt;0 &lt; \varepsilon \leq 1&lt;/math&gt; and &lt;math&gt;0 &lt; \delta \leq 1&lt;/math&gt; it outputs, in a number of calls to the oracle bounded by &lt;math&gt;p\left(\frac{1}{\varepsilon},\frac{1}{\delta},n,size(f)\right)&lt;/math&gt; , a function &lt;math&gt;h \in \mathcal{H}&lt;/math&gt; that satisfies with probability at least &lt;math&gt;1-\delta&lt;/math&gt; the condition &lt;math&gt;error(h) \leq \varepsilon&lt;/math&gt;.

In the following we will define learnability of &lt;math&gt;f&lt;/math&gt; when data have suffered some modification.&lt;ref&gt;Laird, P. D. (1988). ''[https://link.springer.com/content/pdf/bfm%3A978-1-4613-1685-5%2F1.pdf Learning from good and bad data]''. Kluwer Academic Publishers.&lt;/ref&gt;&lt;ref&gt;Kearns, Michael. &quot;Efficient noise-tolerant learning from statistical queries.&quot; Journal of the ACM 45.6 (1998): 983–1006.&lt;/ref&gt;&lt;ref&gt;Brunk, Clifford A., and Michael J. Pazzani. &quot;An investigation of noise-tolerant relational concept learning algorithms.&quot; Proceedings of the 8th International Workshop on Machine Learning. 1991.&lt;/ref&gt;

==Classification noise==
In the classification noise model&lt;ref name = &quot;kv&quot;&gt;Kearns, M. J., &amp; Vazirani, U. V. (1994). An introduction to computational learning theory, chapter 5. MIT press.&lt;/ref&gt; a '''noise rate''' &lt;math&gt;0 \leq \eta &lt; \frac{1}{2}&lt;/math&gt; is introduced. Then, instead of &lt;math&gt;Oracle(x)&lt;/math&gt; that returns always the correct label of example &lt;math&gt;x&lt;/math&gt;, algorithm &lt;math&gt; \mathcal{A}&lt;/math&gt; can only call a faulty oracle &lt;math&gt;Oracle(x,\eta)&lt;/math&gt; that will flip the label of &lt;math&gt;x&lt;/math&gt; with probability &lt;math&gt;\eta&lt;/math&gt;. As in the Valiant case, the goal of a learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; is to choose the best function &lt;math&gt;h \in \mathcal{H}&lt;/math&gt; such that it minimizes &lt;math&gt;error(h) = P_{x \sim \mathcal{D} }( h(x) \neq f(x))&lt;/math&gt;. In applications it is difficult to have access to the real value of &lt;math&gt;\eta&lt;/math&gt;, but we assume we have access to its upperbound &lt;math&gt;\eta_B&lt;/math&gt;.&lt;ref&gt;Angluin, D., &amp; Laird, P. (1988). ''[http://Fhomepages.math.uic.edu/~lreyzin/f14_mcs548/angluin88b.pdf Learning from noisy examples]''. Machine Learning, 2(4), 343–370.&lt;/ref&gt; Note that if we allow the noise rate to be &lt;math&gt;1/2&lt;/math&gt;, then learning becomes impossible in any amount of computation time, because every label conveys no information about the target function.

'''Definition:'''
We say that &lt;math&gt;f&lt;/math&gt; is efficiently learnable using &lt;math&gt;\mathcal{H}&lt;/math&gt; in the '''classification noise model''' if there exists a learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; that has access to &lt;math&gt;Oracle(x,\eta)&lt;/math&gt; and a polynomial &lt;math&gt;p(\cdot,\cdot,\cdot,\cdot)&lt;/math&gt; such that for any &lt;math&gt;0 \leq \eta \leq \frac{1}{2}&lt;/math&gt;,  &lt;math&gt;0\leq \varepsilon \leq 1&lt;/math&gt; and &lt;math&gt;0\leq \delta \leq 1&lt;/math&gt; it outputs, in a number of calls to the oracle bounded by &lt;math&gt;p\left(\frac{1}{1-2\eta_B}, \frac{1}{\varepsilon},\frac{1}{\delta},n,size(f)\right)&lt;/math&gt; , a function &lt;math&gt;h \in \mathcal{H}&lt;/math&gt;  that satisfies with probability at least &lt;math&gt;1-\delta&lt;/math&gt; the condition &lt;math&gt;error(h) \leq \varepsilon&lt;/math&gt;.

==Statistical query learning==
Statistical Query Learning&lt;ref name = &quot;kearns&quot;&gt;Kearns, M. (1998). ''[www.cis.upenn.edu/~mkearns/papers/sq-journal.pdf Efficient noise-tolerant learning from statistical queries]''. Journal of the ACM, 45(6), 983–1006.&lt;/ref&gt; is a kind of [[Active learning (machine learning)|active learning]] problem in which the learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; can decide if to request information about the likelihood &lt;math&gt;P_{f(x)}&lt;/math&gt; that a function &lt;math&gt;f&lt;/math&gt; correctly labels example &lt;math&gt;x&lt;/math&gt;, and receives an answer accurate within a tolerance &lt;math&gt;\alpha&lt;/math&gt;. Formally, whenever the learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; calls the oracle &lt;math&gt;Oracle(x,\alpha)&lt;/math&gt;, it receives as feedback probability &lt;math&gt;Q_{f(x)}&lt;/math&gt;, such that &lt;math&gt;Q_{f(x)} - \alpha \leq P_{f(x)} \leq Q_{f(x)} + \alpha&lt;/math&gt;.

'''Definition:'''
We say that &lt;math&gt;f&lt;/math&gt; is efficiently learnable using &lt;math&gt;\mathcal{H}&lt;/math&gt; in the '''statistical query learning model''' if there exists a learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; that has access to &lt;math&gt;Oracle(x,\alpha)&lt;/math&gt; and polynomials &lt;math&gt;p(\cdot,\cdot,\cdot)&lt;/math&gt;, &lt;math&gt;q(\cdot,\cdot,\cdot)&lt;/math&gt;, and &lt;math&gt;r(\cdot,\cdot,\cdot)&lt;/math&gt; such that for any  &lt;math&gt;0 &lt; \varepsilon \leq 1&lt;/math&gt; the following hold:
# &lt;math&gt;Oracle(x,\alpha)&lt;/math&gt; can evaluate &lt;math&gt;P_{f(x)}&lt;/math&gt; in time &lt;math&gt;q\left(\frac{1}{\varepsilon},n,size(f)\right)&lt;/math&gt;;
# &lt;math&gt;\frac{1}{\alpha}&lt;/math&gt; is bounded by &lt;math&gt;r\left(\frac{1}{\varepsilon},n,size(f)\right)&lt;/math&gt;
# &lt;math&gt;\mathcal{A}&lt;/math&gt; outputs a model &lt;math&gt;h&lt;/math&gt; such that &lt;math&gt;err(h)&lt;\varepsilon&lt;/math&gt;, in a number of calls to the oracle bounded by &lt;math&gt;p\left(\frac{1}{\varepsilon},n,size(f)\right)&lt;/math&gt;.

Note that the confidence parameter &lt;math&gt;\delta&lt;/math&gt; does not appear in the definition of learning. This is because the main purpose of &lt;math&gt;\delta&lt;/math&gt; is to allow the learning algorithm a small probability of failure due to an unrepresentative sample. Since now &lt;math&gt;Oracle(x,\alpha)&lt;/math&gt; always guarantees to meet the approximation criterion &lt;math&gt;Q_{f(x)} - \alpha \leq P_{f(x)} \leq Q_{f(x)} + \alpha&lt;/math&gt;, the failure probability is no longer needed.

The statistical query model is strictly weaker than the PAC model: any efficiently SQ-learnable class is efficiently PAC learnable in the presence of classification noise, but there exist efficient PAC-learnable problems such as [[Parity (mathematics)|parity]] that are not efficiently SQ-learnable.&lt;ref name = &quot;kearns&quot; /&gt;

==Malicious classification==
In the malicious classification model&lt;ref&gt;Kearns, M., &amp; Li, M. (1993). ''[www.cis.upenn.edu/~mkearns/papers/malicious.pdf Learning in the presence of malicious errors]''. SIAM Journal on Computing, 22(4), 807–837.&lt;/ref&gt; an adversary generates errors to foil the learning algorithm. This setting describes situations of [[Burst error|error burst]], which may occur when for a limited time transmission equipment malfunctions repeatedly. Formally, algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; calls an oracle &lt;math&gt;Oracle(x,\beta)&lt;/math&gt; that returns a correctly labeled example &lt;math&gt;x&lt;/math&gt; drawn, as usual, from distribution &lt;math&gt;\mathcal{D}&lt;/math&gt; over the input space with probability &lt;math&gt;1- \beta&lt;/math&gt;, but it returns with probability &lt;math&gt;\beta&lt;/math&gt; an example drawn from a distribution that is not related to &lt;math&gt;\mathcal{D}&lt;/math&gt;.
Moreover, this maliciously chosen example may strategically selected by an adversary who has knowledge of &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;\beta&lt;/math&gt;, &lt;math&gt;\mathcal{D}&lt;/math&gt;, or the current progress of the learning algorithm.

'''Definition:'''
Given a bound &lt;math&gt;\beta_B&lt;  \frac{1}{2} &lt;/math&gt; for &lt;math&gt;0 \leq \beta &lt; \frac{1}{2}&lt;/math&gt;, we say that &lt;math&gt;f&lt;/math&gt; is efficiently learnable using &lt;math&gt;\mathcal{H}&lt;/math&gt; in the malicious classification model, if there exist a learning algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; that has access to &lt;math&gt;Oracle(x,\beta)&lt;/math&gt; and a polynomial &lt;math&gt;p(\cdot,\cdot,\cdot,\cdot,\cdot)&lt;/math&gt; such that for any  &lt;math&gt;0 &lt; \varepsilon \leq 1&lt;/math&gt;, &lt;math&gt;0 &lt; \delta \leq 1&lt;/math&gt; it outputs, in a number of calls to the oracle bounded by &lt;math&gt;p\left(\frac{1}{1/2 - \beta_B},\frac{1}{\varepsilon},\frac{1}{\delta},n,size(f)\right)&lt;/math&gt; , a function &lt;math&gt;h \in \mathcal{H}&lt;/math&gt;  that satisfies with probability at least &lt;math&gt;1-\delta&lt;/math&gt; the condition &lt;math&gt;error(h) \leq \varepsilon&lt;/math&gt;.

==Errors in the inputs: nonuniform random attribute noise==
In the nonuniform random attribute noise&lt;ref&gt;Goldman, S. A., &amp; Robert, H. (1991). Sloan. The difficulty of random attribute noise. Technical Report WUCS 91 29, Washington University, Department of Computer Science.&lt;/ref&gt;&lt;ref&gt;Sloan, R. H. (1989). ''[http://dspace.mit.edu/bitstream/handle/1721.1/38339/20770411.pdf?sequence=1 Computational learning theory: New models and algorithms]'' (Doctoral dissertation, Massachusetts Institute of Technology).&lt;/ref&gt; model the algorithm is learning a [[Boolean function]], a malicious oracle &lt;math&gt;Oracle(x,\nu)&lt;/math&gt; may flip each &lt;math&gt;i&lt;/math&gt;-th bit of example &lt;math&gt;x=(x_1,x_2,\ldots,x_n)&lt;/math&gt; independently with probability &lt;math&gt;\nu_i \leq \nu&lt;/math&gt;.

This type of error can irreparably foil the algorithm, in fact the following theorem holds:

In the nonuniform random attribute noise setting, an algorithm &lt;math&gt;\mathcal{A}&lt;/math&gt; can output a function &lt;math&gt;h \in \mathcal{H}&lt;/math&gt; such that &lt;math&gt;error(h)&lt;\varepsilon &lt;/math&gt; only if &lt;math&gt;\nu &lt; 2\varepsilon &lt;/math&gt;.

== See also==
{{Portal|Artificial intelligence|Machine learning}}
{{columns-list|1|
* [[Machine learning]]
* [[Data mining]]
* [[Probably approximately correct learning]]
* [[Adversarial machine learning]]
}}

==References==
{{Reflist}}



</text>
      <sha1>ioat500t4zo7qhi975lfe26c2ysmbh5</sha1>
    </revision>
  </page>
  <page>
    <title>Multiple instance learning</title>
    <ns>0</ns>
    <id>48841414</id>
    <revision>
      <id>800595633</id>
      <parentid>780509462</parentid>
      <timestamp>2017-09-14T14:28:19Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>clean up spacing around punctuation, replaced: ,or  → , or , ,B → , B (2), ,y → , y using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27330">{{merge from|Multiple-instance_learning|discuss=Talk:Multiple_instance_learning#Merger proposal|date=September 2016}}
{{Machine learning bar}}

Depending on the type and variation in training data, machine learning can be roughly categorized into three frameworks: supervised learning, unsupervised learning, and reinforcement learning. '''Multiple instance learning (MIL)''' falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled “bags”, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags.

Convenient and simple example for MIL was given in.&lt;ref name = Babenko&gt;Babenko, Boris. &quot;Multiple instance learning: algorithms and applications.&quot; View Article PubMed/NCBI Google Scholar (2008).&lt;/ref&gt; Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn’t.

==Background==
Keeler et al.,&lt;ref name = Keeler&gt;Keeler, James D., David E. Rumelhart, and Wee-Kheng Leow. Integrated Segmentation and Recognition of Hand-Printed Numerals. Microelectronics and Computer Technology Corporation, 1991.&lt;/ref&gt; in his work in early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction.&lt;ref name = Dietterich&gt;Dietterich, Thomas G., Richard H. Lathrop, and Tomás Lozano-Pérez. &quot;Solving the multiple instance problem with axis-parallel rectangles.&quot; Artificial intelligence 89.1 (1997): 31-71.&lt;/ref&gt; They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn’t say exactly which of its low-energy shapes are responsible for that.

One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn’t really useful.&lt;ref name=&quot;Dietterich&quot;/&gt; Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning.

Solution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm.&lt;ref name=&quot;Dietterich&quot;/&gt; It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset,&lt;ref name = Musk&gt;C. Blake, E. Keogh, and C.J. Merz. UCI repository of machine learning databases [http://www.ics.uci.edu/amlearn/MLRepository.html], Department of Information and Computer Science, University of California, Irvine, CA, 1998.&lt;/ref&gt; which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but it should be noted that APR was designed with Musk data in mind.

Problem of multi-instance learning is not unique to drug finding. In 1998, Maron and Ratan found another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework.&lt;ref name = Maron&gt;O. Maron and A.L. Ratan. Multiple-instance learning for natural scene classification. In Proceedings of the 15th International Conference on Machine Learning, Madison, WI, pp.341–349, 1998.&lt;/ref&gt; Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction.

== Definitions ==
If the space of instances is &lt;math&gt;\mathcal{X}&lt;/math&gt;, then the set of bags is the set of functions &lt;math&gt;\mathbb{N}^\mathcal{X} = \{B: \mathcal{X} \rightarrow \mathbb{N} \}&lt;/math&gt;, which is isomorphic to the set of multi-subsets of &lt;math&gt;\mathcal{X}&lt;/math&gt;. For each bag &lt;math&gt;B \in \mathbb{N}^\mathcal{X}&lt;/math&gt; and each instance &lt;math&gt;x \in \mathcal{X} &lt;/math&gt;, &lt;math&gt;B(x)&lt;/math&gt; is viewed as the number of times &lt;math&gt;x&lt;/math&gt; occurs in &lt;math&gt;B&lt;/math&gt;.&lt;ref name = Review&gt;Foulds, James, and Eibe Frank. “A review of multi-instance learning assumptions.” The Knowledge Engineering Review 25.01 (2010): 1-25.&lt;/ref&gt; Let &lt;math&gt;\mathcal{Y}&lt;/math&gt; be the space of labels, then a &quot;multiple instance concept&quot; is a map &lt;math&gt;c: \mathbb{N}^\mathcal{X} \rightarrow \mathcal{Y}&lt;/math&gt;. The goal of MIL is to learn such a concept. The remainder of the article will focus on [[binary classification]], where &lt;math&gt;\mathcal{Y} = \{0, 1\}&lt;/math&gt;.

== Assumptions ==
Most of the work on Multiple instance learning, including Dietterich et al. (1997) and Maron &amp; Lozano-P´erez (1997) early papers,&lt;ref name = Dietterich /&gt;&lt;ref name = Perez&gt;Maron, Oded, and Tomás Lozano-Pérez. &quot;A framework for multiple-instance learning.&quot; Advances in neural information processing systems (1998): 570-576&lt;/ref&gt; make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption.

=== Standard assumption ===
The standard assumption takes each instance &lt;math&gt;x \in \mathcal{X}&lt;/math&gt; to have an associated label &lt;math&gt;y \in \{0,1\}&lt;/math&gt; which is hidden to the learner. The pair &lt;math&gt;(x,y)&lt;/math&gt; is called an&quot;instance-level concept&quot;. A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let &lt;math&gt;B = \{ (x_1, y_1), \ldots, (x_n, y_n) \}&lt;/math&gt; be a bag. The label of &lt;math&gt;B&lt;/math&gt; is then &lt;math&gt;c(B) = 1 - \prod_{i=1}^n (1 - y_i)&lt;/math&gt;. Standard MI assumption is asymmetric, which means that if the positive and negative labels are reversed, the assumption has a different meaning. Because of that, when we use this assumption, we need to be clear which label should be the positive one.

Standard assumption might be viewed as too strict, and therefore in the recent years, researchers tried to relax that position, which gave rise to other more loose assumptions.&lt;ref name = Xu&gt;Xu, X. Statistical learning in multiple instance problems. Master’s thesis, University of Waikato (2003).&lt;/ref&gt; Reason for this is the belief that standard MI assumption is appropriate for the Musk dataset, but since MLI can be applied to numerous other problems, some different assumptions could probably be more appropriate. Guided by that idea, Weidmann &lt;ref name = Weidmann&gt;Weidmann, Nils B. “Two-level classification for generalized multi-instance data.” Diss. Albert-Ludwigs-Universität, 2003.&lt;/ref&gt; formulated a hierarchy of generalized instance-based assumptions for MIL. It consists of the standard MI assumption and three types of generalized MI assumptions, each more general than the last, &lt;math&gt;standard \subset presence\text{-}based \subset threshold\text{-}based \subset count\text{-}based&lt;/math&gt;, with the count-based assumption being the most general and the standard assumption being the least general. One would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions.

=== Presence-, threshold-, and count-based assumptions ===
The presence-based assumption is a generalization of the standard assumption, wherein a bag must contain one or more instances that belong to a set of required instance-level concepts in order to be labeled positive. Formally, let &lt;math&gt;C_R \subseteq \mathcal{X} \times \mathcal{Y}&lt;/math&gt; be the set of required instance-level concepts, and let &lt;math&gt;\#(B, c_i)&lt;/math&gt; denote the number of times the instance-level concept &lt;math&gt;c_i&lt;/math&gt; occurs in the bag &lt;math&gt;B&lt;/math&gt;. Then &lt;math&gt;c(B) = 1 \Leftrightarrow \#(B, c_i) \geq 1&lt;/math&gt; for all &lt;math&gt;c_i \in C_R&lt;/math&gt;. Note that, by taking &lt;math&gt;C_R&lt;/math&gt; to contain only one instance-level concept, the presence-based assumption reduces to the standard assumption.

A further generalization comes with the threshold-based assumption, where each required instance-level concept must occur not only once in a bag, but some minimum (threshold) number of times in order for the bag to be labeled positive. With the notation above, to each required instance-level concept &lt;math&gt;c_i \in C_R&lt;/math&gt; is associated a threshold &lt;math&gt;l_i \in \mathbb{N}&lt;/math&gt;. For a bag &lt;math&gt;B&lt;/math&gt;, &lt;math&gt;c(B) = 1 \Leftrightarrow \#(B, c_i) \geq l_i&lt;/math&gt; for all &lt;math&gt;c_i \in C_R&lt;/math&gt;.

The count-based assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag. Each required instance-level concept &lt;math&gt;c_i \in C_R&lt;/math&gt; has a lower threshold &lt;math&gt;l_i \in \mathbb{N}&lt;/math&gt; and upper threshold &lt;math&gt;u_i \in \mathbb{N}&lt;/math&gt; with &lt;math&gt;l_i \leq u_i&lt;/math&gt;. A bag &lt;math&gt;B&lt;/math&gt; is labeled according to &lt;math&gt;c(B) = 1 \Leftrightarrow l_i \leq \#(B, c_i) \leq u_i&lt;/math&gt; for all &lt;math&gt;c_i \in C_R&lt;/math&gt;.

=== GMIL assumption===
Scott, Zhang, and Brown (2005) &lt;ref name = GMIL&gt;Scott, Stephen, Jun Zhang, and Joshua Brown. &quot;On generalized multiple-instance learning.&quot; International Journal of Computational Intelligence and Applications 5.01 (2005): 21-35.&lt;/ref&gt; describe another generalization of the standard model, which they call &quot;generalized multiple instance learning&quot; (GMIL). The GMIL assumption specifies a set of required instances &lt;math&gt;Q \subseteq \mathcal{X}&lt;/math&gt;. A bag &lt;math&gt;X&lt;/math&gt; is labeled positive if it contains instances which are sufficiently close to at least &lt;math&gt;r&lt;/math&gt; of the required instances &lt;math&gt;Q&lt;/math&gt;.&lt;ref name=GMIL /&gt; Under only this condition, the GMIL assumption is equivalent to the presence-based assumption.&lt;ref name = Review /&gt; However, Scott et. al. describe a further generalization in which there is a set of attraction points &lt;math&gt;Q \subseteq \mathcal{X}&lt;/math&gt; and a set of repulsion points &lt;math&gt;\overline{Q} \subseteq \mathcal{X}&lt;/math&gt;. A bag is labeled positive if and only if it contains instances which are sufficiently close to at least &lt;math&gt;r&lt;/math&gt; of the attraction points and are sufficiently close to at most &lt;math&gt;s&lt;/math&gt; of the repulsion points.&lt;ref name=GMIL /&gt; This condition is strictly more general than the presence-based, though it does not fall within the above hierarchy.

=== Collective assumption ===
In contrast to the previous assumptions where the bags were viewed as fixed, the collective assumption views a bag &lt;math&gt;B&lt;/math&gt; as a distribution &lt;math&gt;p(x|B)&lt;/math&gt; over instances &lt;math&gt;\mathcal{X}&lt;/math&gt;, and similarly view labels as a distribution &lt;math&gt;p(y|x)&lt;/math&gt; over instances. The goal of an algorithm operating under the collective assumption is then to model the distribution &lt;math&gt;p(y|B) = \int_\mathcal{X} p(y|x)p(x|B)dx&lt;/math&gt;.

Since &lt;math&gt;p(x|B)&lt;/math&gt; is typically considered fixed but unknown, algorithms instead focus on computing the empirical version: &lt;math&gt;\widehat{p}(y|B) = \frac{1}{n_B} \sum_{i=1}^{n_B} p(y|x_i)&lt;/math&gt;, where &lt;math&gt;n_B&lt;/math&gt; is the number of instances in bag &lt;math&gt;B&lt;/math&gt;. Since &lt;math&gt;p(y|x)&lt;/math&gt; is also typically taken to be fixed but unknown, most collective-assumption based methods focus on learning this distribution, as in the single-instance version.&lt;ref name = Review /&gt;&lt;ref name = Xu /&gt;

While the collective assumption weights every instance with equal importance, Foulds extended the collective assumption to incorporate instance weights. The weighted collective assumption is then that &lt;math&gt;\widehat{p}(y|B) = \frac{1}{w_B} \sum_{i=1}^{n_B} w(x_i) p(y|x_i)&lt;/math&gt;, where &lt;math&gt;w: \mathcal{X} \rightarrow \mathbb{R}^+&lt;/math&gt; is a weight function over instances and &lt;math&gt;w_B = \sum_{x \in B} w(x)&lt;/math&gt;.&lt;ref name = Review /&gt;

== Algorithms ==
[[File:Mildiag.jpg|thumbnail|left|MIL Framework]] There are two major flavors of algorithms for Multiple Instance Learning: instance-based and metadata-based, or embedding-based algorithms. The term &quot;instance-based&quot; denotes that the algorithm attempts to find a set of representative instances based on an MI assumption and classify future bags from these representatives. By contrast, metadata-based algorithms make no assumptions about the relationship between instances and bag labels, and instead try to extract instance-independent information (or metadata) about the bags in order to learn the concept.&lt;ref name = Xu /&gt; For a survey of some of the modern MI algorithms see Foulds and Frank &lt;ref name = Review /&gt;

=== Instance-based algorithms ===
The earliest proposed MI algorithms were a set of &quot;iterated-discrimination&quot; algorithms developed by Dietterich et. al, and Diverse Density developed by Maron and Lozano-Pérez.&lt;ref name = Dietterich /&gt;&lt;ref name = Perez /&gt; Both of these algorithms operated under the standard assumption.

====Iterated-discrimination====
Broadly, all of the iterated-discrimination algorithms consist of two phases. The first phase is to grow an [[Axis-aligned object|axis parallel rectangle]] (APR) which contains at least one instance from each positive bag and no instances from any negative bags. This is done iteratively: starting from a random instance &lt;math&gt;x_1 \in B_1&lt;/math&gt; in a positive bag, the APR is expanded to the smallest APR covering any instance &lt;math&gt;x_2&lt;/math&gt; in a new positive bag &lt;math&gt;B_2&lt;/math&gt;. This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance &lt;math&gt;x_i&lt;/math&gt; contained in the APR is given a &quot;relevance&quot;, corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives.

After the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows: a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability.&lt;ref name = Musk /&gt; Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions.&lt;ref name = Review /&gt;

====Diverse Density====
In its simplest form, Diverse Density (DD) assumes a single representative instance &lt;math&gt;t^*&lt;/math&gt; as the concept. This representative instance must be &quot;dense&quot; in that it is much closer to instances from positive bags than from negative bags, as well as &quot;diverse&quot; in that it is close to at least one instance from each positive bag.

Let &lt;math&gt;\mathcal{B}^+ = \{B_i^+\}_1^m&lt;/math&gt; be the set of positively labeled bags and let &lt;math&gt;\mathcal{B}^- = \{B_i^-\}_1^n&lt;/math&gt; be the set of negatively labeled bags, then the best candidate for the representative instance is given by &lt;math&gt;\hat{t} = \arg \max_t DD(t)&lt;/math&gt;, where the diverse density &lt;math&gt;DD(t) = Pr \left(t|\mathcal{B}^+, \mathcal{B}^- \right) = \arg \max_t \prod_{i=1}^m Pr \left(t|B_i^+\right) \prod_{i=1}^n Pr \left(t|B_i^-\right)&lt;/math&gt; under the assumption that bags are independently distributed given the concept &lt;math&gt;t^*&lt;/math&gt;. Letting &lt;math&gt;B_{ij}&lt;/math&gt; denote the jth instance of bag i, the noisy-or model gives:
: &lt;math&gt; Pr(t|B_i^+) = 1 - \prod_j \left( 1 -Pr\left(t|B_{ij}^+\right) \right)&lt;/math&gt;
: &lt;math&gt; Pr(t|B_i^-) = \prod_j \left( 1 - Pr\left(t|B_{ij}^-\right) \right) &lt;/math&gt;
&lt;math&gt;P(t|B_{ij})&lt;/math&gt; is taken to be the scaled distance &lt;math&gt;P(t|B_{ij}) \propto \exp \left( - \sum_{k} s_k^2 \left( x_k - (B_{ij})_k \right)^2 \right)&lt;/math&gt; where &lt;math&gt;s = (s_k)&lt;/math&gt; is the scaling vector. This way, if every positive bag has an instance close to &lt;math&gt;t&lt;/math&gt;, then &lt;math&gt;Pr(t|B_i^+)&lt;/math&gt; will be high for each &lt;math&gt;i&lt;/math&gt;, but if any negative bag &lt;math&gt;B_i^-&lt;/math&gt; has an instance close to &lt;math&gt;t&lt;/math&gt;, &lt;math&gt;Pr(t|B_i^-)&lt;/math&gt; will be low. Hence, &lt;math&gt;DD(t)&lt;/math&gt; is high only if every positive bag has an instance close to &lt;math&gt;t&lt;/math&gt; and no negative bags have an instance close to &lt;math&gt;t&lt;/math&gt;. The candidate concept &lt;math&gt;\hat{t}&lt;/math&gt; can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to &lt;math&gt;\hat{t}&lt;/math&gt;.&lt;ref name=&quot;Perez&quot;/&gt; Though Diverse Density was originally proposed by Maron et. al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 &lt;ref&gt;Zhang, Qi, and Sally A. Goldman. &quot;EM-DD: An improved multiple-instance learning technique.&quot; Advances in neural information processing systems. (2001): 1073 - 80&lt;/ref&gt; and DD-SVM in 2004,&lt;ref&gt;Chen, Yixin, and James Z. Wang. &quot;Image categorization by learning and reasoning with regions.&quot; The Journal of Machine Learning Research 5 (2004): 913-939&lt;/ref&gt; and MILES in 2006 &lt;ref name = Review /&gt;

A number of single-instance algorithms have also been adapted to a multiple-instance context under the standard assumption, including
* [[Support vector machines]]&lt;ref&gt;Andrews, Stuart, Ioannis Tsochantaridis, and Thomas Hofmann. &quot;Support vector machines for multiple-instance learning.&quot; Advances in neural information processing systems (2003). pp 561 - 658&lt;/ref&gt;
* Artificial [[neural networks]]&lt;ref&gt;Zhou, Zhi-Hua, and Min-Ling Zhang. &quot;Neural networks for multi-instance learning.&quot; Proceedings of the International Conference on Intelligent Information Technology, Beijing, China. (2002). pp 455 - 459&lt;/ref&gt;
* [[Decision trees]]&lt;ref&gt;Blockeel, Hendrik, David Page, and Ashwin Srinivasan. &quot;Multi-instance tree learning.&quot; Proceedings of the 22nd international conference on Machine learning. ACM, 2005. pp 57- 64&lt;/ref&gt;
* [[Boosting (machine learning)|Boosting]]&lt;ref&gt;Auer, Peter, and Ronald Ortner. &quot;A boosting approach to multiple instance learning.&quot; Machine Learning: ECML 2004. Springer Berlin Heidelberg, 2004. 63-74.&lt;/ref&gt;

Post 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above.&lt;ref name = Xu /&gt;

* Weidmann &lt;ref name = Weidmann /&gt; proposes a Two-Level Classification (TLC) algorithm to learn concepts under the count-based assumption. The first step tries to learn instance-level concepts by building a decision tree from each instance in each bag of the training set. Each bag is then mapped to a feature vector based on the counts in the decision tree. In the second step, a single-instance algorithm is run on the feature vectors to learn the concept
* Scott et. al &lt;ref name=GMIL /&gt; proposed an algorithm, GMIL-1, to learn concepts under the GMIL assumption in 2005. GMIL-1 enumerates all axis-parallel rectangles &lt;math&gt;\{R_i\}_{i \in I}&lt;/math&gt; in the original space of instances, and defines a new feature-space of Boolean vectors. A bag &lt;math&gt;B&lt;/math&gt; is mapped to a vector &lt;math&gt;\mathbf{b} = (b_i)_{i \in I}&lt;/math&gt; in this new feature space, where &lt;math&gt;b_i = 1&lt;/math&gt; if APR &lt;math&gt;R_i&lt;/math&gt; covers &lt;math&gt;B&lt;/math&gt;, and &lt;math&gt;b_i = 0&lt;/math&gt; otherwise. A single-instance algorithm can then be applied to learn the concept in this new feature space.

Because of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements.&lt;ref name = Review /&gt;

* Xu (2003) &lt;ref name = Xu /&gt; proposed several algorithms based on logistic regression and boosting methods to learn concepts under the collective assumption.

=== Metadata-based (or embedding-based) algorithms ===
By mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped (embedded) into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features or what type of embedding leads to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based.

* One approach is to let the metadata for each bag be some set of statistics over the instances in the bag. The SimpleMI algorithm takes this approach, where the metadata of a bag is taken to be a simple summary statistic, such as the average or minimum and maximum of each instance variable taken over all instances in the bag. There are other algorithms which use more complex statistics, but SimpleMI was shown to be surprisingly competitive for a number of datasets, despite its apparent lack of complexity.&lt;ref name = Review /&gt;
* Another common approach is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the instance space) between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel).&lt;ref name = Review /&gt; Similar approaches are taken by MILES &lt;ref&gt;{{Cite journal|last=Chen|first=Yixin|last2=Bi|first2=Jinbo|last3=Wang|first3=J. Z.|date=2006-12-01|title=MILES: Multiple-Instance Learning via Embedded Instance Selection|url=http://ieeexplore.ieee.org/document/1717454/?denied|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=28|issue=12|pages=1931–1947|doi=10.1109/TPAMI.2006.248|issn=0162-8828}}&lt;/ref&gt; and MInD.&lt;ref&gt;{{Cite journal|last=Cheplygina|first=Veronika|last2=Tax|first2=David M. J.|last3=Loog|first3=Marco|date=2015-01-01|title=Multiple instance learning with bag dissimilarities|url=http://www.sciencedirect.com/science/article/pii/S0031320314002817|journal=Pattern Recognition|volume=48|issue=1|pages=264–275|doi=10.1016/j.patcog.2014.07.022}}&lt;/ref&gt; MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags.
* A modification of k-nearest neighbors (kNN) can also be considered a metadata-based algorithm with geometric metadata, though the mapping between bags and metadata features is not explicit. However, it is necessary to specify the metric used to compute the distance between bags. Wang and Zucker (2000) &lt;ref&gt;Wang, Jun, and Jean-Daniel Zucker. “Solving multiple-instance problem: A lazy learning approach.” ICML (2000): 1119-25&lt;/ref&gt; suggest the (maximum and minimum, respectively) Hausdorff metrics for bags &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;:
: &lt;math&gt; H(A,B) = \max \left\{ \max_A \min_B \| a - b \|, \max_B \min_A \| a - b \| \right\} &lt;/math&gt;
: &lt;math&gt; h_1(A,B) = \min_A \min_B \| a - b \| &lt;/math&gt;
They define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting.

== Generalizations ==
So far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case.

* One such generalization is the multiple-instance multiple-label problem (MIML), where each bag can now be associated with any subset of the space of labels. Formally, if &lt;math&gt;\mathcal{X}&lt;/math&gt; is the space of features and &lt;math&gt;\mathcal{Y}&lt;/math&gt; is the space of labels, an MIML concept is a map &lt;math&gt;c: \mathbb{N}^\mathcal{X} \rightarrow 2^\mathcal{Y}&lt;/math&gt;. Zhou and Zhang (2006) &lt;ref&gt;Zhou, Zhi-Hua, and Min-Ling Zhang. &quot;Multi-instance multi-label learning with application to scene classification.&quot; Advances in Neural Information Processing Systems. 2006. pp 1609 - 16&lt;/ref&gt; propose a solution to the MIML problem via a reduction to either a multiple-instance or multiple-concept problem.
* Another obvious generalization is to multiple-instance regression. Here, each bag is associated with a single real number as in standard regression. Much like the standard assumption, MI regression assumes there is one instance in each bag, called the &quot;prime instance&quot;, which determines the label for the bag (up to noise). The ideal goal of MI regression would be to find a hyperplane which minimizes the square loss of the prime instances in each bag, but the prime instances are hidden. In fact, Ray and Page (2001) &lt;ref&gt;Ray, Soumya, and David Page. “Multiple instance regression.” ICML. Vol. 1. 2001. pp 425 - 32&lt;/ref&gt; show that finding a best fit hyperplane which fits one instance from each bag is intractable if there are fewer than three instances per bag, and instead develop an algorithm for approximation. Many of the algorithms developed for MI classification may also provide good approximations to the MI regression problem.&lt;ref name = Review /&gt;

==See also==
*[[Supervised learning]]
*[[Multi-label classification]]

== References ==
{{reflist}}

== Multiple Instance Learning ==

</text>
      <sha1>g1nfpxnez6ctv4mfhkhtq3tt9qftwro</sha1>
    </revision>
  </page>
  <page>
    <title>Learnable function class</title>
    <ns>0</ns>
    <id>48827727</id>
    <revision>
      <id>802178390</id>
      <parentid>744939739</parentid>
      <timestamp>2017-09-24T13:51:34Z</timestamp>
      <contributor>
        <username>Onel5969</username>
        <id>10951369</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Duality]] (link changed to [[Duality (mathematics)]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9357">{{Orphan|date=July 2016}}

In [[statistical learning theory]], a '''learnable function class''' is a [[Set (mathematics)|set]] of [[Function (mathematics)|functions]] for which an algorithm can be devised to asymptotically minimize the [[expected risk]], uniformly over all probability distributions. The concept of learnable classes are closely related to [[Regularization (mathematics)|regularization]] in [[machine learning]], and provides large sample justifications for certain learning algorithms.

== Definition ==

=== Background ===
{{See also|Statistical learning theory}}
Let &lt;math&gt;\Omega = \mathcal{X} \times \mathcal{Y} = \{(x, y)\}&lt;/math&gt; be the sample space, where &lt;math&gt;y&lt;/math&gt; are the labels and &lt;math&gt;x&lt;/math&gt; are the covariates (predictors). &lt;math&gt;\mathcal{F} = \{ f: \mathcal{X} \mapsto \mathcal{Y} \}&lt;/math&gt; is a collection of mappings (functions) under consideration to link &lt;math&gt;x&lt;/math&gt; to &lt;math&gt;y&lt;/math&gt;. &lt;math&gt;L: \mathcal{Y} \times \mathcal{Y} \mapsto \mathbb{R}&lt;/math&gt; is a pre-given loss function (usually non-negative). Given a probability distribution &lt;math&gt;P(x, y)&lt;/math&gt; on &lt;math&gt;\Omega&lt;/math&gt;, define the expected risk &lt;math&gt;I_P( f )&lt;/math&gt; to be:
:&lt;math&gt;I_P (f) = \int L( f(x), y ) d P( x, y )&lt;/math&gt;
The general goal in statistical learning is to find the function in &lt;math&gt;\mathcal{F}&lt;/math&gt; that minimizes the expected risk. That is, to find solutions to the following problem:&lt;ref name=&quot;Vapnik2013&quot;&gt;{{cite book|author=Vladimir N. Vapnik|title=The Nature of Statistical Learning Theory|url=https://books.google.com/books?id=EoDSBwAAQBAJ|date=17 April 2013|publisher=Springer Science &amp; Business Media|isbn=978-1-4757-2440-0}}&lt;/ref&gt;
:&lt;math&gt; \hat{f} = \arg \min_{f \in \mathcal{F}} I_P (f) &lt;/math&gt;
But in practice the distribution &lt;math&gt;P&lt;/math&gt; is unknown, and any learning task can only be based on finite samples. Thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk, i.e., to find a sequence of functions &lt;math&gt;\{\hat{f}_n\}_{n=1}^\infty&lt;/math&gt; that satisfies
:&lt;math&gt;\lim_{n \rightarrow \infty} \mathbb{P}( I_P (\hat{f}_n) - \inf_{f \in \mathcal{F}}I_P( f ) &gt; \epsilon ) = 0&lt;/math&gt;
One usual algorithm to find such a sequence is through [[empirical risk minimization]].

=== Learnable function class ===
We can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions. That is:

{{NumBlk|:|&lt;math&gt; \lim_{n \rightarrow \infty} \sup_P \mathbb{P}( I_P (\hat{f}_n) - \inf_{f \in \mathcal{F}}I_P( f ) &gt; \epsilon ) = 0&lt;/math&gt; |{{EquationRef|1}}}}

The intuition behind the more strict requirement is as such: the rate at which sequence &lt;math&gt;\{\hat{f}_n\}&lt;/math&gt; converges to the minimizer of the expected risk can be very different for different &lt;math&gt;P(x, y)&lt;/math&gt;. Because in real world the true distribution &lt;math&gt;P&lt;/math&gt; is always unknown, we would want to select a sequence that performs well under all cases.

However, by the [[no free lunch theorem]], such a sequence that satisfies ({{EquationNote|1}}) does not exist if &lt;math&gt;\mathcal{F}&lt;/math&gt; is too complex. This means we need to be careful and not allow too &quot;many&quot; functions in &lt;math&gt;\mathcal{F}&lt;/math&gt; if we want ({{EquationNote|1}}) to be a meaningful requirement. Specifically, function classes that ensure the existence of a sequence &lt;math&gt;\{\hat{f}_n\}&lt;/math&gt; that satisfies ({{EquationNote|1}}) are known as '''learnable classes'''.&lt;ref name=&quot;Vapnik2013&quot; /&gt;

It is worth noting that at least for supervised classification and regression problems, if a function class is learnable, then the empirical risk minimization automatically satisfies ({{EquationNote|1}}).&lt;ref name=&quot;:0&quot;&gt;{{cite journal |title = Learnability, stability and uniform convergence|journal = The Journal of Machine Learning Research|accessdate = 2015-12-17}}&lt;/ref&gt; Thus in these settings not only do we know that the problem posed by ({{EquationNote|1}}) is solvable, we also immediately have an algorithm that gives the solution.

== Interpretations ==
If the true relationship between &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;x&lt;/math&gt; is &lt;math&gt;y \sim f^*(x)&lt;/math&gt;, then by selecting the appropriate loss function, &lt;math&gt;f^*&lt;/math&gt; can always be expressed as the minimizer of the expected loss across all possible functions. That is,

:&lt;math&gt;f^* = \arg\min_{f \in \mathcal{F}^*} I_P( f )&lt;/math&gt;

Here we let &lt;math&gt;\mathcal{F}^*&lt;/math&gt; be the collection of all possible functions mapping &lt;math&gt;\mathcal{X}&lt;/math&gt; onto &lt;math&gt;\mathcal{Y}&lt;/math&gt;. &lt;math&gt;f^*&lt;/math&gt; can be interpreted as the actual data generating mechanism. However, the no free lunch theorem tells us that in practice, with finite samples we cannot hope to search for the expected risk minimizer over &lt;math&gt;\mathcal{F}^*&lt;/math&gt;. Thus we often consider a subset of &lt;math&gt;\mathcal{F}^*&lt;/math&gt;, &lt;math&gt;\mathcal{F}&lt;/math&gt;, to carry out searches on. By doing so, we risk that &lt;math&gt;f^*&lt;/math&gt; might not be an element of &lt;math&gt;\mathcal{F}&lt;/math&gt;. This tradeoff can be mathematically expressed as

{{NumBlk|:|&lt;math&gt; I_P (\hat{f}_n) - \inf_{f \in \mathcal{F}^*}I_P( f ) = \underbrace{I_P (\hat{f}_n) - \inf_{f \in \mathcal{F}}I_P( f )}_{(a)} + \underbrace{\inf_{f \in \mathcal{F}}I_P( f ) - \inf_{f \in \mathcal{F}^*}I_P( f )}_{(b)} &lt;/math&gt; |{{EquationRef|2}}}}

In the above decomposition, part &lt;math&gt;(b)&lt;/math&gt; does not depend on the data and is non-stochastic. It describes how far away our assumptions (&lt;math&gt;\mathcal{F}&lt;/math&gt;) are from the truth (&lt;math&gt;\mathcal{F}^*&lt;/math&gt;). &lt;math&gt;(b)&lt;/math&gt; will be strictly greater than 0 if we make assumptions that are too strong (&lt;math&gt;\mathcal{F}&lt;/math&gt; too small). On the other hand, failing to put enough restrictions on &lt;math&gt;\mathcal{F}&lt;/math&gt; will cause it to be not learnable, and part &lt;math&gt;(a)&lt;/math&gt; will not stochastically converge to 0. This is the well-known [[overfitting]] problem in statistics and machine learning literature.

== Example: Tikhonov regularization ==
A good example where learnable classes are used is the so-called [[Tikhonov regularization]] in [[reproducing kernel Hilbert space]] (RKHS). Specifically, let &lt;math&gt;\mathcal{F^*}&lt;/math&gt; be an RKHS, and &lt;math&gt;||\cdot||_2&lt;/math&gt; be the norm on &lt;math&gt;\mathcal{F^*}&lt;/math&gt; given by its inner product. It is shown in &lt;ref&gt;{{cite journal |title=Learnability in Hilbert spaces with reproducing kernels |journal=journal of complexity |accessdate=2015-12-17 }}&lt;/ref&gt; that &lt;math&gt;\mathcal{F} = \{f: ||f||_2 \leq \gamma  \}&lt;/math&gt; is a learnable class for any finite, positive &lt;math&gt;\gamma&lt;/math&gt;. The empirical minimization algorithm to the [[Duality (mathematics)|dual form]] of this problem is

:&lt;math&gt;\arg\min_{f \in \mathcal{F}^*} \left\{ \sum_{i = 1}^n L( f(x_i), y_i) + \lambda ||f||_2 \right\}&lt;/math&gt;

This was first introduced by Tikhonov&lt;ref name=&quot;TikhonovArsenin1977&quot;&gt;{{cite book|author1=Andreĭ Nikolaevich Tikhonov|author2=Vasiliĭ I︠A︡kovlevich Arsenin|title=Solutions of ill-posed problems|url=https://books.google.com/books?id=ECrvAAAAMAAJ|year=1977|publisher=Winston|isbn=978-0-470-99124-4}}&lt;/ref&gt; to solve ill-posed problems. Many statistical learning algorithms can be expressed in such a form (for example, the well-known [[ridge regression]]).

The tradeoff between &lt;math&gt;(a)&lt;/math&gt; and &lt;math&gt;(b)&lt;/math&gt; in ({{EquationNote|2}}) is geometrically more intuitive with Tikhonov regularization in RKHS. We can consider a sequence of &lt;math&gt;\{\mathcal{F}_\gamma\}&lt;/math&gt;, which are essentially balls in  &lt;math&gt;\mathcal{F^*}&lt;/math&gt; with centers at 0. As &lt;math&gt;\gamma&lt;/math&gt; gets larger, &lt;math&gt;\mathcal{F}_\gamma&lt;/math&gt; gets closer to the entire space, and &lt;math&gt;(b)&lt;/math&gt; is likely to become smaller. However we will also suffer smaller convergence rates in &lt;math&gt;(a)&lt;/math&gt;. The way to choose an optimal &lt;math&gt;\gamma&lt;/math&gt; in finite sample settings is usually through [[Cross-validation (statistics)|cross-validation]].

== Relationship to empirical process theory ==

Part &lt;math&gt;(a)&lt;/math&gt; in ({{EquationNote|2}}) is closely linked to [[empirical process]] theory in statistics, where the empirical risk &lt;math&gt;\{\sum_{i=1}^n L( y_i, f(x_i) ), f \in \mathcal{F}\}&lt;/math&gt; are known as empirical processes.&lt;ref name=&quot;vaartWellner2013&quot;&gt;{{cite book|author1=A.W. van der vaart|author2=Jon Wellner|title=Weak Convergence and Empirical Processes: With Applications to Statistics|url=https://books.google.com/books?id=zdDkBwAAQBAJ&amp;pg=PA116|date=9 March 2013|publisher=Springer Science &amp; Business Media|isbn=978-1-4757-2545-2|pages=116–}}&lt;/ref&gt; In this field, the function class &lt;math&gt;\mathcal{F}&lt;/math&gt; that satisfies the stochastic convergence

{{NumBlk|:|&lt;math&gt; \sup_P \mathbb{E} \sup_{f \in \mathcal{F}} | \sum_{i=1}^n L( y_i, f(x_i) ) - I_P(f) | = 0  &lt;/math&gt; |{{EquationRef|3}}}}

are known as uniform [[Glivenko–Cantelli class]]es. It has been shown that under certain regularity conditions, learnable classes and uniformly Glivenko-Cantelli classes are equivalent.&lt;ref name=&quot;Vapnik2013&quot; /&gt; Interplay between &lt;math&gt;(a)&lt;/math&gt; and &lt;math&gt;(b)&lt;/math&gt; in statistics literature is often known as the [[bias-variance tradeoff]].

However, note that in &lt;ref name=&quot;:0&quot; /&gt; the authors gave an example of [[stochastic convex optimization]] for [[General Setting of Learning]] where learnability is not equivalent with uniform convergence.

== References ==
&lt;references /&gt;

</text>
      <sha1>pb3of4jwr8xl2zgydk0mbazpozevgdt</sha1>
    </revision>
  </page>
  <page>
    <title>Isotropic position</title>
    <ns>0</ns>
    <id>48987892</id>
    <revision>
      <id>808763301</id>
      <parentid>732559643</parentid>
      <timestamp>2017-11-05T00:28:08Z</timestamp>
      <contributor>
        <ip>70.79.128.103</ip>
      </contributor>
      <comment>corrected definition of convex body in isotropic position</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1699">In the fields of [[machine learning]], the [[theory of computation]], and [[random matrix theory]], a probability distribution over vectors is said to be in '''isotropic position''' if its [[covariance matrix]] is equal to the [[identity matrix]].

== Formal definitions ==
Let &lt;math display=&quot;inline&quot;&gt;D&lt;/math&gt; be a distribution over vectors in the vector space &lt;math display=&quot;inline&quot;&gt;\mathbb{R}^n&lt;/math&gt;.
Then &lt;math display=&quot;inline&quot;&gt;D&lt;/math&gt; is in isotropic position if, for vector &lt;math display=&quot;inline&quot;&gt;v&lt;/math&gt; sampled from the distribution,
:&lt;math&gt;\mathbb{E}\, vv^T = \mathrm{Id}.&lt;/math&gt;

A ''set'' of vectors is said to be in isotropic position if the [[uniform distribution (continuous)|uniform distribution]] over that set is in isotropic position. In particular, every [[orthonormal]] set of vectors is isotropic.

As a related definition, a [[convex body]] &lt;math display=&quot;inline&quot;&gt;K&lt;/math&gt; in &lt;math display=&quot;inline&quot;&gt;\mathbb{R}^n&lt;/math&gt; is called isotropic if it has volume &lt;math display=&quot;inline&quot;&gt;|K|=1&lt;/math&gt;, center of mass at the origin, and there is a constant &lt;math display=&quot;inline&quot;&gt;\alpha&gt;0&lt;/math&gt; such that

:&lt;math&gt;\int_K \langle x, y \rangle^2 dx = \alpha^2 |y|^2,&lt;/math&gt;

for all vectors &lt;math display=&quot;inline&quot;&gt;y&lt;/math&gt; in &lt;math display=&quot;inline&quot;&gt;\mathbb{R}^n&lt;/math&gt;; here &lt;math display=&quot;inline&quot;&gt;|\cdot|&lt;/math&gt; stands
for the standard Euclidean norm.


== See also ==

* [[Whitening transformation]]

== References ==
* {{cite journal |first=M. |last=Rudelson |title=Random Vectors in the Isotropic Position |journal=[[Journal of Functional Analysis]] |volume=164 |year=1999 |issue=1 |pages=60–72 |eprint=math/9608208 }}


</text>
      <sha1>duqf95lbblnf0gvkpz8dnkzelksogtv</sha1>
    </revision>
  </page>
  <page>
    <title>Structured sparsity regularization</title>
    <ns>0</ns>
    <id>48844125</id>
    <revision>
      <id>808160442</id>
      <parentid>786843927</parentid>
      <timestamp>2017-11-01T06:59:16Z</timestamp>
      <contributor>
        <username>Eumolpo</username>
        <id>7953109</id>
      </contributor>
      <comment>orthographic</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24491">{{Orphan|date=December 2015}}

'''Structured sparsity regularization''' is a class of methods, and an area of research in [[statistical learning theory]], that extend and generalize sparsity regularization learning methods.&lt;ref name=&quot;rosPoggio&quot;&gt;{{cite journal|last = Rosasco|first = Lorenzo|author2 = Poggio, Tomasso|title = A Regularization Tour of Machine Learning — MIT-9.520 Lectures Notes|journal = Manuscript, Dec. 2014}}&lt;/ref&gt; Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable &lt;math&gt; Y &lt;/math&gt; (i.e., response, or [[Dependent and independent variables#Dependent variable|dependent variable]]) to be learned can be described by a reduced number of variables in the input space &lt;math&gt; X &lt;/math&gt; (i.e., the [[Domain of a function|domain]], space of [[Feature (machine learning)|features]] or [[Dependent and independent variables#Independent variable|explanatory variables]]). ''Sparsity regularization methods'' focus on selecting the input variables that best describe the output. ''Structured sparsity regularization methods'' generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in &lt;math&gt; X &lt;/math&gt;.&lt;ref name=&quot;groupLasso&quot; /&gt;&lt;ref name=&quot;latentLasso&quot; /&gt;

Common motivation for the use of structured sparsity methods are model interpretability, [[Curse of dimensionality|high-dimensional learning]] (where dimensionality of &lt;math&gt; X &lt;/math&gt; may be higher than the number of observations &lt;math&gt; n &lt;/math&gt;), and reduction of [[Time complexity|computational complexity]].&lt;ref name=&quot;LR18&quot; /&gt; Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups,&lt;ref name=&quot;groupLasso&quot; /&gt; non-overlapping groups, and acyclic graphs.&lt;ref name=&quot;latentLasso&quot; /&gt; Examples of uses of structured sparsity methods include face recognition,&lt;ref name=&quot;face_recognition&quot;&gt;{{cite journal|last = Jia|first = Kui|title = Robust and Practical Face Recognition via Structured Sparsity|year = 2012|display-authors=etal}}&lt;/ref&gt; [[Magnetic resonance imaging|magnetic resonance image (MRI)]] processing,&lt;ref name=&quot;MRI&quot;&gt;{{cite journal|last = Chen|first = Chen|title = Compressive Sensing MRI with Wavelet Tree Sparsity|journal = Proc. of the 26th Annual Conference on Neural Information Processing Systems (NIPS)|year = 2012|display-authors=etal}}&lt;/ref&gt; socio-linguistic analysis in natural language processing,&lt;ref name=&quot;sociolinguistic&quot;&gt;{{cite journal|last = Eisenstein|first = Jacob|title = Discovering Sociolinguistic Associations with Structured Sparsity|journal = Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics|year = 2011|display-authors=etal}}&lt;/ref&gt; and analysis of genetic expression in breast cancer.&lt;ref name=&quot;genetic&quot;&gt;{{cite journal|last = Jacob|first = Laurent|title = Group Lasso with Overlap and Graph Lasso|journal = Proceedings of the 26th International Conference on Machine Learning|year = 2009|display-authors=etal}}&lt;/ref&gt;

== Definition and related concepts ==

=== Sparsity regularization ===
Consider the linear kernel [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with a loss function &lt;math&gt; V(y_i, f(x)) &lt;/math&gt;  and the &lt;math&gt;\ell_0&lt;/math&gt; &quot;norm&quot; as the regularization penalty:
: &lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n V(y_i, \langle w,x_i\rangle)  + \lambda \|w\|_0, &lt;/math&gt;
where &lt;math&gt; x, w \in \mathbb{R^d} &lt;/math&gt;, and &lt;math&gt;\|w\|_0&lt;/math&gt; denotes the &lt;math&gt;\ell_0&lt;/math&gt; &quot;norm&quot;, defined as the number of nonzero entries of the vector &lt;math&gt;w&lt;/math&gt;. &lt;math&gt;f(x) = \langle w,x_i\rangle&lt;/math&gt;  is said to be '''sparse if'''  &lt;math&gt;\|w\|_0 = s &lt; d&lt;/math&gt;. Which means that the output &lt;math&gt;Y&lt;/math&gt; can be described by a small subset of input variables.

More generally, assume a dictionary &lt;math&gt; \phi_j : X \rightarrow \mathbb{R} &lt;/math&gt; with &lt;math&gt;j = 1,...,p &lt;/math&gt;  is given, such that the target function &lt;math&gt;f(x)&lt;/math&gt; of a learning problem can be written as:
: &lt;math&gt;f(x) = \sum_{j=1}^p \phi_j(x) w_j&lt;/math&gt;, &lt;math&gt; \forall x \in X &lt;/math&gt;
The &lt;math&gt;\ell_0&lt;/math&gt; norm &lt;math&gt;\|f\|_0 = \|w\|_0&lt;/math&gt;  as the number of non-zero components of &lt;math&gt;w&lt;/math&gt; is defined as
: &lt;math&gt;\|w\|_0 = | \{ j | w_j \neq 0, j \in\{ 1,...,p \}\} |&lt;/math&gt;, where &lt;math&gt;|A|&lt;/math&gt; is the cardinality of set &lt;math&gt;A&lt;/math&gt;.
&lt;math&gt;f&lt;/math&gt; is said to be sparse if &lt;math&gt;\|f\|_0 = \|w\|_0 = s &lt; d&lt;/math&gt;.

However, while using the &lt;math&gt;\ell_0&lt;/math&gt; norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the &lt;math&gt;\ell_1&lt;/math&gt; norm; this has been shown to still favor sparser solutions and is additionally convex.&lt;ref name=&quot;LR18&quot; /&gt;

=== Structured sparsity regularization ===
Structured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization.&lt;ref name=&quot;groupLasso&quot;&gt;{{cite journal|last = Yuan|first = M.|author2 = Lin, Y.|title = Model selection and estimation in regression with grouped variables|journal = J. R. Stat. Soc. B|year = 2006|volume = 68|issue = 1|pages = 49–67|doi = 10.1111/j.1467-9868.2005.00532.x}}&lt;/ref&gt;&lt;ref name=&quot;latentLasso&quot; /&gt; Consider the above [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with a general kernel and associated feature map &lt;math&gt; \phi_j : X \rightarrow \mathbb{R} &lt;/math&gt; with &lt;math&gt;j = 1,...,p &lt;/math&gt;.
: &lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n V(y_i, \langle w,\Phi(x_i)\rangle)  + \lambda \|w\|_0, &lt;/math&gt;
The regularization term &lt;math&gt;\lambda \|w\|_0 &lt;/math&gt; penalizes each &lt;math&gt;w_j&lt;/math&gt; component independently, which means that the algorithm will suppress input variables independently from each other.

In several situations we may want to impose more structure in the regularization process, so that, for example, input variables are suppressed according to predefined groups. '''Structured sparsity regularization methods''' allow to impose such structure by adding structure to the norms defining the regularization term.

== Structures and norms ==

=== Non-overlapping groups: group Lasso ===
The non-overlapping group case is the most basic instance of structured sparsity. In it, an ''a priori'' partition of the coefficient vector &lt;math&gt;w&lt;/math&gt; in &lt;math&gt;G&lt;/math&gt; non-overlapping groups is assumed. Let &lt;math&gt;w_g&lt;/math&gt; be the vector of coefficients in group &lt;math&gt;g&lt;/math&gt;, we can define a regularization term and its group norm as
: &lt;math&gt;\lambda R(w)=\lambda\sum _{{g=1}}^{G}\|w_{g}\|_{g} &lt;/math&gt;,
where &lt;math&gt; \|w_{g}\|_{g}&lt;/math&gt; is the group &lt;math&gt;\ell_2&lt;/math&gt; norm &lt;math&gt; \|w_{g}\|_{g}= \sqrt{ \sum _{{j=1}}^{{|G_{g}|}}(w_{g}^{j})^{2}} &lt;/math&gt; ,    &lt;math&gt;G_g&lt;/math&gt; is group &lt;math&gt;g&lt;/math&gt;, and &lt;math&gt;w_{g}^{j}&lt;/math&gt; is the ''j-th'' component of group &lt;math&gt;G_g&lt;/math&gt;.

The above norm is also referred to as '''group Lasso'''.&lt;ref name=&quot;groupLasso&quot; /&gt; This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients.

=== Overlapping groups ===
Overlapping groups is the structure sparsity case where a variable can belong to more than one group &lt;math&gt;g&lt;/math&gt;. This case is often of interest as it can represent a more general class of  relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.&lt;ref name=&quot;latentLasso&quot;&gt;{{cite arxiv |last1 = Obozinski|first1 = G.|last2 = Laurent | first2= J. | last3= Vert | first3= J.-P.|title = Group lasso with overlaps: the latent group lasso approach |year = 2011 |arxiv = 1110.0413|class = stat.ML}}&lt;/ref&gt;&lt;ref name=&quot;genetic&quot; /&gt;

There are two types of overlapping group sparsity regularization approaches, which are used to model different types of input variable relationships:

==== Intersection of complements: group Lasso  ====
The ''intersection of complements'' approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to.  Consider again the '''group Lasso''' for a [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem:
: &lt;math&gt;\lambda R(w)=\lambda\sum _{{g=1}}^{G}\|w_{g}\|_{g} &lt;/math&gt;,
where &lt;math&gt; \|w_{g}\|_{g}&lt;/math&gt; is the group &lt;math&gt;\ell_2&lt;/math&gt; norm,    &lt;math&gt;G_g&lt;/math&gt; is group &lt;math&gt;g&lt;/math&gt;, and &lt;math&gt;w_{g}^{j}&lt;/math&gt; is the ''j-th'' component of group &lt;math&gt;G_g&lt;/math&gt;.

As in the non-overlapping groups case, the ''group Lasso'' regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients &lt;math&gt;w_j &gt; 0&lt;/math&gt;. However, as in this case groups may overlap, we take the '''intersection of the complements''' of those groups that are not set to zero.

This ''intersection of complements'' selection criteria implies the modeling choice that we allow some coefficients within a particular group &lt;math&gt;g&lt;/math&gt; to be set to zero, while others within the same group &lt;math&gt;g&lt;/math&gt; may remain positive. In other words, coefficients within a group may differ depending on the several group memberships that each variable within the group may have.

==== Union of groups: latent group Lasso  ====
A different approach is to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.

The formulation of the union of groups approach is also referred to as '''latent group Lasso''', and requires to modify the group &lt;math&gt;\ell_2&lt;/math&gt; norm considered above and introduce the following regularizer &lt;ref name=&quot;latentLasso&quot; /&gt;
: &lt;math&gt;R(w)=inf\left\{\sum _{g}\|w_{{g}}\|_{{g}}:w=\sum _{{g=1}}^{G}{\bar  {w}}_{g}\right\}&lt;/math&gt;
where &lt;math&gt;w\in {\mathbb  {R^{d}}}&lt;/math&gt;,  &lt;math&gt;w_{{g}}\in G_{g}&lt;/math&gt; is the vector of coefficients of group g, and &lt;math&gt;{\bar  {w}}_{g}\in {\mathbb  {R^{d}}}&lt;/math&gt; is a vector with coefficients &lt;math&gt;w_{g}^{j}&lt;/math&gt; for all variables  &lt;math&gt;j&lt;/math&gt;  in group  &lt;math&gt;g&lt;/math&gt; , and  &lt;math&gt;0&lt;/math&gt;  in all others, i.e., &lt;math&gt;{\bar  w}_{g}^{j}=w_{g}^{j}&lt;/math&gt; if  &lt;math&gt;j&lt;/math&gt;  in group  &lt;math&gt;g&lt;/math&gt;  and &lt;math&gt;{\bar  w}_{g}^{j}=0&lt;/math&gt; otherwise.

This regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring &lt;math&gt;w=\sum _{{g=1}}^{G}{\bar  {w}}_{g}&lt;/math&gt; produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to.

=== Issues with Group Lasso regularization and alternative approaches ===
The objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group &lt;math&gt;\ell_1&lt;/math&gt; regularization term.  An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions.&lt;ref name=&quot;:0&quot; /&gt;

An example of a way to fix this is to introduce the squared &lt;math&gt;\ell_2&lt;/math&gt; norm of the weight vector as an additional regularization term while keeping the &lt;math&gt;\ell_1&lt;/math&gt; regularization term from the group lasso approach.&lt;ref name=&quot;:0&quot; /&gt; If the coefficient of the squared  &lt;math&gt;\ell_2&lt;/math&gt; norm term is greater than &lt;math&gt;0&lt;/math&gt;, then because the squared  &lt;math&gt;\ell_2&lt;/math&gt; norm term is strongly convex, the resulting objective function will also be strongly convex.&lt;ref name=&quot;:0&quot; /&gt; Provided that the  &lt;math&gt;\ell_2&lt;/math&gt; coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group  &lt;math&gt;\ell_2&lt;/math&gt; regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach.&lt;ref name=&quot;:0&quot; /&gt; Thus this approach allows for simpler optimization while maintaining sparsity.&lt;ref name=&quot;:0&quot; /&gt;

=== Norms based on the structure over Input variables ===
''See: [[Submodular set function]]''

Besides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a [[directed acyclic graph]] over the variables while in the context of grid-based norms, the structure can be represented using a grid.&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;:3&quot; /&gt;&lt;ref name=&quot;:4&quot; /&gt;&lt;ref name=&quot;:1&quot; /&gt;&lt;ref name=&quot;:5&quot; /&gt;&lt;ref name=&quot;:6&quot; /&gt;

==== Hierarchical Norms ====
''See:'' [[Unsupervised learning]]

Unsupervised learning methods are often used to learn the parameters of [[latent variable model]]s. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, &quot;hierarchies&quot; are assumed between the variables of the system; this system of hierarchies can be represented using directed acyclic graphs.

Hierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents.&lt;ref name=&quot;:3&quot;&gt;Bengio, Y. &quot;Learning deep architectures for AI&quot;. Foundations and Trends in Machine Learning, 2(1), 2009.&lt;/ref&gt;  Hierarchical models using Bayesian non-parametric methods have been used to learn [[topic model]]s,&lt;ref name=&quot;:2&quot;&gt;Blei, D., Ng, A., and Jordan, M. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, 2003.&lt;/ref&gt; which are statistical models for discovering the abstract &quot;topics&quot; that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods.&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|arxiv=0904.3523|last1=Jenatton|first1=Rodolphe|title=Structured Variable Selection with Sparsity-Inducing Norms|journal=Journal of Machine Learning Research |volume=12|issue=2011|pages=2777–2824|last2=Audibert|first2=Jean-Yves|last3=Bach|first3=Francis|year=2009}}&lt;/ref&gt; Hierarchical norms have been applied to bioinformatics,&lt;ref name=&quot;:4&quot;&gt;S. Kim and E. Xing. Tree-guided group Lasso for multi-task regression with structured sparsity. In Proc. ICML, 2010.&lt;/ref&gt; computer vision and topic models.&lt;ref name=&quot;:5&quot;&gt;R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proc. ICML, 2010.&lt;/ref&gt;

==== Norms defined on grids ====
If the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes.&lt;ref name=&quot;:1&quot; /&gt; Such methods have applications in computer vision&lt;ref name=&quot;:6&quot;&gt;R. Jenatton, G. Obozinski, and F. Bach. Structured sparse principal component analysis. In ''Proc. AISTATS'', 2009.&lt;/ref&gt;

== Algorithms for computation ==

=== Best subset selection problem ===
The problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as:&lt;ref name=&quot;LR18&quot;&gt;L. Rosasco. Lecture 10 of the Lecture Notes for 9.520: Statistical Learning Theory and Applications. Massachusetts Institute of Technology, Fall 2014. Available at http://www.mit.edu/~9.520/fall14/slides/class18/class18_sparsity.pdf&lt;/ref&gt;
: &lt;math&gt;\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n V(y_i, w, x_i)  + \lambda \|w\|_0, &lt;/math&gt;
Where &lt;math&gt;\|w\|_0&lt;/math&gt; denotes the &lt;math&gt;\ell_0&lt;/math&gt; &quot;norm&quot;, defined as the number of nonzero entries of the vector &lt;math&gt;w&lt;/math&gt;.

Although this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables.&lt;ref name=&quot;LR18&quot; /&gt;

Two main approaches for solving the optimization problem are: 1) greedy methods, such as [[Stepwise regression|step-wise regression]] in statistics, or [[matching pursuit]] in [[signal processing]]; and 2) convex relaxation formulation approaches and [[Proximal gradient methods for learning|proximal gradient]] optimization methods.

=== Convex relaxation ===
A natural approximation for the best subset selection problem is the &lt;math&gt;\ell_1&lt;/math&gt; norm regularization:&lt;ref name=&quot;LR18&quot; /&gt;
: &lt;math&gt; \min_{w\in\mathbb{R}^d}  \frac{1}{n}\sum_{i=1}^n V(y_i, w, x_i) + \lambda \|w\|_1&lt;/math&gt;
Such as scheme is called [[basis pursuit]] or the [[Lasso (statistics)|Lasso]], which substitutes the &lt;math&gt;\ell_0&lt;/math&gt; &quot;norm&quot; for the convex, non-differentiable &lt;math&gt;\ell_1&lt;/math&gt; norm.

=== Proximal gradient methods ===
{{Main article|Proximal gradient methods for learning|l1=Proximal gradient methods}}

[[Proximal gradient methods for learning|Proximal gradient methods]], also called forward-backward splitting, are optimization methods useful for minimizing functions with a [[Convex function|convex]] and [[Differentiable function|differentiable]] component, and a convex potentially non-differentiable component.

As such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems&lt;ref name=&quot;:0&quot;&gt;{{cite arXiv|last = Villa|first = S.|author2 = Rosasco, L.|author3 = Mosci, S.|author4 = Verri, A.|title = Proximal methods for the latent group lasso penalty|year = 2012|eprint = 1209.0368|class = math.OC}}&lt;/ref&gt; of the following form:
: &lt;math&gt; \min_{w\in\mathbb{R}^d}  \frac{1}{n}\sum_{i=1}^n V(y_i, w, x_i) + R(w)&lt;/math&gt;
Where &lt;math&gt;V(y_i, w, x_i) &lt;/math&gt; is a convex and differentiable [[loss function]] like the [[Loss function#Quadratic loss function|quadratic loss]], and &lt;math&gt;R(w) &lt;/math&gt; is a convex potentially non-differentiable regularizer such as the &lt;math&gt;\ell_1&lt;/math&gt; norm.

== Connections to Other Areas of Machine Learning ==

=== Connection to Multiple Kernel Learning ===
{{main article|Multiple kernel learning}}

Structured Sparsity regularization can be applied in the context of [[multiple kernel learning]].&lt;ref name=&quot;:7&quot;&gt;{{Cite journal|url = |title = MIT 9.520 course notes Fall 2015, chapter 6|last = Rosasco|first = Lorenzo|date = Fall 2015|journal = |doi = |pmid = |access-date = |last2 = Poggio, Tomaso}}&lt;/ref&gt; Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.

In the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of which define hypothesis spaces. Finite dictionaries can be used to define specific kernels, as will be shown.&lt;ref name=&quot;:7&quot; /&gt; Assume for this example that rather than only one dictionary, several finite dictionaries are considered.

For simplicity, the case in which there are only two dictionaries &lt;math&gt;A = \{a_j: X \rightarrow \R, j=1,...,p\} &lt;/math&gt; and &lt;math&gt;B = \{b_t: X \rightarrow \R, t=1,...,q\} &lt;/math&gt; where &lt;math&gt;q&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt; are integers, will be considered. The atoms in &lt;math&gt;A&lt;/math&gt; as well as the atoms in &lt;math&gt;B&lt;/math&gt; are assumed to be linearly independent. Let &lt;math&gt;D = \{d_k: X \rightarrow \R, k=1,...,p+q\} = A \cup B &lt;/math&gt; be the union of the two dictionaries. Consider the linear space of functions &lt;math&gt;H &lt;/math&gt; given by linear combinations of the form

&lt;math&gt;f(x) = \sum_{i=1}^{p+q}{w^j d_j(x)} = \sum_{j=1}^{p}{w_A^j a_j(x)} + \sum_{t=1}^{q}{w_B^t b_t(x)}, x \in X &lt;/math&gt;

for some coefficient vectors &lt;math&gt;w_A \in \R^p, w_B \in \R^q &lt;/math&gt;, where &lt;math&gt;w=(w_A,w_B) &lt;/math&gt;. Assume the atoms in &lt;math&gt;D &lt;/math&gt; to still be linearly independent, or equivalently, that the map &lt;math&gt;w = (w_A, w_B) \mapsto f &lt;/math&gt; is one to one. The functions in the space &lt;math&gt;H &lt;/math&gt; can be seen as the sums of two components, one in the space &lt;math&gt;H_A &lt;/math&gt;, the linear combinations of atoms in  &lt;math&gt;A&lt;/math&gt; and one in &lt;math&gt;H_B &lt;/math&gt;, the linear combinations of the atoms in &lt;math&gt;B&lt;/math&gt;.

One choice of norm on this space is &lt;math&gt;||f|| = ||w_A|| + ||w_B|| &lt;/math&gt;. Note that we can now view &lt;math&gt;H &lt;/math&gt; as a function space in which  &lt;math&gt;H_A &lt;/math&gt;,  &lt;math&gt;H_B &lt;/math&gt; are subspaces. In view of the linear independence assumption, &lt;math&gt;H &lt;/math&gt; can be identified with &lt;math&gt;\R^{p+q} &lt;/math&gt; and &lt;math&gt;H_A, H_B &lt;/math&gt; with &lt;math&gt;\R^p, \R^q &lt;/math&gt; respectively. The norm mentioned above can be seen as the group norm in  &lt;math&gt;H &lt;/math&gt;associated to the subspaces  &lt;math&gt;H_A &lt;/math&gt;,  &lt;math&gt;H_B &lt;/math&gt;, providing a connection to structured sparsity regularization.

Here, &lt;math&gt;H_A &lt;/math&gt;,  &lt;math&gt;H_B &lt;/math&gt; and &lt;math&gt;H &lt;/math&gt; can be seen to be the reproducing kernel Hilbert spaces with corresponding feature maps &lt;math&gt;\Phi_A : X \rightarrow \R^p &lt;/math&gt;, given by &lt;math&gt;\Phi_A(x) = (a_1(x),...,a_p(x)) &lt;/math&gt;, &lt;math&gt;\Phi_B : X \rightarrow \R^q &lt;/math&gt;, given by &lt;math&gt;\Phi_B(x) = (b_1(x),...,b_q(x)) &lt;/math&gt;, and &lt;math&gt;\Phi: X \rightarrow \R^{p+q} &lt;/math&gt;, given by the concatenation of &lt;math&gt;\Phi_A, \Phi_B &lt;/math&gt;, respectively.

In the structured sparsity regularization approach to this scenario, the relevant groups of variables which the group norms consider correspond to the subspaces &lt;math&gt;H_A &lt;/math&gt; and &lt;math&gt;H_B &lt;/math&gt;. This approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients, promoting sparse multiple kernel learning.

The above reasoning directly generalizes to any finite number of dictionaries, or feature maps. It can be extended to feature maps inducing infinite dimensional hypothesis

spaces.&lt;ref name=&quot;:7&quot; /&gt;

==== When Sparse Multiple Kernel Learning is useful ====
Considering sparse multiple kernel learning is useful in several situations including the following:

• Data fusion: When each kernel corresponds to a different kind of modality/feature.

• Nonlinear variable selection: Consider kernels &lt;math&gt;K_g&lt;/math&gt; depending only one dimension of the input.

Generally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important.&lt;ref name=&quot;:7&quot; /&gt;

== Additional uses and applications ==
Structured sparsity regularization methods have been used in a number of settings where it is desired to impose an ''a priori'' input variable structure to the regularization process. Some such applications are:
* [[Compressed sensing|Compressive sensing]] in [[magnetic resonance imaging]] (MRI), reconstructing MR images from a small number of measurements, potentially yielding significant reductions in MR scanning time&lt;ref name=&quot;MRI&quot; /&gt;
* Robust [[Facial recognition system|face recognition]] in the presence of misalignment, occlusion and illumination variation&lt;ref name=&quot;face_recognition&quot; /&gt;
* Uncovering [[Sociolinguistics|socio-linguistic]] associations between lexical frequencies used by Twitter authors, and the socio-demographic variables of their geographic communities&lt;ref name=&quot;sociolinguistic&quot; /&gt;
* Gene selection analysis of breast cancer data using priors of overlapping groups, e.g., biologically meaningful gene sets&lt;ref name=&quot;genetic&quot; /&gt;

== See also ==
* [[Statistical learning theory]]
* [[Regularization (mathematics)#Regularization in statistics and machine learning|Regularization]]
* [[Sparse approximation]]
* [[Proximal gradient method]]s
* [[Convex analysis]]
* [[Feature selection]]

== References ==
{{reflist}}


[[Category:First order methods|First order methods]]
</text>
      <sha1>nntpf5q24zcrb45c8sjt43iwpd23bjf</sha1>
    </revision>
  </page>
  <page>
    <title>MysteryVibe</title>
    <ns>0</ns>
    <id>48976249</id>
    <revision>
      <id>805643946</id>
      <parentid>803959234</parentid>
      <timestamp>2017-10-16T18:39:04Z</timestamp>
      <contributor>
        <username>GoingBatty</username>
        <id>11555324</id>
      </contributor>
      <comment>removed ; added [[Category:British companies established in 2014]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9383">{{EngvarB|date=October 2017}}
{{Use dmy dates|date=October 2017}}
{{multiple issues|
{{COI|date=May 2017}}
{{advert|date=May 2017}}
{{peacock|date=May 2017}}
}}
{{Infobox dot-com company
|name = MysteryVibe
|logo =[[File:MysteryVibe_Logo.png|100px]]
|screenshot =
|caption =
|founded = London, United Kingdom (2014)
|location = London, United Kingdom
|founder = Soumyadip Rakshit&lt;br&gt;Stephanie Alys&lt;br&gt;Rob Weekly&lt;br&gt;Shanshan Xu
| key people = Akash Walia&lt;br&gt;Gabbi Cahane
|industry = Health and Lifestyle
|ipv6 =
|alexa = 168,892&lt;ref&gt;http://www.alexa.com/siteinfo/mysteryvibe.com#&lt;/ref&gt;
|advertising =
|registration =
|footnotes =
|company_type = Privately owned
|current_status = Active
|area_served = Worldwide
|company_slogan = &quot;Your Pleasure, Personalised.&quot;
|website_type =
|num_users =
|launch_date = 2014
|intl =
|website = [https://www.mysteryvibe.com mysteryvibe.com]
}}
'''MysteryVibe''' is a British manufacturer of sex toys.

==History==
MysteryVibe was founded by a group of researchers, engineers and designers.{{citation needed|date=May 2017}} Inspired by trends in [[smartphone]]s like [[Nokia Morph]], the founders came up with the idea of creating a sex toy that would adapt to any body shape and vibrate to any pattern.{{citation needed|date=May 2017}} They continued to research for a number of years before formally starting the company in May 2014, when they were incubated by London-based industrial design firm Seymourpowell.

MysteryVibe released their iOS app on the [[Apple Inc.|Apple]] [[App Store (iOS)|App Store]] in December 2015 and their Android app on [[Google Play]] in September 2016. The apps are designed without any adult themes to support MysteryVibe's wider goal of improving sex education for teenagers and are the only apps in their class to be rated 12+.{{citation needed|date=May 2017}} Their apps have been downloaded more than 250,000 times since launch.&lt;ref&gt;{{Cite web|title = App Annie|url = https://www.appannie.com/apps/ios/app/mysteryvibe/|website = App Annie|access-date = 21 February 2016}}&lt;/ref&gt;

MysteryVibe's flagship product, Crescendo,&lt;ref name=&quot;:9&quot;&gt;{{Cite web|url=http://www.fastcocreate.com/3045887/one-size-doesnt-fit-all-the-making-of-a-next-generation-sex-toy|title=One Size Doesn't Fit All: The Making Of A Next-Generation Sex Toy|website=Co.Create|access-date=13 March 2016}}&lt;/ref&gt; is the world's first vibrator that can be bent to adapt to any body shape.&lt;ref&gt;{{Cite news|url=http://www.huffingtonpost.co.uk/entry/women-work-female-orgasms-omgyes_uk_58c01037e4b0d1078ca2a77b|title=Meet The Women Who Want To Help Other Women Orgasm|date=30 March 2017|work=HuffPost|access-date=13 May 2017}}&lt;/ref&gt; Crescendo was the first crowdfunding project&lt;ref name=&quot;:10&quot;&gt;{{Cite web|url=https://www.born.com/view/631/crescendo-the-worlds-first-truly-personal-adult-toy|title=BORN.COM – Crescendo – The world's first truly personal adult toy by MysteryVibe|last=SA|first=BORN|publisher=born.com|access-date=13 March 2016}}&lt;/ref&gt; to offer its backers 2 versions of their product: ''Pilot'' and ''Retail''. They ran what they called the #''Pilot1000'' programme for their first 1,000 users to get feedback on their Crescendo product. The #''Pilot1000'' users spanned 48 countries and included both backers and experts. MysteryVibe gave all 1,000 users full access to their founding CEO with direct email, phone and [[Skype]]. They then used the feedback they received to make the final ''Retail'' Crescendo.&lt;ref name=&quot;:11&quot;&gt;{{Cite web|url=http://www.bustle.com/articles/140970-the-one-present-to-get-yourself-this-valentines-day|title=Bustle|publisher=bustle.com|access-date=13 March 2016}}&lt;/ref&gt;

== Investment ==
[[File:Soho Works.jpg|alt=MysteryVibe HQ|thumb|MysteryVibe is a member of the [[Soho House (club)|Soho House]] network with their headquarters based at Soho Works in London]]
[[File:Seymourpowelloffice.jpg|alt=Seymourpowell|thumb|MysteryVibe was incubated by the design firm Seymourpowell]]
Due to the [[Lean startup|lean]] model adopted by MysteryVibe with ''collectives'' and ''collaborations'',&lt;ref&gt;{{Cite web|url=http://www.nerve.com/love-sex8739/couple-design-worlds-best-sex-toy|title=Did This Couple Design the World's Best Sex Toy?|publisher=nerve.com|access-date=13 May 2017}}&lt;/ref&gt; they were able to build the company from a sketch to shipped products with less than £1m ($1.5m) in funding.&lt;ref&gt;{{Cite web|title = MysteryVibe {{!}} CrunchBase|url = https://www.crunchbase.com/organization/mysteryvibe#/entity|website = www.crunchbase.com|accessdate = 2 January 2016}}&lt;/ref&gt; They were also unique&lt;ref&gt;{{Cite web|url=https://venturebeat.com/2015/07/20/3-ways-to-fund-your-sex-startup/|title=3 ways to fund your sex startup|date=20 July 2015|website=VentureBeat|access-date=13 May 2017}}&lt;/ref&gt; in raising 100% of the money from [[Angel investor]]s&lt;ref&gt;{{Cite web|title = Mysteryvibe|url = https://angel.co/mysteryvibe|website = angel.co|accessdate = 2 January 2016}}&lt;/ref&gt; without any recourse to [[Venture capital]]. As of Q2 2017, MysteryVibe has raised $3.5m in total funding.&lt;ref&gt;{{Cite web|url=https://angel.co/mysteryvibe|title=Mysteryvibe|website=angel.co|access-date=13 May 2017}}&lt;/ref&gt;

[[File:Keynote talk at Slush 2016 by MysteryVibe co-founder, Stephanie Alys.png|alt=Slush conference, 2016|thumb|MysteryVibe's co-founder giving a talk at Slush conference in Helsinki, 2016]]

===  Awards ===
MysteryVibe has won numerous awards for their company, products and founders. Notable mentions are [[Red Dot]],&lt;ref&gt;{{Cite web|url=http://red-dot.de/pd/online-exhibition/?lang=en|title=Red Dot Product Design Award Winners|website=red-dot.de|archive-url=|archive-date=|dead-url=|access-date=13 May 2017}}&lt;/ref&gt; IDA Design,&lt;ref&gt;{{Cite web|url=https://idesignawards.com/winners/zoom2.php?eid=9-13923-17|title=Design Awards Winners: MysteryVibe|publisher=idesignawards.com|access-date=13 May 2017}}&lt;/ref&gt; The Drum,&lt;ref&gt;{{Cite news|url=http://www.thedrum.com/news/2017/02/13/meet-the-young-women-making-waves-digital-the-drums-50-under-30|title=Meet the young women making waves in digital in The Drum's 50 under 30|work=The Drum|access-date=13 May 2017}}&lt;/ref&gt; and Excellence in Design.&lt;ref&gt;{{Cite web|url=http://www.appliancedesign.com/Excellence-in-Design|title=Appliance Design Excellence in Design Awards|publisher=appliancedesign.com|archive-url=|archive-date=|dead-url=|access-date=13 May 2017}}&lt;/ref&gt; Their biggest recognition has been the Young Guns award.&lt;ref&gt;{{Cite news|url=http://startups.co.uk/young-guns/stephanie-alys-shanshan-xu-and-soumyadip-rakshit-mysteryvibe/|title=Stephanie Alys, Shanshan Xu and Soumyadip Rakshit: MysteryVibe – Young Guns by Startups.co.uk: Start up a successful business|date=16 September 2016|work=Young Guns by Startups.co.uk: Start up a successful business|access-date=13 May 2017}}&lt;/ref&gt;

=== Virgin #VOOM2016 ===
In June 2016, MysteryVibe became the first pleasure product to be featured by [[Virgin Group|Virgin]] in their #VOOM competition. They were showcased under the Export Awards category for exporting their products to over 50 countries worldwide. This led to their selection to the Hardware Club and a nomination&lt;ref name=&quot;:14&quot;&gt;{{Cite web|url=https://techcrunch.com/2016/05/30/vote-now-in-the-europas-conference-and-awards-for-european-startups/|title=Vote now in The Europas Conference &amp; Awards for European startups|last=Butcher|first=Mike|website=TechCrunch|access-date=15 June 2016}}&lt;/ref&gt; for the 'Best Hardware Startup' award at The Europas in London.

==Media attention==

MysteryVibe is the first brand in their category to have been featured on BBC.&lt;ref name=&quot;:12&quot;&gt;{{Cite web|url=http://www.bbc.co.uk/iplayer/episode/p04rg52j/levine-on-love-long-distance-lovers|title=Levine on Love, Long Distance Lovers|website=BBC iPlayer|access-date=10 February 2017}}&lt;/ref&gt; Ranked by European investors as No. 9 of the 100 Slush start-ups in ''Cofounder Magazine'',&lt;ref&gt;{{Cite web|url = https://www.instagram.com/p/98Tho5B5og/?taken-by=mysteryvibe|title = The Slush 100 Start-ups – as picked by Top European Investors|accessdate = 2 January 2016|website = Cofounder Magazine|last = Magazine|first = CoFounder}}&lt;/ref&gt; MysteryVibe has been named as one of the &quot;Top 100 Europe's hottest early-stage Founders&quot; by ''PathFounders,'' ''Europas''.&lt;ref&gt;{{Cite web|title = PathFounders|url = http://theeuropas.com/pathfounders/|website = The Europas {{!}} 14 June 2016 {{!}} London|accessdate = 2 January 2016}}&lt;/ref&gt; They have been listed at No. 7 in the &quot;12 days of start-ups: Spectacular businesses set for stardom in 2016&quot;.&lt;ref&gt;{{Cite web|title = 12 days of start-ups: Spectacular businesses set for stardom in 2016 – Startups.co.uk: Starting a business advice and business ideas|url = http://startups.co.uk/12-spectacular-start-ups-set-to-take-off-in-2016/|publisher = startups.co.uk|accessdate = 2 January 2016}}&lt;/ref&gt;

== References ==
{{reflist}}

[[Category:Manufacturing companies established in 2014]]
[[Category:British companies established in 2014]]
[[Category:Engineering companies of the United Kingdom]]




[[Category:21st-century fashion]]




[[Category:Manufacturing companies of the United Kingdom]]</text>
      <sha1>ao8v235002datzqfdmwchwl9q958ya3</sha1>
    </revision>
  </page>
  <page>
    <title>Validation set</title>
    <ns>0</ns>
    <id>39945557</id>
    <redirect title="Training, test, and validation sets" />
    <revision>
      <id>804584060</id>
      <parentid>804264967</parentid>
      <timestamp>2017-10-09T23:00:22Z</timestamp>
      <contributor>
        <username>Xqbot</username>
        <id>8066546</id>
      </contributor>
      <minor/>
      <comment>Bot: Fixing double redirect to [[Training, test, and validation sets]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="141">#REDIRECT [[Training, test, and validation sets#Validation set]] {{R to section}}


</text>
      <sha1>o2huz0jrzyfsuwi1firhw42rfb3ek5e</sha1>
    </revision>
  </page>
  <page>
    <title>Sparse dictionary learning</title>
    <ns>0</ns>
    <id>48813654</id>
    <revision>
      <id>816165883</id>
      <parentid>800660709</parentid>
      <timestamp>2017-12-19T18:25:19Z</timestamp>
      <contributor>
        <username>Moonlike106</username>
        <id>32567661</id>
      </contributor>
      <minor/>
      <comment>I have just added one of the applications of sparse dictionary learning.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22327">{{Machine learning bar}}

'''Sparse dictionary learning''' is a [[representation learning]] method which aims at finding a [[Sparse matrix|sparse]] representation of the input data (also known as ''sparse coding'') in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called ''atoms'' and they compose a ''dictionary''. Atoms in the dictionary are not required to be [[Orthogonal basis|orthogonal]], and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in [[sparsity]] and flexibility of the representation.

One of the most important applications of sparse dictionary learning is in the field of '''''[[compressed sensing]]''''' or '''''[[Detection theory|signal recovery]]'''''. In compressed sensing, a high dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the '''''[[wavelet transform]]''''' or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like '''''[[basis pursuit]]''''', CoSaMP&lt;ref&gt;{{Cite journal|last=Needell|first=D.|last2=Tropp|first2=J.A.|title=CoSaMP: Iterative signal recovery from incomplete and inaccurate samples|url=https://doi.org/10.1016/j.acha.2008.07.002|journal=Applied and Computational Harmonic Analysis|volume=26|issue=3|pages=301–321|doi=10.1016/j.acha.2008.07.002}}&lt;/ref&gt; or '''''fast non-iterative algorithms'''''&lt;ref&gt;Lotfi, M.; Vidyasagar, M.&quot;[[arxiv:1708.03608|A Fast Non-iterative Algorithm for Compressive Sensing Using Binary Measurement Matrices]]&quot;&lt;/ref&gt; can be used to recover the signal.

One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in [[signal processing]] one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as [[Fourier transform|fourier]] or [[Wavelet transform|wavelet]] transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image [[Noise reduction|denoising]] and [[Image classification|classification]], video and [[Audio signal processing|audio processing]]. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting.
[[File:Dic_learning.jpg|thumb|Image Denoising by Dictionary Learning]]

== Problem statement ==
Given the input dataset &lt;math&gt;X = [x_1, ..., x_K], x_i \in \mathbb{R}^d&lt;/math&gt; we wish to find a dictionary &lt;math&gt;\mathbf{D} \in \mathbb{R}^{d \times n}: D = [d_1, ..., d_n]&lt;/math&gt; and a representation &lt;math&gt;R = [r_1,...,r_K], r_i \in \mathbb{R}^n&lt;/math&gt; such that both &lt;math&gt;\|X-\mathbf{D}R\|^2_F&lt;/math&gt; is minimized and the representations &lt;math&gt;r_i&lt;/math&gt; are sparse enough. This can be formulated as the following [[optimization problem]]:

&lt;math&gt;\underset{\mathbf{D} \in \mathcal{C}, r_i \in \mathbb{R}^n}{\text{argmin}} \sum_{i=1}^K\|x_i-\mathbf{D}r_i\|_2^2+\lambda \|r_i\|_0&lt;/math&gt;, where &lt;math&gt;\mathcal{C} \equiv \{\mathbb{D} \in \mathbb{R}^{d \times n}: \|d_i\|_2 \leq 1 \,\, \forall i =1,...,n \}&lt;/math&gt;

&lt;math&gt;\mathcal{C}&lt;/math&gt; is required to constrain &lt;math&gt;\mathbf{D}&lt;/math&gt; so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of &lt;math&gt;r_i&lt;/math&gt;.

The minimization problem above is not convex because of the [[L0 norm|ℓ&lt;sub&gt;0&lt;/sub&gt;-&quot;norm&quot;]] and solving this problem is NP-hard.&lt;ref&gt;A. M. Tillmann, &quot;[[doi:10.1109/LSP.2014.2345761|On the Computational Intractability of Exact and Approximate Dictionary Learning]]&quot;, IEEE Signal Processing Letters 22(1), 2015: 45–49.&lt;/ref&gt; In some cases ''[[L1-norm|L]]''&lt;sup&gt;[[L1-norm|1]]&lt;/sup&gt;[[L1-norm|-norm]] is known to ensure sparsity&lt;ref&gt;{{Cite journal|title = For most large underdetermined systems of linear equations the minimal 𝓁1-norm solution is also the sparsest solution|url = http://onlinelibrary.wiley.com/doi/10.1002/cpa.20132/abstract|journal = Communications on Pure and Applied Mathematics|date = 2006-06-01|issn = 1097-0312|pages = 797–829|volume = 59|issue = 6|doi = 10.1002/cpa.20132|first = David L.|last = Donoho}}&lt;/ref&gt; and so the above becomes a [[convex optimization]] problem with respect to each of the variables &lt;math&gt;\mathbf{D}&lt;/math&gt; and &lt;math&gt;\mathbf{R}&lt;/math&gt; when the other one is fixed, but it is not jointly convex in &lt;math&gt;(\mathbf{D}, \mathbf{R})&lt;/math&gt;.


=== Properties of the dictionary ===
The dictionary &lt;math&gt;\mathbf{D}&lt;/math&gt; defined above can be &quot;undercomplete&quot; if &lt;math&gt;n &lt; d&lt;/math&gt; or &quot;overcomplete&quot; in case &lt;math&gt;n&gt;d&lt;/math&gt; with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.

Undercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to [[dimensionality reduction]] and techniques like [[principal component analysis]] which require atoms &lt;math&gt;d_1,...,d_n&lt;/math&gt; to be orthogonal.   The choice of these subspaces is crucial for efficient dimensionality reduction, but it is not trivial.  And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.

Overcomplete dictionaries, however, do not require the atoms to be orthogonal (they will never be a [[Basis (linear algebra)|basis]] anyway) thus allowing for more flexible dictionaries and richer data representations.

An overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix (wavelets transform, fourier transform) or it can be formulated so that its elements are changed in such a way that it sparsely represents given signal in a best way.  Learned dictionaries are capable to give more sparse solution as compared to predefined transform matrices.

== Algorithms ==
As the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.

The problem of finding an optimal sparse coding &lt;math&gt;R&lt;/math&gt; with a given dictionary &lt;math&gt;\mathbf{D}&lt;/math&gt; is known as [[sparse approximation]] (or sometimes just sparse coding problem). There has been developed a number of algorithms to solve it (such as [[matching pursuit]] and [[Lasso (statistics)|LASSO]]) which are incorporated into the algorithms described below.

=== Method of optimal directions (MOD) ===
The method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem.&lt;ref&gt;{{Cite journal|title = Method of optimal directions for frame design|url = http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=760624&amp;url=http%253A%252F%252Fieeexplore.ieee.org%252Fxpls%252Fabs_all.jsp%253Farnumber%253D760624|journal =  1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999. Proceedings|date = 1999-01-01|pages = 2443–2446 vol.5|volume = 5|doi = 10.1109/ICASSP.1999.760624|first = K.|last = Engan|first2 = S.O.|last2 = Aase|first3 = J.|last3 = Hakon Husoy}}&lt;/ref&gt; The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:

&lt;math&gt;\min_{\mathbf{D}, R}\{\|X-\mathbf{D}R\|^2_F\} \,\, \text{s.t.}\,\, \forall i \,\,\|r_i\|_0 \leq T &lt;/math&gt;

Here, &lt;math&gt;F&lt;/math&gt; denotes the [[Frobenius norm]]. MOD alternates between getting the [[Sparse approximation|sparse coding]] using a method such as [[matching pursuit]] and updating the dictionary by computing the analytical solution of the problem given by &lt;math&gt;\mathbf{D} = XR^+ &lt;/math&gt; where &lt;math&gt;R^+ &lt;/math&gt; is a [[Moore–Penrose pseudoinverse|Moore-Penrose pseudoinverse]]. After this update &lt;math&gt;\mathbf{D} &lt;/math&gt; is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence (or until a sufficiently small residue).

MOD has proved to be a very efficient method for low-dimensional input data &lt;math&gt;X &lt;/math&gt; requiring just a few iterations to converge. However, due to the high complexity of the matrix-inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.

=== K-SVD ===
{{main|K-SVD}}[[K-SVD]] is an algorithm that performs [[Singular value decomposition|SVD]] at its core to update the atoms of the dictionary one by one and basically is a generalization of [[K-means clustering|K-means]]. It enforces that each element of the input data &lt;math&gt;x_i&lt;/math&gt; is encoded by a linear combination of not more than &lt;math&gt;T_0 &lt;/math&gt; elements in a way identical to the MOD approach:

&lt;math&gt;\min_{\mathbf{D}, R}\{\|X-\mathbf{D}R\|^2_F\} \,\, \text{s.t.}\,\, \forall i \,\,\|r_i\|_0 \leq T_0 &lt;/math&gt;

This algorithm's essence is to first fix the dictionary, find the best possible &lt;math&gt;R &lt;/math&gt; under the above constraint (using [[Matching pursuit#Extensions|Orthogonal Matching Pursuit]]) and then iteratively update the atoms of dictionary &lt;math&gt;\mathbf{D}&lt;/math&gt; in the following manner:

&lt;math&gt;
\|X - \mathbf{D}R\|^2_F =  \left| X - \sum_{i = 1}^K d_i x^i_T\right|^2_F = \| E_k - d_k x^k_T\|^2_F
&lt;/math&gt;

The next steps of the algorithm include [[Low-rank approximation|rank-1 approximation]] of the residual matrix &lt;math&gt;
E_k
&lt;/math&gt;, updating &lt;math&gt;
d_k
&lt;/math&gt; and enforcing the sparsity of &lt;math&gt;
x_k
&lt;/math&gt; after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima.

=== Stochastic gradient descent ===
{{Main|Stochastic gradient descent}}One can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem.&lt;ref&gt;{{Cite journal|title = Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary|url = http://epubs.siam.org/doi/pdf/10.1137/07070156X|journal = SIAM Journal on Imaging Sciences|pages = 228–247|volume = 1|issue = 3|doi = 10.1137/07070156x|first = Michal|last = Aharon|first2 = Michael|last2 = Elad}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = Yair Censor and Stavros A. Zenios, Parallel Optimization — Theory, Algorithms, and Applications. Oxford University Press, New York/Oxford, 1997, xxviii+539 pages. (US $ 85.00) |isbn=0-19-510062-X |url = https://link.springer.com/article/10.1023/A%3A1008311628080|journal = Journal of Global Optimization|date = 2000-01-01|issn = 0925-5001|pages = 107–108|volume = 16|issue = 1|doi = 10.1023/A:1008311628080|first = János D.|last = Pintér}}&lt;/ref&gt; The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set &lt;math&gt;\mathcal{C}&lt;/math&gt;. The step that occurs at i-th iteration is described by this expression:

&lt;math&gt;\mathbf{D}_i = \text{proj}_{\mathcal{C}} \left\{\mathbf{D}_{i-1}-\delta_i\nabla_{\mathbf{D}}\sum_{i \in S}\|x_i-\mathbf{D}r_i\|_2^2+\lambda\|r_i\|_1 \right\}&lt;/math&gt;, where &lt;math&gt;S&lt;/math&gt; is a random subset of &lt;math&gt;\{1...K\}&lt;/math&gt; and &lt;math&gt;\delta_i&lt;/math&gt; is a gradient step.

=== Lagrange dual method ===
An algorithm based on solving a [[Duality (optimization)|dual Lagrangian problem]] provides an efficient way to solve for the dictionary having no complications induced by the sparsity function.&lt;ref&gt;Lee, Honglak, et al. &quot;Efficient sparse coding algorithms.&quot; ''Advances in neural information processing systems''. 2006.&lt;/ref&gt; Consider the following Lagrangian:

&lt;math&gt;\mathcal{L}(\mathbf{D}, \Lambda) = \text{tr}\left((X-\mathbf{D}R)^T(X-\mathbf{D}R)\right) + \sum_{j=1}^n\lambda_i \left({\sum_{i=1}^d\mathbf{D}_{ij}^2-c} \right)&lt;/math&gt;, where &lt;math&gt;c&lt;/math&gt; is a constraint on the norm of the atoms and &lt;math&gt;\lambda_i&lt;/math&gt; are the so-called dual variables forming the diagonal matrix &lt;math&gt;\Lambda&lt;/math&gt;.

We can then provide an analytical expression for the Lagrange dual after minimization over &lt;math&gt;\mathbf{D}&lt;/math&gt;:

&lt;math&gt;\mathcal{D}(\Lambda) = \min_{\mathbf{D}}\mathcal{L}(\mathbf{D}, \Lambda) = \text{tr}(X^TX-XR^T(RR^T+\Lambda)^{-1}(XR^T)^T-c\Lambda)&lt;/math&gt;.

After applying one of the optimization methods to the value of the dual (such as [[Newton's method in optimization|Newton's method]] or [[Conjugate gradient method|conjugate gradient]]) we get the value of &lt;math&gt;\mathbf{D}&lt;/math&gt;:

&lt;math&gt;\mathbf{D}^T=(RR^T+\Lambda)^{-1}(XR^T)^T&lt;/math&gt;

Solving this problem is less computational hard because the amount of dual variables &lt;math&gt;n&lt;/math&gt; is a lot of times much less than the amount of variables in the primal problem.

=== Parametric training methods ===
Parametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones.&lt;ref&gt;{{Cite journal|title = Dictionaries for Sparse Representation Modeling|url = http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=5452966&amp;url=http%253A%252F%252Fieeexplore.ieee.org%252Fxpls%252Fabs_all.jsp%253Farnumber%253D5452966|journal = Proceedings of the IEEE|date = 2010-06-01|issn = 0018-9219|pages = 1045–1057|volume = 98|issue = 6|doi = 10.1109/JPROC.2010.2040551|first = R.|last = Rubinstein|first2 = A.M.|last2 = Bruckstein|first3 = M.|last3 = Elad}}&lt;/ref&gt; This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include:
* Translation-invariant dictionaries.&lt;ref&gt;{{Cite journal|title = Family of Iterative LS-based Dictionary Learning Algorithms, ILS-DLA, for Sparse Signal Representation|url = https://dx.doi.org/10.1016/j.dsp.2006.02.002|journal = Digit. Signal Process.|date = 2007-01-01|issn = 1051-2004|pages = 32–49|volume = 17|issue = 1|doi = 10.1016/j.dsp.2006.02.002|first = Kjersti|last = Engan|first2 = Karl|last2 = Skretting|first3 = John H\a akon|last3 = Husøy}}&lt;/ref&gt; These dictionaries are composed by the translations of the atoms originating from the dictionary constructed for a finite-size signal patch. This allows the resulting dictionary to provide a representation for the arbitrary-sized signal.
* Multiscale dictionaries.&lt;ref&gt;{{Cite journal|title = Learning Multiscale Sparse Representations for Image and Video Restoration|url = http://epubs.siam.org/doi/abs/10.1137/070697653|journal = Multiscale Modeling &amp; Simulation|date = 2008-01-01|issn = 1540-3459|pages = 214–241|volume = 7|issue = 1|doi = 10.1137/070697653|first = J.|last = Mairal|first2 = G.|last2 = Sapiro|first3 = M.|last3 = Elad}}&lt;/ref&gt; This method focuses on constructing a dictionary that is composed of differently scaled dictionaries to improve sparsity.
* Sparse dictionaries.&lt;ref&gt;{{Cite journal|title = Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation|url = http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=5325694&amp;url=http%253A%252F%252Fieeexplore.ieee.org%252Fxpls%252Fabs_all.jsp%253Farnumber%253D5325694|journal = IEEE Transactions on Signal Processing|date = 2010-03-01|issn = 1053-587X|pages = 1553–1564|volume = 58|issue = 3|doi = 10.1109/TSP.2009.2036477|first = R.|last = Rubinstein|first2 = M.|last2 = Zibulevsky|first3 = M.|last3 = Elad}}&lt;/ref&gt; This method focuses on not only providing a sparse representation but also constructing a sparse dictionary which is enforced by the expression &lt;math&gt;\mathbf{D} = \mathbf{B}\mathbf{A}  &lt;/math&gt; where &lt;math&gt;\mathbf{B}&lt;/math&gt; is some pre-defined analytical dictionary with desirable properties such as fast computation and &lt;math&gt;\mathbf{A}&lt;/math&gt; is a sparse matrix. Such formulation allows to directly combine the fast implementation of analytical dictionaries with the flexibility of sparse approaches.

=== Online dictionary learning ===
Many common approaches to sparse dictionary learning rely on the fact that the whole input data &lt;math&gt;X&lt;/math&gt; (or at least a large enough training dataset) is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a [[Stream (computing)|stream]]. Such cases lie in the field of study of [[Online machine learning|online learning]] which essentially suggests iteratively updating the model upon the new data points &lt;math&gt;x&lt;/math&gt; becoming available.

A dictionary can be learned in an online manner the following way:&lt;ref&gt;{{Cite journal|title = Online Learning for Matrix Factorization and Sparse Coding|url = http://dl.acm.org/citation.cfm?id=1756006.1756008|journal = J. Mach. Learn. Res.|date = 2010-03-01|issn = 1532-4435|pages = 19–60|volume = 11|first = Julien|last = Mairal|first2 = Francis|last2 = Bach|first3 = Jean|last3 = Ponce|first4 = Guillermo|last4 = Sapiro}}&lt;/ref&gt;
# For &lt;math&gt;t = 1...T:&lt;/math&gt;
# Draw a new sample &lt;math&gt;x_t&lt;/math&gt;
# Find a sparse coding using [[Least-angle regression|LARS]]: &lt;math&gt;r_t = \underset{r \in \mathbb{R}^n}{\text{argmin}}\left(\frac{1}{2}\|x_t-\mathbf{D}_{t-1}r\|+\lambda\|r\|_1\right)&lt;/math&gt;
# Update dictionary using [[Coordinate descent|block-coordinate]] approach: &lt;math&gt;\mathbf{D}_t = \underset{\mathbf{D} \in \mathcal{C}}{\text{argmin}}\frac{1}{t}\sum_{i=1}^t\left(\frac{1}{2}\|x_i-\mathbf{D}r_i\|^2_2+\lambda\|r_i\|_1\right)&lt;/math&gt;

This method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).

== Applications ==
The dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to state-of-art results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.

It also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation.&lt;ref&gt;Aharon, M, M Elad, and A Bruckstein. 2006. &quot;K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.&quot; Signal Processing, IEEE Transactions on 54 (11): 4311-4322&lt;/ref&gt;

Sparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis&lt;ref&gt;{{Cite journal|title = Sparse Modeling of Textures|url = https://link.springer.com/article/10.1007/s10851-008-0120-3|journal = Journal of Mathematical Imaging and Vision|date = 2008-11-06|issn = 0924-9907|pages = 17–31|volume = 34|issue = 1|doi = 10.1007/s10851-008-0120-3|first = Gabriel|last = Peyré}}&lt;/ref&gt; and unsupervised clustering.&lt;ref&gt;{{Cite journal|title = Classification and clustering via dictionary learning with structured incoherence and shared features|url = http://www.computer.org/csdl/proceedings/cvpr/2010/6984/00/05539964-abs.html|publisher = IEEE Computer Society|journal = 2014 IEEE Conference on Computer Vision and Pattern Recognition|date = 2010-01-01|location = Los Alamitos, CA, USA|isbn = 978-1-4244-6984-0|pages = 3501–3508|volume = 0|doi = 10.1109/CVPR.2010.5539964|first = Ignacio|last = Ramirez|first2 = Pablo|last2 = Sprechmann|first3 = Guillermo|last3 = Sapiro}}&lt;/ref&gt; In evaluations with the [[Bag-of-words model in computer vision|Bag-of-Words]] model,&lt;ref&gt;{{Cite journal|last=Koniusz|first=Piotr|last2=Yan|first2=Fei|last3=Mikolajczyk|first3=Krystian|date=2013-05-01|title=Comparison of mid-level feature coding approaches and pooling strategies in visual concept detection|url=https://dx.doi.org/10.1016/j.cviu.2012.10.010|journal=Computer Vision and Image Understanding|volume=117|issue=5|pages=479–492|doi=10.1016/j.cviu.2012.10.010|issn=1077-3142}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Koniusz|first=Piotr|last2=Yan|first2=Fei|last3=Gosselin|first3=Philippe Henri|last4=Mikolajczyk|first4=Krystian|date=2017-02-24|title=Higher-order occurrence pooling for bags-of-words: Visual concept detection|url=http://ieeexplore.ieee.org/document/7439823|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=39|issue=2|pages=313–326|doi=10.1109/TPAMI.2016.2545667|issn=0162-8828}}&lt;/ref&gt; sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks.

Dictionary learning is used to analyse medical signals in detail. Such medical signals include those from electroencephalography (EEG), electrocardiography (ECG), magnetic resonance imaging (MRI), fuctional MRI (fMRI), and ultrasound computer tomography (USCT), where different assumptions are used to analyze each signal.

== See also ==
* [[Sparse approximation]]
* [[Sparse PCA]]
* [[Matrix factorization]]
* [[K-SVD]]
* [[Sparse coding|Neural sparse coding]]

== References ==
&lt;references /&gt;


</text>
      <sha1>2s4y4u6vwwqoz97mwprnv07mt73g5y5</sha1>
    </revision>
  </page>
  <page>
    <title>Darkforest</title>
    <ns>0</ns>
    <id>49316492</id>
    <revision>
      <id>814814418</id>
      <parentid>808618962</parentid>
      <timestamp>2017-12-11T02:29:06Z</timestamp>
      <contributor>
        <username>Neo-Jay</username>
        <id>561624</id>
      </contributor>
      <comment>completed the author list of [[DeepMind]]'s paper on [[AlphaGo]] published in the journal [[Nature (journal)|Nature]] on [[28 January]] [[2016]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12757">{{About|Facebook's Go-playing computer program||Dark Forest (disambiguation){{!}}Dark Forest}}
'''Darkforest''' is a [[computer go]] program developed by [[Facebook]], based on [[deep learning]] techniques using a [[convolutional neural network]]. Its updated version &lt;!-- not a typo --&gt;'''Darkfores2'''&lt;!-- not a typo --&gt; combines the techniques of its predecessor with  [[Monte Carlo tree search]].&lt;ref name=&quot;facebook-paper2&quot;&gt;{{Cite arXiv|eprint=1511.06410v1|last1=Tian|first1=Yuandong|title=Better Computer Go Player with Neural Network and Long-term Prediction|last2=Zhu|first2=Yan|class=cs.LG|year=2015}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://www.technologyreview.com/s/544181/how-facebooks-ai-researchers-built-a-game-changing-go-engine/|title=How Facebook’s AI Researchers Built a Game-Changing Go Engine|date=December 4, 2015|website=MIT Technology Review|access-date=2016-02-03}}&lt;/ref&gt; The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them.&lt;ref name=&quot;:1&quot;&gt;{{Cite web|url=http://www.techtimes.com/articles/128636/20160128/facebook-ai-go-player-gets-smarter-with-neural-network-and-long-term-prediction-to-master-worlds-hardest-game.htm|title=Facebook AI Go Player Gets Smarter With Neural Network And Long-Term Prediction To Master World's Hardest Game|date=2016-01-28|website=Tech Times|access-date=2016-04-24}}&lt;/ref&gt; With the update, the system is known as '''Darkfmcts3'''.&lt;ref name=&quot;:2&quot;&gt;{{Cite web|url=https://venturebeat.com/2016/01/26/facebooks-artificially-intelligent-go-player-is-getting-smarter/|title=Facebook’s artificially intelligent Go player is getting smarter|website=VentureBeat|access-date=2016-04-24}}&lt;/ref&gt;

Darkforest is of similar strength to programs like [[Crazy Stone (software)|CrazyStone]] and Zen.&lt;ref&gt;http://livestream.com/oxuni/StracheyLectureDrDemisHassabis&lt;/ref&gt; It has been tested against a professional human player at the 2016 [[Computer Go UEC Cup|UEC cup]]. [[Google]]'s [[AlphaGo]] program won against a professional player in October 2015 using a similar combination of techniques.&lt;ref&gt;{{Cite news|url=https://www.theguardian.com/technology/2016/jan/28/go-playing-facebook-spoil-googles-ai-deepmind|title=No Go: Facebook fails to spoil Google's big AI day|last=90210|first=HAL|date=2016-01-28|newspaper=The Guardian|language=en-GB|issn=0261-3077|access-date=2016-02-01}}&lt;/ref&gt;

Darkforest is named after [[Liu Cixin]]'s science fiction novel ''[[The Dark Forest]]''.&lt;ref&gt;{{cite web|url=http://xw.qq.com/tech/20160301024236/TEC201603010242360D|title=FB围棋项目负责人谈人机大战|date=2016-03-01|publisher=Tencent|language=Chinese|trans-title=FB Go Project Manager Discusses Man vs Machine Showdown}}&lt;/ref&gt;

== Background ==
Competing with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go’s high [[branching factor]] makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s [[evaluation function]] could change drastically with one stone change. However, by using a Deep [[Convolutional neural network|Convolutional Neural Network]] designed for long-term predictions, '''Darkforest''' has been able to substantially improve the win rate for bots over more traditional [[Monte Carlo tree search|Monte Carlo Tree Search]] based approaches.

=== Matches ===
Against human players, '''Darkfores2''' achieves a stable ''[[Go ranks and ratings|3d ranking]]'' on [[KGS Go Server]], which roughly corresponds to an advanced amateur human player. However, after adding [[Monte Carlo tree search|Monte Carlo Tree Search]] to '''Darkfores2''' to create a much stronger player named '''darkfmcts3,''' it can achieve a ''5d ranking'' on the KGS Go Server.

==== Against other AI ====
'''darkfmcts3''' is on par with state-of-the-art Go AIs such as Zen, DolBaram and [[Crazy Stone (software)|Crazy Stone]] but lags behind AlphaGo.&lt;ref&gt;{{Cite journal|title = Mastering the game of Go with deep neural networks and tree search|url = https://www.nature.com/nature/journal/v529/n7587/full/nature16961.html|journal = [[Nature (journal)|Nature]]| issn= 0028-0836|pages = 484–489|volume = 529|issue = 7587|doi = 10.1038/nature16961|pmid = 26819042|first1 = David|last1 = Silver|author-link1=David Silver (programmer)|first2 = Aja|last2 = Huang|author-link2=Aja Huang|first3 = Chris J.|last3 = Maddison|first4 = Arthur|last4 = Guez|first5 = Laurent|last5 = Sifre|first6 = George van den|last6 = Driessche|first7 = Julian|last7 = Schrittwieser|first8 = Ioannis|last8 = Antonoglou|first9 = Veda|last9 = Panneershelvam|first10= Marc|last10= Lanctot|first11= Sander|last11= Dieleman|first12=Dominik|last12= Grewe|first13= John|last13= Nham|first14= Nal|last14= Kalchbrenner|first15= Ilya|last15= Sutskever|author-link15=Ilya Sutskever|first16= Timothy|last16= Lillicrap|first17= Madeleine|last17= Leach|first18= Koray|last18= Kavukcuoglu|first19= Thore|last19= Graepel|first20= Demis |last20=Hassabis|author-link20=Demis Hassabis|date= 28 January 2016|bibcode = 2016Natur.529..484S|accessdate=10 December 2017}}{{closed access}}&lt;/ref&gt; It won 3rd place in [https://www.gokgs.com/tournEntrants.jsp?sort=s&amp;id=1005 January 2016 KGS Bot Tournament] against other Go AIs.

=== News Coverage ===
After [[Google]]'s [[AlphaGo]] won against [[Fan Hui]] in 2015, [[Facebook]] made its AI's hardware designs public, alongside releasing the code behind DarkForest as open-source, along with heavy recruiting to strengthen its team of AI engineers.&lt;ref name=&quot;:1&quot; /&gt;

== Style of play ==
'''Darkforest''' uses a neural network to sort through the 10&lt;sup&gt;100&lt;/sup&gt; board positions, and find the most powerful next move.&lt;ref name=&quot;:0&quot;&gt;{{Cite web|url=https://www.technologyreview.com/s/544181/how-facebooks-ai-researchers-built-a-game-changing-go-engine/|title=How Facebook’s AI Researchers Built a Game-Changing Go Engine|website=MIT Technology Review|access-date=2016-04-24}}&lt;/ref&gt; However, neural networks alone cannot match the level of good amateur players or the best search-based Go engines, and so '''Darkfores2''' combines the neural network approach with a search-based machine. A database of 250,000 real Go games were used in the development of '''Darkforest''', with 220,000 used as a training set and the rest used to test the neural network's ability to predict the next moves played in the real games.  This allows Darkforest to accurately evaluate the global state of the board, but local tactics were still poor. Search-based engines have poor global evaluation, but are good at local tactics. Combining these two approaches is difficult because search-based engines work much faster than neural networks, a problem which was solved in '''Darkfores2''' by running the processes in parallel with frequent communication between the two.&lt;ref name=&quot;:0&quot; /&gt;

=== Conventional strategies ===
Go is generally played by analyzing the position of the stones on the board. Some advanced players have described it as playing in some part subconsciously. Unlike chess and checkers, where AI players can simply look farther forward at moves than human players, but with each round of Go having on average 250 possible moves, that approach is ineffective. Instead, neural networks copy human play by training the AI systems on images of successful moves, the AI can effectively learn how to interpret how the board looks, as many grandmasters do.&lt;ref name=&quot;:3&quot;&gt;{{Cite web|url=https://www.wired.com/2015/12/google-and-facebook-race-to-solve-the-ancient-game-of-go/|title=Google and Facebook Race to Solve the Ancient Game of Go With AI|website=WIRED|language=en-US|access-date=2016-04-24}}&lt;/ref&gt; In November 2015, Facebook demonstrated the combination of MCTS with neural networks, which played with a style that &quot;felt human&quot;.&lt;ref name=&quot;:3&quot; /&gt;

=== Flaws ===
It has been noted that Darkforest still has flaws in its play style. Sometimes the bot plays [[tenuki]] (&quot;move elsewhere&quot;) pointlessly when local powerful moves are required. When the bot is losing, it shows the typical behavior of MCTS, it plays bad moves and loses more. The Facebook AI team has acknowledged these as areas of future improvement.&lt;ref&gt;{{Cite web|url=http://www.bbc.com/news/technology-35419141|title=Facebook trains AI to beat humans at Go board game - BBC News|website=BBC News|language=en-GB|access-date=2016-04-24}}&lt;/ref&gt;

== Program architecture ==
The family of '''Darkforest''' computer go programs is based on [[Convolutional neural network|convolution neural networks]].&lt;ref name=&quot;:1&quot; /&gt; The most recent advances in '''Darkfmcts3''' combined convolutional neural networks with more traditional [[Monte Carlo tree search]].&lt;ref name=&quot;:1&quot; /&gt; Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced convolutional neural network architecture from Darkfores2 with a [[Monte Carlo tree search]].

'''Darkfmcts3''' relies on a [[Convolutional neural network|convolution neural networks]] that predicts the next k moves based on the current state of play. It treats the board as a 19x19 image with multiple channels. Each channel represents a different aspect of board information based upon the specific style of play. For standard and extended play, there are 21 and 25 different channels, respectively. In standard play, each players [[List of Go terms#Liberty|liberties]] are represented as six binary channels or planes. The respective plane is true if the player one, two, or three or more liberties available. [[List of Go terms#Ko|Ko]] (i.e. illegal moves) is represented as one binary plane. Stone placement for each opponent and empty board positions are represented as three binary planes, and the duration since a stone has been placed is represented as real numbers on two planes, one for each player. Lastly, the opponents rank is represented by nine binary planes, where if all are true, the player is a 9d level, if 8 are true, a 8d level, and so forth. Extended play additionally considers the boarder (binary plane that is true at the border), position mask (represented as distance from the board center, i.e. &lt;math&gt; x^{(-0.5 * distance^2)}&lt;/math&gt;, where &lt;math&gt;x&lt;/math&gt; is a real number at a position), and each player's territory (binary, based on which player a location is closer to).

Darkfmct3 uses a 12-layer full convolutional network with a width of 384 nodes without weight sharing or pooling. Each convolutional layer is followed by a [[Rectifier (neural networks)|rectified linear unit]], a popular activation function for deep neural networks.&lt;ref&gt;{{cite journal|last2=Bengio|first2=Yoshua|last3=Hinton|first3=Geoffrey|date=27 May 2015|title=Deep learning|journal=Nature|volume=521|issue=7553|pages=436–444|doi=10.1038/nature14539|last1=LeCun|first1=Yann|pmid=26017442}}&lt;/ref&gt; A key innovation of Darkfmct3 compared to previous approaches is that it uses only one [[softmax function]] to predict the next move, which enables the approach to reduce the overall number of parameters.&lt;ref name=&quot;:1&quot; /&gt; Darkfmct3 was trained against 300 random selected games from an empirical dataset representing different game stages. The learning rate was determined by vanilla [[stochastic gradient descent]].

Darkfmct3 [[Synchronization (computer science)|synchronously]] couples a convolutional neural network with a [[Monte Carlo tree search]]. Because the convolutional neural network is computationally taxing, the Monte Carlo tree search focuses computation on the more likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, it is possible to guarantee that each node is expanded by the moves predicted by the neural network.

== Comparison with other systems ==
'''Darkfores2''' beats '''Darkforest''', its neural network-only predecessor, around 90% of the time, and Pachi, one of the best search-based engines, around 95% of the time.&lt;ref name=&quot;:0&quot; /&gt; On the [[Go ranks and ratings|Kyu rating system]], Darkforest holds a 1-2d level. '''Darkfores2''' achieves a stable 3d level on KGS Go Server as a ranked bot.&lt;ref name=&quot;facebook-paper2&quot; /&gt; With the added [[Monte Carlo tree search]], '''Darkfmcts3''' with 5,000 rollouts beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament.&lt;ref name=&quot;:2&quot; /&gt;

== See also ==
* [[Go and mathematics]]

== References ==
{{Reflist}}

== External links ==

*[https://github.com/facebookresearch/darkforestGo Source code on Github]





</text>
      <sha1>55xun1eujqmtdnhivz9wpcb2p788fjk</sha1>
    </revision>
  </page>
  <page>
    <title>Inauthentic text</title>
    <ns>0</ns>
    <id>5008963</id>
    <revision>
      <id>729903860</id>
      <parentid>703305514</parentid>
      <timestamp>2016-07-15T11:03:50Z</timestamp>
      <contributor>
        <username>Boson</username>
        <id>13662</id>
      </contributor>
      <comment>Tag as unreferenced</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2319">{{Unreferenced|date=July 2016}}
An '''inauthentic text''' is a computer-generated expository document meant to appear as genuine, but which is actually meaningless.  Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with [[Spam blog]]s.  They are also carried along in email in order to fool [[spam filter]]s by giving the spam the superficial characteristics of legitimate text.

Sometimes nonsensical documents are created with computer assistance for humorous effect, as with [[Dissociated press]] or [[Flarf poetry]].  They have also been used to challenge the veracity of a publication&amp;mdash;[[MIT]] students submitted papers generated by a computer program called [[SCIgen]] to a conference, where they were initially accepted.  This led the students to claim that the bar for submissions was too low.

With the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two.  Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics.  [[Noam Chomsky]] coined the phrase &quot;[[Colorless green ideas sleep furiously]]&quot; giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning.

The first group to use the expression in this regard can be found below from [[Indiana University]].  Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace.  The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not.  Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score.

==See also==
* [[Scraper site]]
* [[Spamdexing]]

==External links==
*[http://www.inauthentic.org An Inauthentic Paper Detector] from [[Indiana University]] School of Informatics




</text>
      <sha1>jugog2ah6b0zr2deaus880sxdzpctr7</sha1>
    </revision>
  </page>
  <page>
    <title>Bayesian structural time series</title>
    <ns>0</ns>
    <id>50211107</id>
    <revision>
      <id>809118620</id>
      <parentid>801121204</parentid>
      <timestamp>2017-11-07T06:26:24Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Jennifer A. Hoeting]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4148">{{more footnotes|date=April 2016}}
'''Bayesian structural time series''' ('''BSTS''') model is a [[machine learning]] technique used for [[feature selection]], time series forecasting, [[Nowcasting (economics)|nowcasting]], inferring causal impact and other. The model is designed to work with [[time series]] data.

The model has also promising application in the field of analytical [[marketing]]. In particular, it can be used in order to assess how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators ([[Difference in differences|difference-in-differences]] model is a usual alternative approach in this case).&lt;ref name=&quot;:0&quot; /&gt; &quot;In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls.&quot;&lt;ref name=&quot;:0&quot; /&gt;

== General model description ==
The model consists of three main parts:
# '''[[Kalman filter]]'''. The technique for time series decomposition. In this step, a researcher can add different state variables: trend, seasonality, regression, and others.
# '''[[Spike-and-slab variable selection|Spike-and-slab]] method.''' In this step, the most important regression predictors are selected.
# '''[[Ensemble learning|Bayesian model averaging]].''' Combining the results and prediction calculation.
The model seems to discover not only correlations, but also causations in the underlying data.&lt;ref name=&quot;:0&quot;&gt;{{Cite web|url=http://research.google.com/pubs/pub41854.html|title=Inferring causal impact using Bayesian structural time-series models|website=research.google.com|access-date=2016-04-17}}&lt;/ref&gt;

A possible drawback of the model can be its relatively complicated mathematical underpinning and difficult implementation as a computer program. However, the programming language [[R (programming language)|R]] has ready-to-use packages for calculating the BSTS model,&lt;ref&gt;{{Cite web|url=https://cran.r-project.org/web/packages/bsts/bsts.pdf|title=bsts}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://google.github.io/CausalImpact/CausalImpact.html|title=CausalImpact|website=google.github.io|access-date=2016-04-17}}&lt;/ref&gt; which do not require strong mathematical background from a researcher.

== See also ==
* [[Bayesian inference using Gibbs sampling]]
* [[Correlation does not imply causation]]

== References ==
{{Reflist}}
* Scott, S. L., &amp; Varian, H. R. 2014a. [http://people.ischool.berkeley.edu/~hal/Papers/2012/fat.pdf Bayesian variable selection for nowcasting economic time series]. ''Economic Analysis of the Digital Economy.''
* Scott, S. L., &amp; Varian, H. R. 2014b. [http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-with-bsts.pdf Predicting the present with bayesian structural time series]. ''International Journal of Mathematical Modelling and Numerical Optimisation.''
* Varian, H. R. 2014. [http://people.ischool.berkeley.edu/~hal/Papers/2013/ml.pdf Big Data: New Tricks for Econometrics]. ''Journal of Economic Perspectives''
* Brodersen, K. H., Gallusser, F., Koehler, J., Remy, N., &amp; Scott, S. L. 2015. [http://research.google.com/pubs/pub41854.html Inferring causal impact using Bayesian structural time-series models]. ''The Annals of Applied Statistics.''
* R package [https://cran.r-project.org/web/packages/bsts/bsts.pdf &quot;bsts&quot;].
* R package [https://google.github.io/CausalImpact/CausalImpact.html &quot;CausalImpact&quot;].
* O’Hara, R. B., &amp; Sillanpää, M. J. 2009. [https://projecteuclid.org/euclid.ba/1340370391 A review of Bayesian variable selection methods: what, how and which]. ''Bayesian analysis.''
* [[Jennifer A. Hoeting|Hoeting, J. A.]], Madigan, D., Raftery, A. E., &amp; Volinsky, C. T. 1999. [https://projecteuclid.org/euclid.ss/1009212519 Bayesian model averaging: a tutorial]. ''Statistical science.''


</text>
      <sha1>jjkybyljagur3eurz45ddqk0qbkngkg</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic folding</title>
    <ns>0</ns>
    <id>50222574</id>
    <revision>
      <id>800109360</id>
      <parentid>777995898</parentid>
      <timestamp>2017-09-11T15:24:57Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12419">'''Semantic folding''' theory describes a procedure for encoding the [[semantics]] of [[natural language]] text in a semantically grounded [[Binary number|binary representation]]. This approach provides a framework for modelling how language data is processed by the [[neocortex]].{{r|webber}}

==Theory==
Semantic folding theory draws inspiration from [[Douglas Hofstadter|Douglas R. Hofstadter]]'s ''Analogy as the Core of Cognition'' which suggests that the brain makes sense of the world by identifying and applying [[Analogy|analogies]].{{r|hofstadter}} The theory hypothesises that semantic data must therefore be introduced to the neocortex in such a form as to allow the application of a [[similarity measure]] and offers, as a solution, the [[sparse matrix|sparse]] [[bit array|binary vector]] employing a two-dimensional topographic [[semantic space]] as a distributional reference frame. The theory builds on the computational theory of the human cortex known as [[hierarchical temporal memory]] (HTM), and positions itself as a complementary theory for the representation of language semantics.

A particular strength claimed by this approach is that the resulting binary representation enables complex semantic operations to be performed simply and efficiently at the most basic computational level.

== Two-dimensional semantic space ==
Analogous to the structure of the neocortex, Semantic Folding theory posits the implementation of a semantic space as a two-dimensional grid. This grid is populated by context-vectors&lt;ref group=&quot;note&quot;&gt;A context-vector is defined as a vector containing all the words in a particular context.&lt;/ref&gt; in such a way as to place similar context-vectors closer to each other, for instance, by using competitive learning principles. This [[vector space model]] is presented in the theory as an equivalence to the well known word space model&lt;ref name=&quot;:0&quot;&gt;{{Cite web|url=http://su.diva-portal.org/smash/get/diva2:189276/FULLTEXT01|title=The Word-Space Model|last=Sahlgreen|first=Magnus|date=2006|website=|publisher=|access-date=}}&lt;/ref&gt; described in the [[Information retrieval|Information Retrieval]] literature.

Given a semantic space (implemented as described above) a word-vector&lt;ref group=&quot;note&quot;&gt;A word-vector or word-SDR is referred to as a Semantic Fingerprint in Semantic Folding theory.&lt;/ref&gt; can be obtained for any given word Y by employing the following [[algorithm]]:

For each position X in the semantic map (where X represents [[Cartesian coordinate system|cartesian coordinates]])
     if the word Y is contained in the context-vector at position X
          then add 1 to the corresponding position in the word-vector for Y
     else
          add 0 to the corresponding position in the word-vector for Y

The result of this process will be a word-vector containing all the contexts in which the word Y appears and will therefore be representative of the semantics of that word in the semantic space. It can be seen that the resulting word-vector is also in a sparse distributed representation (SDR) format [Schütze, 1993] &amp; [Sahlgreen, 2006].&lt;ref name=&quot;:0&quot; /&gt;&lt;ref&gt;{{Cite web|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8856|title=Word Space|last=Schütze|first=Hinrich|date=1993|website=|publisher=|access-date=}}&lt;/ref&gt; Some properties of word-SDRs that are of particular interest with respect to computational semantics are:&lt;ref name=&quot;:1&quot; /&gt;
* high [[Noise and Resistance|noise resistance]]: As a result of similar contexts being placed closer together in the underlying map, word-SDRs are highly tolerant of false or shifted &quot;bits&quot;.
* [[Boolean algebra|boolean]] logic: It is possible to manipulate word-SDRs in a meaningful way using boolean (OR, AND, exclusive-OR) and/or [[Arithmetical operations|arithmetical]] (SUBtract) functions .
* sub-sampling: Word-SDRs can be sub-sampled to a high degree without any appreciable loss of semantic information.
* topological two-dimensional representation: The SDR representation maintains the topological distribution of the underlying map therefore words with similar meanings will have similar word-vectors. This suggests that a variety of measures can be applied to the calculation of [[semantic similarity]], from a simple overlap of vector elements, to a range of distance measures such as: [[Euclidean distance]], [[Hamming distance]], [[Jaccard index|Jaccard distance]], [[cosine similarity]], [[Levenshtein distance]], [[Sørensen–Dice coefficient|Sørensen-Dice index]], etc.

== Semantic spaces ==
Semantic spaces&lt;ref group=&quot;note&quot;&gt;also referred to as distributed semantic spaces or distributed semantic memory&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Baroni|first=Marco|last2=Lenci|first2=Alessandro|title=Distributional Memory: A General Framework for Corpus-Based Semantics|url=http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00016|journal=Computational Linguistics|volume=36|issue=4|pages=673–721|doi=10.1162/coli_a_00016}}&lt;/ref&gt; in the natural language domain aim to create representations of natural language that are capable of capturing meaning. The original motivation for semantic spaces stems from two core challenges of natural language: [[Vocabulary mismatch]] (the fact that the same meaning can be expressed in many ways) and [[ambiguity]] of natural language (the fact that the same term can have several meanings).

The application of semantic spaces in [[natural language processing]] (NLP) aims at overcoming limitations of [[Rule-based system|rule-based]] or model-based approaches operating on the [[Keyword research|keyword]] level. The main drawback with these approaches is their brittleness, and the large manual effort required to create either rule-based NLP systems or training corpora for model learning.&lt;ref&gt;{{Cite journal|author1=Scott C. Deerwester |author2=Susan T. Dumais |author3=Thomas K. Landauer |author4=George W. Furnas |author5=Richard A. Harshen |date=1990|title=Indexing by Latent Semantic Analysis|url=http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf|journal=Journal of the American Society of Information Science|doi=|pmid=|access-date=}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|author1=Xing Wei |author2=W. Bruce Croft |date=2007|title=Investigating retrieval performance with manually-built topic models|url=http://dl.acm.org/citation.cfm?id=1931390.1931423|journal=Proceeding RIAO '07 Large Scale Semantic Access to Content (Text, Image, Video, and Sound)|doi=|pmid=|access-date=}}&lt;/ref&gt; Rule-based and [[machine learning]] based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models.

Research in semantic spaces dates back more than 20 years. In 1996, two papers were published that raised a lot of attention around the general idea of creating semantic spaces: [[latent semantic analysis]]&lt;ref&gt;{{Cite web|url=http://lsa.colorado.edu/papers/plato/plato.annote.html|title=LSA: A Solution to Plato's Problem|website=lsa.colorado.edu|access-date=2016-04-19}}&lt;/ref&gt; from [[Microsoft]] and [[Hyperspace Analogue to Language]]&lt;ref&gt;{{Cite journal|last=Lund|first=Kevin|last2=Burgess|first2=Curt|date=1996-06-01|title=Producing high-dimensional semantic spaces from lexical co-occurrence|url=https://link.springer.com/article/10.3758/BF03204766|journal=Behavior Research Methods, Instruments, &amp; Computers|language=en|volume=28|issue=2|pages=203–208|doi=10.3758/BF03204766|issn=0743-3808}}&lt;/ref&gt; from the [[University of California, Riverside|University of California]]. However, their adoption was limited by the large computational effort required to construct and use those semantic spaces. A breakthrough with regard to the [[Accuracy and precision|accuracy]] of modelling associative relations between words (e.g. &quot;spider-web&quot;, &quot;lighter-cigarette&quot;, as opposed to synonymous relations such as &quot;whale-dolphin&quot;, &quot;astronaut-driver&quot;) was achieved by [[explicit semantic analysis]] (ESA)&lt;ref&gt;{{Cite journal|author1=Evgeniy Gabrilovich  |author2=Shaul Markovitch |lastauthoramp=yes |date=2007|title=Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis|url=http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf|journal=Proc. 20th Int'l Joint Conf. on Artificial Intelligence (IJCAI). pp. 1606–1611.|doi=|pmid=|access-date=}}&lt;/ref&gt; in 2007. ESA was a novel (non-machine learning) based approach that represented words in the form of vectors with 100,000 [[dimension]]s (where each dimension represents an Article in [[Wikipedia]]). However practical applications of the approach are limited due to the large number of required dimensions in the vectors.

More recently, advances in [[Neural networks|neural networking]] techniques in combination with other new approaches ([[tensor]]s) led to a host of new recent developments: [[Word2vec]]&lt;ref&gt;{{cite arXiv|arxiv=1310.4546|title=Distributed Representations of Words and Phrases and their Compositionality|author1=Tomas Mikolov |author2=Ilya Sutskever |author3=Kai Chen |author4=Greg Corrado |author5=Jeffrey Dean |date=2013|website=|publisher=|access-date=}}&lt;/ref&gt; from [[Google]] and [[GloVe (machine learning)|GloVe]]&lt;ref&gt;{{Cite web|url=http://www-nlp.stanford.edu/pubs/glove.pdf|title=GloVe: Global Vectors for Word Representation|author1=Jeffrey Pennington |author2=Richard Socher |author3=Christopher D. Manning |date=2014|website=|publisher=|access-date=}}&lt;/ref&gt; from [[Stanford University]].

Semantic folding represents a novel, biologically inspired approach to semantic spaces where each word is represented as a sparse binary vector with 16,000 dimensions (a semantic fingerprint) in a 2D semantic map (the semantic universe). Sparse binary representation are advantageous in terms of computational efficiency, and allow for the storage of very large numbers of possible patterns.&lt;ref name=&quot;:1&quot;&gt;{{cite arXiv|arxiv=1503.07469|title=Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory|author1=Subutai Ahmad |author2=Jeff Hawkins |date=2015|publisher=|access-date=}}&lt;/ref&gt;

==Visualization==
[[File:Semantic fingerprint comparing the terms &quot;dog&quot; and &quot;car&quot;.png|thumb|Semantic fingerprint image comparing the terms &quot;dog&quot; and &quot;car&quot;.]]
[[File:Semantic fingerprint comparing the terms &quot;jaguar&quot; and &quot;Porsche&quot;.png|thumb|Semantic fingerprint image comparing the terms &quot;jaguar&quot; and &quot;Porsche&quot;]]
The topological distribution over a two-dimensional grid (outlined above) lends itself to a [[bitmap]] type visualization of the semantics of any word or text, where each active semantic feature can be displayed as e.g. a [[pixel]]. As can be seen in the images shown here, this representation allows for a direct visual comparison of the semantics of two (or more) linguistic items.

Image 1 clearly demonstrates that the two disparate terms &quot;dog&quot; and &quot;car&quot; have, as expected, very obviously different semantics.

Image 2 shows that only one of the meaning contexts of  &quot;jaguar&quot;, that of &quot;Jaguar&quot; the car, overlaps with the meaning of Porsche (indicating partial similarity). Other meaning contexts of &quot;jaguar&quot; e.g. &quot;jaguar&quot; the animal clearly have different non-overlapping contexts.

Note also that the visualization of semantic similarity using Semantic Folding bears a strong resemblance to the [[fMRI]] images produced in a research study conducted by A.G. Huth et al.,&lt;ref&gt;{{cite journal|last1=Huth|first1=Alexander|title=Natural speech reveals the semantic maps that tile human cerebral cortex|journal=Nature|date=27 April 2016|issue=7600|pages=453–458|doi=10.1038/nature17637|url=http://www.nature.com/nature/journal/v532/n7600/abs/nature17637.html|accessdate=19 October 2016|volume=532|pmid=27121839|pmc=4852309}}&lt;/ref&gt; where it is claimed that words are grouped in the brain by meaning.

== Notes ==
{{reflist|group=note}}

== References ==
{{reflist|30em|refs=
&lt;ref name=hofstadter&gt;{{Cite web|url=https://mitpress.mit.edu/books/analogical-mind|title=The Analogical Mind|website=MIT Press|access-date=2016-04-18}}&lt;/ref&gt;
&lt;ref name=webber&gt;{{Cite journal|last=De Sousa Webber|first=Francisco|date=2015|title=Semantic Folding theory and its Application in Semantic Fingerprinting|arxiv=1511.08855|journal=Cornell University Library|doi=|pmid=|access-date=}}&lt;/ref&gt;

}}




</text>
      <sha1>bg0cd0wvy3m33rt89jwnwupaikv9lnw</sha1>
    </revision>
  </page>
  <page>
    <title>Spike-and-slab variable selection</title>
    <ns>0</ns>
    <id>50227596</id>
    <revision>
      <id>790464139</id>
      <parentid>782498926</parentid>
      <timestamp>2017-07-13T22:37:11Z</timestamp>
      <contributor>
        <username>Reneschaub</username>
        <id>31509116</id>
      </contributor>
      <minor/>
      <comment>The spike and slab is the distribution of the combined inclusion*value. Therefore the spike at zero indicates the Dirac portion of the distribution where the inclusion is zero.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6740">'''Spike-and-slab regression''' is a Bayesian [[Feature selection|variable selection]] technique that is particularly useful when the number of possible predictors is larger than the number of observations.&lt;ref&gt;{{Cite web|url=http://people.ischool.berkeley.edu/~hal/Papers/2013/ml.pdf|title=Big Data: New Tricks for Econometrics|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

Initially, the idea of the spike-and-slab model was proposed by Mitchell &amp; Beauchamp (1988).&lt;ref&gt;{{Cite web|url=https://www2.stat.duke.edu/courses/Fall05/sta395/joelucas1.pdf|title=Bayesian variable selection in linear regression|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; The approach was further significantly developed by Madigan &amp; Raftery (1994)&lt;ref&gt;{{Cite web|url=http://www.stat.cmu.edu/~fienberg/Statistics36-756/MadiganRaftery-JASA-1994.pdf|title=Model selection and accounting for model uncertainty in graphical models using occam’s window|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; and George &amp; McCulloch (1997).&lt;ref&gt;{{Cite web|url=http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/GeorgeMcCulloch97.pdf|title=Approaches for Bayesian variable selection|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; The final adjustments to the model were done by Ishwaran &amp; Rao (2005).&lt;ref&gt;{{Cite web|url=http://arxiv.org/pdf/math/0505633.pdf|title=Spike and slab variable selection: frequentist and Bayesian strategies|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

== Model description ==
Suppose we have ''P'' possible predictors in some model. Vector ''γ'' has a length equal to ''P'' and consists of zeros and ones. This vector indicates whether a particular variable is included in the regression or not. If no specific prior information on initial inclusion probabilities of particular variables is available, a [[Bernoulli distribution|Bernoulli prior]] distribution is a common default choice.&lt;ref name=&quot;:0&quot;&gt;{{Cite web|url=http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-with-bsts.pdf|title=Predicting the present with bayesian structural time series|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; Conditional on a predictor being in the regression, we identify a [[Prior probability|prior distribution]] for the model coefficient, which corresponds to that variable (''β''). A common choice on that step is to use a [[Normal distribution|Normal]] prior with mean equal to zero and a large variance calculated based on &lt;math&gt;(X^TX)^{-1}&lt;/math&gt; (where &lt;math&gt;X&lt;/math&gt; is a [[design matrix]] of explanatory variables of the model).&lt;ref&gt;{{Cite web|url=http://people.ischool.berkeley.edu/~hal/Papers/2012/fat.pdf|title=Bayesian variable selection for nowcasting economic time series|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

A draw of ''γ'' from its prior distribution is a list of the variables included in the regression. Conditional on this set of selected variables, we take a draw from the prior distribution of the regression coefficients (if ''γ''&lt;sub&gt;''i''&lt;/sub&gt; = 1 then ''β''&lt;sub&gt;''i''&lt;/sub&gt; ≠ 0 and if ''γ''&lt;sub&gt;''i''&lt;/sub&gt; = 0 then ''β''&lt;sub&gt;''i''&lt;/sub&gt; = 0). ''βγ'' denotes the subset of ''β'' for which ''γ''&lt;sub&gt;''i''&lt;/sub&gt; = 1. In the next step, we calculate a [[posterior probability]] distribution for both inclusion and coefficients by applying a standard statistical procedure.&lt;ref&gt;{{Cite web|url=http://research.google.com/pubs/pub41854.html|title=Inferring causal impact using Bayesian structural time-series models|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; All steps of the described algorithm are repeated thousands of times using [[Markov chain Monte Carlo]] (MCMC) technique. As a result, we obtain a posterior distribution of ''γ'' (variable inclusion in the model), ''β'' (regression coefficient values) and the corresponding prediction of ''y''.

The model got its name (spike-and-slab) due to the shape of the two prior distributions. The &quot;spike&quot; is the probability of a particular coefficient in the model to be zero. The &quot;slab&quot; is the prior distribution for the regression coefficient values.

An advantage of Bayesian variable selection techniques is that they are able to make use of prior knowledge about the model. In the absence of such knowledge, some reasonable default values can be used: &quot;For the analyst who prefers simplicity at the cost of some reasonable assumptions, useful prior information can be reduced to an expected model size, an expected ''R''&lt;sup&gt;2&lt;/sup&gt;, and a sample size ''ν'' determining the weight given to the guess at ''R''&lt;sup&gt;2&lt;/sup&gt;.&quot;&lt;ref name=&quot;:0&quot; /&gt; Some researchers suggest the following default values: ''R''&lt;sup&gt;2&lt;/sup&gt; = 0.5, ''ν'' = 0.01, and {{pi}} = 0.5 (parameter of a prior Bernoulli distribution).&lt;ref name=&quot;:0&quot; /&gt;

A possible drawback of the Spike-and-Slab model can be its mathematical complexity (in comparison to linear regression). A deep understanding of this model requires sound knowledge in [[stochastic process]]es. On the other hand, some modern statistical software (e.g. [[R (programming language)|R]]) have ready-to-use solutions for calculating various Bayesian variable selection models.&lt;ref&gt;{{Cite web|url=https://cran.r-project.org/web/packages/spikeslab/spikeslab.pdf|title=spikeslab|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://cran.r-project.org/web/packages/spikeSlabGAM/vignettes/UsingSpikeSlabGAM.pdf|title=spikeSlabGAM|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://cran.r-project.org/web/packages/bsts/bsts.pdf|title=bsts|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; In this case, it would be enough for a researcher to know the idea of the method, required model parameters and input variables. The analysis of the model outcomes (distribution of ''γ'', ''β'', and corresponding predictions of ''y'') can be more challenging in comparison to linear regression case. The spike-and-slab model produces inclusion probabilities for each of possible predictors. This can cause difficulties when comparing results to the studies with simple regression (usually only regression coefficients with corresponding statistics are available).

Spike-and-slab regression is a part of [[Bayesian structural time series]] model (which is used for feature selection, time series forecasting, nowcasting, inferring causation, and other).

== See also ==
* [[Bayesian inference using Gibbs sampling]]
* [[Bayesian structural time series]]

== Notes ==

{{Empty section|date=April 2016}}

==References==
{{Reflist}}



</text>
      <sha1>1zojed3dh0hfhiiv25jtjbp48q7pizo</sha1>
    </revision>
  </page>
  <page>
    <title>Glossary of artificial intelligence</title>
    <ns>0</ns>
    <id>50336055</id>
    <revision>
      <id>813654189</id>
      <parentid>811746657</parentid>
      <timestamp>2017-12-04T15:30:11Z</timestamp>
      <contributor>
        <username>LearnMore</username>
        <id>134016</id>
      </contributor>
      <minor/>
      <comment>/* See also */ Format</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13984">{{Use dmy dates|date=September 2017}}


''Contributors are needed to write definitions for terms in this glossary.  Please keep the definitions brief and in accordance with Wikipedia's policies and guidelines.''

This '''glossary of artificial intelligence terms''' is about [[artificial intelligence]], its sub-disciplines, and related fields.


{{compact ToC|side=yes|center=yes|nobreak=yes|seealso=yes|refs=yes|}}

{{Science}}
{{Complex systems}}

== A ==

*'''[[Abductive logic programming]]''' –
*'''[[Abductive reasoning]]''' –
*'''[[Abstract data type]]''' –
*'''[[Abstraction (software engineering)|Abstraction]]''' –
*'''[[Accelerating change]]''' –
*'''[[Action language]]''' –
*'''[[Action model learning]]''' –
*'''[[Action selection]]''' –
*'''[[Adaptive algorithm]]''' –
*'''[[Adaptive neuro fuzzy inference system]]''' –
*'''[[Admissible heuristic]]''' –
*'''[[Affective computing]]''' –
*'''[[Agent architecture]]''' –
*'''[[AI accelerator (computer hardware)|AI accelerator]]''' –
*'''[[AI-complete]]''' –
*'''[[Algorithm]]''' –
*'''[[Algorithmic efficiency]]''' –
*'''[[Algorithmic probability]]''' –
*'''[[AlphaGo]]''' –
*'''[[Ambient intelligence]]''' –
*'''[[Analysis of algorithms]]''' –
*'''[[Answer set programming]]''' –
*'''[[Anytime algorithm]]''' –
*'''[[Application programming interface]]''' –
*'''[[Approximate string matching]]''' –
*'''[[Approximation error]]''' –
*'''[[Argumentation framework]]''' –
*'''[[Artificial immune system]]''' –
*'''[[Artificial intelligence]]''' –
*'''[[AIML|Artificial Intelligence Markup Language]]''' –
*'''[[Artificial neural network]]''' –
*'''[[Association for the Advancement of Artificial Intelligence]]''' –
*'''[[Asymptotic computational complexity]]''' –
*'''[[Attributional calculus]]''' –
*'''[[Augmented reality]]''' –
*'''[[Automata theory]]''' –
*'''[[Automated planning and scheduling]]''' –
*'''[[Automated reasoning]]''' –
*'''[[Autonomic computing]]''' –
*'''[[Autonomous car]]''' –
*'''[[Autonomous robot]]''' –



{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==B==

*'''[[Backpropagation]]''' –
*'''[[Backward chaining]]''' –
*'''[[Batch normalisation]]''' –
*'''[[Bayesian programming]]''' –
*'''[[Bees algorithm]]''' –
*'''[[Behavior informatics]]''' –
*'''[[Behavior tree (artificial intelligence, robotics and control)|Behavior tree]]''' –
*'''[[Belief-desire-intention software model]]''' –
*'''[[Bias–variance tradeoff]]''' –
*'''[[Big data]]''' –
*'''[[Big O notation]]''' –
*'''[[Binary tree]]''' –
*'''[[Bio-inspired computing]]''' –
*'''[[Blackboard system]]''' –
*'''[[Boolean satisfiability problem]]''' –
*'''[[Brain technology]]''' –
*'''[[Branching factor]]''' –
*'''[[Brute-force search]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==C==

*'''[[Case-based reasoning]]''' –
*'''[[Chatterbot]]''' –
*'''[[Cloud robotics]]''' –
*'''[[Cluster analysis]]''' –
*'''[[Cobweb (clustering)|Cobweb]]''' –
*'''[[Cognitive architecture]]''' –
*'''[[Cognitive computing]]''' –
*'''[[Cognitive science]]''' –
*'''[[Combinatorial optimization]]''' –
*'''[[Committee machine]]''' –
*'''[[Commonsense knowledge (artificial intelligence)|Commonsense knowledge]]''' –
*'''[[Commonsense reasoning]]''' –
*'''[[Computational chemistry]]''' –
*'''[[Computational complexity theory]]''' –
*'''[[Computational creativity]]''' –
*'''[[Computational humor]]''' –
*'''[[Computational intelligence]]''' –
*'''[[Computational learning theory]]''' –
*'''[[Computational linguistics]]''' –
*'''[[Computational mathematics]]''' – the mathematical research in areas of science where [[computation|computing]] plays an essential role.
*'''[[Computational neuroscience]]''' –
*'''[[Computational number theory]]''' – also known as ''algorithmic number theory'', it is the study of [[algorithm]]s for performing [[number theory|number theoretic]] [[computations]].
*'''[[Computational problem]]''' –
*'''[[Computational statistics]]''' –
*'''[[Computational vision]]''' –
*'''[[Computer-automated design]]''' –
*'''[[Computer science]]''' –
*'''[[Computer vision]]''' –
*'''[[Connectionism]]''' –
*'''[[Consistent heuristic]]''' –
*'''[[Constraint logic programming]]''' –
*'''[[Constraint programming]]''' –
*'''[[Constructed language]]''' –
*'''[[Control theory]]''' –
*'''[[Convolutional neural network]]''' –
*'''[[Crossover (genetic algorithm)|Crossover]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==D==

*'''[[Darkforest]]''' –
*'''[[Dartmouth workshop]]''' –
*'''[[Data fusion]]''' –
*'''[[Data integration]]''' –
*'''[[Data mining]]''' –
*'''[[Data science]]''' –
*'''[[Data set]]''' –
*'''[[Data warehouse]]''' –
*'''[[Datalog]]''' –
*'''[[Decision boundary]]''' –
*'''[[Decision support system]]''' –
*'''[[Decision theory]]''' –
*'''[[Declarative programming]]''' –
*'''[[Deductive classifier]]''' –
*'''[[Deep Blue (chess computer)|Deep Blue]]''' –
*'''[[Deep learning]]''' –
*'''[[Default logic]]''' –
*'''[[Description logic]]''' –
*'''[[Developmental robotics]]''' –
*'''[[Diagnosis (artificial intelligence)|Diagnosis]]''' –
*'''[[Dialog system]]''' –
*'''[[Dimensionality reduction]]''' –
*'''[[Discrete system]]''' –
*'''[[Distributed artificial intelligence]]''' –
*'''[[Dynamic epistemic logic]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==E==

*'''[[Embodied agent]]''' –
*'''[[Embodied cognitive science]]''' –
*'''[[Ensemble averaging (machine learning)|Ensemble averaging]]''' –
*'''[[Ethics of artificial intelligence]]''' –
*'''[[Evolutionary algorithm]]''' –
*'''[[Evolutionary computation]]''' –
*'''[[Evolving classification function]]''' –
*'''[[Existential risk from artificial general intelligence|Existential risk]]''' –
*'''[[Expert systems]]''' –

==F==

*'''[[Feature extraction]]''' –
*'''[[Feature selection]]''' –
*'''[[First-order logic]]''' –
*'''[[Fluent (artificial intelligence)|Fluent]]''' –
*'''[[Formal language]]''' –
*'''[[Forward chaining]]''' –
*'''[[Frame (artificial intelligence) |Frame]]''' –
*'''[[Frame language]]''' –
*'''[[Frame problem]]''' –
*'''[[Friendly artificial intelligence]]''' –
*'''[[Futures studies]]''' –
*'''[[Fuzzy control system]]''' –
*'''[[Fuzzy logic]]''' –
*'''[[Fuzzy set]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==G==

*'''[[Game theory]]''' –
*'''[[Genetic algorithm]]''' –
*'''[[Genetic operator]]''' –
*'''[[Glowworm swarm optimization]]''' –
*'''[[Google DeepMind]]''' –
*'''[[Graph (abstract data type)|Graph]]''' –
*'''[[Graph (discrete mathematics)|Graph]]''' –
*'''[[Graph database]]''' –
*'''[[Graph theory]]''' –
*'''[[Graph traversal]]''' –

==H==

*'''[[Heuristic (computer science)|Heuristic]]''' –
*'''{{anchor|hidden layer}}Hidden layer''' – an internal layer of neurons in an [[artificial neural network]], not dedicated to input or output
*'''{{anchor|hidden unit}}Hidden unit''' – an neuron in a hidden layer in an [[artificial neural network]]
*'''[[Hyper-heuristic]]''' –

==I==

*'''[[IEEE Computational Intelligence Society]]''' –
*'''[[Incremental learning]]''' –
*'''[[Inference engine]]''' –
*'''[[Information integration]]''' –
*'''[[Information Processing Language]]''' –
*'''[[Intelligence amplification]]''' –
*'''[[Intelligence explosion]]''' –
*'''[[Intelligent agent]]''' –
*'''[[Intelligent control]]''' –
*'''[[Intelligent personal assistant]]''' –
*'''[[Interpretation (logic)|Interpretation]]''' –
*'''[[Issue trees]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==J==

==K==

*'''[[Kernel method]]''' –
*'''[[KL-ONE]]''' –
*'''[[Knowledge acquisition]]''' –
*'''[[Knowledge-based systems]]''' –
*'''[[Knowledge engineering]]''' –
*'''[[Knowledge extraction]]''' –
*'''[[Knowledge Interchange Format]]''' –
*'''[[Knowledge representation and reasoning]]''' –

==L==

*'''[[Linked data]]''' –
*'''[[Lisp]]''' –
*'''[[Logic programming]]''' –

==M==

*'''[[Machine vision]]''' –
*'''[[Markov chain]]''' –
*'''[[Markov decision process]]''' –
*'''[[Mathematical optimization]]''' –
*'''[[Machine learning]]''' –
*'''[[Machine listening]]''' –
*'''[[Machine perception]]''' –
*'''[[Mechanism design]]''' –
*'''[[Mechatronics]]''' –
*'''[[Metabolic network modelling]]''' –
*'''[[Metaheuristic]]''' –
*'''[[Model checking]]''' –
*'''[[Modus ponens]]''' –
*'''[[Modus tollens]]''' –
*'''[[Monte Carlo tree search]]''' –
*'''[[Multi-agent system]]''' –
*'''[[Multi-swarm optimization]]''' –
*'''[[Mutation (genetic algorithm)|Mutation]]''' –
*'''[[Mycin]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

== N ==

*'''[[Name binding]]''' –
*'''[[Named-entity recognition]]''' –
*'''[[Named graph]]''' –
*'''[[Natural language processing]]''' –
*'''[[Natural language programming]]''' –
*'''[[Network motif]]''' –
*'''[[Neuro-fuzzy]]''' –
*'''[[Neurocybernetics]]''' –
*'''[[Node (computer science)|Node]]''' –
*'''[[Nondeterministic algorithm]]''' –
*'''[[Nouvelle AI]]''' –
*'''[[NP (complexity)|NP]]''' –
*'''[[NP-completeness]]''' –
*'''[[NP-hardness]]''' –

==O==

*'''[[Offline learning]]''' –
*'''[[Online machine learning|Online learning]]''' –
*'''[[Ontology engineering]]''' –
*'''[[Ontology learning]]''' –
*'''[[OpenAI]]''' –
*'''[[OpenCog]]''' –
*'''[[Open Mind Common Sense]]''' –
*'''[[Open-source software]]''' –

==P==
*'''[[Partial order reduction]]''' –
*'''[[Partially observable Markov decision process]]''' –
*'''[[Particle swarm optimization]]''' –
*'''[[Pathfinding]]''' –
*'''[[Pattern recognition]]''' –
*'''[[Planner (programming language)|Planner]]''' –
*'''[[Predicate logic]]''' –
*'''[[Principal Component Analysis]]''' –
*'''[[Principle of rationality]]''' –
*'''[[Probabilistic programming language]]''' –
*'''[[Production Rule Representation]]''' –
*'''[[Production system (computer science) |Production system]]''' –
*'''[[Programming language]]''' –
*'''[[Prolog]]''' –
*'''[[Propositional calculus]]''' –
*'''[[Python (programming language)|Python]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==Q==

*'''[[Qualification problem]]''' –
*'''[[Quantifier (logic)|Quantifier]]''' –
*'''[[Query language]]''' –

==R==

*'''[[R (programming language) | R programming language]]''' –
*'''[[Reasoning system]]''' –
*'''[[Recurrent neural network]]''' –
*'''[[Region connection calculus]]''' –
*'''[[Reinforcement learning]]''' –
*'''[[Resource Description Framework]]''' –
*'''[[Rete algorithm]]''' –
*'''[[Robotics]]''' –
*'''[[Rule-based system]]''' –

==S==

*'''[[Satisfiability]]''' –
*'''[[Search algorithm]]''' –
*'''[[Selection (genetic algorithm)|Selection]]''' –
*'''[[Self-management (computer science)|Self-management]]''' –
*'''[[Semantic network]]''' –
*'''[[Semantic reasoner]]''' –
*'''[[Semantic query]]''' –
*'''[[Semantics (computer science)|Semantics]]''' –
*'''[[Sensor fusion]]''' –
*'''[[Separation logic]]''' –
*'''[[Similarity learning]]''' –
*'''[[Simulated annealing]]''' –
*'''[[Artificial intelligence, situated approach|Situated approach]]''' –
*'''[[Situation calculus]]''' –
*'''[[SLD resolution]]''' –
*'''[[Soft computing]]''' –
*'''[[Software]]''' –
*'''[[Software engineering]]''' –
*'''[[Spatial-temporal reasoning]]''' –
*'''[[SPARQL]]''' –
*'''[[Speech recognition]]''' –
*'''[[Spiking neural network]]''' –
*'''[[State (computer science) |State]]''' –
*'''[[Statistical classification]]''' –
*'''[[Statistical relational learning]]''' –
*'''[[Stochastic optimization]]''' –
*'''[[Stochastic semantic analysis]]'''
*'''[[STRIPS]]''' –
*'''[[Subject-matter expert]]''' –
*'''[[Superintelligence]]''' –
*'''[[Supervised learning]]''' –
*'''[[Swarm intelligence]]''' –
*'''[[Symbolic artificial intelligence]]''' –
*'''[[Synthetic intelligence]]''' –
*'''[[Systems neuroscience]]''' –


{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

==T==

*'''[[Technological singularity]]''' –
*'''[[Temporal difference learning]]''' –
*'''[[Tensor network theory]]''' –
*'''[[TensorFlow]]''' –
*'''[[Theoretical computer science]]''' –
*'''[[Theory of computation]]''' –
*'''[[Thompson sampling]]''' –
*'''[[Time complexity]]''' –
*'''[[Transhumanism]]''' –
*'''[[Transition system]]''' –
*'''[[Tree traversal]]''' –
*'''[[True quantified Boolean formula]]''' –
*'''[[Turing test]]''' –
*'''[[Type system]]''' –

==U==

*'''[[Unsupervised learning]]''' –

==V==
* '''[[Vision processing unit]]''' –

==W==

*'''[[Watson (computer)|Watson]]''' –
*'''[[Weak AI]]''' –
*'''[[World Wide Web Consortium]]''' –

==X==

==Y==

==Z==



{{Compact ToC|side=yes|center=yes|top=yes|num=yes|extlinks=yes|seealso=yes|refs=yes|nobreak=yes|}}

== See also ==

*[[Artificial intelligence]]
*[[Glossary of areas of mathematics]]
*[[Glossary of calculus]]


== References ==







{{Computer science}}
{{Evolutionary computation}}
{{Emerging technologies}}
{{Robotics}}

[[Category:Glossaries of science|Artificial intelligence]]

[[Category:Artificial intelligence|Glossary of artificial intelligence]]

[[Category:Wikipedia glossaries|Artificial intelligence]]</text>
      <sha1>mce148lefi0byfsea7ot2mbelm17dzu</sha1>
    </revision>
  </page>
  <page>
    <title>Multilinear subspace learning</title>
    <ns>0</ns>
    <id>30909817</id>
    <revision>
      <id>799048789</id>
      <parentid>782037610</parentid>
      <timestamp>2017-09-05T09:38:24Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16816">[[File:Video represented as a third-order tensor.jpg|right|thumb|300px|A video or an image sequence represented as a third-order tensor of column x row x time for multilinear subspace learning.]]
'''Multilinear subspace learning''' is an approach to dimensionality reduction.&lt;ref name=&quot;Vasilescu2003&quot;&gt;M. A. O. Vasilescu, D. Terzopoulos (2003) [http://www.cs.toronto.edu/~maov/tensorfaces/cvpr03.pdf &quot;Multilinear Subspace Analysis of Image Ensembles&quot;], &quot;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’03), Madison, WI, June, 2003&quot;&lt;/ref&gt;&lt;ref name=&quot;Vasilescu2002tensorfaces&quot;&gt;M. A. O. Vasilescu, D. Terzopoulos (2002) [http://www.cs.toronto.edu/~maov/tensorfaces/Springer%20ECCV%202002_files/eccv02proceeding_23500447.pdf &quot;Multilinear Analysis of Image Ensembles: TensorFaces&quot;],  Proc. 7th European Conference on Computer Vision (ECCV'02), Copenhagen, Denmark, May, 2002&lt;/ref&gt;&lt;ref name=&quot;Vasilescu2002hms&quot;&gt;M. A. O. Vasilescu,(2002) [http://www.media.mit.edu/~maov/motionsignatures/hms_icpr02_corrected.pdf &quot;Human Motion Signatures: Analysis, Synthesis, Recognition&quot;], &quot;Proceedings of International Conference on Pattern Recognition (ICPR 2002), Vol. 3, Quebec City, Canada, Aug, 2002, 456–460.&quot;&lt;/ref&gt;&lt;ref name=&quot;Vasilescu2007&quot;/&gt;&lt;ref name=&quot;MSLbook&quot;&gt;{{cite book
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.crcpress.com/product/isbn/9781439857243
 |title=Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data
 |series=Chapman &amp; Hall/CRC Press Machine Learning and Pattern Recognition Series
 |publisher=Taylor and Francis
 |isbn=978-1-4398572-4-3
 |year=2013
}}&lt;/ref&gt;
[[Dimension reduction|Dimensionality reduction]] can be performed on a data [[tensor]] whose observations have been vectorized&lt;ref name=&quot;Vasilescu2003&quot;/&gt; and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor.&lt;ref name=&quot;MSLsurvey&quot;&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt;&lt;ref name=&quot;TSAnips&quot;&gt;X. He, D. Cai, P. Niyogi, [http://books.nips.cc/papers/files/nips18/NIPS2005_0249.pdf Tensor subspace analysis], in: [[Advances in Neural Information Processing Systems]]c 18 (NIPS), 2005.&lt;/ref&gt;  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor [[image]]s (2D/3D), [[video]] sequences (3D/4D), and [[Hyperspectral imaging|hyperspectral cubes]] (3D/4D).

The mapping from a [[high-dimensional vector space]] to a set of lower dimensional [[vector space|vector spaces]] is a [[Multilinear subspace learning#Multilinear projection|multilinear projection]].&lt;ref name=&quot;Vasilescu2007&quot;&gt;{{cite conference
 |first=M.A.O. |last=Vasilescu
 |first2=D. |last2=Terzopoulos
 |title=Multilinear Projection for Appearance-Based Recognition in the Tensor Framework
 |conference=IEEE 11th [[International Conference on Computer Vision]]
 |pages=1–8 |year=2007
 |doi=10.1109/ICCV.2007.4409067
}}&lt;/ref&gt;&lt;ref name=&quot;MSLsurvey&quot;/&gt;

[[Multilinear subspace learning#Algorithms|Multilinear subspace learning algorithms]] are higher-order generalizations of [[linear subspace]] learning methods such as [[principal component analysis]] (PCA), [[independent component analysis]] (ICA), [[linear discriminant analysis]] (LDA) and [[canonical correlation|canonical correlation analysis]] (CCA).

== Background ==
With the advances in [[data acquisition]] and [[Computer data storage|storage technology]], [[big data]] (or massive data sets) are being generated on a daily basis in a wide range of emerging applications. Most of these big data are multidimensional. Moreover, they are usually very-[[high-dimensional]], with a large amount of redundancy, and only occupying a part of the input space. Therefore, [[Dimension reduction|dimensionality reduction]] is frequently employed to map [[high-dimensional data]] to a low-dimensional space while retaining as much information as possible.

[[Linear subspace]] learning algorithms are traditional dimensionality reduction techniques that represent input data as [[Coordinate vector|vector]]s and solve for an optimal [[linear mapping]] to a lower-dimensional space. Unfortunately, they often become inadequate when dealing with massive multidimensional data. They result in very-high-dimensional vectors, lead to the estimation of a large number of parameters.&lt;ref name=&quot;Vasilescu2003&quot;/&gt;&lt;ref name=&quot;MSLsurvey&quot;/&gt;&lt;ref name=&quot;TSAnips&quot;/&gt;&lt;ref name=&quot;MPCA-Lu2008&quot;&gt;H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &quot;[https://dx.doi.org/10.1109/TNN.2007.901277 MPCA: Multilinear principal component analysis of tensor objects],&quot; IEEE Trans. Neural Netw., vol. 19, no. 1, pp. 18–39, January  2008.&lt;/ref&gt;&lt;ref name=&quot;DATER&quot;&gt;S. Yan, D. Xu, Q. Yang, L. Zhang, X. Tang, and H.-J. Zhang, &quot;[http://portal.acm.org/citation.cfm?id=1068959 Discriminant analysis with tensor representation],&quot; in Proc. [[IEEE Conference on Computer Vision and Pattern Recognition]], vol. I, June 2005, pp. 526–532.&lt;/ref&gt;

Multilinear Subspace Learning employ different types of data tensor analysis tools for dimensionality reduction.  Multilinear Subspace learning can be applied to observations whose measurements were vectorized and organized into a data tensor,&lt;ref name=&quot;Vasilescu2003&quot;/&gt; or whose measurements are treated as a matrix and concatenated into a tensor.&lt;ref&gt;{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}&lt;/ref&gt;

== Algorithms ==

=== [[Multilinear PCA|Multilinear Principal Component Analysis]] ===
Historically, [[Multilinear PCA|Multilinear Principal Component Analysis]] has been referred to as &quot;M-mode PCA&quot;, a terminology which was coined by Peter Kroonenberg.&lt;ref name=&quot;Kroonenberg1980&quot;&gt;P. M. Kroonenberg and J. de Leeuw, [http://www.springerlink.com/content/c8551t1p31236776/ Principal component analysis of three-mode data by means of alternating least squares algorithms], Psychometrika, 45 (1980), pp. 69–97.&lt;/ref&gt;  In 2005, [[M. Alex O. Vasilescu|Vasilescu]] and [[Demetri Terzopoulos|Terzopoulos]] introduced the Multilinear PCA&lt;ref name=&quot;MPCA-MICA2005&quot;&gt;M. A. O. Vasilescu, D. Terzopoulos (2005) [http://www.media.mit.edu/~maov/mica/mica05.pdf &quot;Multilinear Independent Component Analysis&quot;], &quot;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’05), San Diego, CA, June 2005, vol.1, 547–553.&quot;&lt;/ref&gt; terminology as a way to better differentiate between linear tensor decompositions and multilinear tensor decomposition, as well as, to better differentiate between analysis approaches that computed 2nd order statistics associated with each data tensor mode(axis)s,&lt;ref name=&quot;Vasilescu2003&quot;/&gt;&lt;ref name=&quot;Vasilescu2002tensorfaces&quot;/&gt;&lt;ref name=&quot;Vasilescu2002hms&quot;/&gt;&lt;ref name=&quot;MPCA-Lu2008&quot;/&gt;&lt;ref name=&quot;Vasilescu2004&quot;&gt;M.A.O. Vasilescu, D. Terzopoulos (2004) [http://www.media.mit.edu/~maov/tensortextures/Vasilescu_siggraph04.pdf &quot;TensorTextures: Multilinear Image-Based Rendering&quot;, M. A. O. Vasilescu and D. Terzopoulos, Proc. ACM SIGGRAPH 2004 Conference Los Angeles, CA, August, 2004, in Computer Graphics Proceedings, Annual Conference Series, 2004, 336–342. ]&lt;/ref&gt; and subsequent work on Multilinear Independent Component Analysis&lt;ref name=&quot;MPCA-MICA2005&quot;/&gt; that computed higher order statistics associated with each tensor mode/axis. MPCA is an extension of [[Principal Component Analysis|PCA]].

=== [[Multilinear ICA|Multilinear Independent Component Analysis]] ===
[[Multilinear Independent Component Analysis]]&lt;ref name=&quot;MPCA-MICA2005&quot;/&gt; is an extension of [[Independent Component Analysis|ICA]].

=== Multilinear Linear Descriminant Analysis ===
*Multilinear extension of [[linear discriminant analysis|LDA]]
**TTP-based: Discriminant Analysis with Tensor Representation (DATER)&lt;ref name=&quot;DATER&quot;/&gt;
**TTP-based: General tensor discriminant analysis (GTDA)&lt;ref&gt;D. Tao, X. Li, X. Wu, and S. J. Maybank, &quot;[https://dx.doi.org/10.1109/TPAMI.2007.1096  General tensor discriminant analysis and gabor features for gait recognition],&quot; IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 10, pp. 1700–1715, October  2007.&lt;/ref&gt;
**TVP-based: Uncorrelated Multilinear Discriminant Analysis (UMLDA)&lt;ref&gt;H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &quot;[https://dx.doi.org/10.1109/TNN.2008.2004625 Uncorrelated multilinear discriminant analysis with regularization and aggregation for tensor object recognition],&quot; IEEE Trans. Neural Netw., vol. 20, no. 1, pp. 103–123, January  2009.&lt;/ref&gt;

=== Multilinear canonical correlation analysis ===
*Multilinear extension of [[canonical correlation analysis|CCA]]
**TTP-based: Tensor Canonical Correlation Analysis (TCCA)&lt;ref&gt;
T.-K. Kim and R. Cipolla. &quot;[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4547427  Canonical correlation analysis of video volume tensors for action categorization and detection],&quot;  IEEE Trans. Pattern Anal. Mach. Intell.,  vol. 31, no. 8, pp. 1415–1428, 2009.&lt;/ref&gt;
**TVP-based: Multilinear Canonical Correlation Analysis (MCCA)&lt;ref&gt;H.  Lu, &quot;[http://www.dsp.utoronto.ca/~haiping/Publication/MCCA_IJCAI2013.pdf Learning Canonical Correlations of Paired Tensor Sets via Tensor-to-Vector Projection],&quot; Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013), Beijing, China, August 3–9, 2013.&lt;/ref&gt;
**TVP-based: Bayesian Multilinear Canonical Correlation Analysis (BMTF)&lt;ref&gt;{{Cite book|url=https://link.springer.com/chapter/10.1007/978-3-662-44848-9_42|title=Machine Learning and Knowledge Discovery in Databases|last=Khan|first=Suleiman A.|last2=Kaski|first2=Samuel|date=2014-09-15|publisher=Springer Berlin Heidelberg|isbn=9783662448472|editor-last=Calders|editor-first=Toon|series=Lecture Notes in Computer Science|pages=656–671|language=en|doi=10.1007/978-3-662-44848-9_42|editor-last2=Esposito|editor-first2=Floriana|editor-last3=Hüllermeier|editor-first3=Eyke|editor-last4=Meo|editor-first4=Rosa}}&lt;/ref&gt;

*A TTP is a direct projection of a high-dimensional tensor to a low-dimensional tensor of the same order, using ''N'' projection matrices for an ''N''th-order tensor. It can be performed in ''N'' steps with each step performing a tensor-matrix multiplication (product). The ''N'' steps are exchangeable.&lt;ref name=&quot;HOSVD&quot;&gt;L.D.  Lathauwer,  B.D.  Moor,  J.  Vandewalle,  [http://portal.acm.org/citation.cfm?id=354398 A  multilinear  singular  value decomposition], SIAM Journal of Matrix Analysis and Applications vol. 21, no. 4, pp. 1253–1278, 2000&lt;/ref&gt; This projection is an extension of the [[higher-order singular value decomposition]]&lt;ref name=&quot;HOSVD&quot;/&gt; (HOSVD) to subspace learning.&lt;ref name=&quot;MPCA-Lu2008&quot;/&gt; Hence, its origin is traced back to the [[Tucker decomposition]]&lt;ref&gt;{{Cite journal
 | author = [[Ledyard R Tucker]]
 | title = Some mathematical notes on three-mode factor analysis
 | journal = [[Psychometrika]]
 | volume = 31
 | issue = 3
 |date=September 1966
 | doi = 10.1007/BF02289464
 | pages = 279–311
}}&lt;/ref&gt; in 1960s.

*A TVP is a direct projection of a high-dimensional tensor to a low-dimensional vector, which is also referred to as the rank-one projections. As TVP projects a tensor to a vector, it can be viewed as multiple projections from a tensor to a scalar. Thus, the TVP of a tensor to a ''P''-dimensional vector consists of ''P'' projections from the tensor to a scalar. The projection from a tensor to a scalar is an elementary multilinear projection (EMP). In EMP, a tensor is projected to a point through ''N'' unit projection vectors. It is the projection of a tensor on a single line (resulting a scalar), with one projection vector in each mode. Thus, the TVP of a tensor object to a vector in a ''P''-dimensional vector space consists of ''P'' EMPs. This projection is an extension of the [[CP decomposition|canonical decomposition]],&lt;ref&gt;{{Cite journal
 | author = J. D. Carroll &amp; J. Chang
 | title = Analysis of individual differences in multidimensional scaling via an ''n''-way generalization of 'Eckart–Young' decomposition
 | journal = [[Psychometrika]]
 | volume = 35
 | pages = 283–319
 | year = 1970
 | doi = 10.1007/BF02310791
}}&lt;/ref&gt; also known as the [[PARAFAC|parallel factors]] (PARAFAC) decomposition.&lt;ref&gt;R. A. Harshman, [http://publish.uwo.ca/~harshman/wpppfac0.pdf Foundations of the PARAFAC procedure: Models and conditions for an &quot;explanatory&quot; multi-modal factor analysis]. UCLA Working Papers in Phonetics, 16, pp. 1–84, 1970.&lt;/ref&gt;

=== Typical approach in MSL ===
There are ''N'' sets of parameters to be solved, one in each mode. The solution to one set often depends on the other sets (except when ''N=1'', the linear case). Therefore, the suboptimal iterative procedure in&lt;ref&gt;L.  D.  Lathauwer,  B.  D.  Moor,  J.  Vandewalle,  [http://portal.acm.org/citation.cfm?id=354405 On  the  best  rank-1  and rank-(R1, R2, ..., RN ) approximation of higher-order tensors], SIAM Journal of Matrix Analysis and Applications 21 (4) (2000) 1324–1342.&lt;/ref&gt; is followed.

#Initialization of the projections in each mode
#For each mode, fixing the projection in all the other mode, and solve for the projection in the current mode.
#Do the mode-wise optimization for a few iterations or until convergence.

This is originated from the alternating least square method for multi-way data analysis.&lt;ref name=&quot;Kroonenberg1980&quot;/&gt;

=== Pros and cons ===
[[File:Compactness Comparison of Linear and Multilinear Projections.png|right|thumb|360px|This figure compares the number of parameters to be estimated for the same amount of [[dimension reduction]] by vector-to-vector projection (VVP), (i.e., linear projection,) tensor-to-vector projection (TVP), and tensor-to-tensor projection (TTP).  Multilinear projections require much fewer parameters and the representations obtained are more compact. (This figure is produced based on Table 3 of the survey paper&lt;ref name=&quot;MSLsurvey&quot;/&gt;)]]
The advantages of MSL are:&lt;ref name=&quot;MSLsurvey&quot;/&gt;&lt;ref name=&quot;TSAnips&quot;/&gt;&lt;ref name=&quot;MPCA-Lu2008&quot;/&gt;&lt;ref name=&quot;DATER&quot;/&gt;

*It preserves the structure and correlation in the original data before projection by operating on natural tensorial representation of multidimensional data.
*It can learn more compact representations than its linear counterpart. It needs to estimate a much smaller number of parameters and it has fewer problems in the small sample size scenario.
*It can handle big tensor data more efficiently with computations in much lower dimensions than linear methods. Thus, it leads to lower demand on computational resources.

The disadvantages of MSL are:&lt;ref name=&quot;MSLsurvey&quot;/&gt;&lt;ref name=&quot;TSAnips&quot;/&gt;&lt;ref name=&quot;MPCA-Lu2008&quot;/&gt;&lt;ref name=&quot;DATER&quot;/&gt;

* Most MSL algorithm are iterative. They may be affected by initialization method and have convergence problem.
* The solution obtained is [[local optimum]].

== Pedagogical resources ==
* '''Survey''': [https://dx.doi.org/10.1016/j.patcog.2011.01.004 A survey of multilinear subspace learning for tensor data] ([http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf open access version]).
* '''Lecture''': [http://videolectures.net/icml08_lu_ump/ Video lecture on UMPCA] at the 25th International Conference on Machine Learning (ICML 2008).

== Code ==
* [http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/ MATLAB Tensor Toolbox] by [[Sandia National Laboratories]].
* [http://www.mathworks.com/matlabcentral/fileexchange/26168 The MPCA algorithm written in Matlab (MPCA+LDA included)].
* [http://www.mathworks.com/matlabcentral/fileexchange/35432 The UMPCA algorithm written in Matlab (data included)].
* [http://www.mathworks.fr/matlabcentral/fileexchange/35782 The UMLDA algorithm written in Matlab (data included)].

== Tensor data sets ==
* 3D gait data (third-order tensors): [http://www.dsp.utoronto.ca/~haiping/CodeData/USFGait17_128x88x20.zip 128x88x20(21.2M)]; [http://www.dsp.utoronto.ca/~haiping/CodeData/USFGait17_64x44x20.zip 64x44x20(9.9M)]; [http://www.dsp.utoronto.ca/~haiping/CodeData/USFGait17_32x22x10.zip 32x22x10(3.2M)];

== See also ==
*[[CP decomposition]]
*[[Dimension reduction]]
*[[Multilinear algebra]]
*[[Multilinear PCA|Multilinear Principal Component Analysis]]
*[[Tensor]]
*[[Tensor decomposition]]
*[[Tensor software]]
*[[Tucker decomposition]]

== References ==
{{Reflist|2}}




</text>
      <sha1>rudhf5l6l4ues0hi1rqhy1q8grikiyg</sha1>
    </revision>
  </page>
  <page>
    <title>Bing Predicts</title>
    <ns>0</ns>
    <id>50646178</id>
    <revision>
      <id>770354997</id>
      <parentid>769548148</parentid>
      <timestamp>2017-03-14T22:23:24Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>/* History */cleanup including punct move per WP:REFPUNCT using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3746">{{Notability|Products|date=October 2016}}
'''Bing Predicts''' is a prediction engine developed by [[Microsoft]] that uses [[machine learning]] from data on trending [[social media]] topics (and [[Market sentiment|sentiment]] towards those topics), along with trending searches on [[Bing (search engine)|Bing]]. It predicts the outcomes of [[election|political election]]s, popular [[reality show]]s, and major [[sporting event]]s. Predictions can be accessed through the [[Bing (search engine)|Bing]] search engine.&lt;ref name=&quot;microsoft1&quot;&gt;{{cite web|last=Chen |first=David |url=http://blogs.microsoft.com/next/2014/06/11/how-does-bing-predict-the-future/#sm.000ppef2l11fqd3mw1o1sgbrjx04w |title=How does Bing predict the future? - Next at Microsoft |publisher=Blogs.microsoft.com |date=2014-06-11 |accessdate=2016-05-26}}&lt;/ref&gt;

==History==
The idea for a prediction engine was first suggested by Walter Sun, Development Manager for the Core Ranking team at Bing, when he noticed that school districts were more frequently searched before a major weather event in the area was forecasted, because searchers wanted to find out if a closing or delay was caused. He concluded that the time and location of major weather events could accurately be predicted without referring to a [[weather forecast]] by observing major increases in search frequency of school districts in the area. This inspired [[Bing (search engine)|Bing]] to use its search data to infer outcomes of certain events, such as winners of [[reality show]]s.&lt;ref name=&quot;microsoft1&quot;/&gt; Bing Predicts launched on April 21, 2014. The first reality shows to be featured on Bing Predicts were ''[[The Voice (U.S. TV series)|The Voice]]'', ''[[American Idol]]'', and ''[[Dancing with the Stars]]''.&lt;ref&gt;{{cite web|url=http://blogs.bing.com/search/2014/04/21/predictions-with-bing/ |title=Predictions with Bing &amp;#124; Bing Search Blog |publisher=Blogs.bing.com |date= |accessdate=2016-05-26}}&lt;/ref&gt;

The prediction accuracy for Bing Predicts is 80% for ''American Idol'', and 85% for ''The Voice''. Bing Predicts also predicts the outcomes of major political elections in the United States. Bing Predicts had 97% accuracy for the [[United States Senate elections, 2014|2014 United States Senate elections]], 96% accuracy for the [[United States House of Representatives elections, 2014|2014 United States House of Representatives elections]], and an 89% accuracy for the [[United States gubernatorial elections, 2014|2014 United States gubernatorial elections]]. Bing Predicts is also making predictions for the results of the 2016 United States presidential primaries.&lt;ref&gt;{{cite web|url=http://www.bing.com/explore/predicts |title=Bing predicts |publisher=Bing.com |date=2015-03-15 |accessdate=2016-05-26}}&lt;/ref&gt; It has also done predictions in sports, including a perfect 15 for 15 in the 2014 World Cup,&lt;ref&gt;{{cite web|url=http://www.geekwire.com/2014/microsoft-bing-15-16-world-cup/ |title=Microsoft Bing beats Google in World Cup predictions}}&lt;/ref&gt; leading to positive press such as a Business Insider story on its successes&lt;ref&gt;{{cite web|url=http://www.businessinsider.com/how-microsoft-got-so-good-at-predicting-who-will-win-nfl-games-2015-10 | title=How Microsoft Got So Good at Predicting who will win NFL game}}&lt;/ref&gt; and a PC World article on how Microsoft CEO Satya Nadella did well in his March Madness bracket entry.&lt;ref&gt;{{cite web|url=http://www.pcworld.com/article/2906714/bing-reigns-supreme-in-march-madness-picks-as-satya-nadella-destroys-competition.html | title=Bing reigns supreme in March Madness}}&lt;/ref&gt;

==References==
{{reflist}}

{{Bing}}




[[Category:Computer-related introductions in 2014]]</text>
      <sha1>ra3ifuq7ncn1eym2ipkp792q3nyreac</sha1>
    </revision>
  </page>
  <page>
    <title>Movidius</title>
    <ns>0</ns>
    <id>49786340</id>
    <revision>
      <id>806745541</id>
      <parentid>805357396</parentid>
      <timestamp>2017-10-23T22:07:36Z</timestamp>
      <contributor>
        <username>IonPike</username>
        <id>32220741</id>
      </contributor>
      <comment>/* Myriad X */ section is copied from the source and also reads like a promotion without a lot of substance</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12115">{{Infobox company
| name = Movidius
| logo =
| type = Subsidiary
| fate = Acquired by [[Intel]]
| predecessor = &lt;!-- or: | predecessors = --&gt;
| successor = &lt;!-- or: | successors = --&gt;
| founded = {{Start date and age|2005}}
| founder = &lt;!-- or: | founders = --&gt;
| defunct = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| hq_location_city = [[San Mateo, California]]

| key_people =
| products = [[Computer vision]] and [[deep-learning]] processor chips
| owner = [[Intel]]
| num_employees =
| num_employees_year = &lt;!-- Year of num_employees data (if known) --&gt;
| parent =
| website = &lt;!-- {{URL|example.com}} --&gt;
}}

'''Movidius''' is a company based in [[San Mateo, California]] that designs specialised [[low-power processor]] chips for [[computer vision]] and [[deep-learning]]. It was announced that the company was to be acquired by [[Intel]] in September 2016.

==History==
Movidius was co-founded in [[Dublin]] in 2005 by Sean Mitchell and Dr. David Moloney, with Dr. Valentin Muresan heading up the Timisoara (Romania) design-centre and Martin Mellody as VP of SW Engineering.&lt;ref name=&quot;Irish-Times&quot;&gt;{{cite web|last1=Newenham|first1=Pamela|title=Sean Mitchell and David Moloney, Movidius|url=http://www.irishtimes.com/business/sean-mitchell-and-david-moloney-movidius-1.1798580|website=The Irish Times|accessdate=15 March 2016}}&lt;/ref&gt;&lt;ref name=&quot;Irishbusiness-founding&quot;&gt;{{cite web|title=There are 100 jobs coming at this cutting-edge Irish company|url=http://businessetc.thejournal.ie/movidius-funding-2047538-Apr2015/|website=Thejournal.ie|accessdate=15 March 2016}}&lt;/ref&gt;

David and Sean's vision for the company when they set it up was nothing less than to revolutionise the fabless semiconductor industry which until that point was focused on building processor based SoCs (System on Chip) with hardware acceleration to do most of the intensive processing which couldn't be handled by the CPU.

Fergal Connor (SHAVE processor architect) and Cormac Brick (formerly VP of SW Engineering and now Director of Machine Intelligence Group or MIG) joined the company in late 2006 and Brendan Barry (VP of HW Engineering) joined in March 2007 to head up Movidius SoC development. John Bourke joined the company as CFO in 2008 and spearheaded Movidius fundraising activities along with Sean Mitchell. Paul Costigan joined the company as COO in 2009 and ran the Movidius Hong Kong operation for the duration of his tenure in Movidius.

Between 2006 and 2016, it raised nearly $90 million in capital funding.&lt;ref&gt;http://www.economist.com/technology-quarterly/2016-03-12/after-moores-law#section-3&lt;/ref&gt; In May, 2013 the company appointed [[Remi El-Ouazzane]] as CEO.&lt;ref&gt;{{cite news|title=Movidius Raises $16 Million to Boost Augmented Reality Portfolios|url=http://siliconangle.com/blog/2013/07/10/movidius-raises-16-million-to-boost-augmented-reality-portfolios/|accessdate=4 August 2016|publisher=SiliconAngle|date=July 10, 2013}}&lt;/ref&gt; In January, 2016 the company announced a partnership with [[Google]].&lt;ref name=&quot;Google-collab&quot;&gt;{{cite web|last1=Weckler|first1=Adrian|title=Dublin tech firm Movidius to power Google's new virtual reality headset|url=http://www.independent.ie/business/technology/news/dublin-tech-firm-movidius-to-power-googles-new-virtual-reality-headset-34449883.html|website=Independent.ie|accessdate=15 March 2016}}&lt;/ref&gt; Movidius has been active in the Google [[Project Tango]] project.&lt;ref&gt;{{Cite web|url=https://www.theverge.com/2016/3/16/11242578/movidius-myriad-2-chip-computer-vision-dji-phantom-4|title=The chipmaker behind Google’s project Tango is powering DJI’s autonomous drone|last=Popper|first=Ben|date=2016-03-16|website=The Verge|access-date=2017-07-22}}&lt;/ref&gt; Movidius announced a planned acquisition by [[Intel]] in September 2016.&lt;ref&gt;http://www.movidius.com/news/ceo-post-september-2016&lt;/ref&gt;

In 2016, the company was ranked #24 on the [[Deloitte Fast 500#Fast 500 North America| Deloitte Fast 500 North America]] list.&lt;ref&gt;{{cite web|url=https://www2.deloitte.com/content/dam/Deloitte/us/Documents/technology-media-telecommunications/us-tmt-2016-fast-500-winners-by-rank.pdf|title=2016 Winners by rank|publisher=[[Deloitte]]|accessdate=14 October 2017}}&lt;/ref&gt;

==Products==

===ISAAC===

After the first year of feasibility study on accelerating [[game-physics]], Movidius joined the Hothouse [[business incubator|incubation]] programme in [[DIT Dublin Docklands]] centre and started to ramp up the [[IC design team]] to get the first test chip “ISAAC” off the ground. The ISAAC design was already very advanced by the time the company took on the first external funding with investment from Enterprise Ireland in August 2007 followed by an investment round from angel investors in November 2007. ISAAC was a fully functional SoC with all of the interfaces necessary to fully test the core communications and computation subsystem and contained eight SHAVE v1.0 processors and a LEON3 RISC as system controller with peripherals and caches. The ISAAC test chip was taped out at the end of 2007 and we had working silicon back on [[65nm HCMOS LP process technology]] from [[TSMC]] by March 2008. While the chip design was going on, the software team were working to build development tools and the [[game physics engine software]]. At the time it was calculated that the ISAAC testchip had cost around $1M to build starting from a blank sheet of paper including the SHAVE processor and CMX multiported memory subsystem and software tools. During the timeframe much more straightforward conventional SoCs were being built using licensed IP in competing fabless companies for on the order of $10M.

===Myriad 1===

Using the ISAAC testchip Movidius started to test the market mainly with customers like Samsung and LG in Korea who started the company more towards imaging and video processing applications which in turn fed into the product definition of the production version of ISAAC code-named “SABRE” which eventually became known as Myriad 1. Myriad1 was manufactured in the same TSMC 65&amp;nbsp;nm HCMOS LP process as ISAAC. The entire system ran off a 180&amp;nbsp;MHz clock generated by an onboard PLL as in the case of ISAAC.

The first foray into image and video processing came when the company, together with Toshiba, developed a Myriad1 version in 2010 with a 512Mb stacked DRAM in package and began the development of the first Image Signal Processing (ISP) pipelines and 3D rectification processing for stereoscopic capture from the 2 camera interfaces of Myriad 1 via a pair of MIPI to parallel converter chips from Toshiba which were integrated into a SiP (System in Package) device.

Movidius CTO David Moloney gave the first information on the Myriad1 architecture at HotChips in 2011 which generated a lot of interest in the platform. This interest resulted in Movidius being introduced to Motorola's ATAP division via Kartik Venkataraman, CTO of Pelican Imaging. The relationship with ATAP blossomed, championed by Victor Vedovato and Johnny Lee within Regina Dugan's DARPA-inspired team aimed at building rapid prototypes, in this case using the depth solution developed dual camera module developed using Myriad1.

===Myriad 2===

With input from the team at ATAP Movidius began to define a new architecture aimed at high performance computer vision applications codenamed “Fragrak” (Myriad 2), targeting 28&amp;nbsp;nm technology. The engagement with ATAP was formalised in late 2012 around a computer vision project for object tracking called “Project Pink”. Project Pink eventually became the Google Project “Tango” when the ATAP team were transferred into Google proper.

The strong interest in Myriad2 from Google ATAP and other customers with the skillful negotiation of Sean Mitchell led to renewed VC interest in funding the company which had been struggling financially and ultimately led to a reboot of the company with new VCs in 2013 and with the reboot came a focus on the US market and a Silicon Valley HQ and new CEO.

{{as of| 2016| 02|}}, Movidius's latest Myriad 2 chip is an always-on [[manycore]] [[Vision processing unit]] that can function on power constrained devices.&lt;ref name=&quot;Google-collab&quot;/&gt; It is a [[Heterogeneous computing|heterogeneous architecture]], combining twelve SHAVE (Streaming Hybrid Architecture Vector Engine) 128bit [[VLIW]] [[SIMD]] processors connected to a multiported [[Scratchpad memory]], a pair of LEON4 [[UltraSPARC]] ISA processors for control, and a number of [[fixed function units]] to [[Hardware acceleration|accelerate]] specific [[video processing]] tasks (such as small [[Convolution]]s and color conversion lookups). It includes camera interface hardware, bypassing the need for external memory buffers when handling realtime image inputs. In terms of software, a [[Visual programming language]] allows workflows to be devised, and there is support for [[OpenCL]].

===Neural Compute Stick (NCS)===

''Neural Compute Stick (NCS)'' (originally known as Fathom) is a USB stick containing a Myriad 2 processor, allowing a [[AI accelerator (computer hardware)|vision accelerator]] to be easily added to devices using [[ARM architecture|ARM]] processors including [[PCs]], [[Unmanned aerial vehicle|drones]], [[robot]]s, [[Internet of Things|IoT]] devices and [[video surveillance]] for tasks such as identifying people or objects. It can run between 80 and 150 [[FLOPS|GFLOPS]] performance at below 1W of power. The company switched from a previous [[Die shrink|65nm process]] to a 28&amp;nbsp;nm one to increase its chip’s efficiency by 20-30x. NCS was expected to cost under $100 per unit &lt;ref&gt;{{Cite web|url=http://www.tomshardware.com/news/movidius-fathom-neural-compute-stick,31694.html|title=Deep Learning On A Stick: Movidius' 'Fathom' Neural Compute Stick (Updated)|date=2016-04-28|website=Tom's Hardware|access-date=2016-05-28}}&lt;/ref&gt;. After Intel's acquisition of Movidius, the Movidius™ Neural Compute Stick was released on July 21, 2017 at a cost of $79 in the USA.&lt;ref&gt;{{Cite web|url=https://newsroom.intel.com/news/intel-democratizes-deep-learning-application-development-launch-movidius-neural-compute-stick/|title=intel-democratizes-deep-learning-application-development-launch-movidius-neural-compute-stick|date=July 21, 2017|website=Intel.com}}&lt;/ref&gt;

===Myriad X===
{{Advert section|date=October 2017}}

Myriad X &lt;ref&gt;https://www.movidius.com/myriadx&lt;/ref&gt; was launched on 28 August 2017 and is the third generation and most advanced VPU from Movidius, and Intel company. Myriad X is the first VPU to feature the Neural Compute Engine - a dedicated hardware accelerator for deep neural network inferences. The Neural Compute Engine in conjunction with the 16 powerful SHAVE cores and an ultra-high throughput intelligent memory fabric makes Myriad X the industry leader for on-device deep neural networks and computer vision applications. Myriad X has received additional upgrades to imaging and vision engines including additional programmable SHAVE cores, upgraded and expanded vision accelerators, and a new native 4K ISP pipeline with support for up to 8 HD sensors connecting directly to the VPU.  Myriad X features the all-new Neural Compute Engine - a purpose-built hardware accelerator designed to dramatically increase performance of deep neural networks without compromising the low power characteristics of the Myriad VPU product line. Featuring an array of MAC blocks and directly interfacing with the intelligent memory fabric, the Neural Compute Engine is able to rapidly perform the calculations necessary for deep inference without hitting the so-called &quot;data wall&quot; bottleneck encountered by other processor designs. Combining the neural network performance of the 16 proprietary SHAVE cores with the neural compute engine, Myriad X delivers 10X the performance compared to previous generations*.

==See also==
* [[Vision processing unit]]
* [[MPSoC]]
* [[Coprocessor]]
* [[Convolutional neural network]]

==References==
{{reflist}}



[[Category:Companies based in San Mateo, California]]</text>
      <sha1>fl5niushbjkp6amauyak9xyf2qsvttv</sha1>
    </revision>
  </page>
  <page>
    <title>Timeline of machine learning</title>
    <ns>0</ns>
    <id>50828755</id>
    <revision>
      <id>800982085</id>
      <parentid>798425862</parentid>
      <timestamp>2017-09-16T23:15:17Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23034">This page is a '''timeline of [[machine learning]]'''. Major discoveries, achievements, milestones and other major events are included.

==Overview==

{| class=&quot;wikitable sortable&quot;
|-
! Decade !! Summary
|-
| &lt;1950s|| Statistical methods are discovered and refined.
|-
| 1950s || Pioneering [[machine learning]] research is conducted using simple algorithms.
|-
| 1960s || [[Bayesian method]]s are introduced for [[Bayesian inference|probabilistic inference]] in machine learning&lt;ref&gt;Solomonoff, Ray J. &quot;A formal theory of inductive inference. Part II.&quot; Information and control 7.2 (1964): 224-254.&lt;/ref&gt;.
|-
| 1970s || '[[AI Winter]]' caused by pessimism about machine learning effectiveness.
|-
| 1980s || Rediscovery of [[backpropagation]] causes a resurgence in machine learning research.
|-
| 1990s || Work on machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions — or “learn” — from the results.&lt;ref&gt;{{cite web|last1=Marr|first1=Marr|title=A Short History of Machine Learning - Every Manager Should Read|url=https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/#2a1a75f9323f|website=Forbes|accessdate=28 Sep 2016}}&lt;/ref&gt; [[Support vector machines]] and [[recurrent neural networks]] become popular.
|-
| 2000s || [[Kernel methods]] grow in popularity&lt;ref&gt;Hofmann, Thomas, Bernhard Schölkopf, and Alexander J. Smola. &quot;Kernel methods in machine learning.&quot; The annals of statistics (2008): 1171-1220.&lt;/ref&gt;, and competitive machine learning becomes more widespread&lt;ref&gt;Bennett, James, and Stan Lanning. &quot;The netflix prize.&quot; Proceedings of KDD cup and workshop. Vol. 2007. 2007.&lt;/ref&gt;.
|-
| 2010s || [[Deep learning]] becomes feasible, which leads to machine learning becoming integral to many widely used software services and applications.
|}

==Timeline==

[[File:A simple neural network with two input units and one output unit.png|thumb|A simple neural network with two input units and one output unit]]



&lt;!-- Commented out: [[File:Watson Jeopardy.jpg|thumb|[[Ken Jennings]], Watson, and [[Brad Rutter]] in their Jeopardy! exhibition match]] --&gt;

{| class=&quot;wikitable sortable&quot;
|-
! Year !! Event Type !! Caption !! Event
|-
| 1763 || Discovery || The Underpinnings of [[Bayes' theorem|Bayes' Theorem]] || [[Thomas Bayes]]'s work ''[[An Essay towards solving a Problem in the Doctrine of Chances]]'' is published two years after his death, having been amended and edited by a friend of Bayes, [[Richard Price]].&lt;ref&gt;{{cite journal|last1=Bayes|first1=Thomas|title=An Essay towards solving a Problem in the Doctrine of Chance|journal=Philosophical Transactions|date=1 January 1763|volume=53|pages=370–418|doi=10.1098/rstl.1763.0053|url=http://rstl.royalsocietypublishing.org/content/53/370.full.pdf|accessdate=15 June 2016}}&lt;/ref&gt; The essay presents work which underpins [[Bayes theorem]].
|-
| 1805 || Discovery || Least Squares || [[Adrien-Marie Legendre]] describes the &quot;méthode des moindres carrés&quot;, known in English as the [[least squares]] method.&lt;ref&gt;{{cite book|last1=Legendre|first1=Adrien-Marie|title=Nouvelles méthodes pour la détermination des orbites des comètes|date=1805|publisher=Firmin Didot|location=Paris|page=viii|url=https://books.google.com/books/about/Nouvelles_m%C3%A9thodes_pour_la_d%C3%A9terminati.html?id=FRcOAAAAQAAJ&amp;redir_esc=y|accessdate=13 June 2016|language=French}}&lt;/ref&gt; The least squares method is used widely in [[data fitting]].
|-
| 1812 || || [[Bayes' theorem|Bayes' Theorem]] || [[Pierre-Simon Laplace]] publishes ''Théorie Analytique des Probabilités'', in which he expands upon the work of Bayes and defines what is now known as [[Bayes' Theorem]].&lt;ref&gt;{{cite web|last1=O'Connor|first1=J J|last2=Robertson|first2=E F|title=Pierre-Simon Laplace|url=http://www-history.mcs.st-and.ac.uk/Biographies/Laplace.html|publisher=School of Mathematics and Statistics, University of St Andrews, Scotland|accessdate=15 June 2016}}&lt;/ref&gt;
|-
| 1913 || Discovery || Markov Chains || [[Andrey Markov]] first describes techniques he used to analyse a poem. The techniques later become known as [[Markov chains]].&lt;ref&gt;{{cite journal|last1=Hayes|first1=Brian|title=First Links in the Markov Chain|url=http://www.americanscientist.org/issues/pub/first-links-in-the-markov-chain/|accessdate=15 June 2016|work=American Scientist|issue=March–April 2013|publisher=Sigma Xi, The Scientific Research Society|page=92|doi=10.1511/2013.101.1|quote=Delving into the text of Alexander Pushkin’s novel in verse Eugene Onegin, Markov spent hours sifting through patterns of vowels and consonants. On January 23, 1913, he summarized his findings in an address to the Imperial Academy of Sciences in St. Petersburg. His analysis did not alter the understanding or appreciation of Pushkin’s poem, but the technique he developed—now known as a Markov chain—extended the theory of probability in a new direction.|volume=101}}&lt;/ref&gt;
|-
| 1950 || || Turing's Learning Machine || [[Alan Turing]] proposes a 'learning machine' that could learn and become artificially intelligent. Turing's specific proposal foreshadows [[genetic algorithms]].&lt;ref&gt;{{cite journal|last1=Turing|first1=Alan|title=COMPUTING MACHINERY AND INTELLIGENCE|journal=MIND|date=October 1950|volume=59|issue=236|pages=433–460|doi=10.1093/mind/LIX.236.433|url=http://mind.oxfordjournals.org/content/LIX/236/433|accessdate=8 June 2016}}&lt;/ref&gt;
|-
| 1951 || || First Neural Network Machine || [[Marvin Minsky]] and Dean Edmonds build the first neural network machine, able to learn, the [[Stochastic neural analog reinforcement calculator|SNARC]].&lt;ref&gt;{{Harvnb|Crevier|1993|pp=34–35}} and {{Harvnb|Russell|Norvig|2003|p=17}}&lt;/ref&gt;
|-
| 1952 || || Machines Playing Checkers || [[Arthur Samuel]] joins IBM's Poughkeepsie Laboratory and begins working on some of the very first machine learning programs, first creating programs that play checkers.&lt;ref name=&quot;aaai&quot;&gt;{{cite news|last1=McCarthy|first1=John|last2=Feigenbaum|first2=Ed|title=Arthur Samuel: Pioneer in Machine Learning|url=http://www.aaai.org/ojs/index.php/aimagazine/article/view/840/758|accessdate=5 June 2016|work=AI Magazine|issue=3|publisher=Association for the Advancement of Artificial Intelligence|page=10}}&lt;/ref&gt;
|-
| 1957 || Discovery || Perceptron || [[Frank Rosenblatt]] invents the [[perceptron]] while working at the [[Cornell Aeronautical Laboratory]].&lt;ref&gt;{{cite journal|last1=Rosenblatt|first1=Frank|title=THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN|journal=Psychological Review|date=1958|volume=65|issue=6|pages=386–408|doi=10.1037/h0042519 |url=http://www.staff.uni-marburg.de/~einhaeus/GRK_Block/Rosenblatt1958.pdf}}&lt;/ref&gt; The invention of the perceptron generated a great deal of excitement and widely covered in the media.&lt;ref&gt;{{cite news|last1=Mason|first1=Harding|last2=Stewart|first2=D|last3=Gill|first3=Brendan|title=Rival|url=http://www.newyorker.com/magazine/1958/12/06/rival-2|accessdate=5 June 2016|work=The New Yorker|date=6 December 1958}}&lt;/ref&gt;
|-
| 1967 || || Nearest Neighbor || The nearest neighbor algorithm was created, which is the start of basic pattern recognition. The algorithm was used to map routes.&lt;ref&gt;{{cite web|last1=Marr|first1=Marr|title=A Short History of Machine Learning - Every Manager Should Read|url=https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/#2a1a75f9323f|website=Forbes|accessdate=28 Sep 2016}}&lt;/ref&gt;
|-
| 1969 || || Limitations of Neural Networks || [[Marvin Minsky]] and [[Seymour Papert]] publish their book ''[[Perceptrons (book)|Perceptrons]]'', describing some of the limitations of perceptrons and neural networks. The interpretation that the book shows that neural networks are fundamentally limited is seen as a hindrance for research into neural networks.&lt;ref&gt;{{cite web|last1=Cohen|first1=Harvey|title=The Perceptron|url=http://harveycohen.net/image/perceptron.html|accessdate=5 June 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Colner|first1=Robert|title=A brief history of machine learning|url=http://www.slideshare.net/bobcolner/a-brief-history-of-machine-learning|website=SlideShare|accessdate=5 June 2016}}&lt;/ref&gt;
|-
| 1970 || || Automatic Differentation (Backpropagation)  || [[Seppo Linnainmaa]] publishes the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.&lt;ref name=&quot;lin1970&quot;&gt;[[Seppo Linnainmaa]] (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 6-7.&lt;/ref&gt;&lt;ref name=&quot;lin1976&quot;&gt;[[Seppo Linnainmaa]] (1976). Taylor expansion of the accumulated rounding error. BIT Numerical Mathematics, 16(2), 146-160.&lt;/ref&gt; This corresponds to the modern version of backpropagation, but is not yet named as such.&lt;ref name=&quot;grie2012&quot;&gt;Griewank, Andreas (2012). Who Invented the Reverse Mode of Differentiation?. Optimization Stories, Documenta Matematica, Extra Volume ISMP (2012), 389-400.&lt;/ref&gt;&lt;ref name=&quot;grie2008&quot;&gt;Griewank, Andreas and Walther, A.. Principles and Techniques of Algorithmic Differentiation, Second Edition. SIAM, 2008.&lt;/ref&gt;&lt;ref name=&quot;schmidhuber2015&quot;&gt;[[Jürgen Schmidhuber]] (2015). Deep learning in neural networks: An overview. Neural Networks 61 (2015): 85-117. [http://arxiv.org/abs/1404.7828 ArXiv]&lt;/ref&gt;&lt;ref name=&quot;scholarpedia2015&quot;&gt;[[Jürgen Schmidhuber]] (2015). Deep Learning. Scholarpedia, 10(11):32832. [http://www.scholarpedia.org/article/Deep_Learning#Backpropagation Section on Backpropagation]&lt;/ref&gt;
|-
| 1979 || || Stanford Cart || Students at Stanford University develop a cart that can navigate and avoid obstacles in a room.&lt;ref&gt;{{cite web|last1=Marr|first1=Marr|title=A Short History of Machine Learning - Every Manager Should Read|url=https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/#2a1a75f9323f|website=Forbes|accessdate=28 Sep 2016}}&lt;/ref&gt;
|-
| 1980 || Discovery || Neocognitron || [[Kunihiko Fukushima]] first publishes his work on the [[Neocognitron]], a type of [[artificial neural network]].&lt;ref&gt;{{cite journal|last1=Fukushima|first1=Kunihiko|title=Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern The Recognitron Unaffected by Shift in Position|journal=Biological Cybernetics|date=1980|volume=36|pages=193–202|url=http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf|accessdate=5 June 2016|doi=10.1007/bf00344251|pmid=7370364}}&lt;/ref&gt; [[Neocognitron|Neocognition]] later inspires [[convolutional neural networks]].&lt;ref&gt;{{cite web|last1=Le Cun|first1=Yann|title=Deep Learning|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.6176&amp;rep=rep1&amp;type=pdf|accessdate=5 June 2016}}&lt;/ref&gt;
|-
| 1981 || || Explanation Based Learning || Gerald Dejong introduces Explanation Based Learning, where a computer algorithm analyses data and creates a general rule it can follow and discard unimportant data.&lt;ref&gt;{{cite web|last1=Marr|first1=Marr|title=A Short History of Machine Learning - Every Manager Should Read|url=https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/#2a1a75f9323f|website=Forbes|accessdate=28 Sep 2016}}&lt;/ref&gt;
|-
| 1982 || Discovery || Recurrent Neural Network || [[John Hopfield]] popularizes [[Hopfield networks]], a type of [[recurrent neural network]] that can serve as [[content-addressable memory]] systems.&lt;ref&gt;{{cite journal|last1=Hopfield|first1=John|title=Neural networks and physical systems with emergent collective computational abilities|journal=Proceedings of the National Academy of Sciences of the United States of America|date=April 1982|volume=79|pages=2554–2558|url=http://www.pnas.org/content/79/8/2554.full.pdf|accessdate=8 June 2016|doi=10.1073/pnas.79.8.2554|pmid=6953413|pmc=346238}}&lt;/ref&gt;
|-
| 1985 || || NetTalk || A program that learns to pronounce words the same way a baby does, is developed by Terry Sejnowski.&lt;ref&gt;{{cite web|last1=Marr|first1=Marr|title=A Short History of Machine Learning - Every Manager Should Read|url=https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/#2a1a75f9323f|website=Forbes|accessdate=28 Sep 2016}}&lt;/ref&gt;
|-
| 1986 || Discovery || Backpropagation || The process of [[backpropagation]] is described by [[David Rumelhart]], [[Geoff Hinton]] and [[Ronald J. Williams]].&lt;ref&gt;{{cite journal|last1=Rumelhart|first1=David|last2=Hinton|first2=Geoffrey|last3=Williams|first3=Ronald|title=Learning representations by back-propagating errors|journal=Nature|date=9 October 1986|volume=323|pages=533–536|url=http://elderlab.yorku.ca/~elder/teaching/cosc6390psyc6225/readings/hinton%201986.pdf|accessdate=5 June 2016|doi=10.1038/323533a0}}&lt;/ref&gt;
|-
| 1989 || Discovery || Reinforcement Learning || Christopher Watkins develops [[Q-learning]], which greatly improves the practicality and feasibility of [[reinforcement learning]].&lt;ref&gt;{{cite journal|last1=Watksin|first1=Christopher|title=Learning from Delayed Rewards|date=1 May 1989|url=http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf}}&lt;/ref&gt;
|-
| 1989 || Commercialization || Commercialization of Machine Learning on Personal Computers || Axcelis, Inc. releases [[Evolver (software)|Evolver]], the first software package to commercialize the use of genetic algorithms on personal computers.&lt;ref&gt;{{cite news|last1=Markoff|first1=John|title=BUSINESS TECHNOLOGY; What's the Best Answer? It's Survival of the Fittest|url=https://www.nytimes.com/1990/08/29/business/business-technology-what-s-the-best-answer-it-s-survival-of-the-fittest.html|accessdate=8 June 2016|work=New York Times|date=29 August 1990}}&lt;/ref&gt;
|-
| 1992 || Achievement || Machines Playing Backgammon || Gerald Tesauro develops [[TD-Gammon]], a computer [[backgammon]] program that utilises an [[artificial neural network]] trained using [[temporal-difference learning]] (hence the 'TD' in the name). TD-Gammon is able to rival, but not consistently surpass, the abilities of top human backgammon players.&lt;ref&gt;{{cite journal|last1=Tesauro|first1=Gerald|title=Temporal Difference Learning and TD-Gammon|journal=Communications of the ACM|date=March 1995|volume=38|issue=3|doi=10.1145/203330.203343|url=http://www.bkgm.com/articles/tesauro/tdl.html}}&lt;/ref&gt;
|-
| 1995 || Discovery || Random Forest Algorithm || Tin Kam Ho publishes a paper describing [[Random forest|Random decision forests]].&lt;ref&gt;{{cite journal|last1=Ho|first1=Tin Kam|title=Random Decision Forests|journal=Proceedings of the Third International Conference on Document Analysis and Recognition|date=August 1995|volume=1|pages=278–282|doi=10.1109/ICDAR.1995.598994|url=http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf|accessdate=5 June 2016|publisher=IEEE|location=Montreal, Quebec|isbn=0-8186-7128-9}}&lt;/ref&gt;
|-
| 1997 || || IBM Deep Blue Beats Kasparov || IBM’s Deep Blue beats the world champion at chess.&lt;ref&gt;{{cite web|last1=Marr|first1=Marr|title=A Short History of Machine Learning - Every Manager Should Read|url=https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/#2a1a75f9323f|website=Forbes|accessdate=28 Sep 2016}}&lt;/ref&gt;
|-
| 1995 || Discovery || Support Vector Machines || [[Corinna Cortes]] and [[Vladimir Vapnik]] publish their work on [[support vector machines]].&lt;ref name=&quot;bhml&quot;&gt;{{cite web|last1=Golge|first1=Eren|title=BRIEF HISTORY OF MACHINE LEARNING|url=http://www.erogol.com/brief-history-machine-learning/|website=A Blog From a Human-engineer-being|accessdate=5 June 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Cortes|first1=Corinna|last2=Vapnik|first2=Vladimir|title=Support-vector networks|journal=Machine Learning|date=September 1995|volume=20|issue=3|pages=273–297|doi=10.1007/BF00994018|url=http://download.springer.com/static/pdf/467/art%253A10.1007%252FBF00994018.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2FBF00994018&amp;token2=exp=1465109699~acl=%2Fstatic%2Fpdf%2F467%2Fart%25253A10.1007%25252FBF00994018.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252FBF00994018*~hmac=133f5211871b237411d6dcc05047fc16cdc99abc25ab4e74be863808ea53bfd7|accessdate=5 June 2016|publisher=Kluwer Academic Publishers|issn=0885-6125}}&lt;/ref&gt;
|-
| 1997 || Discovery || LSTM || [[Sepp Hochreiter]] and [[Jürgen Schmidhuber]] invent Long-short term memory recurrent neural networks,&lt;ref&gt;{{cite journal|last1=Hochreiter|first1=Sepp|last2=Schmidhuber|first2=Jürgen|title=LONG SHORT-TERM MEMORY|journal=Neural Computation|date=1997|volume=9|issue=8|pages=1735–1780|url=http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf|doi=10.1162/neco.1997.9.8.1735|pmid=9377276}}&lt;/ref&gt; greatly improving the efficiency and practicality of recurrent neural networks.
|-
| 1998 || || MNIST database || A team led by [[Yann LeCun]] releases the [[MNIST database]], a dataset comprising a mix of handwritten digits from [[American Census Bureau]] employees and American high school students.&lt;ref&gt;{{cite web|last1=LeCun|first1=Yann|last2=Cortes|first2=Corinna|last3=Burges|first3=Christopher|title=THE MNIST DATABASE of handwritten digits|url=http://yann.lecun.com/exdb/mnist/|accessdate=16 June 2016}}&lt;/ref&gt; The MNIST database has since become a benchmark for evaluating handwriting recognition.
|-
| 2002 || || Torch Machine Learning Library || [[Torch (machine learning)|Torch]], a software library for machine learning, is first released.&lt;ref&gt;{{cite journal|last1=Collobert|first1=Ronan|last2=Benigo|first2=Samy|last3=Mariethoz|first3=Johnny|title=Torch: a modular machine learning software library|date=30 October 2002|url=http://www.idiap.ch/ftp/reports/2002/rr02-46.pdf|accessdate=5 June 2016}}&lt;/ref&gt;
|-
| 2006 || || The Netflix Prize || The [[Netflix Prize]] competition is launched by [[Netflix]]. The aim of the competition was to use machine learning to beat Netflix's own recommendation software's accuracy in predicting a user's rating for a film given their ratings for previous films by at least 10%.&lt;ref&gt;{{cite web|title=The Netflix Prize Rules|url=http://www.netflixprize.com/rules|website=Netflix Prize|publisher=Netflix|accessdate=16 June 2016}}&lt;/ref&gt; The prize was won in 2009.
|-
| 2010 || || Kaggle Competition || [[Kaggle]], a website that serves as a platform for machine learning competitions, is launched.&lt;ref&gt;{{cite web|title=About|url=https://www.kaggle.com/about|website=Kaggle|publisher=Kaggle Inc|accessdate=16 June 2016}}&lt;/ref&gt;
|-
| 2011 || Achievement || Beating Humans in Jeopardy || Using a combination of machine learning, [[natural language processing]] and information retrieval techniques, [[IBM]]'s [[Watson (computer)|Watson]] beats two human champions in a [[Jeopardy!]] competition.&lt;ref&gt;{{cite news|last1=Markoff|first1=John|title=Computer Wins on ‘Jeopardy!’: Trivial, It’s Not|url=https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?pagewanted=all&amp;_r=0|accessdate=5 June 2016|work=New York Times|date=17 February 2011|page=A1}}&lt;/ref&gt;
|-
| 2012 || Achievement || Recognizing Cats on YouTube || The [[Google Brain]] team, led by [[Andrew Ng]] and [[Jeff Dean (computer scientist) | Jeff Dean]], create a neural network that learns to recognise cats by watching unlabeled images taken from frames of [[YouTube]] videos.&lt;ref&gt;{{cite journal|last1=Le|first1=Quoc|last2=Ranzato|first2=Marc’Aurelio|last3=Monga|first3=Rajat|last4=Devin|first4=Matthieu|last5=Chen|first5=Kai|last6=Corrado|first6=Greg|last7=Dean|first7=Jeff|last8=Ng|first8=Andrew|title=Building High-level Features Using Large Scale Unsupervised Learning|journal=CoRR|date=12 July 2012|arxiv=1112.6209}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last1=Markoff|first1=John|title=How Many Computers to Identify a Cat? 16,000|url=https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html|accessdate=5 June 2016|work=New York Times|date=26 June 2012|page=B1}}&lt;/ref&gt;
|-
| 2014 || || Leap in Face Recognition || [[Facebook]] researchers publish their work on [[DeepFace]], a system that uses neural networks that identifies faces with 97.35% accuracy. The results are an improvement of more than 27% over previous systems and rivals human performance.&lt;ref&gt;{{cite journal|last1=Taigman|first1=Yaniv|last2=Yang|first2=Ming|last3=Ranzato|first3=Marc’Aurelio|last4=Wolf|first4=Lior|title=DeepFace: Closing the Gap to Human-Level Performance in Face Verification|journal=Conference on Computer Vision and Pattern Recognition|date=24 June 2014|url=https://research.facebook.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/|accessdate=8 June 2016}}&lt;/ref&gt;
|-
| 2014 || || Sibyl || Researchers from [[Google]] detail their work on Sibyl,&lt;ref&gt;{{cite web|last1=Canini|first1=Kevin|last2=Chandra|first2=Tushar|last3=Ie|first3=Eugene|last4=McFadden|first4=Jim|last5=Goldman|first5=Ken|last6=Gunter|first6=Mike|last7=Harmsen|first7=Jeremiah|last8=LeFevre|first8=Kristen|last9=Lepikhin|first9=Dmitry|last10=Llinares|first10=Tomas Lloret|last11=Mukherjee|first11=Indraneel|last12=Pereira|first12=Fernando|last13=Redstone|first13=Josh|last14=Shaked|first14=Tal|last15=Singer|first15=Yoram|title=Sibyl: A system for large scale supervised machine learning|url=https://users.soe.ucsc.edu/~niejiazhong/slides/chandra.pdf|website=Jack Baskin School Of Engineering|publisher=UC Santa Cruz|accessdate=8 June 2016}}&lt;/ref&gt; a proprietary platform for massively parallel machine learning used internally by Google to make predictions about user behavior and provide recommendations.&lt;ref&gt;{{cite news|last1=Woodie|first1=Alex|title=Inside Sibyl, Google’s Massively Parallel Machine Learning Platform|url=http://www.datanami.com/2014/07/17/inside-sibyl-googles-massively-parallel-machine-learning-platform/|accessdate=8 June 2016|work=Datanami|publisher=Tabor Communications|date=17 July 2014}}&lt;/ref&gt;
|-
| 2016 || Achievement || Beating Humans in Go ||Google's [[AlphaGo]] program becomes the first [[Computer Go]] program to beat an unhandicapped professional human player&lt;ref&gt;{{cite web|title=Google achieves AI 'breakthrough' by beating Go champion|url=http://www.bbc.com/news/technology-35420579|website=BBC News|publisher=BBC|accessdate=5 June 2016|date=27 January 2016}}&lt;/ref&gt; using a combination of machine learning and tree search techniques.&lt;ref&gt;{{cite web|title=AlphaGo|url=https://www.deepmind.com/alpha-go.html|website=Google DeepMind|publisher=Google Inc|accessdate=5 June 2016}}&lt;/ref&gt;
|}

==See also==
* [[History of artificial intelligence]]
* [[Machine learning]]
* [[Timeline of artificial intelligence]]
* [[Timeline of machine translation]]

==References==
{{Reflist|30em}}


[[Category:Computing timelines|Machine learning]]</text>
      <sha1>dzjgu9y2tuqskapl3bd9v9sl980tqyk</sha1>
    </revision>
  </page>
  <page>
    <title>Savi Technology</title>
    <ns>0</ns>
    <id>995455</id>
    <revision>
      <id>795625640</id>
      <parentid>769728782</parentid>
      <timestamp>2017-08-15T12:45:27Z</timestamp>
      <contributor>
        <username>Shyamsunder</username>
        <id>800815</id>
      </contributor>
      <comment>-; ±[[Category:Commercial item transport and distribution]]→[[Category:Logistics industry in the United States]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3489">{{multiple issues|
{{Advert|date=April 2009}}
{{Refimprove|date=April 2009}}
}}
{{Infobox company
| name = Savi Technology, Inc.
| logo = Savi Logo.png
| type =
| industry =
| fate =
| predecessor = &lt;!-- or: | predecessors = --&gt;
| successor = &lt;!-- or: | successors = --&gt;
| founded = 1989 in [[Alexandria, Virginia]], [[Virginia]], [[United States]]
| founder = &lt;!-- or: | founders = --&gt;
| defunct = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| hq_location_city =
| hq_location_country =
| area_served = Worldwide
| key_people = William Clark&lt;br /&gt;({{small|President and CEO}})&lt;br /&gt;Brian Daum &lt;br /&gt;({{small|Senior Vice President and CFO}})&lt;br /&gt;Andy Souders&lt;br /&gt;({{small|Senior Vice President, Products and Strategy}})
| products =
| owner = &lt;!-- or: | owners = --&gt;
| num_employees =
| num_employees_year = &lt;!-- Year of num_employees data (if known) --&gt;
| parent =
| website = &lt;!-- {{URL|http://www.savi.com}} --&gt;
}}

== About ==
'''Savi Technology''' was founded in 1989 and is based in [[Alexandria, Virginia]].

Savi provides the most complete Sensor Analytics solutions for organizations that face critical decisions based on the location and status of their assets.

== Solutions ==
'''Savi Technology''' offers sensor analytics solutions for [[logistics]] and [[supply chain]] operations. It tracks shipment locations in real time and applies analytics to accurately predict arrival of goods. The company provides Savi Insight, a solution that offers predictive and prescriptive [[supply chain]] analytics to forecast future outcomes, prevent operational disruptions, and reduce risk; Savi Tracking, a solution that monitors and provides [[operational intelligence]] for [[asset tracking]], journey management, and electronic cargo tracking assets in motion; ETAaaS, a [[SaaS]] analytics solution that processes multiple real-time data sources, [[enterprise resource planning]] (ERP), and historical information; and Savi Now, a mobile application for tracking and tracing high-value assets. It also offers tags that enable organizations to access real-time information on the location, condition, and security status of assets and shipments; fixed and mobile readers; [[radio-frequency identification]] devices and sensors; and portable deployment kits. In addition, the company provides professional services, including program management, systems integration, system and network design, support, and hosting. It serves the U.S. Department of Defense, the U.S. and allied militaries, civilian governmental organizations, and commercial companies, as well as transportation, pharmaceuticals, retail, life sciences, and manufacturing industries worldwide.

== References ==
[http://www.savi.com Savi Company Website]
[https://www.linkedin.com/company/savi-technology LinkedIn]
[https://www.bloomberg.com/research/stocks/private/snapshot.asp?privcapId=110583 Savi Technology Overview]

==External links==
* [http://www.savi.com Official Website]
* [https://www.washingtonpost.com/business/capitalbusiness/washington-area-pops-onto-tech-radar-as-alternative-to-silicon-valley/2014/01/20/b16003c4-6662-11e3-a0b9-249bbb34602c_story.html The Washington Post]
* [https://www.linkedin.com/company/savi-technology LinkedIn]

[[Category:Radio-frequency identification]]




[[Category:Logistics industry in the United States]]
</text>
      <sha1>opmnkwh512m8j1pn7vl11560ey9rufr</sha1>
    </revision>
  </page>
  <page>
    <title>Cognitive robotics</title>
    <ns>0</ns>
    <id>2934910</id>
    <revision>
      <id>794823356</id>
      <parentid>765375497</parentid>
      <timestamp>2017-08-10T07:27:17Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 6 sources and tagging 0 as dead. #IABot (v1.5beta)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8379">{{more footnotes|date=February 2012}}

'''Cognitive robotics''' is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to [[Robot learning|learn]] and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of [[embodied cognitive science]] and [[embodied embedded cognition]].

==Core issues==

While traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. [[philosophy of perception|Perception]] and [[motor cognition|action]] and the notion of [[Mental representation|symbolic representation]] are therefore core issues to be addressed in cognitive robotics.

==Starting point==

Cognitive robotics views animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional [[Artificial Intelligence]] techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of [[intelligent agent]]s in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world.

==Learning techniques==

===Motor Babble===
{{main|Motor babbling}}

A preliminary robot learning technique called [[motor babbling]] involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to ''expect'' a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped.

===Imitation===

Once a robot can coordinate its motors to produce a desired result, the technique of ''learning by imitation'' may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition.

===Knowledge acquisition===

A more complex learning approach is &quot;autonomous knowledge acquisition&quot;: the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed.

A somewhat more directed mode of exploration can be achieved by &quot;curiosity&quot; algorithms, such as Intelligent Adaptive Curiosity&lt;ref&gt;http://www.pyoudeyer.com/ims.pdf&lt;/ref&gt;&lt;ref&gt;http://www.pyoudeyer.com/oudeyer-kaplan-neurorobotics.pdf&lt;/ref&gt; or Category-Based Intrinsic Motivation.&lt;ref&gt;http://science.slc.edu/~jmarshall/papers/cbim-epirob09.pdf&lt;/ref&gt; These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an [[Artificial Neural Network]]) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest.

==Other architectures==

Some researchers in cognitive robotics have tried using architectures such as ([[ACT-R]] and [[Soar (cognitive architecture)]]) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into a set of symbols and their relationships.

==Questions==

Some of the fundamental questions to still be answered in cognitive robotics are:
* How much human programming should or can be involved to support the learning processes?
* How can one quantify progress? Some of the adopted ways is the reward and punishment. But what kind of reward and what kind of punishment? In humans, when teaching a child for example, the reward would be candy or some encouragement, and the punishment can take many forms. But what is an effective way with robots?

== Books ==
Cognitive Robotics book &lt;ref&gt;{{Cite web|title = Cognitive Robotics|url = https://www.crcpress.com/Cognitive-Robotics/Samani/9781482244564|website = CRC Press|accessdate = 2015-10-07}}&lt;/ref&gt; by Hooman Samani,&lt;ref&gt;{{Cite web|title = Hooman Samani|url = http://www.hoomansamani.com/|website = www.hoomansamani.com|accessdate = 2015-10-07}}&lt;/ref&gt; takes a multidiciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects.

==See also==
*[[Artificial intelligence]]
*[[Intelligent agent]]
*[[Cognitive science]]
*[[Cybernetics]]
*[[Developmental robotics]]
*[[Embodied cognitive science]]
*[[Developmental robotics|Epigenetic robotics]]
*[[Evolutionary robotics]]
*[[Hybrid intelligent system]]
*[[Intelligent control]]

== References ==
{{Reflist}}
*[https://web.archive.org/web/20090314232056/http://www.ss-rics.org/ The Symbolic and Subsymbolic Robotic Intelligence Control System (SS-RICS)]
*[https://web.archive.org/web/20060617202124/http://www.cs.uu.nl/groups/IS/robotics/robotics.html Intelligent Systems Group - University of Utrecht]
*[http://www.cs.toronto.edu/cogrobo/main/ The Cognitive Robotics Group - University of Toronto]
* The [[IDSIA]] [http://robotics.idsia.ch/ Robotics Lab] and [http://www.idsia.ch/~juergen/cogbotlab.html Cognitive Robotics Lab] of [[Juergen Schmidhuber]]
*[https://web.archive.org/web/20060218074249/http://www.inl.gov/adaptiverobotics/humanoidrobotics/future.shtml What Does the Future Hold for Cognitive Robots? - Idaho National Laboratory]
*[http://www.nrl.navy.mil/aic/iss/aas/CognitiveRobots.php Cognitive Robotics at the Naval Research Laboratory]
*[http://cogrob.ensta.fr/ Cognitive robotics at ENSTA] autonomous embodied systems, evolving in complex and non-constraint environments, using mainly vision as sensor.
*[https://web.archive.org/web/20070504203412/http://eecs.vanderbilt.edu/CIS/cisHome.shtml The Center for Intelligent Systems - Vanderbilt University]
*[http://www.cor-lab.de/ Institute for Cognition and Robotics (CoR-Lab) at Bielefeld University]
*[http://mmi.tudelft.nl/SocioCognitiveRobotics/ SocioCognitive Robotics at Delft University of Technology]
*[http://www.aslab.upm.es/ Autonomous Systems Laboratory at Universidad Politecnica de Madrid]
*[http://www.ida.liu.se/conferences/cogrob2014/ The Cognitive Robotics Association], founded in 1998, directed by Gerhard Lakemeyer, University of Aachen, organizes every two year the Cognitive Robotics Workshop and it is generously supported by [http://www.journals.elsevier.com/artificial-intelligence/ the AI journal]

== External links ==
* [http://www.defpro.com/news/details/4056/ iRobis Announces Complete Cognitive Software System for Robots]
* [http://www.xpero.org The Xpero project]
* [[iCub]]
* [http://www.irobis.com/ Institute of Robotics in Scandinavia AB] ([[iRobis]])
* [https://web.archive.org/web/20070619021836/http://mensnewsdaily.com/2007/05/16/robobusiness-robots-with-imagination/ RoboBusiness: Robots that Dream of Being Better]
* [https://web.archive.org/web/20071008234511/http://www.conscious-robots.com/en/conscious-machines/the-field-of-machine-consciousness/cognitive-rob.html www.Conscious-Robots.com]
* [http://mailman.rwth-aachen.de/mailman/listinfo/cogrob-sc The cognitive Robotics Association]
{{Robotics}}



</text>
      <sha1>o2hh96qju7p20g9sou2vvh860vvspq6</sha1>
    </revision>
  </page>
  <page>
    <title>Dataiku</title>
    <ns>0</ns>
    <id>51112472</id>
    <revision>
      <id>807886562</id>
      <parentid>800762595</parentid>
      <timestamp>2017-10-30T17:22:23Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: removing misplaced special [[no-break space]] character</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5157">{{Infobox company
| name = Dataiku
| logo = Dataiku_logo.png
| logo_size = 250
| native_name_lang = fr
| former_name =
| type = [[Privately held company|Private]]
| industry =
| founded = {{start date and age|2013|02|14}} in [[Paris]], [[France]]
| founder =  Florian Douetteau, Clément Stenac, Marc Batty, Thomas Cabrol
| location_city = [[New York City]]
| area_served = [[United States|U.S.]], [[France]], [[U.K.]], [[Germany]]
| key_people =
| products = Dataiku Data Science Studio
| services =
| owner = Battery Ventures, FirstMark Capital, Serena Capital, Alven Capital
| num_employees =100
| num_employees_year = 2017
| website = {{URL|http://www.dataiku.com/}}
| location_country = [[United States]]
}}

'''Dataiku''' is a [[computer software]] company headquartered in [[New York City]]. The company develops collaborative [[data science]] software marketed for [[big data]].

== History ==

The company was founded in [[Paris]] in 2013 by 4 co-founders. Two of them met while working at French [[search engine]] company [[Exalead]], including chief executive Florian Douetteau, and Clément Sténac.&lt;ref&gt;{{cite web |url= http://www.rudebaguette.com/2013/10/10/paris-exalead-mafia/|title= Paris has its own Paypal Mafia: How Exalead spawned dozens of startups|author= |date= 10 October 2013|author1=Liam Boogar|website=RudeBaguette|language=en}}&lt;/ref&gt;

For its first two years, the company relied on its own capital. In January 2015, Dataiku raised $3.6 million from Serena Capital and Alven Capital, two French technology [[venture capital]] funds.&lt;ref name=&quot;TechCrunch&quot;&gt;{{cite web |url=https://techcrunch.com/2015/01/19/french-startup-dataiku-grabs-3-6m-to-keep-developing-big-data-software/|title= French Startup Dataiku Grabs $3.6M To Continue Developing Big Data Software|author= |date= 19 January 2015|author1=Ron Miller|website= TechCrunch|language=en}}&lt;/ref&gt; This was followed by $14 million raised with [[FirstMark Capital]], a New York City-based [[venture capital]] firm in October 2016.&lt;ref name=&quot;TechCrunch2016&quot;&gt;{{cite web |url=https://techcrunch.com/2016/10/25/dataiku-grabs-14-million-for-its-collaborative-data-science-platform/|title= Dataiku grabs $14 million for its collaborative data science platform|author= |date= 25 October 2016|author1=Romain Dillet|website= TechCrunch|language=en}}&lt;/ref&gt; In September 2017 the company raised a $28 million Series B investment from [[Battery Ventures]], as well as historic investors.&lt;ref&gt;{{Cite news|url=https://techcrunch.com/2017/09/06/dataiku-to-enhance-data-tools-with-28-million-infusion-led-by-battery-ventures/|title=Dataiku to enhance data tools with $28 million investment led by Battery Ventures|last=Miller|first=Ron|work=TechCrunch|access-date=2017-09-15|language=en}}&lt;/ref&gt;

Dataiku opened an office in [[New York City]] in 2015&lt;ref&gt;{{cite web |url= http://www.businesswire.com/news/home/20160317005806/en/Dataiku-developer-Advanced-Big-Data-Analytics-Software|title= Dataiku, developer of Advanced Big Data Analytics Software, hires two new VP’s and continues expansion in North America|author= |date= 17 March 2016|website=Business Wire|language=en}}&lt;/ref&gt; which became the company headquarters&lt;ref&gt;{{Cite web|url=http://www.bigdatanews.datasciencecentral.com/group/bdn-daily-press-releases/forum/topics/dataiku-unveils-new-headquarters-in-downtown-new-york|title=Dataiku unveils new headquarters in downtown New York|website=www.bigdatanews.datasciencecentral.com|language=en|access-date=2017-09-15}}&lt;/ref&gt;. They opened an office in London in the summer of 2016&lt;ref&gt;{{Cite web|url=https://www.cnbc.com/video/2016/07/22/theres-a-big-window-of-opportunity-in-the-uk-cto.html|title=There's a big window of opportunity in the UK: CTO|website=CNBC|access-date=2017-09-15}}&lt;/ref&gt;.

The software Dataiku Data Science Studio (DSS) was announced in 2014, supporting [[predictive modelling]] to build business applications.&lt;ref name=&quot;TechCrunch&quot; /&gt; Later versions of DSS added other features.&lt;ref&gt;{{cite web |url=http://www.cioreview.com/news/dataiku-launches-dss-3-for-advanced-data-metrics-monitoring-nid-13769-cid-84.html|title= Dataiku Launches DSS 3 for Advanced Data Metrics Monitoring|author= |date= 19 May 2016|website= CIO Review|language=en}}&lt;/ref&gt;

Dataiku offers a free edition and enterprise versions with additional features, such as multi-user collaboration or real-time scoring.

In 2017, Dataiku entered the [[Gartner]] Magic Quadrant for Data Science Platforms as a &quot;visionary&quot;&lt;ref&gt;{{Cite web|url=http://www.kdnuggets.com/2017/02/gartner-2017-mq-data-science-platforms-gainers-losers.html|title=Gartner 2017 Magic Quadrant for Data Science Platforms: gainers and losers|website=www.kdnuggets.com|language=en-US|access-date=2017-09-15}}&lt;/ref&gt;.

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.dataiku.com/}}





[[Category:American companies established in 2013]]
[[Category:French companies established in 2013]]

</text>
      <sha1>kp4rod5ot7hkxqyekf24g4vchm36hd0</sha1>
    </revision>
  </page>
  <page>
    <title>Algorithm selection</title>
    <ns>0</ns>
    <id>50773876</id>
    <revision>
      <id>801858441</id>
      <parentid>801858265</parentid>
      <timestamp>2017-09-22T10:23:04Z</timestamp>
      <contributor>
        <username>Tony1</username>
        <id>332841</id>
      </contributor>
      <comment>[[User:Ohconfucius/script|Script]]-assisted fixes: per , , </comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13006">{{Use dmy dates|date=September 2017}}
{{Orphan|date=August 2016}}

'''Algorithm selection''' (sometimes also called '''per-instance algorithm selection''' or '''offline algorithm selection''') is a meta-algorithmic technique to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, algorithms have different performances. That is, while one algorithm performs well on some instances, it performs poorly on others and vice versa for another algorithm. If we can identify when to use which algorithm, we can get the best of both worlds and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms.

== Definition==

Given a portfolio &lt;math&gt;\mathcal{P}&lt;/math&gt; of algorithms &lt;math&gt;\mathcal{A} \in \mathcal{P}&lt;/math&gt;, a set of instances &lt;math&gt;i \in \mathcal{I}&lt;/math&gt; and a cost metric &lt;math&gt;m: \mathcal{P} \times \mathcal{I} \to \mathbb{R}&lt;/math&gt;, the algorithm selection problem consists of finding a mapping &lt;math&gt;s: \mathcal{I} \to \mathcal{P}&lt;/math&gt; from instances &lt;math&gt;\mathcal{I}&lt;/math&gt; to algorithms &lt;math&gt;\mathcal{P}&lt;/math&gt; such that the cost &lt;math&gt; \sum_{i \in \mathcal{I}} m(s(i),i)&lt;/math&gt; across all instances is optimized.&lt;ref&gt;J. Rice: The Algorithm Selection Problem. Advances in Computers 15: 65–118 (1976)&lt;/ref&gt;&lt;ref&gt;B. Bischl, P. Kerschke, L. Kotthoff, M. Lindauer, Y. Malitsky, A. Frechétte, H. Hoos, F. Hutter, K. Leyton-Brown, K. Tierney and J. Vanschoren. ASlib: A Benchmark Library for Algorithm Selection In: Artificial Intelligence Journal (AIJ) (2016)&lt;/ref&gt;

== Examples==

=== Boolean satisfiability problem (and other hard combinatorial problems)===
A well-known application of algorithm selection is the Boolean satisfiability problem. Here, the portfolio of algorithms is a set of (complementary) SAT solvers, the instances are Boolean formulas, the cost metric is for example average runtime or number of unsolved instances. So, the goal is to select a well-performing SAT solver for each individual instance. In the same way, algorithm selection can be applied to many other &lt;math&gt;\mathcal{NP}&lt;/math&gt;-hard problems (such as [[Linear programming|mixed integer programming]], [[Constraint satisfaction problem|CSP]], [[Automated planning and scheduling|AI planning]], [[Travelling salesman problem|TSP]], [[MAXSAT]], [[QBF]] and [[answer set programming]]). Competition-winning systems in SAT are SATzilla,&lt;ref name=&quot;zilla08&quot;&gt;{{Cite journal|last=L. Xu and F. Hutter and H. Hoos and K. Leyton-Brown|date=2008|title=SATzilla: Portfolio-based Algorithm Selection for SAT|url=|journal=Journal of Artificial Intelligence Research 32|doi=|pmid=|access-date=}}&lt;/ref&gt; 3S&lt;ref&gt;{{Cite journal|author1=S. Kadioglu |author2=Y. Malitsky |author3=A. Sabharwal |author4=H. Samulowitz |author5=M. Sellmann: |date=2011|title=Algorithm Selection and Scheduling|url=|journal=Proceedings of CP|doi=|pmid=|access-date=}}&lt;/ref&gt; and CSHC&lt;ref name=&quot;CSHC&quot;&gt;{{Cite journal|author1=Y. Malitsky |author2=A. Sabharwal |author3=H. Samulowitz |author4=M. Sellmann: |date=2013|title=Algorithm Portfolios Based on Cost-Sensitive Hierarchical Clustering|url=|journal=Proceedings of IJCAI|doi=|pmid=|access-date=}}&lt;/ref&gt;

=== Machine learning===
In machine learning, algorithm selection is better known as [[Meta learning (computer science)|meta-learning]]. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, the goal is to predict which machine learning algorithm will have a small error on each data set.

== Instance features==

The algorithm selection problem is mainly solved with machine learning techniques. By representing the problem instances by numerical features &lt;math&gt;f&lt;/math&gt;, algorithm selection can be seen as a multi-class classification problem by learning a mapping &lt;math&gt; f_{i} \mapsto \mathcal{A}&lt;/math&gt; for a given instance &lt;math&gt;i&lt;/math&gt;.

Instance features are numerical representations of instances. For example, we can count the number of variables, clauses, average clause length for Boolean formulas,&lt;ref&gt;{{Cite journal|author1=E. Nudelman |author2=K. Leyton-Brown |author3=H. Hoos |author4=A. Devkar |author5=Y. Shoham |date=2004|title=Understanding Random SAT: Beyond the Clauses-to-Variables Ratio|url=|journal=Proccedings of CP|doi=|pmid=|access-date=}}&lt;/ref&gt; or number of samples, features, class balance for ML data sets to get an impression about their characteristics.

=== Static vs. probing features===

We distinguish between two kinds of features:
# Static features are in most cases some counts and statistics (e.g., clauses-to-variables ratio in SAT). These features ranges from very cheap features (e.g. number of variables) to very complex features (e.g., statistics about variable-clause graphs).
# Probing features (sometimes also called landmarking features) are computed by running some analysis of algorithm behavior on an instance (e.g., accuracy of a cheap decision tree algorithm on an ML data set, or running for a short time a stochastic local search solver on a Boolean formula). These feature often cost more than simple static features.

=== Feature costs===

Depending on the used performance metric &lt;math&gt; m &lt;/math&gt;, feature computation can be associated with costs.
For example, if we use running time as performance metric, we include the time to compute our instance features into the performance of an algorithm selection system.
SAT solving is a concrete example, where such feature costs cannot be neglected, since instance features for CNF formulas can be either very cheap (e.g., to get the number of variables can be done in constant time for CNFs in the DIMACs format) or very expensive (e.g., graph features which can cost tens or hundreds of seconds).

It is important to take the overhead of feature computation into account in practice in such scenarios; otherwise a misleading impression of the performance of the algorithm selection approach is created. For example, if the decision which algorithm to choose can be made with prefect accuracy, but the features are the running time of the portfolio algorithms, there is no benefit to the portfolio approach. This would not be obvious if feature costs were omitted.

== Approaches==

=== Regression approach===

One of the first successful algorithm selection approaches predicted the performance of each algorithm &lt;math&gt;\hat{m}_{\mathcal{A}}: \mathcal{I} \to \mathbb{R}&lt;/math&gt; and selecting the algorithm with the best predicted performance &lt;math&gt;arg\min_{\mathcal{A}\in\mathcal{P}} \hat{m}_{\mathcal{A}}(i) &lt;/math&gt; for a new instance &lt;math&gt;i&lt;/math&gt;.&lt;ref name=&quot;zilla08&quot; /&gt;

=== Clustering approach===

A common assumption is that the given set of instances &lt;math&gt;\mathcal{I}&lt;/math&gt; can be clustered into homogeneous subsets
and for each of these subsets, there is one well-performing algorithm for all instances in there.
So, the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster.
A new instance is assigned to a cluster and the associated algorithm selected.&lt;ref&gt;{{Cite journal|author1=S. Kadioglu |author2=Y. Malitsky |author3=M. Sellmann |author4=K. Tierney |date=2010|title=ISAC – Instance-Specific Algorithm Configuration|url=|journal=Proceedings of the European Conference on Artificial Intelligence|doi=|pmid=|access-date=}}&lt;/ref&gt;

A more modern approach is cost-sensitive [[hierarchical clustering]]&lt;ref name=&quot;CSHC&quot;/&gt; using supervised learning to identify the homogeneous instance subsets.

=== Pairwise cost-sensitive classification approach===

A common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms)
and choose the class that was predicted most often by the pairwise models.
We can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms.
This is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference.
Therefore, each instance &lt;math&gt;i&lt;/math&gt; for training a classification model &lt;math&gt;\mathcal{A}_1&lt;/math&gt; vs &lt;math&gt;\mathcal{A}_2&lt;/math&gt; is associated with a cost &lt;math&gt;|m(\mathcal{A}_1,i) - m(\mathcal{A}_2,i)| &lt;/math&gt;.&lt;ref&gt;{{Cite journal|author1=L. Xu |author2=F. Hutter |author3=J. Shen |author4=H. Hoos |author5=K. Leyton-Brown |date=2012|title=SATzilla2012: Improved Algorithm Selection Based on Cost-sensitive Classification Models|url=|journal=Proceedings of the SAT Challenge 2012: Solver and Benchmark Descriptions|doi=|pmid=|access-date=}}&lt;/ref&gt;

== Requirements==
[[File:Portfolio correlation as.png|thumb|Clustering of SAT solvers from SAT12-INDU ASlib scenario according to the correlation coefficient of spearman.]]
[[File:Shapley Values on SAT12-INDU ASlib Scenario.png|thumb|Shapley values for complementary analysis on SAT12-INDU ASlib Scenario&lt;ref&gt;{{Cite journal|last=A. Frechette, L. Kotthoff, T. Michalak, T. Rahwan, H. Hoos and K. Leyton{-}Brown|first=|date=2016|title=Using the Shapley Value to Analyze Algorithm Portfolios|url=|journal=Proceedings of the International Conference on AAAI|doi=|pmid=|access-date=}}&lt;/ref&gt;]]
The algorithm selection problem can be effectively applied under the following assumptions:
* The portfolio &lt;math&gt;\mathcal{P}&lt;/math&gt; of algorithms is complementary with respect to the instance set &lt;math&gt;\mathcal{I}&lt;/math&gt;, i.e.,  there is no single algorithm &lt;math&gt;\mathcal{A} \in \mathcal{P}&lt;/math&gt; that dominates the performance of all other algorithms on &lt;math&gt;\mathcal{I}&lt;/math&gt; (see figures to the right for examples on complementary analysis).
* In some application, the computation of instance features is associated with a cost. For example, if the cost metric is running time, we have also to consider the time to compute the instance features. In such cases, the cost to compute features should not be larger than the performance gain through algorithm selection.

== Application domains==

Algorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied.
Application domains include:

* hard combinatorial problems: [[Boolean satisfiability problem|SAT]], [[Linear programming|Mixed Integer Programming]], [[Constraint satisfaction problem|CSP]], [[Automated planning and scheduling|AI Planning]], [[Travelling salesman problem|TSP]], [[MAXSAT]], [[QBF]] and [[Answer Set Programming]]
* combinatorial auctions
* in machine learning, the problem is known as [[Meta learning (computer science)|meta-learning]]
* software design
* black-box optimization
* multi-agent systems
* numerical optimization
* linear algebra, differential equations
* evolutionary algorithms
* vehicle routing problem
* power systems

For an extensive list of literature about algorithm selection, we refer to a literature overview.

== Variants of algorithm selection==

=== Online selection===

Online algorithm selection in [[Hyper-heuristic]] refers to switching between different algorithms during the solving process. In contrast, (offline) algorithm selection is an one-shot game where we select an algorithm for a given instance only once.

=== Computation of schedules===

An extension of algorithm selection is the per-instance algorithm scheduling problem, in which we do not select only one solver, but we select a time budget for each algorithm on a per-instance base. This approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely.&lt;ref&gt;{{Cite journal|author1=M. Lindauer, R. Bergdoll  |author2=F. Hutter|date=2016|title=An Empirical Study of Per-Instance Algorithm Scheduling|url=|journal=Proceedings of the International Conference on Learning and Intelligent Optimization|doi=|pmid=|access-date=}}&lt;/ref&gt;

=== Selection of parallel portfolios===

Given the increasing importance of parallel computation,
an extension of algorithm selection for parallel computation is parallel portfolio selection,
in which we select a subset of the algorithms to simultaneously run in a parallel portfolio.&lt;ref&gt;{{Cite journal|last=M. Lindauer and H. Hoos and F. Hutter|date=2015|title=From Sequential Algorithm Selection to Parallel Portfolio Selection|url=|journal= Proceedings of the International Conference on Learning and Intelligent Optimization|doi=|pmid=|access-date=}}&lt;/ref&gt;

== External links==

* [http://www.coseal.net/aslib/ Algorithm Selection Library (ASlib)]
* [https://larskotthoff.github.io/assurvey/ Algorithm selection literature]

== References==
{{Reflist}}

</text>
      <sha1>3jid8hnzl9wlzsf60brujrfaofyxrm3</sha1>
    </revision>
  </page>
  <page>
    <title>Transfer learning</title>
    <ns>0</ns>
    <id>3920550</id>
    <revision>
      <id>803423325</id>
      <parentid>801182269</parentid>
      <timestamp>2017-10-02T13:04:27Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor/>
      <comment>[[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5180">'''Transfer learning''' or inductive transfer is a research problem in [[machine learning]] that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.&lt;ref&gt;{{cite web |last1=West |first1=Jeremy |first2=Dan |last2=Ventura |first3=Sean |last3=Warnick |url=http://cpms.byu.edu/springresearch/abstract-entry?id=861 |title=Spring Research Presentation: A Theoretical Foundation for Inductive Transfer |publisher=Brigham Young University, College of Physical and Mathematical Sciences |year=2007 |accessdate=2007-08-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20070801120743/http://cpms.byu.edu/springresearch/abstract-entry?id=861 |archivedate=2007-08-01 |df= }} &lt;/ref&gt; For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.  This area of research bears some relation to the long history of psychological literature on [[transfer of learning]], although formal ties between the two fields are limited.

==History==
The earliest cited work on transfer in [[machine learning]] is attributed to Lorien Pratt, who formulated the discriminability-based transfer (DBT) algorithm in 1993.&lt;ref&gt;{{cite book|url={{google books|plainurl=y|id=6tGHlwEACAAJ|page=204}}|title=NIPS Conference: Advances in Neural Information Processing Systems 5|last=Pratt|first=L. Y.|publisher=Morgan Kaufmann Publishers|year=1993|pp=204-211|chapter=Discriminability-based transfer between neural networks|chapter-url=http://papers.nips.cc/paper/641-discriminability-based-transfer-between-neural-networks.pdf}}&lt;/ref&gt;

In 1997, the journal ''Machine Learning''  published a special issue devoted to transfer learning,&lt;ref&gt;{{Cite web|url=https://link.springer.com/journal/10994/28/1/page/1|title=Machine Learning - Special Issue on Inductive Transfer|last=Pratt|first=L. Y.|authorlink=|last2=Thrun|first2=Sebastian|date=July 1997|website=link.springer.com|publisher=Springer|language=en|access-date=2017-08-10|volume=28|issue=1}}&lt;/ref&gt; and by 1998, the field had advanced to include [[multi-task learning]],&lt;ref&gt;Caruana, R., &quot;Multitask Learning&quot;, pp. 95-134 in {{Harvnb|Pratt|Thrun|1998}}&lt;/ref&gt;  along with a more formal analysis of its theoretical foundations.&lt;ref&gt;Baxter, J., &quot;Theoretical Models of Learning to Learn&quot;, pp. 71-95 {{Harvnb|Pratt|Thrun|1998}}&lt;/ref&gt; ''Learning to Learn'',{{sfn|Thrun|Pratt|2012}} edited by Pratt and [[Sebastian Thrun]], is a 2012 review of the subject.

Transfer learning has also been applied in cognitive science, with the journal ''Connection Science''
publishing a special issue on reuse of neural networks through transfer in 1996.&lt;ref&gt;{{Cite web|url=http://www.tandfonline.com/toc/ccos20/8/2|title=Special Issue: Reuse of Neural Networks through Transfer|last=Pratt|first=L.|last2=|date=|year=1996 |language=en|access-date=2017-08-10|journal=Connection Science|volume=8|issue=2}}&lt;/ref&gt;

Notably, scientists have developed algorithms for transfer learning in [[Markov logic network]]s&lt;ref&gt;{{citation|title=Learning Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI-2007)|date=July 2007|last1=Mihalkova|last2=Huynh|last3=Mooney|first1=Lilyana|first2=Tuyen|first3=Raymond J.|contribution=Mapping and Revising Markov Logic Networks for Transfer|contribution-url=http://www.cs.utexas.edu/users/ml/papers/mihalkova-aaai07.pdf|location=Vancouver, BC|accessdate=2007-08-05|pp=608-614}}&lt;/ref&gt; and [[Bayesian networks]].&lt;ref&gt;{{citation|last=Niculescu-Mizil|title=Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS 2007)|date=March 21–24, 2007|last2=Caruana|first1=Alexandru|first2=Rich|contribution=Inductive Transfer for Bayesian Network Structure Learning|contribution-url=http://www.stat.umn.edu/~aistat/proceedings/data/papers/043.pdf|access-date=2007-08-05}}&lt;/ref&gt; Researchers have also applied techniques for transfer to problems in [[Document classification|text classification]],&lt;ref&gt;Do, Chuong B. and Andrew Y. Ng. http://papers.nips.cc/paper/2843-transfer-learning-for-text-classification.pdf Transfer learning for text classification Neural Information Processing Systems Foundation, NIPS*2005 Online Papers. Retrieved on 2007-08-05.&lt;/ref&gt;&lt;ref&gt;Raina, Rajat, Andrew Y. Ng, and Daphne Koller. http://ai.stanford.edu/~ang/papers/icml06-transferinformativepriors.pdf Constructing Informative Priors using Transfer Learning ''Proceedings of the Twenty-third International Conference on Machine Learning'', 2006. Retrieved on 2007-08-05.&lt;/ref&gt; and [[E-mail filtering|spam filtering]].&lt;ref&gt;Bickel, Steffen. http://www.ecmlpkdd2006.org/discovery_challenge2006_overview.pdf ECML-PKDD Discovery Challenge 2006 Overview Proceedings of the ECML-PKDD Discovery Challenge Workshop, 2006. Retrieved on 2007-08-05.&lt;/ref&gt;

==See also==
* [[Multi-task learning]]
* [[Domain Adaptation]]

== Sources ==
* {{cite book|url={{google books|plainurl=y|id=X_jpBwAAQBAJ}}|title=Learning to Learn|last1=Thrun|first1=Sebastian|last2=Pratt|first2=Lorien|date=6 December 2012|publisher=Springer Science &amp; Business Media|isbn=978-1-4615-5529-2}}
==References==
{{Reflist|2}}

</text>
      <sha1>kryt3eywezz27i8avsxtlk67x2b95la</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Genetic programming</title>
    <ns>14</ns>
    <id>3061615</id>
    <revision>
      <id>746268286</id>
      <parentid>388502966</parentid>
      <timestamp>2016-10-26T09:48:48Z</timestamp>
      <contributor>
        <username>Kri</username>
        <id>253188</id>
      </contributor>
      <comment>+ </comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="571">'''Genetic programming''' ('''GP''') is an automated methodology inspired by [[biological evolution]] to find [[computer programs]] that best perform a user-defined task. It is therefore a particular [[machine learning]] technique that uses an [[evolutionary algorithm]] to optimize a population of computer programs according to a [[fitness landscape]] determined by a program's ability to perform a given computational task.

{{Cat main|Genetic programming}}


[[Category:Genetics|Programming, genetic]]
</text>
      <sha1>gp1vlcmpkozot0o9kr1rtvx1o0fl60t</sha1>
    </revision>
  </page>
  <page>
    <title>Evolutionary programming</title>
    <ns>0</ns>
    <id>460689</id>
    <revision>
      <id>784483321</id>
      <parentid>778025240</parentid>
      <timestamp>2017-06-08T15:41:36Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2202">'''Evolutionary programming''' is one of the four major [[evolutionary algorithm]] paradigms.  It is similar to [[genetic programming]], but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.

It was first used by [[Lawrence J. Fogel]] in the US in 1960 in order to use simulated [[evolution]] as a learning process aiming to generate [[artificial intelligence]]. Fogel used [[finite-state machine]]s as predictors and evolved them.
Currently evolutionary programming is a wide [[evolutionary computing]] dialect with no fixed structure or ([[Genetic representation|representation]]), in contrast with some of the other dialects. It is becoming harder to distinguish from [[Evolution strategy|evolutionary strategies]].

Its main variation operator is [[Mutation (genetic algorithm)|mutation]]; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ){{Explain|date=April 2017}} [[Selection (genetic algorithm)|survivor selection]].

==See also==
* [[Artificial intelligence]]
* [[Genetic algorithm]]
* [[Genetic operator]]

==References==
* Fogel, L.J., Owens, A.J., Walsh, M.J. (1966), ''Artificial Intelligence through Simulated Evolution'', John Wiley.
* Fogel, L.J. (1999), ''Intelligence through Simulated Evolution : Forty Years of Evolutionary Programming'', John Wiley.
* Eiben, A.E., Smith, J.E. (2003), [http://www.cs.vu.nl/~gusz/ecbook/ecbook.html ''Introduction to Evolutionary Computing''], [http://www.springer.de Springer]. {{ISBN|3-540-40184-9}}

==External links==
* [http://www.aip.de/~ast/EvolCompFAQ/Q1_2.htm The Hitch-Hiker's Guide to Evolutionary Computation: What's Evolutionary Programming (EP)?]
* [http://www.cleveralgorithms.com/nature-inspired/evolution/evolutionary_programming.html Evolutionary Programming by Jason Brownlee (PhD)]

{{Evolutionary computation}}

{{Authority control}}

{{DEFAULTSORT:Evolutionary Programming}}




{{compu-sci-stub}}

[[de:Evolutionäre Programmierung]]</text>
      <sha1>bnlvjye3flcuewgq02735nxe05ffrgt</sha1>
    </revision>
  </page>
  <page>
    <title>Genetic algorithm</title>
    <ns>0</ns>
    <id>40254</id>
    <revision>
      <id>816164599</id>
      <parentid>815133139</parentid>
      <timestamp>2017-12-19T18:15:36Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v478)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="60843">{{Use dmy dates|date=July 2013}}
[[Image:St 5-xband-antenna.jpg|thumb|The 2006 NASA [[Space Technology 5|ST5]] spacecraft antenna. This complicated shape was found by an evolutionary computer design program to create the best radiation pattern. It is known as an [[evolved antenna]].]]
&lt;!-- Deleted image removed: [[Image:ESA JAXA HUMIES Trajectory.png|thumb|The ESA/JAXA interplanetary Trajectory recipient of the [http://www.genetic-programming.org/combined.php 2013 gold HUMIES ] award. This complex tour of the Jovian Moons was found with the help of an evolutionary technique based on self-adaptation]] --&gt;
In [[computer science]] and [[operations research]], a '''genetic algorithm''' ('''GA''') is a [[metaheuristic]] inspired by the process of [[natural selection]] that belongs to the larger class of [[evolutionary algorithm]]s (EA). Genetic algorithms are commonly used to generate high-quality solutions to [[Optimization (mathematics)|optimization]] and [[Search algorithm|search problem]]s by relying on bio-inspired operators such as [[Mutation (genetic algorithm)|mutation]], [[crossover (genetic algorithm)|crossover]] and [[selection (genetic algorithm)|selection]].{{sfn|Mitchell|1996|p=2}}

== Methodology ==

=== Optimization problems ===
In a genetic algorithm, a [[population]] of [[candidate solution]]s (called individuals, creatures, or [[phenotype]]s) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its [[chromosome]]s or [[genotype]]) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.{{sfn|Whitley|1994|p=66}}

The evolution usually starts from a population of randomly generated individuals, and is an [[Iteration|iterative process]], with the population in each iteration called a ''generation''. In each generation, the [[fitness (biology)|fitness]] of every individual in the population is evaluated; the fitness is usually the value of the [[objective function]] in the optimization problem being solved. The more fit individuals are [[Stochastics|stochastically]] selected from the current population, and each individual's genome is modified ([[Crossover (genetic algorithm)|recombined]] and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the [[algorithm]]. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.

A typical genetic algorithm requires:

# a [[genetic representation]] of the solution domain,
# a [[fitness function]] to evaluate the solution domain.

A standard representation of each candidate solution is as an [[bit array|array of bits]].{{sfn|Whitley|1994|p=66}} Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple [[Crossover (genetic algorithm)|crossover]] operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in [[genetic programming]] and graph-form representations are explored in [[evolutionary programming]]; a mix of both linear chromosomes and trees is explored in [[gene expression programming]].

Once the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.

==== Initialization ====
The population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the ''search space''). Occasionally, the solutions may be &quot;seeded&quot; in areas where optimal solutions are likely to be found.

==== Selection ====
{{Main article|Selection (genetic algorithm)}}
During each successive generation, a portion of the existing population is [[selection (genetic algorithm)|selected]] to breed a new generation. Individual solutions are selected through a ''fitness-based'' process, where [[Fitness (biology)|fitter]] solutions (as measured by a [[fitness function]]) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.

The fitness function is defined over the genetic representation and measures the ''quality'' of the represented solution. The fitness function is always problem dependent. For instance, in the [[knapsack problem]] one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The ''fitness'' of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.

In some problems, it is hard or even impossible to define the fitness expression; in these cases, a [[Computer simulation|simulation]] may be used to determine the fitness function value of a [[phenotype]] (e.g. [[computational fluid dynamics]] is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even [[Interactive evolutionary computation|interactive genetic algorithms]] are used.

==== Genetic operators ====
{{Main article|Crossover (genetic algorithm)|Mutation (genetic algorithm)}}

The next step is to generate a second generation population of solutions from those selected through a combination of [[genetic operator]]s: [[crossover (genetic algorithm)|crossover]] (also called recombination), and [[mutation (genetic algorithm)|mutation]].

For each new solution to be produced, a pair of &quot;parent&quot; solutions is selected for breeding from the pool selected previously. By producing a &quot;child&quot; solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its &quot;parents&quot;. New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.
Although reproduction methods that are based on the use of two parents are more &quot;biology inspired&quot;, some research&lt;ref&gt;Eiben, A. E. et al (1994). &quot;Genetic algorithms with multi-parent recombination&quot;. PPSN III: Proceedings of the International Conference on Evolutionary Computation. The Third Conference on Parallel Problem Solving from Nature: 78&amp;ndash;87. {{ISBN|3-540-58484-6}}.&lt;/ref&gt;&lt;ref&gt;Ting, Chuan-Kang (2005). &quot;On the Mean Convergence Time of Multi-parent Genetic Algorithms Without Selection&quot;. Advances in Artificial Life: 403&amp;ndash;412. {{ISBN|978-3-540-28848-0}}.&lt;/ref&gt; suggests that more than two &quot;parents&quot; generate higher quality chromosomes.

These processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.

Opinion is divided over the importance of crossover versus mutation. There are many references in [[David B. Fogel|Fogel]] (2006) that support the importance of mutation-based search.

Although crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.&lt;ref&gt;Akbari, Ziarati (2010). &quot;A multilevel evolutionary algorithm for optimizing numerical functions&quot; IJIEC 2 (2011): 419&amp;ndash;430  [http://growingscience.com/ijiec/Vol2/IJIEC_2010_11.pdf]&lt;/ref&gt;

It is worth tuning parameters such as the [[Mutation (genetic algorithm)|mutation]] probability, [[Crossover (genetic algorithm)|crossover]] probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to [[genetic drift]]  (which is non-[[Ergodicity|ergodic]] in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless [[#Elitism|elitist selection]] is employed.

==== Heuristics ====

In addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The ''speciation'' heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution.&lt;ref&gt;{{Cite book|url=https://pdfs.semanticscholar.org/a525/1192091cc6c138cf8010e43d72b9dfb0d022.pdf|title=Handbook of Evolutionary Computation|last=Deb|first=Kalyanmoy|last2=Spears|first2=William M.|publisher=Institute of Physics Publishing|year=1997|isbn=|location=|pages=|chapter=C6.2: Speciation methods}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://link.springer.com/referenceworkentry/10.1007/978-3-540-92910-9_32|title=Handbook of Natural Computing|last=Shir|first=Ofer M.|date=2012|publisher=Springer Berlin Heidelberg|year=|isbn=9783540929093|editor-last=Rozenberg|editor-first=Grzegorz|location=|pages=1035–1069|language=en|chapter=Niching in Evolutionary Algorithms|doi=10.1007/978-3-540-92910-9_32|editor-last2=Bäck|editor-first2=Thomas|editor-last3=Kok|editor-first3=Joost N.}}&lt;/ref&gt;

==== Termination ====
This generational process is repeated until a termination condition has been reached. Common terminating conditions are:

* A solution is found that satisfies minimum criteria
* Fixed number of generations reached
* Allocated budget (computation time/money) reached
* The highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results
* Manual inspection
* Combinations of the above
*

== The building block hypothesis ==
Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:

# A description of a heuristic that performs adaptation by identifying and recombining &quot;building blocks&quot;, i.e. low order, low defining-length [[Schema (genetic algorithms)|schemata]] with above average fitness.
# A hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic.

Goldberg describes the heuristic as follows:

:&quot;Short, low order, and highly fit schemata are sampled, [[crossover (genetic algorithm)|recombined]] [crossed over], and resampled to form strings of potentially higher fitness. In a way, by working with these particular schemata [the building blocks], we have reduced the complexity of our problem; instead of building high-performance strings by trying every conceivable combination, we construct better and better strings from the best partial solutions of past samplings.

:&quot;Because highly fit schemata of low defining length and low order play such an important role in the action of genetic algorithms, we have already given them a special name: building blocks. Just as a child creates magnificent fortresses through the arrangement of simple blocks of wood, so does a genetic algorithm seek near optimal performance through the juxtaposition of short, low-order, high-performance schemata, or building blocks.&quot;{{sfn|Goldberg|1989|p=41}}

Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many [[estimation of distribution algorithm]]s, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold.&lt;ref&gt;{{cite journal|last1=Harik|first1=Georges R.|last2=Lobo|first2=Fernando G.|last3=Sastry|first3=Kumara|title=Linkage Learning via Probabilistic Modeling in the Extended Compact Genetic Algorithm (ECGA)|journal=Scalable Optimization via Probabilistic Modeling|date=1 January 2006|pages=39–61|doi=10.1007/978-3-540-34954-9_3|url=https://link.springer.com/chapter/10.1007/978-3-540-34954-9_3|publisher=Springer Berlin Heidelberg|language=en}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Cantú-Paz|first3=Erick|title=BOA: The Bayesian Optimization Algorithm|journal=Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1|date=1 January 1999|pages=525–532|url=http://dl.acm.org/citation.cfm?id=2933973|publisher=Morgan Kaufmann Publishers Inc.}}&lt;/ref&gt; Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.&lt;ref&gt;{{cite journal|last1=Coffin|first1=David|last2=Smith|first2=Robert E.|title=Linkage Learning in Estimation of Distribution Algorithms|journal=Linkage in Evolutionary Computation|date=1 January 2008|pages=141–156|doi=10.1007/978-3-540-85068-7_7|url=https://link.springer.com/chapter/10.1007/978-3-540-85068-7_7|publisher=Springer Berlin Heidelberg|language=en}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Echegoyen|first1=Carlos|last2=Mendiburu|first2=Alexander|last3=Santana|first3=Roberto|last4=Lozano|first4=Jose A.|title=On the Taxonomy of Optimization Problems Under Estimation of Distribution Algorithms|journal=Evolutionary Computation|date=8 November 2012|volume=21|issue=3|pages=471–495|doi=10.1162/EVCO_a_00095|url=http://www.mitpressjournals.org/doi/abs/10.1162/EVCO_a_00095#.V7ILDN9vG00|issn=1063-6560}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Sadowski|first1=Krzysztof L.|last2=Bosman|first2=Peter A.N.|last3=Thierens|first3=Dirk|title=On the Usefulness of Linkage Processing for Solving MAX-SAT|journal=Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation|date=1 January 2013|pages=853–860|doi=10.1145/2463372.2463474|url=http://dl.acm.org/citation.cfm?id=2463474|publisher=ACM}}&lt;/ref&gt;

== Limitations ==
There are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:

* Repeated [[fitness function]] evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive [[fitness function]] evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods can not deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an [[fitness approximation|approximated fitness]] that is computationally efficient. It is apparent that amalgamation of [[fitness approximation|approximate models]] may be one of the most promising approaches to convincingly use GA to solve complex real life problems.
* Genetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, and airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts.
* The &quot;better&quot; solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem.
* In many problems, GAs may have a tendency to converge towards [[local optimum|local optima]] or even arbitrary points rather than the [[global optimum]] of the problem. This means that it does not &quot;know how&quot; to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the [[fitness landscape]]: certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions,&lt;ref&gt;{{cite journal|last=Taherdangkoo|first=Mohammad|last2=Paziresh |first2=Mahsa |last3=Yazdi |first3=Mehran |last4= Bagheri |first4=Mohammad Hadi |title=An efficient algorithm for function optimization: modified stem cells algorithm|journal=Central European Journal of Engineering|date=19 November 2012|volume=3|issue=1|pages=36–50|doi=10.2478/s13531-012-0047-8}}&lt;/ref&gt; although the [[No free lunch in search and optimization|No Free Lunch theorem]]&lt;ref&gt;Wolpert, D.H., Macready, W.G., 1995. No Free Lunch Theorems for Optimisation. Santa Fe Institute, SFI-TR-05-010, Santa Fe.&lt;/ref&gt; proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a &quot;niche penalty&quot;, wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, which will reduce the representation of that group in subsequent generations, permitting other (less similar) individuals to be maintained in the population. This trick, however, may not be effective, depending on the landscape of the problem. Another possible technique would be to simply replace part of the population with randomly generated individuals, when most of the population is too similar to each other. Diversity is important in genetic algorithms (and [[genetic programming]]) because crossing over a homogeneous population does not yield new solutions. In [[Evolution strategy|evolution strategies]] and [[evolutionary programming]], diversity is not essential because of a greater reliance on mutation.
* Operating on dynamic data sets is difficult, as genomes begin to converge early on towards solutions which may no longer be valid for later data. Several methods have been proposed to remedy this by increasing genetic diversity somehow and preventing early convergence, either by increasing the probability of mutation when the solution quality drops (called ''triggered hypermutation''), or by occasionally introducing entirely new, randomly generated elements into the gene pool (called ''random immigrants''). Again, [[Evolution strategy|evolution strategies]] and [[evolutionary programming]] can be implemented with a so-called &quot;comma strategy&quot; in which parents are not maintained and new parents are selected only from offspring. This can be more effective on dynamic problems.
* GAs cannot effectively solve problems in which the only fitness measure is a single right/wrong measure (like [[decision problem]]s), as there is no way to converge on the solution (no hill to climb). In these cases, a random search may find a solution as quickly as a GA. However, if the situation allows the success/failure trial to be repeated giving (possibly) different results, then the ratio of successes to failures provides a suitable fitness measure.
* For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include [[Evolution strategy|evolution strategies]], [[evolutionary programming]], [[simulated annealing]], [[Gaussian adaptation]], [[hill climbing]], and [[swarm intelligence]] (e.g.: [[ant colony optimization]], [[particle swarm optimization]]) and methods based on [[integer linear programming]]. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.

== Variants ==

=== Chromosome representation ===
{{main article | genetic representation }}
The simplest algorithm represents each chromosome as a [[Bit array|bit string]]. Typically, numeric parameters can be represented by [[integer]]s, though it is possible to use [[floating point]] representations. The floating point representation is natural to [[Evolution strategy|evolution strategies]] and [[evolutionary programming]]. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by [[John Henry Holland]] in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a [[linked list]], [[associative array|hashes]], [[object (computer science)|objects]], or any other imaginable [[data structure]]. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.

When bit-string representations of integers are used, [[Gray coding]] is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so called ''Hamming walls'', in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.

Other approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a ''virtual alphabet'' (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.&lt;ref name=Goldberg1991&gt;{{cite journal|last=Goldberg|first=David E.|title=The theory of virtual alphabets|journal=Parallel Problem Solving from Nature, Lecture Notes in Computer Science|year=1991|volume=496|pages=13–22|doi=10.1007/BFb0029726|url=https://link.springer.com/chapter/10.1007/BFb0029726|accessdate=2 July 2013}}&lt;/ref&gt;&lt;ref name=Janikow1991&gt;{{cite journal|last=Janikow|first=C. Z.|first2=Z. |last2=Michalewicz |title=An Experimental Comparison of Binary and Floating Point Representations in Genetic Algorithms|journal=Proceedings of the Fourth International Conference on Genetic Algorithms|year=1991|pages=31–36|url=http://www.cs.umsl.edu/~janikow/publications/1991/GAbin/text.pdf|accessdate=2 July 2013}}&lt;/ref&gt;

An expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome.&lt;ref name=Patrascu2014&gt;{{cite journal|last=Patrascu|first=M.|last2=Stancu|first2=A.F.|last3=Pop|first3=F.|title=HELGA: a heterogeneous encoding lifelike genetic algorithm for population evolution modeling and simulation|journal=Soft Computing|year=2014|volume=18|pages=2565–2576|url=https://link.springer.com/article/10.1007/s00500-014-1401-y|doi=10.1007/s00500-014-1401-y}}&lt;/ref&gt; This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.

=== Elitism ===
A practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as ''elitist selection'' and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.&lt;ref&gt;{{cite conference |last1=Baluja |first1=Shumeet |first2=Rich |last2=Caruana |title=Removing the genetics from the standard genetic algorithm |conference=[[International Conference on Machine Learning|ICML]] |year=1995 |url=http://www.ri.cmu.edu/pub_files/pub2/baluja_shumeet_1995_1/baluja_shumeet_1995_1.pdf}}&lt;/ref&gt;

=== Parallel implementations ===
Parallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.
Other variants, like genetic algorithms for [[online optimization]] problems, introduce time-dependence or noise in the fitness function.

=== Adaptive GAs ===
Genetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of ''pc'' and ''pm'', AGAs utilize the population information in each generation and adaptively adjust the ''pc'' and ''pm'' in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm),&lt;ref&gt;[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=286385 Srinivas. M and Patnaik. L, &quot;Adaptive probabilities of crossover and mutation in genetic algorithms,&quot; IEEE Transactions on System, Man and Cybernetics, vol.24, no.4, pp.656&amp;ndash;667, 1994.]&lt;/ref&gt; the adjustment of ''pc'' and ''pm'' depends on the fitness values of the solutions. In ''CAGA'' (clustering-based adaptive genetic algorithm),&lt;ref&gt;[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4220690 ZHANG. J, Chung. H and Lo. W. L, &quot;Clustering-Based Adaptive Crossover and Mutation Probabilities for Genetic Algorithms&quot;, IEEE Transactions on Evolutionary Computation vol.11, no.3, pp. 326&amp;ndash;335, 2007.]&lt;/ref&gt; through the use of clustering analysis to judge the optimization states of the population, the adjustment of ''pc'' and ''pm'' depends on these optimization states.
It can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA {{Citation needed|date=July 2016}} while overcoming the lack of robustness of hill climbing.

This means that the rules of genetic variation may have a different meaning in the natural case. For instance &amp;ndash; provided that steps are stored in consecutive order &amp;ndash; crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the [[Chromosomal inversion|inversion operator]] has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency. (See for instance &lt;ref&gt;[http://www.thisurlisfalse.com/evolution-in-a-nutshell/ Evolution-in-a-nutshell]&lt;/ref&gt; or example in [[travelling salesman problem]], in particular the use of an [[edge recombination operator]].)

A variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.

A number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA,&lt;ref&gt;[http://www.complex-systems.com/issues/03-5.html D.E. Goldberg, B. Korb, and K. Deb. &quot;Messy genetic algorithms: Motivation, analysis, and first results&quot;. Complex Systems, 5(3):493–530, October 1989.]&lt;/ref&gt; GEMGA&lt;ref&gt;Gene expression: The missing link in evolutionary computation&lt;/ref&gt; and LLGA.&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=269517 G. Harik. Learning linkage to efficiently solve problems of bounded difficulty using genetic algorithms. PhD thesis, Dept. Computer Science, University of Michigan, Ann Arbour, 1997]&lt;/ref&gt;

== Problem domains ==
Problems which appear to be particularly appropriate for solution by genetic algorithms include [[Timeline|timetabling]] and scheduling problems, and many scheduling software packages are based on GAs{{Citation needed|date=December 2011}}. GAs have also been applied to [[engineering]].&lt;ref&gt;Tomoiagă B, Chindriş M, Sumper A, Sudria-Andreu A, Villafafila-Robles R. [http://www.mdpi.com/1996-1073/6/3/1439/pdf Pareto Optimal Reconfiguration of Power Distribution Systems Using a Genetic Algorithm Based on NSGA-II. ] Energies. 2013; 6(3):1439-1455.&lt;/ref&gt; Genetic algorithms are often applied as an approach to solve [[global optimization]] problems.

As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex [[fitness landscape]] as mixing, i.e., [[Mutation (genetic algorithm)|mutation]] in combination with [[Crossover (genetic algorithm)|crossover]], is designed to move the population away from [[local optima]] that a traditional [[hill climbing]] algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a [[Markov chain]]).

Examples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector,&lt;ref&gt;{{cite web|last=Gross|first=Bill|title=A solar energy system that tracks the sun|url=http://www.ted.com/talks/bill_gross_on_new_energy.html|work=TED|accessdate=20 November 2013}}&lt;/ref&gt; antennae designed to pick up radio signals in space,&lt;ref&gt;{{citation |first=G. S. |last=Hornby |first2=D. S. |last2=Linden |first3=J. D. |last3=Lohn |url=http://ti.arc.nasa.gov/m/pub-archive/1244h/1244%20(Hornby).pdf |title=Automated Antenna Design with Evolutionary Algorithms}}&lt;/ref&gt; and walking methods for computer figures.&lt;ref&gt;http://goatstream.com/research/papers/SA2013/index.html&lt;/ref&gt;

In his ''Algorithm Design Manual'', [[Steven Skiena|Skiena]] advises against genetic algorithms for any task:

{{blockquote|[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution—where significant progress require [sic] millions of years—can be quite appropriate.
[...]
I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to [[simulated annealing]] for your heuristic search voodoo needs.|Steven Skiena&lt;ref&gt;{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title = The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year = 2010 |isbn=1-849-96720-2}}&lt;/ref&gt;{{rp|267}}}}

== History ==
In 1950, [[Alan Turing]] proposed a &quot;learning machine&quot; which would parallel the principles of evolution.&lt;ref name=&quot;mind.oxfordjournals.org&quot;&gt;{{cite journal|last1=Turing|first1=Alan M.|title=Computing machinery and intelligence|journal=Mind|volume=LIX|issue=238|pages=433–460|doi=10.1093/mind/LIX.236.433|url=http://mind.oxfordjournals.org/content/LIX/236/433}}&lt;/ref&gt; Computer simulation of evolution started as early as in 1954 with the work of [[Nils Aall Barricelli]], who was using the computer at the [[Institute for Advanced Study]] in [[Princeton, New Jersey]].&lt;ref name=&quot;Barricelli 1954 45–68&quot;&gt;{{cite journal|last=Barricelli|first=Nils Aall|year=1954|authorlink=Nils Aall Barricelli|title=Esempi numerici di processi di evoluzione|journal=Methodos|pages=45–68}}&lt;/ref&gt;&lt;ref name=&quot;Barricelli 1957 143–182&quot;&gt;{{cite journal|last=Barricelli|first=Nils Aall|year=1957|authorlink=Nils Aall Barricelli|title=Symbiogenetic evolution processes realized by artificial methods|journal=Methodos|pages=143–182}}&lt;/ref&gt;  His 1954 publication was not widely noticed. Starting in 1957,&lt;ref name=&quot;Fraser 1957 484–491&quot;&gt;{{cite journal|last=Fraser|first=Alex|authorlink=Alex Fraser (scientist)|year=1957|title=Simulation of genetic systems by automatic digital computers. I. Introduction|journal=Aust. J. Biol. Sci.|volume=10|pages=484–491}}&lt;/ref&gt;  the Australian quantitative geneticist [[Alex Fraser (scientist)|Alex Fraser]] published a series of papers on simulation of [[artificial selection]] of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970)&lt;ref name=&quot;Fraser 1970&quot;&gt;{{cite book|last=Fraser|first=Alex|authorlink=Alex Fraser (scientist)|first2=Donald |last2=Burnell|year=1970|title=Computer Models in Genetics|publisher=McGraw-Hill|location=New York|isbn=0-07-021904-4}}&lt;/ref&gt; and Crosby (1973).&lt;ref name=&quot;Crosby 1973&quot;&gt;{{cite book|last=Crosby|first=Jack L.|year=1973|title=Computer Simulation in Genetics|publisher=John Wiley &amp; Sons|location=London|isbn=0-471-18880-8}}&lt;/ref&gt; Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, [[Hans-Joachim Bremermann]] published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms.&lt;ref&gt;[http://berkeley.edu/news/media/releases/96legacy/releases.96/14319.html 02.27.96 - UC Berkeley's Hans Bremermann, professor emeritus and pioneer in mathematical biology, has died at 69&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by [[David B. Fogel|Fogel]] (1998).&lt;ref&gt;{{cite book|last=Fogel|first=David B. (editor)|year=1998|title=Evolutionary Computation: The Fossil Record|publisher=IEEE Press|location=New York|isbn=0-7803-3481-7}}&lt;/ref&gt;

Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game,&lt;ref&gt;{{cite journal|last=Barricelli|first=Nils Aall|year=1963|title=Numerical testing of evolution theories. Part II. Preliminary tests of performance, symbiogenesis and terrestrial life|journal=Acta Biotheoretica|issue=16|pages=99–126}}&lt;/ref&gt; [[artificial evolution]] became a widely recognized optimization method as a result of the work of [[Ingo Rechenberg]] and [[Hans-Paul Schwefel]] in the 1960s and early 1970s &amp;ndash; Rechenberg's group was able to solve complex engineering problems through [[Evolution strategy|evolution strategies]].&lt;ref&gt;{{cite book|last=Rechenberg|first=Ingo|year=1973|title=Evolutionsstrategie|place=Stuttgart|publisher=Holzmann-Froboog|isbn=3-7728-0373-3}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Schwefel|first=Hans-Paul|year=1974|title=Numerische Optimierung von Computer-Modellen (PhD thesis)}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Schwefel|first=Hans-Paul|year=1977|title=Numerische Optimierung von Computor-Modellen mittels der Evolutionsstrategie : mit einer vergleichenden Einführung in die Hill-Climbing- und Zufallsstrategie|place=Basel; Stuttgart | publisher=Birkhäuser| isbn=3-7643-0876-1}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Schwefel|first=Hans-Paul|year=1981|title=Numerical optimization of computer models (Translation of 1977 Numerische Optimierung von Computor-Modellen mittels der Evolutionsstrategie|place=Chichester ; New York|publisher=Wiley|isbn=0-471-09988-0}}&lt;/ref&gt;  Another approach was the evolutionary programming technique of [[Lawrence J. Fogel]], which was proposed for generating artificial intelligence. [[Evolutionary programming]] originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of [[John Henry Holland|John Holland]] in the early 1970s, and particularly his book ''Adaptation in Natural and Artificial Systems'' (1975). His work originated with studies of [[cellular automata]], conducted by [[John Henry Holland|Holland]] and his students at the [[University of Michigan]]. Holland introduced a formalized framework for predicting the quality of the next generation, known as [[Holland's Schema Theorem]]. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in [[Pittsburgh, Pennsylvania]].

===Commercial products===
In the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=-MszVdu_PAMC&amp;dq=general+electric+genetic+algorithm+mainframe&amp;source=gbs_navlinks_s|title=An Approach to Designing an Unmanned Helicopter Autopilot Using Genetic Algorithms and Simulated Annealing|last=Aldawoodi|first=Namir|publisher=ProQuest|year=2008|isbn=0549773495|location=|pages=99|quote=|via=Google Books}}&lt;/ref&gt;
In 1989, Axcelis, Inc. released [[Evolver (software)|Evolver]], the world's first commercial GA product for desktop computers. [[The New York Times]] technology writer [[John Markoff]] wrote&lt;ref&gt;{{cite news|last=Markoff|first=John|title=What's the Best Answer? It's Survival of the Fittest|publisher=New York Times|url=https://www.nytimes.com/1990/08/29/business/business-technology-what-s-the-best-answer-it-s-survival-of-the-fittest.html|accessdate=2016-07-13|date=29 August 1990}}&lt;/ref&gt; about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995.&lt;ref&gt;Ruggiero, Murray A.. (2009-08-01) [http://www.futuresmag.com/2009/08/01/fifteen-years-and-counting?t=technology&amp;page=2 Fifteen years and counting]. Futuresmag.com. Retrieved on 2013-08-07.&lt;/ref&gt; Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version.&lt;ref&gt;[http://www.palisade.com/evolver/ Evolver: Sophisticated Optimization for Spreadsheets]. Palisade. Retrieved on 2013-08-07.&lt;/ref&gt;

== Related techniques ==
{{See also|List of genetic algorithm applications}}

===Parent fields===
Genetic algorithms are a sub-field of:
*[[Evolutionary algorithms]]
*[[Evolutionary computing]]
*[[Metaheuristic]]s
*[[Stochastic optimization]]
*[[Optimization (mathematics)|Optimization]]

===Related fields===

====Evolutionary algorithms====
{{Refimprove section|date=May 2011}}
{{main article|Evolutionary algorithm}}
Evolutionary algorithms is a sub-field of [[Evolutionary Computation|evolutionary computing]].

* [[Evolution strategy|Evolution strategies]] (ES, see Rechenberg, 1994) evolve individuals by means of mutation and intermediate or discrete recombination. ES algorithms are designed particularly to solve problems in the real-value domain. They use self-adaptation to adjust control parameters of the search. De-randomization of self-adaptation has led to the contemporary Covariance Matrix Adaptation Evolution Strategy ([[CMA-ES]]).
* [[Evolutionary programming]] (EP) involves populations of solutions with primarily mutation and selection and arbitrary representations. They use self-adaptation to adjust parameters, and can include other variation operations such as combining information from multiple parents.
* [[Estimation of Distribution Algorithm]] (EDA) substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as Probabilistic Graphical Models, from which new solutions can be sampled&lt;ref&gt;{{cite journal|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Cantú-Paz|first3=Erick|title=BOA: The Bayesian Optimization Algorithm|journal=Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1|date=1 January 1999|pages=525–532|url=http://dl.acm.org/citation.cfm?id=2933973|publisher=Morgan Kaufmann Publishers Inc.}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Pelikan|first1=Martin|title=Hierarchical Bayesian optimization algorithm : toward a new generation of evolutionary algorithms|date=2005|publisher=Springer|location=Berlin [u.a.]|isbn=978-3-540-23774-7|edition=1st}}&lt;/ref&gt; or generated from guided-crossover.&lt;ref&gt;{{cite journal|last1=Thierens|first1=Dirk|title=The Linkage Tree Genetic Algorithm|journal=Parallel Problem Solving from Nature, PPSN XI|date=11 September 2010|pages=264–273|doi=10.1007/978-3-642-15844-5_27|url=https://link.springer.com/chapter/10.1007/978-3-642-15844-5_27|publisher=Springer Berlin Heidelberg|language=en}}&lt;/ref&gt;
* [[Gene expression programming]] (GEP) also uses populations of computer programs. These complex computer programs are encoded in simpler linear chromosomes of fixed length, which are afterwards expressed as expression trees. Expression trees or computer programs evolve because the chromosomes undergo mutation and recombination in a manner similar to the canonical GA. But thanks to the special organization of GEP chromosomes, these genetic modifications always result in valid computer programs.&lt;ref&gt;{{cite web|last=Ferreira|first=C|title=Gene Expression Programming: A New Adaptive Algorithm for Solving Problems|url= http://www.gene-expression-programming.com/webpapers/GEP.pdf|publisher= Complex Systems, Vol. 13, issue 2: 87-129.}}&lt;/ref&gt;
* [[Genetic programming]] (GP) is a related technique popularized by [[John Koza]] in which computer programs, rather than function parameters, are optimized. Genetic programming often uses [[Tree (data structure)|tree-based]] internal [[data structure]]s to represent the computer programs for adaptation instead of the [[List (computing)|list]] structures typical of genetic algorithms.
* [[Grouping genetic algorithm]] (GGA) is an evolution of the GA where the focus is shifted from individual items, like in classical GAs, to groups or subset of items.&lt;ref name=&quot;Falkenauer&quot;&gt;{{cite book|last=Falkenauer|first=Emanuel|authorlink=Emanuel Falkenauer|year=1997|title=Genetic Algorithms and Grouping Problems|publisher=John Wiley &amp; Sons Ltd|location=Chichester, England|isbn=978-0-471-97150-4}}&lt;/ref&gt; The idea behind this GA evolution proposed by [[Emanuel Falkenauer]] is that solving some complex problems, a.k.a. ''clustering'' or ''partitioning'' problems where a set of items must be split into disjoint group of items in an optimal way, would better be achieved by making characteristics of the groups of items equivalent to genes. These kind of problems include [[bin packing problem|bin packing]], line balancing, [[cluster analysis|clustering]] with respect to a distance measure, equal piles, etc., on which classic GAs proved to perform poorly. Making genes equivalent to groups implies chromosomes that are in general of variable length, and special genetic operators that manipulate whole groups of items. For bin packing in particular, a GGA hybridized with the Dominance Criterion of Martello and Toth, is arguably the best technique to date.
* [[Interactive evolutionary algorithm]]s are evolutionary algorithms that use human evaluation. They are usually applied to domains where it is hard to design a computational fitness function, for example, evolving images, music, artistic designs and forms to fit users' aesthetic preference.

====Swarm intelligence====
{{main article|Swarm intelligence}}
Swarm intelligence is a sub-field of [[Evolutionary Computation|evolutionary computing]].

* [[Ant colony optimization]] ('''ACO''') uses many ants (or agents) equipped with a pheromone model to traverse the solution space and find locally productive areas. Although considered an [[Estimation of distribution algorithm]],&lt;ref&gt;{{cite journal|last1=Zlochin|first1=Mark|last2=Birattari|first2=Mauro|last3=Meuleau|first3=Nicolas|last4=Dorigo|first4=Marco|title=Model-Based Search for Combinatorial Optimization: A Critical Survey|journal=Annals of Operations Research|date=1 October 2004|volume=131|issue=1-4|pages=373–395|doi=10.1023/B:ANOR.0000039526.52305.af|url=https://link.springer.com/article/10.1023/B:ANOR.0000039526.52305.af|language=en|issn=0254-5330}}&lt;/ref&gt;
* [[Particle swarm optimization]] (PSO) is a computational method for multi-parameter optimization which also uses population-based approach. A population (swarm) of candidate solutions (particles) moves in the search space, and the movement of the particles is influenced both by their own best known position and swarm's global best known position. Like genetic algorithms, the PSO method depends on information sharing among population members. In some problems the PSO is often more computationally efficient than the GAs, especially in unconstrained problems with continuous variables.&lt;ref&gt;Rania Hassan, Babak Cohanim, Olivier de Weck, Gerhard Vente
r (2005) [http://www.mit.edu/~deweck/PDF_archive/3%20Refereed%20Conference/3_50_AIAA-2005-1897.pdf A comparison of particle swarm optimization and the genetic algorithm]&lt;/ref&gt;

====Other evolutionary computing algorithms====

Evolutionary computation is a sub-field of the [[metaheuristic]] methods.

* [[Memetic algorithm]] (MA), often called ''hybrid genetic algorithm'' among others, is a population-based method in which solutions are also subject to local improvement phases. The idea of memetic algorithms comes from [[meme]]s, which unlike genes, can adapt themselves. In some problem areas they are shown to be more efficient than traditional evolutionary algorithms.
* [[Bacteriologic algorithm]]s (BA) inspired by [[evolutionary ecology]] and, more particularly, bacteriologic adaptation. Evolutionary ecology is the study of living organisms in the context of their environment, with the aim of discovering how they adapt. Its basic concept is that in a heterogeneous environment, there is not one individual that fits the whole environment. So, one needs to reason at the population level. It is also believed BAs could be successfully applied to complex positioning problems (antennas for cell phones, urban planning, and so on) or data mining.&lt;ref&gt;{{cite journal|url=http://www.irisa.fr/triskell/publis/2005/Baudry05d.pdf|format=PDF|first=Benoit|last=Baudry |author2=Franck Fleurey |author3=[[Jean-Marc Jézéquel]] |author4=Yves Le Traon|title=Automatic Test Case Optimization: A Bacteriologic Algorithm|date=March–April 2005|pages=76–82|journal=IEEE Software|issue=2|publisher=IEEE Computer Society|doi=10.1109/MS.2005.30|volume=22|accessdate=9 August 2009}}&lt;/ref&gt;
* [[Cultural algorithm]] (CA) consists of the population component almost identical to that of the genetic algorithm and, in addition, a knowledge component called the belief space.
* [[Differential search algorithm]] (DS) inspired by migration of superorganisms.&lt;ref&gt;{{cite journal|last=Civicioglu|first=P.|title=Transforming Geocentric Cartesian Coordinates to Geodetic Coordinates by Using Differential Search Algorithm|journal=Computers &amp;Geosciences|year=2012|volume=46|pages=229–247|doi=10.1016/j.cageo.2011.12.011}}&lt;/ref&gt;
* [[Gaussian adaptation]] (normal or natural adaptation, abbreviated NA to avoid confusion with GA) is intended for the maximisation of manufacturing yield of signal processing systems. It may also be used for ordinary parametric optimisation. It relies on a certain theorem valid for all regions of acceptability and all Gaussian distributions. The efficiency of NA relies on information theory and a certain theorem of efficiency. Its efficiency is defined as information divided by the work needed to get the information.&lt;ref&gt;{{cite journal|last=Kjellström|first=G.|title= On the Efficiency of Gaussian Adaptation|journal=Journal of Optimization Theory and Applications|volume=71|issue=3|pages=589–597|date=December 1991|doi= 10.1007/BF00941405}}&lt;/ref&gt; Because NA maximises mean fitness rather than the fitness of the individual, the landscape is smoothed such that valleys between peaks may disappear. Therefore it has a certain &quot;ambition&quot; to avoid local peaks in the fitness landscape. NA is also good at climbing sharp crests by adaptation of the moment matrix, because NA may maximise the disorder ([[average information]]) of the Gaussian simultaneously keeping the [[mean fitness]] constant.

====Other metaheuristic methods====

Metaheuristic methods broadly fall within [[Stochastic optimization|stochastic]] optimisation methods.

* [[Simulated annealing]] (SA) is a related global optimization technique that traverses the search space by testing random mutations on an individual solution. A mutation that increases fitness is always accepted. A mutation that lowers fitness is accepted probabilistically based on the difference in fitness and a decreasing temperature parameter. In SA parlance, one speaks of seeking the lowest energy instead of the maximum fitness. SA can also be used within a standard GA algorithm by starting with a relatively high rate of mutation and decreasing it over time along a given schedule.
* [[Tabu search]] (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest energy of those generated. In order to prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.
* [[Extremal optimization]] (EO) Unlike GAs, which work with a population of candidate solutions, EO evolves a single solution and makes [[local search (optimization)|local]] modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (&quot;fitness&quot;). The governing principle behind this algorithm is that of ''emergent'' improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is decidedly at odds with a GA that selects good solutions in an attempt to make better solutions.

====Other stochastic optimisation methods====

* The [[Cross-entropy method|cross-entropy (CE) method]] generates candidates solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.
* Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular [[reinforcement learning]], [[Active learning (machine learning)|active or query learning]], [[neural networks]], and [[metaheuristics]].

==See also==
* [[List of genetic algorithm applications]]
* [[particle filter|Genetic algorithms in signal processing (a.k.a. particle filters)]]
* [[Propagation of schema]]
* [[Universal Darwinism]]
* [[Metaheuristics]]
* [[Learning classifier system]]
* [[Rule-based machine learning]]

== References ==
{{Reflist|30em}}

== Bibliography ==
{{Refbegin}}
* {{Cite book|title=Genetic Programming &amp;ndash; An Introduction | last1=Banzhaf | first1=Wolfgang | last2=Nordin | first2=Peter | last3 = Keller | first3=Robert | last4=Francone | first4=Frank | year=1998 | isbn=978-1558605107 | publisher=Morgan Kaufmann | location=San Francisco, CA | ref=harv}}
* {{Cite journal|last=Bies|first=Robert R. |last2=Muldoon |first2=Matthew F. |last3=Pollock |first3=Bruce G. |last4=Manuck |first4=Steven |last5=Smith |first5=Gwenn |last6=Sale |first6=Mark E. |year=2006|title=A Genetic Algorithm-Based, Hybrid Machine Learning Approach to Model Selection|journal=Journal of Pharmacokinetics and Pharmacodynamics|publisher=Springer|location=Netherlands|pages=196–221}}
* {{Cite journal|last=Cha|first=Sung-Hyuk|last2=Tappert |first2=Charles C. |year=2009|title=A Genetic Algorithm for Constructing Compact Binary Decision Trees|journal=Journal of Pattern Recognition Research |volume=4|issue=1|pages=1–13|url=http://www.jprr.org/index.php/jprr/article/view/44/25|doi=10.13176/11.44}}
* {{Cite journal|last=Fraser|first=Alex S.|year=1957|title=Simulation of Genetic Systems by Automatic Digital Computers. I. Introduction|journal=Australian Journal of Biological Sciences|volume=10|pages=484–491}}
* {{Cite book| last=Goldberg | first=David | year=1989 | title=Genetic Algorithms in Search, Optimization and Machine Learning | isbn=978-0201157673 | publisher=Addison-Wesley Professional | location=Reading, MA | ref=harv}}
* {{Cite book| last=Goldberg | first=David | year=2002 | title=The Design of Innovation: Lessons from and for Competent Genetic Algorithms | publisher=Kluwer Academic Publishers | location=Norwell, MA | isbn=978-1402070983 | ref=harv}}
* {{Cite book| last=Fogel | first=David | title=Evolutionary Computation: Toward a New Philosophy of Machine Intelligence | publisher=IEEE Press | location=Piscataway, NJ | edition=3rd | isbn=978-0471669517 | ref=harv}}
* {{Cite book | last=Holland | first=John | title=Adaptation in Natural and Artificial Systems | publisher=MIT Press | location=Cambridge, MA | year=1992 | isbn=978-0262581110 | ref=harv}}
* {{Cite book | last=Koza | first=John | title=Genetic Programming: On the Programming of Computers by Means of Natural Selection | year = 1992 | publisher=MIT Press | location=Cambridge, MA | isbn=978-0262111706 | ref=harv}}
* {{Cite book | last=Michalewicz | first=Zbigniew | year=1996 | title=Genetic Algorithms + Data Structures = Evolution Programs | publisher=Springer-Verlag | isbn=978-3540606765 | ref=harv}}
* {{Cite book | last=Mitchell | first=Melanie | title=An Introduction to Genetic Algorithms | year=1996 | publisher=MIT Press | location=Cambridge, MA | isbn = 9780585030944 | ref=harv}}
* {{Cite book |last1=Poli |first1=R. |last2=Langdon |first2=W. B. |last3=McPhee |first3=N. F. |year=2008 |title=A Field Guide to Genetic Programming | publisher=Lulu.com, freely available from the internet | isbn = 978-1-4092-0073-4}}
* Rechenberg, Ingo (1994): Evolutionsstrategie '94, Stuttgart: Fromman-Holzboog.
* Schmitt, Lothar M; Nehaniv, Chrystopher L; Fujii, Robert H (1998), ''Linear analysis of genetic algorithms'', Theoretical Computer Science 208: 111&amp;ndash;148
* Schmitt, Lothar M (2001),  ''Theory of Genetic Algorithms'', Theoretical Computer Science 259: 1&amp;ndash;61
* Schmitt, Lothar M (2004),  ''Theory of Genetic Algorithms II: models for genetic operators over the string-tensor representation of populations and convergence to global optima for arbitrary fitness function under scaling'', Theoretical Computer Science 310: 181&amp;ndash;231
* Schwefel, Hans-Paul (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).
* {{Cite book | last=Vose | first=Michael | year=1999 | title=The Simple Genetic Algorithm: Foundations and Theory | publisher=MIT Press | location=Cambridge, MA | isbn=978-0262220583 | ref=harv}}
* {{Cite journal | last=Whitley | first=Darrell | title=A genetic algorithm tutorial | journal=Statistics and Computing | doi=10.1007/BF00175354 | volume=4 | issue=2 | pages=65–85 | year=1994 | ref=harv | url=http://www.cs.uga.edu/~potter/CompIntell/ga_tutorial.pdf }}&lt;!--| accessdate=5 January 2013--&gt;
* {{Cite book | last1=Hingston | first1=Philip | last2=Barone | first2=Luigi | last3=Michalewicz | first3=Zbigniew | title=Design by Evolution: Advances in Evolutionary Design | year=2008 | publisher=Springer | isbn=978-3540741091 | ref=harv}}
* {{Cite book | last1=Eiben | first1=Agoston | last2=Smith | first2=James | year=2003 | title=Introduction to Evolutionary Computing | publisher=Springer | isbn=978-3540401841 | ref=harv}}
{{Refend}}

== External links ==

=== Resources ===
* [https://web.archive.org/web/20160303215222/http://www.geneticprogramming.com/ga/index.htm] Provides a list of resources in the genetic algorithms field

=== Tutorials ===
* [http://www2.econ.iastate.edu/tesfatsi/holland.gaintro.htm Genetic Algorithms - Computer programs that &quot;evolve&quot; in ways that resemble natural selection can solve complex problems even their creators do not fully understand] An excellent introduction to GA by John Holland and with an application to the Prisoner's Dilemma
* [http://userweb.eng.gla.ac.uk/yun.li/ga_demo/ An online interactive GA tutorial for a reader to practise or learn how a GA works]: Learn step by step or watch global convergence in batch, change the population size, crossover rates/bounds, mutation rates/bounds and selection mechanisms, and add constraints.
* [https://web.archive.org/web/20130615042000/http://samizdat.mines.edu/ga_tutorial/ga_tutorial.ps A Genetic Algorithm Tutorial by Darrell Whitley Computer Science Department Colorado State University] An excellent tutorial with lots of theory
* [http://cs.gmu.edu/~sean/book/metaheuristics/ &quot;Essentials of Metaheuristics&quot;], 2009 (225 p). Free open text by Sean Luke.
* [http://www.it-weise.de/projects/book.pdf Global Optimization Algorithms &amp;ndash; Theory and Application]
* [https://mpatacchiola.github.io/blog/2017/03/14/dissecting-reinforcement-learning-5.html Genetic Algorithms in Python] Tutorial with the intuition behind GAs and Python implementation.

{{Authority control}}

{{DEFAULTSORT:Genetic Algorithm}}
[[Category:Genetic algorithms| ]]








[[sv:Genetisk programmering#Genetisk algoritm]]</text>
      <sha1>61l1o1l080q8lm569dr463sl0jzqtjt</sha1>
    </revision>
  </page>
  <page>
    <title>Multiplicative weight update method</title>
    <ns>0</ns>
    <id>52242050</id>
    <revision>
      <id>811608788</id>
      <parentid>811607439</parentid>
      <timestamp>2017-11-22T19:16:49Z</timestamp>
      <contributor>
        <username>Siddharthist</username>
        <id>28122572</id>
      </contributor>
      <comment>added  using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22688">{{Use dmy dates|date=September 2017}}
'''Multiplicative weight update method''' is a meta-algorithm. It is an algorithmic technique which &quot;maintains a distribution on a certain set of interest, and updates it iteratively by multiplying the probability mass of elements by suitably chosen factors based on feedback obtained by running another algorithm on the distribution&quot;.&lt;ref name = ref1&gt;{{cite web|url=ftp://ftp.cs.princeton.edu/techreports/2007/804.pdf |format=PDF |title=Efficient Algorithms Using The Multiplicative Weights Update Method |date=November 2007}}&lt;/ref&gt; It was discovered repeatedly in very diverse fields such as machine learning ([[AdaBoost]], [[Winnow (algorithm)|Winnow]], Hedge), [[Mathematical optimization|optimization]] (solving [[Linear programming|linear programs]]), theoretical computer science (devising fast algorithm for [[Linear programming|LPs]] and [[Semidefinite programming|SDPs]]), and [[game theory]].

==Name==
&quot;Multiplicative weights&quot; implies the iterative rule used in algorithms derived from the multiplicative weight update method.&lt;ref name=ref2&gt;{{cite web |url=https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/notes/lecture16.pdf |format=PDF |title= The Multiplicative Weights Algorithm* |access-date=2016-11-09}}&lt;/ref&gt; It is given with different names in the different fields where it was discovered or rediscovered.

==History and background==
The earliest known version of this technique was in an algorithm named &quot;[[fictitious play]]&quot; which was proposed in [[game theory]] in the early 1950s. Grigoriadis and Khachiyan&lt;ref&gt;{{cite web |format=PDF |title= A sublinear-time randomized approximation algorithm for matrix games. |date=1995 |access-date=2016-11-09}}&lt;/ref&gt; applied a randomized variant of &quot;fictitious play&quot; to solve two-player [[zero-sum game]]s efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In [[machine learning]], Littlestone applied the earliest form of the multiplicative weights update rule in his famous [[Winnow (algorithm)|winnow algorithm]], which is similar to Minsky and Papert's earlier [[Perceptron|perceptron learning algorithm]]. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm.

The multiplicative weights algorithm is also widely applied in [[computational geometry]] such as [[Kenneth L. Clarkson|Clarkson's]] algorithm for [[Linear programming|linear programming (LP)]] with a bounded number of variables in linear time.&lt;ref name=&quot;KENNETH L. CLARKSON pp. 452&quot;&gt;KENNETH L. CLARKSON. '' A Las Vegas algorithm for linear programming when the dimension is small.'', In Proc. 29th FOCS, pp. 452–456. IEEE Comp. Soc. Press, 1988.[doi:10.1109/SFCS.1988.21961] 123, 152.&lt;/ref&gt;&lt;ref name=&quot;KENNETH L. CLARKSON 1995&quot;&gt;KENNETH L. CLARKSON. ''A Las Vegas algorithm for linear and integer programming when the dimension is small.'', J. ACM, 42:488–499, 1995. [doi:10.1145/201019.201036] 123, 152.&lt;/ref&gt; Later, Bronnimann and Goodrich employed analogous methods to find [[Set cover problem|set covers]] for [[hypergraph]]s with small [[VC dimension]].&lt;ref name=&quot;M.T. GOODRICH. 1995&quot;&gt;H. BRONNIMANN AND ¨ M.T. GOODRICH. ''Almost optimal set covers in finite VC-dimension.'', Discrete Comput. Geom., 14:463–479, 1995. Preliminary version in 10th Ann. Symp. Comp. Geom. (SCG'94). [doi:10.1007/BF02570718] 123, 152&lt;/ref&gt;

In [[Operations research|operation research]] and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.

In computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavan's method of pessimistic estimators for derandomization of randomized rounding algorithms; Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yao's XOR Lemma; Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases.&lt;ref name=ref4&gt;{{cite web |url=http://www.satyenkale.com/papers/mw-survey.pdf |format=PDF |title= The Multiplicative Weights Update Method: A Meta-Algorithm and Applications |date=2012}}&lt;/ref&gt;

==General setup==
A binary decision needs to be made based on n experts’ opinions to attain an associated payoff. In the first round, all experts’ opinions have the same weight. The decision maker will make the first decision based on the majority of the experts' prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each expert's opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down.

==Algorithm analysis==

===Halving algorithm&lt;ref name=ref2&gt;{{cite web |url=https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/notes/lecture16.pdf |format=PDF |title= The Multiplicative Weights Algorithm* |access-date=2016-11-09}}&lt;/ref&gt;===

Given a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistake will all be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most  {{math|log&lt;sub&gt;''2''&lt;/sub&gt;(''N'')}} mistakes.&lt;ref name=ref2&gt;{{cite web |url=https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/notes/lecture16.pdf |format=PDF |title= The Multiplicative Weights Algorithm* |access-date=2016-11-09}}&lt;/ref&gt;

===Weighted majority algorithm&lt;ref name=ref4 /&gt;&lt;ref name=ref5&gt;{{cite web |url=https://www.cs.princeton.edu/courses/archive/fall13/cos521/lecnotes/lec8.pdf |format=PDF |title= Lecture 8: Decision-making under total uncertainty: the multiplicative weight algorithm |date=2013}}&lt;/ref&gt;===

Unlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same &quot;expert advice&quot; setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts.
The very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0.&lt;ref name=ref4 /&gt; This would make fewer mistakes compared to halving algorithm.

    '''Initialization''':
       Fix an &lt;math&gt;\eta&lt;/math&gt;≤1/2. For each expert, associate the weight &lt;math&gt;{w_i}^1&lt;/math&gt;≔1.
    '''For''' &lt;math&gt;t&lt;/math&gt; = &lt;math&gt;\mathit{1}&lt;/math&gt;, &lt;math&gt;\mathit{2}&lt;/math&gt;,…,&lt;math&gt;T&lt;/math&gt;
       '''1'''. Make the prediction that is the weighted majority of the experts' predictions based on the weights &lt;math&gt;\mathbb{w_1}^t,..., \mathbb{w_n}^t&lt;/math&gt;. That is, making a binary choice depending on which prediction has a higher total weight of experts advising it (breaking ties arbitrarily).
       '''2'''. For every expert i who predicts wrongly, decrease his weight for the next round by multiplying it by a factor of (1-η):
                                                              &lt;math&gt;w_{i}^{t+1}&lt;/math&gt;=&lt;math&gt;(1-\eta) w_{i}^{t}&lt;/math&gt; (update rule)

If &lt;math&gt;\eta =0&lt;/math&gt;, the weight of the expert's advice will remain the same. When &lt;math&gt;\eta&lt;/math&gt; increases, the weight of the expert's advice will decrease. Note that some researchers fix &lt;math&gt;\eta =1/2&lt;/math&gt; in weighted majority algorithm.

After &lt;math&gt;T&lt;/math&gt; steps, let &lt;math&gt;m_i^T&lt;/math&gt; be the number of mistakes of expert i and &lt;math&gt;M^T&lt;/math&gt; be the number of mistakes our algorithm has made. Then we have the following bound for every &lt;math&gt;i&lt;/math&gt;:

                              &lt;math&gt;M^T \leq 2(1+\eta) m_i^T+ \frac{2 \ln(n)}{\eta}&lt;/math&gt;.

In particular, this holds for i which is the best expert. Since the best expert will have the least &lt;math&gt;m_i^T&lt;/math&gt;, it will give the best bound on the number of mistakes made by the algorithm as a whole.

===Randomized weighted majority algorithm&lt;ref name=ref2 /&gt;&lt;ref name=ref6&gt;{{cite web |url=http://www.cs.princeton.edu/courses/archive/spr06/cos511/scribe_notes/0330.pdf |format=PDF |title=COS 511: Foundations of Machine Learning |date=20 March 2006}}&lt;/ref&gt;===
Given the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction:

predict
:&lt;math&gt;
f(x) = \begin{cases}1 &amp; \text{with probability} \frac{q_1}{W}\\0 &amp; \text{otherwise}\end{cases}
&lt;/math&gt;

where
                &lt;math&gt;W= \sum_{i} { w_i} = q_0 + q_1&lt;/math&gt;.

The number of mistakes made by the randomized weighted majority algorithm is bounded as:
                 &lt;math&gt;E\left [ \# \text{mistakes of the learner} \right ] \leq \alpha_\beta \left ( \# \text{ mistakes of the best expert} \right ) + c_\beta \ln(N) &lt;/math&gt;

where     &lt;math&gt;\alpha_\beta= \frac{\ln(\frac{1}{beta})}{1-\beta}&lt;/math&gt; and          &lt;math&gt;c_\beta=\frac{1}{1-\beta}&lt;/math&gt;.

Note that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction.
In this randomized algorithm, &lt;math&gt;\alpha_\beta \rightarrow 1&lt;/math&gt; if &lt;math&gt;\beta \rightarrow 1&lt;/math&gt;. Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make.&lt;ref name=ref7 /&gt; However, it is important to note that in some research, people define &lt;math&gt;\eta =1/2&lt;/math&gt; in weighted majority algorithm and allow &lt;math&gt;0\leq \eta \leq 1&lt;/math&gt; in [[randomized weighted majority algorithm]].&lt;ref name=ref2 /&gt;

==Applications==
The multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event.&lt;ref name=ref1 /&gt;

===Solving zero-sum games approximately (Oracle algorithm):&lt;ref name=ref1 /&gt;&lt;ref name=ref4 /&gt;&lt;ref name=ref7&gt;{{cite web |url= https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe24.pdfformat=PDF |title= An Algorithmist's Toolkit |date=8 December 2009 |access-date=2016-11-09}}&lt;/ref&gt;===

Suppose we were given the distribution &lt;math&gt;P&lt;/math&gt; on experts. Let &lt;math&gt;A&lt;/math&gt; = payoff matrix of a finite two-player zero-sum game, with &lt;math&gt;n&lt;/math&gt; rows.

When the row player &lt;math&gt;p_r&lt;/math&gt; uses plan &lt;math&gt;i&lt;/math&gt; and the column player &lt;math&gt;p_c&lt;/math&gt; uses plan &lt;math&gt;j&lt;/math&gt;, the payoff of player &lt;math&gt;p_c&lt;/math&gt; is &lt;math&gt;A \left ( i, j \right)&lt;/math&gt;≔&lt;math&gt;A_{ij}&lt;/math&gt;, assuming &lt;math&gt;A \left( i, j\right ) \in \left [ 0, 1 \right ]&lt;/math&gt;.

If player &lt;math&gt;p_r&lt;/math&gt; chooses action &lt;math&gt;i&lt;/math&gt; from a distribution &lt;math&gt;P&lt;/math&gt; over the rows, then the expected result for player &lt;math&gt;p_c&lt;/math&gt; selecting action &lt;math&gt;j&lt;/math&gt; is &lt;math&gt;A \left (P, j \right )=E_{i \in P} \left [A \left(i,j \right) \right]&lt;/math&gt;.

To maximize &lt;math&gt;A \left (P, j \right)&lt;/math&gt;, player &lt;math&gt;p_c&lt;/math&gt; is should choose plan &lt;math&gt;j&lt;/math&gt;. Similarly, the expected payoff for player &lt;math&gt;p_l&lt;/math&gt; is &lt;math&gt;A \left (i,P\right )=E_{j\in P} \left [A \left(i,j \right) \right ]&lt;/math&gt;. Choosing plan &lt;math&gt;i&lt;/math&gt; would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain:

                                           &lt;math&gt;\min_P \max_j A\left( P, j \right) = \max_Q \min_i A\left( i, Q \right) &lt;/math&gt;
where P and i changes over the distributions over rows, Q and j changes over the columns.

Then, let &lt;math&gt;\lambda^*&lt;/math&gt; denote the common value of above quantities, also named as the &quot;value of the game&quot;. Let &lt;math&gt;\delta&gt;0&lt;/math&gt; be an error parameter. To solve the zero-sum game bounded by additive error of &lt;math&gt;\delta&lt;/math&gt;,

                                                  &lt;math&gt;\lambda^* - \delta \leq \min_i  A \left (i,q \right ) &lt;/math&gt;
                                                  &lt;math&gt;\max_j A \left(p, j \right) \leq \lambda^* +\delta &lt;/math&gt;

So there is an algorithm solving zero-sum game up to an additive factor of δ using O({{math|log&lt;sub&gt;''2''&lt;/sub&gt;(''n'')}}/&lt;math&gt;\delta^2&lt;/math&gt;) calls to ORACLE, with an additional processing time of O(n) per call&lt;ref name =ref7 /&gt;

===Machine learning===
In machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm.&lt;ref&gt;DEAN P. FOSTER AND RAKESH VOHRA (1999). ''Regret in the on-line decision problem'', p. 29. Games and Economic Behaviour&lt;/ref&gt; Later, Freund and Schapire generalized it in the form of hedge algorithm.&lt;ref name=ref8 /&gt; AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method.&lt;ref name=ref4 /&gt;

====Winnow algorithm====
Based on current knowledge in algorithms, multiplicative weight update method was first used in Littlestone's winnow algorithm.&lt;ref name=ref4 /&gt; It is used in machine learning to solve a linear program.

Given &lt;math&gt;m&lt;/math&gt; labeled examples &lt;math&gt; \left (a_1, l_1 \right ),\text{…} ,\left (a_m, l_m \right ) &lt;/math&gt; where &lt;math&gt;a_j \in \mathbb{R}^n&lt;/math&gt; are feature vectors, and &lt;math&gt;l_j \in \left \{-1,1 \right \} \quad&lt;/math&gt; are their labels.

The aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that &lt;math&gt;l_j a_j x \geq 0&lt;/math&gt; for all &lt;math&gt;j&lt;/math&gt;. Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine &lt;math&gt;a_j&lt;/math&gt; to be &lt;math&gt;l_j a_j&lt;/math&gt;, the problem reduces to finding a solution to the following LP:

                      &lt;math&gt;\forall j=1,2,\text{…}, m : a_j x \geq 0&lt;/math&gt;,
                      &lt;math&gt;1*x=1&lt;/math&gt;,
                      &lt;math&gt;\forall i : x_i \geq 0&lt;/math&gt;.

This is general form of LP.

====Hedge algorithm &lt;ref name=ref2 /&gt;====
The hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different.&lt;ref name=ref2 /&gt;
It is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update.&lt;ref name=ref16&gt;{{cite web |url=http://www.shivani-agarwal.net/Teaching/E0370/Aug-2011/Lectures/20-scribe1.pdf |format=PDF |title= Online Learning from Experts: Weighed Majority and Hedge |access-date=2016-12-07}}&lt;/ref&gt;

=====Analysis=====
Assume &lt;math&gt;\epsilon \leq 1&lt;/math&gt; and for &lt;math&gt;t \in [T]&lt;/math&gt;, &lt;math&gt;p^t&lt;/math&gt; is picked by Hedge. Then for all experts &lt;math&gt;i&lt;/math&gt;,

                                 &lt;math&gt;\sum_{t \leq T} p^t m^t \leq \sum_{t \leq T} m^t +\frac{\ln(N)}{\epsilon}+\epsilon T&lt;/math&gt;

'''Initialization''': Fix an &lt;math&gt;\eta \leq 12&lt;/math&gt;. For each expert, associate the weight &lt;math&gt;w_i^1&lt;/math&gt; ≔1
'''For''' t=1,2,…,T:
       1. Pick the distribution &lt;math&gt;p_j^t= \frac{w_1^t}{\Phi t}&lt;/math&gt; where &lt;math&gt;\Phi t=\sum_i w_i^t&lt;/math&gt;.
       2. Observe the cost of the decision &lt;math&gt;m^t&lt;/math&gt;.
       3. Set
                               &lt;math&gt;w_i^{t + 1} = w_i^t*&lt;/math&gt; exp(&lt;math&gt;-\epsilon * m_i^t&lt;/math&gt;).

====AdaBoost algorithm====

[[AdaBoost|This algorithm]]&lt;ref name=ref8&gt;Yoav, Freund. Robert, E. Schapire (1996). ''TA Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting*'', p. 55. journal of computer and system sciences.&lt;/ref&gt; maintains a set of weights &lt;math&gt;w^t&lt;/math&gt; over the training examples. On every iteration &lt;math&gt;t&lt;/math&gt;, a distribution &lt;math&gt;p^t&lt;/math&gt; is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis &lt;math&gt;h_t&lt;/math&gt; that (hopefully) has small error with respect to the distribution. Using the new hypothesis &lt;math&gt;h_t&lt;/math&gt;, AdaBoost generates the next weight vector &lt;math&gt;w^{t+1}&lt;/math&gt;. The process repeats. After T such iterations, the final hypothesis &lt;math&gt;h_f&lt;/math&gt; is the output. The hypothesis &lt;math&gt;h_f&lt;/math&gt; combines the outputs of the T weak hypotheses using a weighted majority vote.&lt;ref name=ref8 /&gt;

 '''Input''':
       Sequence of &lt;math&gt;N&lt;/math&gt; labeled examples (&lt;math&gt;x_1&lt;/math&gt;,&lt;math&gt;y_1&lt;/math&gt;),…,(&lt;math&gt;x_N&lt;/math&gt;, &lt;math&gt;y_N&lt;/math&gt;)
       Distribution &lt;math&gt;D&lt;/math&gt; over the &lt;math&gt;N&lt;/math&gt; examples
       Weak learning algorithm &quot;'WeakLearn&quot;'
       Integer &lt;math&gt;T&lt;/math&gt; specifying number of iterations
 '''Initialize''' the weight vector: &lt;math&gt;w_{i}^{1} = D(i)&lt;/math&gt; for &lt;math&gt;i=1&lt;/math&gt;,..., &lt;math&gt;N&lt;/math&gt;.
 '''Do for''' &lt;math&gt;t=1&lt;/math&gt;,..., &lt;math&gt;N&lt;/math&gt;
       '''1'''. Set &lt;math&gt;p^t=\frac{w^t}{\sum_{i=1}^{N} w_{i}^{t}}&lt;/math&gt;.
       '''2'''. Call '''WeakLearn''', providing it with the distribution &lt;math&gt;p^t&lt;/math&gt;; get back a hypothesis &lt;math&gt;h_t: X\rightarrow&lt;/math&gt; [0,1].
       '''3'''. Calculate the error of &lt;math&gt;h_t:\epsilon_t = \sum_{i=1}^{N} p_{i}^{t}&lt;/math&gt;|&lt;math&gt;h_t(x_i)&lt;/math&gt;.
       '''4'''. Set &lt;math&gt;\beta_t = \frac{\epsilon_t}{1-\epsilon_t}&lt;/math&gt;.
       '''5'''. Set the new weight vector to be &lt;math&gt;w_{i}^{t+1}=w_{i}^{t}\beta_{t}^{1-|h_t(x_i)-y_i|}&lt;/math&gt;.

 '''Output''' the hypothesis:

       &lt;math&gt;
       f(x) = \begin{cases}1 &amp; \text{if} \sum_{t=1}^{T} \log(1/\beta_t) h_{t}(x) \geq \frac{1}{2}\sum_{t=1}^{T} \log(1/\beta_t) \frac{q_1}{W}\\0 &amp; \text{otherwise}\end{cases}
       &lt;/math&gt;

===Solving linear programs approximately&lt;ref name=ref11&gt;{{cite web |url=http://tcs.epfl.ch/files/content/sites/tcs/files/Lec2-Fall14-Ver2.pdf |format=PDF |title= Fundamentals of Convex Optimization |access-date=2016-11-09}}&lt;/ref&gt;===

====Problem====
Given a &lt;math&gt;m \times n&lt;/math&gt; matrix &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;b \in \mathbb{R}^n&lt;/math&gt;, is there a &lt;math&gt;x&lt;/math&gt; such that &lt;math&gt;A x \geq b&lt;/math&gt;?

                       &lt;math&gt;\exists ? x: A x \geq b &lt;/math&gt;              (1)

====Assumption====
Using the oracle algorithm in solving zero-sum problem, with an error parameter &lt;math&gt; \epsilon &gt; 0&lt;/math&gt;, the output would either be a point &lt;math&gt;x&lt;/math&gt; such that &lt;math&gt;A x \geq b-\epsilon&lt;/math&gt; or a proof that &lt;math&gt;x&lt;/math&gt; does not exist, i.e., there is no solution to this linear system of inequalities.

====solution====
Given vector &lt;math&gt;p \in \Delta_n&lt;/math&gt;, solves the following relaxed problem

                      &lt;math&gt;\exists ? x: p^{\textsf T}\!\!A x\geq p^\textsf{T}\!b&lt;/math&gt;             (2)

If there exists a x satisfying (1), then x satisfies (2) for all &lt;math&gt; p\in \Delta_n&lt;/math&gt;. The contrapositive of this statement is also true.
Suppose if oracle returns a feasible solution for a &lt;math&gt;p&lt;/math&gt;, the solution &lt;math&gt;x&lt;/math&gt; it returns has bounded width &lt;math&gt;\max_i |{(A x)}_i - b_i | \leq 1&lt;/math&gt;.
So if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of &lt;math&gt;2\epsilon&lt;/math&gt;. The algorithm makes at most &lt;math&gt;\frac{\ln(m)}{\epsilon^2}&lt;/math&gt; calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case.

===Other applications===

====Operations research and online statistical decision-making&lt;ref name =ref4 /&gt;====
In [[operations research]] and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.

====Computational geometry====
The multiplicative weights algorithm is also widely applied in [[computational geometry]]&lt;ref name =ref4 /&gt;, such as [[Kenneth L. Clarkson|Clarkson]]'s algorithm for [[Linear programming|linear programming (LP)]] with a bounded number of variables in linear time.&lt;ref name=&quot;KENNETH L. CLARKSON pp. 452&quot;/&gt;&lt;ref name=&quot;KENNETH L. CLARKSON 1995&quot;/&gt; Later, Bronnimann and Goodrich employed analogous methods to find [[Set cover problem|Set Covers]] for [[hypergraph]]s with small [[VC dimension]].&lt;ref name=&quot;M.T. GOODRICH. 1995&quot;/&gt;

====[[Gradient descent|Gradient descent method]]&lt;ref name=ref1 /&gt;====

====[[Matrix (mathematics)|Matrix]] multiplicative weights update&lt;ref name=ref1 /&gt;====

====Plotkin, Shmoys, Tardos framework for [[Packing problems|packing]]/[[Covering problems|covering LPs]]&lt;ref name=ref4 /&gt;====

====Approximating [[multi-commodity flow problem]]s&lt;ref name=ref4 /&gt;====

====O (logn)- approximation for many [[NP-hardness|NP-hard problems]]&lt;ref name=ref4 /&gt;====

====[[Learning theory (education)|Learning theory]] and [[Boosting (machine learning)|boosting]]&lt;ref name=ref4 /&gt;====

====Hard-core sets and the XOR lemma&lt;ref name=ref4 /&gt;====

====Hannan's algorithm and multiplicative weights&lt;ref name=ref4 /&gt;====

====Online [[convex optimization]]&lt;ref name=ref4 /&gt;====

==References==
{{Reflist}}

==External links==



</text>
      <sha1>lyy2xi7n6mi4elsglwx0zxg4mvrl36w</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-task learning</title>
    <ns>0</ns>
    <id>938663</id>
    <revision>
      <id>814991683</id>
      <parentid>814991314</parentid>
      <timestamp>2017-12-12T02:43:58Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>/* See also */ replacing a link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30525">'''Multi-task learning''' (MTL) is a subfield of [[machine learning]] in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.&lt;ref&gt;Baxter, J. (2000). A model of inductive bias learning&quot; ''Journal of Artificial Intelligence Research'' 12:149--198, [http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume12/baxter00a.pdf On-line paper]&lt;/ref&gt;&lt;ref&gt;[[Sebastian Thrun|Thrun, S.]] (1996). Is learning the n-th thing any easier than learning the first?. In Advances in Neural Information Processing Systems 8, pp. 640--646. MIT Press. [http://citeseer.ist.psu.edu/thrun96is.html Paper at Citeseer]&lt;/ref&gt;&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|url = http://www.cs.cornell.edu/~caruana/mlj97.pdf|title = Multi-task learning|last = Caruana|first = R.|date = 1997|journal = Machine Learning|doi = 10.1023/A:1007379606734|pmid = |access-date =|volume=28|pages=41–75}}&lt;/ref&gt; Early versions of MTL were called &quot;hints&quot;&lt;ref&gt;Suddarth, S., Kergosien, Y. (1990). Rule-injection hints as a means of improving network performance and learning time. EURASIP Workshop. Neural Networks pp. 120-129. Lecture Notes in Computer Science. Springer.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Abu-Mostafa | first1 = Y. S. | year = 1990 | title = Learning from hints in neural networks | url = | journal = Journal of Complexity | volume = 6 | issue = | pages = 192–198 | doi=10.1016/0885-064x(90)90006-y}}&lt;/ref&gt;

In a widely cited 1997 paper, Rich Caruana gave the following characterization:&lt;blockquote&gt;Multitask Learning is an approach to [[inductive transfer]] that improves [[Generalization error|generalization]] by using the domain information contained in the training signals of related tasks as an [[inductive bias]]. It does this by learning tasks in parallel while using a shared [[Representation learning|representation]]; what is learned for each task can help other tasks be learned better.&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|url = http://www.cs.cornell.edu/~caruana/mlj97.pdf|title = Multi-task learning|last = Caruana|first = R.|date = 1997|journal = Machine Learning|doi = 10.1023/A:1007379606734|pmid = |access-date =|volume=28|pages=41–75}}&lt;/ref&gt;&lt;/blockquote&gt;

In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance.&lt;ref name=&quot;:0&quot;&gt;{{Cite web|url = http://www.cs.cornell.edu/~kilian/research/multitasklearning/multitasklearning.html|title = Multi-task Learning|date = |accessdate = |website = |publisher = |last = Weinberger|first = Kilian}}&lt;/ref&gt; Further examples of settings for MTL include [[multiclass classification]] and [[multi-label classification]].&lt;ref name=&quot;:1&quot;&gt;{{Cite arxiv|arxiv = 1504.03101|title = Convex Learning of Multiple Tasks and their Structure|last = Ciliberto|first = C.|date = 2015 }}&lt;/ref&gt;

Multi-task learning works because [[Regularization (mathematics)|regularization]] induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents [[overfitting]] by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled.&lt;ref name=&quot;:0&quot; /&gt; However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.&lt;ref name=&quot;:3&quot;&gt;Romera-Paredes, B., Argyriou, A., Bianchi-Berthouze, N., &amp; Pontil, M., (2012) Exploiting Unrelated Tasks in Multi-Task Learning. http://jmlr.csail.mit.edu/proceedings/papers/v22/romera12/romera12.pdf&lt;/ref&gt;

==Methods==

===Task grouping and overlap===
Within the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with [[Sparse array|sparsity]], overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases.&lt;ref&gt;Kumar, A., &amp; Daume III, H., (2012) Learning Task Grouping and Overlap in Multi-Task Learning. http://icml.cc/2012/papers/690.pdf&lt;/ref&gt; Task relatedness can be imposed a priori or learned from the data.&lt;ref name=&quot;:1&quot;/&gt;&lt;ref&gt;Jawanpuria, P., &amp; Saketha Nath, J., (2012) A Convex Feature Learning Formulation for Latent Task Structure Discovery. http://icml.cc/2012/papers/90.pdf&lt;/ref&gt; Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly.&lt;ref&gt;Zweig, A. &amp; Weinshall, D. Hierarchical Regularization Cascade for Joint Learning. Proceedings: of 30th International Conference on Machine Learning (ICML), Atlanta GA, June 2013. http://www.cs.huji.ac.il/~daphna/papers/Zweig_ICML2013.pdf&lt;/ref&gt;

===Exploiting unrelated tasks===
One can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be [[orthogonal]]. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.&lt;ref name=&quot;:3&quot;&gt;Romera-Paredes, B., Argyriou, A., Bianchi-Berthouze, N., &amp; Pontil, M., (2012) Exploiting Unrelated Tasks in Multi-Task Learning. http://jmlr.csail.mit.edu/proceedings/papers/v22/romera12/romera12.pdf&lt;/ref&gt;

=== Transfer of knowledge ===
Related to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep [[convolutional neural network]] GoogLeNet,&lt;ref&gt;{{Cite journal|arxiv = 1409.4842|title = Going Deeper with Convolutions|last = Szegedy|first = C.|date = 2014|journal = Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on|doi = 10.1109/CVPR.2015.7298594|pmid = }}&lt;/ref&gt; an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.&lt;ref&gt;{{Cite web|url = http://www.mit.edu/~9.520/fall15/slides/class24/deep_learning_overview.pdf|title = Deep Learning Overview|date = |accessdate = |website = |publisher = |last = Roig|first = Gemma}}&lt;/ref&gt;

=== Group online adaptive learning ===
Traditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL).&lt;ref&gt;Zweig, A. &amp; Chechik, G. Group online adaptive learning. Machine Learning, DOI 10.1007/s10994-017- 5661-5, August 2017. http://rdcu.be/uFSv&lt;/ref&gt; Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.

== Mathematics ==

=== Reproducing Hilbert space of vector valued functions (RKHSvv) ===
The MTL problem can be cast within the context of RKHSvv (a [[Complete metric space|complete]] [[inner product space]] of [[vector-valued function]]s equipped with a [[Reproducing kernel Hilbert space|reproducing kernel]]). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.&lt;ref name=&quot;:1&quot; /&gt;

==== RKHSvv concepts ====
Suppose the training data set is &lt;math&gt;\mathcal{S}_t =\{(x_i^t,y_i^t)\}_{i=1}^{n_t}&lt;/math&gt;, with &lt;math&gt;x_i^t\in\mathcal{X}&lt;/math&gt;, &lt;math&gt;y_i^t\in\mathcal{Y}&lt;/math&gt;, where &lt;math&gt;t&lt;/math&gt; indexes task, and &lt;math&gt;t \in 1,...,T&lt;/math&gt;. Let &lt;math&gt;n=\sum_{t=1}^Tn_t &lt;/math&gt;. In this setting there is a consistent input and output space and the same [[loss function]] &lt;math&gt; \mathcal{L}:\mathbb{R}\times\mathbb{R}\rightarrow \mathbb{R}_+ &lt;/math&gt; for each task: . This results in the regularized machine learning problem:
{{NumBlk|:|&lt;math display=&quot;block&quot; id=&quot;1&quot;&gt; \min_{f \in \mathcal{H}}\sum _{t=1} ^T \frac{1}{n_t} \sum _{i=1} ^{n_t} \mathcal{L}(y_i^t, f_t(x_i^t))+\lambda ||f||_\mathcal{H} ^2 &lt;/math&gt;|{{EquationRef|1}}}}
where &lt;math&gt; \mathcal{H} &lt;/math&gt; is a vector valued reproducing kernel Hilbert space with functions &lt;math&gt; f:\mathcal X \rightarrow \mathcal{Y}^T &lt;/math&gt; having components &lt;math&gt; f_t:\mathcal{X}\rightarrow \mathcal {Y} &lt;/math&gt;.

The reproducing kernel for the space &lt;math&gt; \mathcal{H} &lt;/math&gt; of functions   &lt;math&gt; f:\mathcal X \rightarrow \mathbb{R}^T &lt;/math&gt; is a symmetric matrix-valued function &lt;math&gt; \Gamma :\mathcal X\times \mathcal X \rightarrow \mathbb{R}^{T \times T} &lt;/math&gt;  , such that &lt;math&gt; \Gamma (\cdot ,x)c\in \mathcal{H} &lt;/math&gt; and the following reproducing property holds:
{{NumBlk|:|&lt;math display=&quot;block&quot;&gt; \langle f(x),c \rangle _ {\mathbb{R}^T} = \langle f,\Gamma (x,\cdot ) c \rangle _ {\mathcal {H}} &lt;/math&gt;|{{EquationRef|2}}}} The reproducing kernel gives rise to a representer theorem showing that any solution to equation {{EquationNote|1}} has the form:
{{NumBlk|:|&lt;math display=&quot;block&quot;&gt; f(x)=\sum _{t=1}^T \sum _{i=1}^{n_t} \Gamma(x,x_i^t)c_i^t &lt;/math&gt;|{{EquationRef|3}}}}

==== Separable kernels ====
The form of the kernel &lt;math&gt;\Gamma &lt;/math&gt; induces both the representation of the [[feature space]] and structures the output across tasks. A natural simplification is to choose a ''separable kernel,'' which factors into separate kernels on the input space &lt;math&gt; \mathcal X &lt;/math&gt; and on the tasks &lt;math&gt; \{1,...,T\} &lt;/math&gt;. In this case the kernel relating scalar components &lt;math&gt; f_t &lt;/math&gt; and &lt;math&gt; f_s &lt;/math&gt; is given by &lt;math display=&quot;inline&quot;&gt; \gamma((x_i,t),(x_j,s )) = k(x_i,x_j)k_T(s,t)=k(x_i,x_j)A_{s,t} &lt;/math&gt;. For vector valued functions  &lt;math&gt; f\in \mathcal H &lt;/math&gt;  we can write &lt;math&gt;\Gamma(x_i,x_j)=k(x_i,x_j)A&lt;/math&gt;, where &lt;math&gt;k&lt;/math&gt; is a scalar reproducing kernel, and &lt;math&gt;A&lt;/math&gt; is a symmetric positive semi-definite &lt;math&gt;T\times T&lt;/math&gt; matrix. Henceforth denote &lt;math&gt; S_+^T=\{\text{PSD matrices} \} \subset \mathbb R^{T \times T} &lt;/math&gt; .

This factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by &lt;math&gt;A&lt;/math&gt;. Methods for non-separable kernels &lt;math&gt;\Gamma &lt;/math&gt; is an current field of research.

For the separable case, the representation theorem is reduced to &lt;math display=&quot;inline&quot;&gt;f(x)=\sum _{i=1} ^N k(x,x_i)Ac_i&lt;/math&gt;. The model output on the training data is then &lt;math&gt;KCA&lt;/math&gt; , where &lt;math&gt;K&lt;/math&gt; is the &lt;math&gt;n \times n&lt;/math&gt; empirical kernel matrix with entries &lt;math display=&quot;inline&quot;&gt;K_{i,j}=k(x_i,x_j)&lt;/math&gt;, and &lt;math&gt;C&lt;/math&gt;  is the &lt;math&gt;n \times T&lt;/math&gt; matrix of rows &lt;math&gt;c_i&lt;/math&gt;.

With the separable kernel, equation  {{EquationNote|1}} can be rewritten as

{{NumBlk|:|&lt;math display=&quot;block&quot; id=&quot;1&quot;&gt; \min _{C\in \mathbb{R}^{n\times T}} V(Y,KCA) + \lambda tr(KCAC^{\top})&lt;/math&gt;|{{EquationRef|P}}}}

where &lt;math&gt;V&lt;/math&gt; is a (weighted) average of &lt;math&gt;\mathcal{L}&lt;/math&gt; applied entry-wise to Y and KCA. (The weight is zero if &lt;math&gt; Y_i^t &lt;/math&gt; is a missing observation).

Note the second term in {{EquationNote|P}} can be derived as follows:

&lt;math&gt;||f||^2_\mathcal{H} = \langle \sum _{i=1} ^n k(\cdot,x_i)Ac_i, \sum _{j=1} ^n k(\cdot ,x_j)Ac_j\rangle_{\mathcal H } &lt;/math&gt;

&lt;math&gt;= \sum _{i,j=1} ^n  \langle   k(\cdot,x_i)A c_i, k(\cdot ,x_j)Ac_j\rangle_{\mathcal H }   &lt;/math&gt; (bilinearity)

&lt;math&gt;= \sum _{i,j=1} ^n \langle k(x_i,x_j)A c_i, c_j\rangle_{\mathbb R^T }   &lt;/math&gt; (reproducing property)

&lt;math&gt;= \sum _{i,j=1} ^n k(x_i,x_j) c_i^\top A  c_j=tr(KCAC^\top ) &lt;/math&gt;

==== Known task structure ====

===== Task structure representations =====
There are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.

'''Regularizer''' - With the separable kernel, it can be shown (below) that &lt;math display=&quot;inline&quot;&gt;||f||^2_\mathcal{H} = \sum_{s,t=1}^T A^\dagger _{t,s} \langle f_s, f_t \rangle _{\mathcal H_k} &lt;/math&gt;, where &lt;math&gt;A^\dagger _{t,s} &lt;/math&gt; is the  &lt;math&gt; t,s &lt;/math&gt; element of the pseudoinverse of &lt;math&gt; A &lt;/math&gt;, and &lt;math&gt;\mathcal H_k &lt;/math&gt; is the RKHS based on the scalar kernel &lt;math&gt; k &lt;/math&gt;, and &lt;math display=&quot;inline&quot;&gt; f_t(x)=\sum _{i=1} ^n k(x,x_i)A_t^\top c_i &lt;/math&gt;. This formulation shows that &lt;math&gt;A^\dagger _{t,s} &lt;/math&gt; controls the weight of the penalty associated with &lt;math display=&quot;inline&quot;&gt;\langle f_s, f_t \rangle _{\mathcal H_k} &lt;/math&gt;. (Note that &lt;math display=&quot;inline&quot;&gt;\langle f_s, f_t \rangle _{\mathcal H_k} &lt;/math&gt; arises from &lt;math display=&quot;inline&quot;&gt;||f_t||_{\mathcal H_k} = \langle f_t, f_t \rangle _{\mathcal H_k} &lt;/math&gt;.)

Proof:

&lt;math&gt;||f||^2_\mathcal{H} = \langle \sum _{i=1} ^n \gamma ((x_i,t_i),\cdot )c_i^{t_i}, \sum _{j=1} ^n \gamma ((x_j,t_j), \cdot )c_j^{t_j}\rangle_{\mathcal H } &lt;/math&gt;

&lt;math&gt;=\sum _{i,j=1} ^n c_i^{t_i} c_j^{t_j}  \gamma ((x_i,t_i),(x_j,t_j)) &lt;/math&gt;

&lt;math&gt;=\sum _{i,j=1} ^n \sum _{s,t=1} ^T c_i^{t} c_j^{s}  k(x_i,x_j)A_{s,t} &lt;/math&gt;

&lt;math&gt;=\sum _{i,j=1} ^n    k(x_i,x_j) \langle  c_i, A c_j\rangle_{\mathbb R^T} &lt;/math&gt;

&lt;math&gt;=\sum _{i,j=1} ^n    k(x_i,x_j) \langle  c_i, A A^\dagger A c_j\rangle_{\mathbb R^T} &lt;/math&gt;

&lt;math&gt;=\sum _{i,j=1} ^n    k(x_i,x_j) \langle  Ac_i,  A^\dagger A c_j\rangle_{\mathbb R^T} &lt;/math&gt;

&lt;math&gt;=\sum _{i,j=1} ^n \sum _{s,t=1} ^T  (Ac_i)^t (A c_j)^s  k(x_i,x_j) A^\dagger_{s,t}  &lt;/math&gt;

&lt;math&gt;= \sum _{s,t=1} ^T  A^\dagger_{s,t} \langle \sum _{i=1} ^n k(x_i,\cdot )(Ac_i)^t, \sum _{j=1} ^n   k(x_j,\cdot )(A c_j)^s   \rangle  _{\mathcal H_k}   &lt;/math&gt;

&lt;math&gt;= \sum _{s,t=1} ^T  A^\dagger_{s,t} \langle f_t, f_s  \rangle  _{\mathcal H_k}   &lt;/math&gt;

'''Output metric''' - an alternative output metric on &lt;math&gt;\mathcal Y^T &lt;/math&gt; can be induced by the inner product &lt;math&gt;\langle y_1,y_2 \rangle _\Theta=\langle y_1,\Theta y_2 \rangle_{\mathbb R^T}  &lt;/math&gt;. With the squared loss there is an equivalence between the separable kernels &lt;math&gt;k(\cdot,\cdot)I_T &lt;/math&gt; under the alternative metric, and &lt;math&gt;k(\cdot,\cdot)\Theta &lt;/math&gt;, under the canonical metric.

'''Output mapping''' - Outputs can be mapped as  &lt;math&gt;L:\mathcal Y^T \rightarrow \mathcal \tilde Y &lt;/math&gt;  to a higher dimensional space to encode complex structures such as trees, graphs and strings.  For linear maps &lt;math&gt;L&lt;/math&gt;, with appropriate choice of separable kernel, it can be shown that  &lt;math&gt;A=L^\top L&lt;/math&gt;.

===== Task structure examples =====
Via the regularizer formulation, one can represent a variety of task structures easily.
* Letting &lt;math display=&quot;inline&quot;&gt;A^\dagger = \gamma I_T + ( \gamma - \lambda)\frac {1} T \bold{1}\bold{1}^\top &lt;/math&gt; (where &lt;math&gt;I_T &lt;/math&gt; is the ''T''x''T'' identity matrix, and &lt;math display=&quot;inline&quot;&gt;\bold{1}\bold{1}^\top &lt;/math&gt; is the ''T''x''T'' matrix of ones) is equivalent to letting &lt;math&gt;\gamma &lt;/math&gt; control the variance &lt;math display=&quot;inline&quot;&gt;\sum_t || f_t - \bar f|| _{\mathcal H_k} &lt;/math&gt;  of tasks from their mean &lt;math display=&quot;inline&quot;&gt;\frac 1 T \sum_t f_t  &lt;/math&gt;. For example, blood levels of some biomarker may be taken on &lt;math&gt;T&lt;/math&gt; patients at &lt;math&gt;n_t&lt;/math&gt; time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients.
* Letting &lt;math&gt; A^\dagger = \alpha I_T +(\alpha - \lambda )M &lt;/math&gt; , where &lt;math&gt; M_{t,s} = \frac 1 {|G_r|} \mathbb I(t,s\in G_r) &lt;/math&gt; is equivalent to letting &lt;math&gt; \alpha &lt;/math&gt; control the variance measured with respect to a group mean: &lt;math&gt; \sum _{r} \sum _{t \in G_r } ||f_t - \frac 1 {|G_r|} \sum _{s\in G_r)} f_s||  &lt;/math&gt;. (Here &lt;math&gt; |G_r| &lt;/math&gt; the cardinality of group r, and &lt;math&gt;  \mathbb I &lt;/math&gt; is the indicator function). For example, people in different political parties (groups) might be regularized together with respect to predicting the favorability rating of a politician. Note that this penalty reduces to the first when all tasks are in the same group.
* Letting &lt;math&gt; A^\dagger = \delta I_T + (\delta -\lambda)L  &lt;/math&gt;, where &lt;math&gt; L=D-M&lt;/math&gt; is the L[[Laplacian matrix|aplacian]] for the graph with adjacency matrix ''M'' giving pairwise similarities of tasks. This is equivalent to giving a larger penalty to the distance separating tasks ''t'' and ''s'' when they are more similar (according to the weight &lt;math&gt; M_{t,s} &lt;/math&gt;,) i.e. &lt;math&gt;\delta &lt;/math&gt; regularizes &lt;math&gt; \sum _{t,s}||f_t - f_s ||_{\mathcal H _k }^2 M_{t,s} &lt;/math&gt;.
* All of the above choices of A also induce the additional regularization term  &lt;math display=&quot;inline&quot;&gt;\lambda \sum_t ||f|| _{\mathcal H_k} ^2 &lt;/math&gt; which penalizes complexity in f more broadly.

==== Learning tasks together with their structure ====
Learning problem {{EquationNote|P}} can be generalized to admit learning task matrix A as follows:
{{NumBlk|:|&lt;math display=&quot;block&quot; id=&quot;1&quot;&gt; \min _{C \in \mathbb{R}^{n\times T},A \in S_+^T} V(Y,KCA) + \lambda tr(KCAC^{\top})+F(A)&lt;/math&gt;|{{EquationRef|Q}}}}

Choice of &lt;math&gt;F:S_+^T\rightarrow \mathbb R_+&lt;/math&gt; must be designed to learn matrices ''A'' of a given type. See &quot;Special cases&quot; below.

===== Optimization of {{EquationNote|Q}} =====
Restricting to the case of [[Convex optimization|convex]] losses and [[Coercive function|coercive]] penalties Ciliberto ''et al.'' have shown that although {{EquationNote|Q}} is not convex jointly in ''C'' and ''A,'' a related problem is jointly convex.

Specifically on the convex set &lt;math&gt; \mathcal C=\{(C,A)\in \mathbb R^{n \times T}\times S_+^T | Range(C^\top KC)\subseteq Range(A)\}&lt;/math&gt;, the equivalent problem

{{NumBlk|:|&lt;math display=&quot;block&quot; id=&quot;1&quot;&gt; \min _{C ,A \in \mathcal C } V(Y,KC) + \lambda tr(A^\dagger C^{\top}KC)+F(A)&lt;/math&gt;|{{EquationRef|R}}}}

is convex with the same minimum value. And if &lt;math&gt; (C_R, A_R)&lt;/math&gt; is a minimizer for {{EquationNote|R}} then &lt;math&gt; (C_R A^\dagger _R, A_R)&lt;/math&gt; is a minimizer for {{EquationNote|Q}}.

{{EquationNote|R}} may be solved by a barrier method on a closed set by introducing the following perturbation:

{{NumBlk|:|&lt;math display=&quot;block&quot; id=&quot;1&quot;&gt; \min _{C \in \mathbb{R}^{n\times T},A \in S_+^T} V(Y,KC) + \lambda tr(A^\dagger (C^{\top}KC+\delta^2I_T))+F(A)&lt;/math&gt;|{{EquationRef|S}}}}

The perturbation via the barrier &lt;math&gt;\delta ^2 tr(A^\dagger)&lt;/math&gt; forces the objective functions to be equal to &lt;math&gt;+\infty&lt;/math&gt; on the boundary of &lt;math&gt;  R^{n \times T}\times S_+^T&lt;/math&gt; .

{{EquationNote|S}} can be solved with a block coordinate descent method, alternating in ''C'' and ''A.'' This results in a sequence of minimizers &lt;math&gt; (C_m,A_m)&lt;/math&gt; in {{EquationNote|S}} that converges to the solution in {{EquationNote|R}} as &lt;math&gt; \delta_m \rightarrow 0&lt;/math&gt;, and hence gives the solution to {{EquationNote|Q}}.

===== Special cases =====
'''[[Regularization by spectral filtering|Spectral penalties]]''' - Dinnuzo ''et al''&lt;ref&gt;{{Cite journal|url = http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Dinuzzo_54.pdf |title = Learning output kernels with block coordinate descent.|last = Dinuzzo|first = Francesco|date = 2011|journal = Proceedings of the 28th International Conference on Machine Learning (ICML-11)|doi = |pmid = |access-date = }}&lt;/ref&gt; suggested setting ''F'' as the Frobenius norm &lt;math&gt; \sqrt{tr(A^\top A)}&lt;/math&gt;. They optimized {{EquationNote|Q}} directly using block coordinate descent, not accounting for difficulties at the boundary of &lt;math&gt;\mathbb R^{n\times T} \times S_+^T&lt;/math&gt;.

'''Clustered tasks learning''' - Jacob ''et al''&lt;ref&gt;{{Cite journal|url = |title = Clustered multi-task learning: A convex formulation|last = Jacob|first = Laurent|date = 2009|journal = Advances in neural information processing systems|doi = |pmid = |access-date = }}&lt;/ref&gt; suggested to learn ''A'' in the setting where ''T''  tasks are organized in ''R'' disjoint clusters. In this case let &lt;math&gt; E\in \{0,1\}^{T\times R}&lt;/math&gt; be the matrix with &lt;math&gt; E_{t,r}=\mathbb I (\text{task }t\in \text{group }r)&lt;/math&gt;. Setting &lt;math&gt; M = I - E^\dagger E^T&lt;/math&gt;, and  &lt;math&gt; U = \frac 1 T \bold{11}^\top &lt;/math&gt;, the task matrix &lt;math&gt; A^\dagger  &lt;/math&gt;  can be parameterized as a function of &lt;math&gt; M  &lt;/math&gt;: &lt;math&gt; A^\dagger(M) = \epsilon _M U+\epsilon_B (M-U)+\epsilon (I-M)  &lt;/math&gt; , with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation &lt;math&gt; \mathcal S_c = \{M\in S_+^T:I-M\in S_+^T \and tr(M) = r \} &lt;/math&gt;. In this formulation,  &lt;math&gt; F(A)=\mathbb I(A(M)\in \{A:M\in \mathcal S_C\})  &lt;/math&gt;.

===== Generalizations =====
'''Non-convex penalties''' - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.

'''Non-separable kernels''' - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.

==Applications==

===Spam filtering===
Using the principles of MTL, techniques for collaborative [[spam filtering]] that facilitates personalization have been proposed. In large scale open membership email systems, most users do not label enough messages for an individual local [[classifier (mathematics)|classifier]] to be effective, while the data is too noisy to be used for a global filter across all users. A hybrid global/individual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public. This can be accomplished while still providing sufficient quality to users with few labeled instances.&lt;ref&gt;Attenberg, J., Weinberger, K., &amp; Dasgupta, A. Collaborative Email-Spam Filtering with the Hashing-Trick. http://www.cse.wustl.edu/~kilian/papers/ceas2009-paper-11.pdf&lt;/ref&gt;

===Web search===
Using boosted [[decision trees]], one can enable implicit data sharing and regularization. This learning method can be used on web-search ranking data sets. One example is to use ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. It has been demonstrated that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.&lt;ref&gt;Chappelle, O., Shivaswamy, P., &amp; Vadrevu, S. Multi-Task Learning for Boosting
with Application to Web Search Ranking. http://www.cse.wustl.edu/~kilian/papers/multiboost2010.pdf&lt;/ref&gt;

=== RoboEarth ===
In order to facilitate transfer of knowledge, IT infrastructure is being developed. One such project, RoboEarth, aims to set up an open source internet database that can be accessed and continually updated from around the world. The goal is to facilitate a cloud-based interactive knowledge base, accessible to technology companies and academic institutions, which can enhance the sensing, acting and learning capabilities of robots and other artificial intelligence agents.&lt;ref name=&quot;RoboEarth&quot;&gt;[http://www.roboearth.org/motivation Description of RoboEarth Project]&lt;/ref&gt;

==Software package==
The Multi-Task Learning via StructurAl Regularization (MALSAR) Matlab package&lt;ref&gt;Zhou, J., Chen, J. and Ye, J. MALSAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University, 2012. http://www.public.asu.edu/~jye02/Software/MALSAR. [http://www.public.asu.edu/~jye02/Software/MALSAR/Manual.pdf On-line manual]&lt;/ref&gt;  implements the following multi-task learning algorithms:
* Mean-Regularized Multi-Task Learning&lt;ref&gt;Evgeniou, T., &amp; Pontil, M. (2004). [https://pdfs.semanticscholar.org/1ea1/91c70559d21be93a4d128f95943e80e1b4ff.pdf Regularized multi–task learning]. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 109–117).&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Evgeniou | first1 = T. | last2 = Micchelli | first2 = C. | last3 = Pontil | first3 = M. | year = 2005 | title = Learning multiple tasks with kernel methods | url = http://jmlr.org/papers/volume6/evgeniou05a/evgeniou05a.pdf | format = PDF | journal = Journal of Machine Learning Research | volume = 6 | issue = | page = 615 }}&lt;/ref&gt;
* Multi-Task Learning with Joint Feature Selection&lt;ref&gt;{{cite journal | last1 = Argyriou | first1 = A. | last2 = Evgeniou | first2 = T. | last3 = Pontil | first3 = M. | year = 2008a | title = Convex multi-task feature learning | url = | journal = Machine Learning | volume = 73 | issue = | pages = 243–272 | doi=10.1007/s10994-007-5040-8}}&lt;/ref&gt;
* Robust Multi-Task Feature Learning&lt;ref&gt;Chen, J., Zhou, J., &amp; Ye, J. (2011). [http://www.academia.edu/download/44101186/Integrating_low-rank_and_group-sparse_st20160325-15067-1mftmbg.pdf Integrating low-rank and group-sparse structures for robust multi-task learning]. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.&lt;/ref&gt;
* Trace-Norm Regularized Multi-Task Learning&lt;ref&gt;Ji, S., &amp; Ye, J. (2009). [http://www.machinelearning.org/archive/icml2009/papers/151.pdf An accelerated gradient method for trace norm minimization]. Proceedings of the 26th Annual International Conference on Machine Learning (pp. 457–464).&lt;/ref&gt;
* Alternating Structural Optimization&lt;ref&gt;{{cite journal | last1 = Ando | first1 = R. | last2 = Zhang | first2 = T. | year = 2005 | title = A framework for learning predictive structures from multiple tasks and unlabeled data | url = http://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf | journal = The Journal of Machine Learning Research | volume = 6 | issue = | pages = 1817–1853 }}&lt;/ref&gt;&lt;ref&gt;Chen, J., Tang, L., Liu, J., &amp; Ye, J. (2009). [http://leitang.net/papers/ICML09_CASO.pdf A convex formulation for learning shared structures from multiple tasks]. Proceedings of the 26th Annual International Conference on Machine Learning (pp. 137–144).&lt;/ref&gt;
* Incoherent Low-Rank and Sparse Learning&lt;ref&gt;Chen, J., Liu, J., &amp; Ye, J. (2010). [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3783291/ Learning incoherent sparse and low-rank patterns from multiple tasks]. Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1179–1188).&lt;/ref&gt;
* Robust Low-Rank Multi-Task Learning
* Clustered Multi-Task Learning&lt;ref&gt;Jacob, L., Bach, F., &amp; Vert, J. (2008). [https://hal-ensmp.archives-ouvertes.fr/docs/00/32/05/73/PDF/cmultitask.pdf Clustered multi-task learning: A convex formulation]. Advances in Neural Information Processing Systems， 2008&lt;/ref&gt;&lt;ref&gt;Zhou, J., Chen, J., &amp; Ye, J. (2011). [http://papers.nips.cc/paper/4292-clustered-multi-task-learning-via-alternating-structure-optimization.pdf Clustered multi-task learning via alternating structure optimization]. Advances in Neural Information Processing Systems.&lt;/ref&gt;
* Multi-Task Learning with Graph Structures

==See also==
* [[Artificial Intelligence]]
* [[Artificial neural network]]
* [[Evolutionary computation]]
* [[Human-based genetic algorithm]]
* [[Kernel methods for vector output]]
* [[Machine Learning]]
* [[Robot learning]]

==References==

{{Reflist}}

==External links==
* [http://big.cs.uiuc.edu/webpage/cumulativeLearning/cumulativeLearning.html The Biosignals Intelligence Group at UIUC]
* [http://www.cse.wustl.edu/~kilian/research/multitasklearning/multitasklearning.html Washington University at St. Louis Depart. of Computer Science]

===Software===
* [http://www.public.asu.edu/~jye02/Software/MALSAR/index.html The Multi-Task Learning via Structural Regularization Package]
* [http://klcl.pku.edu.cn/member/sunxu/code.htm Online Multi-Task Learning Toolkit (OMT)] A general-purpose online multi-task learning toolkit based on [[conditional random field]] models and [[stochastic gradient descent]] training ([[C Sharp (programming language)|C#]], [[.NET Framework|.NET]])

</text>
      <sha1>avnz59iwyzr2t3vfonp4cwmjmc5gh8u</sha1>
    </revision>
  </page>
  <page>
    <title>AIVA</title>
    <ns>0</ns>
    <id>52642349</id>
    <revision>
      <id>795341555</id>
      <parentid>790829404</parentid>
      <timestamp>2017-08-13T16:38:40Z</timestamp>
      <contributor>
        <ip>2003:4E:2C4C:6F00:5C9A:46C7:D93D:5BC7</ip>
      </contributor>
      <comment>/* Discography */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6638">{{Infobox artist
 | name                 = Aiva
 | nationality  = [[Luxembourgish]]
 | style = [[Classical music]]
 | website              = {{URL|www.aiva.ai}}
}}
'''AIVA''' (Artificial Intelligence Virtual Artist) is a [[deep learning]] algorithm applied to music composition. In June 2016, it became the first system of [[algorithmic composition]] to be registered, as a [[composer]], in an authors' right Society [[SACEM]].&lt;ref&gt;{{cite web|url=http://www.siliconluxembourg.lu/aiva-the-artificial-intelligence-composing-classical-music/ |title=AIVA the Artificial Intelligence composing Classical Music |date= |accessdate=2016-10-21}}&lt;/ref&gt;

==Description==
Created in February 2016, AIVA specializes in [[Classical music|Classical]] and [[Symphonic music]] composition.&lt;ref&gt;{{cite web|url=http://www.siliconluxembourg.lu/aiva-the-artificial-intelligence-composing-classical-music/ |title=AIVA the Artificial Intelligence composing Classical Music |date= |accessdate=2016-10-21}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://iq.intel.fr/aiva-lia-qui-compose-de-la-musique-classique/ |title=AIVA : l’IA qui compose de la musique classique |date= |accessdate=2016-11-29}}&lt;/ref&gt; It became the world’s first virtual composer to be recognized by a music society ([[SACEM]]).&lt;ref&gt;{{cite web|url=https://repertoire.sacem.fr/droit-auteur/AIVA%20SYMPHONIC%20FANTASY%20OPUS%207%20THE%20AWAKENING%20IN%20G%20SHARP%20MINOR/3552274701?query=aiva&amp;filters=parties#searchBtn
 |title=AIVA composer of Classical Music}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.tf1.fr/tf1/jt-we/videos/demain-l-intelligence-artificielle-entre-fascination-apprehension.html
 |title=TF1 News, subject &quot;A.I. between fascination and apprehension&quot; 25th June, 2017}}&lt;/ref&gt;
By reading a large collection of existing works of classical music (written by human composers such as [[Bach]], [[Beethoven]], [[Mozart]]) AIVA is capable of understanding concepts of music theory and composing on its own.&lt;ref&gt;{{cite web|url=http://radio.rtl.lu/emissiounen/greis-valerius/1560993.html/1560993.html
 |title=AIVA - RTL.lu Radio Luxembourg}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=RYDyhYaSevU
 |title=France 2, Stupéfiant ! Les robots vont-ils remplacer les Artistes, 16th May, 2017}}&lt;/ref&gt; The algorithm AIVA is based on [[deep learning]] and [[reinforcement learning]] architectures&lt;ref&gt;{{cite web|url=http://www.wort.lu/de/kultur/aiva-une-jeune-start-up-qui-ne-manque-pas-d-ambitions-la-musique-classique-recomposee-57fbba6b5061e01abe83a1c2|title=La musique Classique recomposée |date= |accessdate=2016-10-11}}&lt;/ref&gt;

== Discography ==
AIVA is a published composer;&lt;ref&gt;SACEM Database, https://repertoire.sacem.fr/resultats?filters=parties&amp;query=aiva&amp;nbWorks=20&lt;/ref&gt; its first studio album “Genesis” was released in November 2016 and counts 20 original and 4 orchestrated works composed by AIVA. The tracks were recorded by human musicians:
Olivier Hecho as the Conductor of the Aiva Sinfonietta Orchestra and [[Eric Breton]] as a Pianist.&lt;ref&gt;CD Aiva album &quot;Genesis&quot; released November 2016.&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://medium.com/@aivatech/composing-the-music-of-the-future-4af560603988#.t8d6dkxi8 |title=Composing the music of the future |date= |accessdate=2016-09-24}}&lt;/ref&gt;
[[File:GENESIS_AIVA_CD.jpg|thumb|right|AIVA album &quot;Genesis&quot;]]
* 2016 CD album « Genesis » Hv-Com – LEPM 048427

Track listing:
{{Track listing
| headline        = AIVA, &quot;Genesis&quot;. All compositions by AIVA. Produced by Pierre Barreau
| title1          = Celtic Dance, Op. 14, in A minor
| length1         = 02:23
| title2          = Symphonic Fantasy in G sharp minor, Op. 7, ''The Awakening''
| length2         = 03:23
| title3          = Symphonic Fantasy in A minor, Op. 21, ''Genesis''
| length3         = 02:50
| title4          = Octet No. 1 in D major, Op. 3, &quot;A little chamber music&quot;
| length4         = 01:40
| title5          = Aiva, Op. 1 for piano solo in D major
| length5         = 04:15
| title6          = Aiva Op. 2 for piano solo
| length6         = 01:48
| title7          = Aiva Op. 3 for piano solo
| length7         = 01:31
| title8          = Aiva Op. 4 for piano solo
| length8         = 01:43
| title9          = Aiva Op. 5 for piano solo [or harpsichord]
| length9         = 02:08
| title10         = Aiva Op. 6 for piano solo
| length10        = 01:08
| title11         = Aiva Op. 8 For piano solo
| length11        = 02:27
| title12         = Aiva Op. 9 for piano solo
| length12        = 01:51
| title13         = Aiva Op. 10 for piano solo
| length13        = 03:43
| title14         = Aiva Op. 11 for piano solo &quot;Rhapsody&quot;
| length14        = 03:55
| title15         = Variation Op. 12 for piano solo
| length15        = 03:00
| title16         = Aiva Op. 13 for piano solo
| length16        = 02:49
| title17         = Aiva Op. 15 for piano solo
| length17        = 02:30
| title18         = Aiva Op. 16 for piano solo
| length18        = 01:55
| title19         = Aiva Op. 17 for piano solo
| length19        = 03:00
| title20         = Aiva Op. 18 for piano solo
| length20        = 02:16
| title21         = Aiva Op. 19 for piano solo
| length21        = 01:45
| title22         = Aiva Op. 20 for piano solo
| length22        = 01:57
| title23         = Aiva Op. 21 for piano solo, ''Genesis''
| length23        = 03:00
| title24 = Aiva Op. 22 for piano solo
| length24        = 03:32
}}


Avignon Symphonic Orchestra [ORAP] also performed Aiva's compositions[http://www.ledauphine.com/search?q=orap+aiva+intelligence+artificielle&amp;x=1&amp;y=1] in April 2017.&lt;ref&gt;{{Cite news|url=|title=Europe 1 France, chronicle by Thierry Geffrotin, April 16th, 2017|last=|first=|date=|work=|access-date=|archive-url=|archive-date=|dead-url=}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=|title=Le Dauphiné Libéré / Vaucluse Matin, April 17th, 2017, &quot;De la musique née d'un cerveau artificiel&quot;|last=|first=|date=|work=|access-date=|archive-url=|archive-date=|dead-url=}}&lt;/ref&gt;

==Example of scores composed by AIVA==
This is the preview of the score Op. n°3 for piano solo &quot;A little chamber music&quot;, composed by AIVA.

[[File:Opus 3 for Piano Solo.pdf|This is the score for AIVA's Opus 3 for Piano Solo, composed by the Artificial Intelligence]]
==See also==
{{Portal|music|artificial intelligence}}
*[[Music and Artificial Intelligence]]
*[[Applications of artificial intelligence|Applications of Artificial Intelligence]]
*[[Computer Music]]

==References==

&lt;references /&gt;






</text>
      <sha1>7tszp8q4l5qfrkforeh6j72u183hngf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Unsupervised learning</title>
    <ns>14</ns>
    <id>52763828</id>
    <revision>
      <id>776659011</id>
      <parentid>758107912</parentid>
      <timestamp>2017-04-22T12:41:29Z</timestamp>
      <contributor>
        <username>Olexa Riznyk</username>
        <id>9148555</id>
      </contributor>
      <comment>+cat main</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="43">{{Cat main}}

</text>
      <sha1>oskod0rsljwgi83u1nf88yucs3ce3zg</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Semisupervised learning</title>
    <ns>14</ns>
    <id>52763829</id>
    <revision>
      <id>758107927</id>
      <timestamp>2017-01-03T14:57:18Z</timestamp>
      <contributor>
        <username>Kri</username>
        <id>253188</id>
      </contributor>
      <comment>Created category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29"></text>
      <sha1>qcspncnn8d2jwjscqlboh74nlrgod3b</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Supervised learning</title>
    <ns>14</ns>
    <id>52763867</id>
    <revision>
      <id>758108726</id>
      <timestamp>2017-01-03T15:04:10Z</timestamp>
      <contributor>
        <username>Kri</username>
        <id>253188</id>
      </contributor>
      <comment>Created category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29"></text>
      <sha1>qcspncnn8d2jwjscqlboh74nlrgod3b</sha1>
    </revision>
  </page>
  <page>
    <title>RoboEarth</title>
    <ns>0</ns>
    <id>38782554</id>
    <revision>
      <id>813843080</id>
      <parentid>782489589</parentid>
      <timestamp>2017-12-05T14:52:11Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v477)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5249">{{refimprove|date=March 2013}}
'''RoboEarth''' is a network and database repository where [[robot]]s can share information and learn from each other and a cloud for outsourcing heavy computation tasks. The project that has been described as a &quot;[[World Wide Web]]&lt;!--and [[Wikipedia]]--&gt; for robots&quot;. The project brings together researchers from five major universities in Germany, the Netherlands and Spain and is backed by the [[European Union]].&lt;ref&gt;{{cite web|title=Europe launches RoboEarth: 'Wikipedia for robots'|url=https://www.usatoday.com/story/tech/2014/01/17/robot-robotics-roboearth-europe-munich-netherlands/4575021/|publisher=USA TODAY|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=European researchers have created a hive mind for robots and it's being demoed this week|url=https://www.engadget.com/2014/01/14/roboearth-demo/|publisher=Engadget|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Robots test their own world wide web, dubbed RoboEarth|url=http://www.bbc.com/news/technology-25727110|publisher=BBC News|accessdate=4 January 2017|date=14 January 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title='Wikipedia for robots': Because bots need an Internet too|url=https://www.cnet.com/news/wikipedia-for-robots-because-bots-need-an-internet-too/|publisher=CNET|accessdate=4 January 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=New Worldwide Network Lets Robots Ask Each Other Questions When They Get Confused|url=http://www.popsci.com/technology/article/2013-03/new-cloud-engine-robots-can-learn-each-other|publisher=Popular Science|accessdate=4 January 2017}}&lt;/ref&gt;

==Purpose==&lt;!--/Use/Description/...--&gt;
It allows robots to:

* Store and share information: Robots can use the common representation provided by the RoboEarth language and the scalable storage provided by the RoboEarth database to store and share information. This has the following key advantages:
** Significantly increases the speed of learning by leveraging the experience of other robots.
***e.g.: Robots sharing articulation models.
** Allows developers to create general robot task instructions rather than programming individual robots on a case-by-case basis.
***e.g., Sharing a common action-recipe to serve drinks with two different robots in different environments.
* Offload computation: Robots can use the vast computational infrastructure available on the web for computationally heavy tasks including planning, probabilistic inference, and mapping, among many others.
**e.g. Cloud-based collaborative mapping with low-cost robots, knowledge processing as a service with KnowRob, and the cloud robotics framework [[Rapyuta]].
* Collaborate: Robots can use the cloud as a common medium to collaborate and achieve a common task.
**e.g., Two robots collaboratively performing a serve-a-drink task in a mock hospital room, and generating and maintaining consistent world state estimates based on object detections by multiple robots using WIRE.

In addition to the cloud-based infrastructure, RoboEarth offers ROS-compatible, robot-unspecific components for high level control of the robot. See software-components for more details.

RoboEarth offers a Cloud Robotics infrastructure, which includes everything needed to close the loop from robot to the cloud and back to the robot. RoboEarth’s World-Wide-Web style database stores knowledge generated by humans – and robots – in a machine-readable format. Data stored in the RoboEarth knowledge base include software components, maps for navigation (e.g., object locations, world models), task knowledge (e.g., action recipes, manipulation strategies), and object recognition models (e.g., images, object models).

The RoboEarth Cloud Engine (also called Rapyuta) makes powerful computation available to robots. It allows robots to offload their heavy computation to secure computing environments in the cloud with minimal configuration. The Cloud Engine’s computing environments provide high bandwidth access to the RoboEarth knowledge repository enabling robots to benefit from the experience of other robots
&lt;ref&gt;{{cite web|title=RoboEarth.Org |url=http://www.roboearth.org/project-scope}}&lt;/ref&gt;
&lt;/blockquote&gt;

In late 2009, the RoboEarth project was awarded a 4-year funding grant from the [[European Commission]]’s Cognitive Systems and Robotics Initiative in order to develop their networked database platform, [[Rapyuta]], and to develop proof-of-concept systems to demonstrate its use. In January 2014, it was officially announced that 'Wikipedia for Robots' had been launched.&lt;ref&gt;{{cite web|title=Europe Launches RoboEarth Wikipedia for Robots|url=http://dailydigest.com/22734/europe-launches-roboearth-wikipedia-for-robots/|publisher=Daily Digest|accessdate=17 January 2014}}&lt;/ref&gt;

==Spin offs==
RoboEarth has a spin off called [[Robohow]].&lt;ref&gt;{{cite web|title=These Robots Can Teach Other Robots How to Do New Things|url=https://singularityhub.com/2017/05/26/these-robots-can-teach-other-robots-how-to-do-new-things/|publisher=SingularityHub|accessdate=27 May 2017}}&lt;/ref&gt;

== References ==
{{reflist}}





&lt;!--?--&gt;

{{robotics-stub}}</text>
      <sha1>b0j8zd5nbtjh2qi5tv2ucy3begw888b</sha1>
    </revision>
  </page>
  <page>
    <title>VGG Image Annotator</title>
    <ns>0</ns>
    <id>52992310</id>
    <revision>
      <id>801542418</id>
      <parentid>793899989</parentid>
      <timestamp>2017-09-20T09:08:55Z</timestamp>
      <contributor>
        <username>Adutta.np</username>
        <id>5714698</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2600">{{Infobox software
| name = VGG Image Annotator
| logo = [[File:VGG Image Annotator Logo.png]]
| logo alt =
| logo caption =
| screenshot = &lt;!-- Image name is enough. --&gt;
| screenshot alt =
| caption =
| collapsible = &lt;!-- Any text here will collapse the screenshot. --&gt;
| author = Abhishek Dutta
| developer =
| discontinued = &lt;!-- Set to yes if software is discontinued, otherwise omit. --&gt;
| ver layout = &lt;!-- simple (default) or stacked --&gt;
| latest release version = 1.0.3
| latest release date = 07 August 2017
| latest preview version =
| latest preview date = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| status = Stable Release
| programming language = Javascript, HTML, CSS
| operating system = All
| platform = Web Browser
| size = ~ 200 KB
| language = English
| language count = &lt;!-- Number only --&gt;
| language footnote =
| genre =
| license = BSD-2 clause license
| alexa =
| website = http://www.robots.ox.ac.uk/~vgg/software/via/
| repo = https://gitlab.com/vgg/via
| standard =
| AsOf =
}}

'''VGG Image Annotator''' (VIA) is an open source project developed at the Visual Geometry Group&lt;ref&gt;{{Cite web|url=http://www.robots.ox.ac.uk/~vgg/|title=Visual Geometry Group Home Page|website=www.robots.ox.ac.uk|access-date=2017-04-04}}&lt;/ref&gt; and released under the BSD-2 clause license.&lt;ref&gt;{{cite web|title=VIA License|url=https://gitlab.com/vgg/via/blob/develop/LICENSE|accessdate=26 January 2017}}&lt;/ref&gt; With this standalone application, you can define regions in an image and create a textual description of those regions. Such image regions and descriptions are useful for [[Supervised learning|supervised training]] of [[machine learning]] algorithms.&lt;ref&gt;{{cite web|title=VGG Image Annotator (VIA)|url=http://www.robots.ox.ac.uk/~vgg/software/via/|accessdate=26 January 2017}}&lt;/ref&gt;

== Features ==
* based solely on [[HTML]], [[Cascading Style Sheets|CSS]] and [[JavaScript|Javascript]] (no external Javascript libraries)
* can be used off-line (full application in a single html file of size &lt; 200KB)
* requires nothing more than a modern [[web browser]]
* supported region shapes: rectangle, circle, ellipse, polygon and point
* import/export region data in [[Comma-separated values|CSV]] and [[JSON]] file format

== External links ==
* http://www.robots.ox.ac.uk/~vgg/software/via/
* https://gitlab.com/vgg/via

== See also ==
* [[List of Manual Image Annotation Tools]]

== References ==
{{Reflist}}




</text>
      <sha1>9p9v8ebnjj6rmvipnpythhsal50j1bn</sha1>
    </revision>
  </page>
  <page>
    <title>User:Maria Schuld/sandbox</title>
    <ns>2</ns>
    <id>53049812</id>
    <revision>
      <id>768712765</id>
      <parentid>768565959</parentid>
      <timestamp>2017-03-05T10:24:22Z</timestamp>
      <contributor>
        <username>CommonsDelinker</username>
        <id>2304267</id>
      </contributor>
      <comment>Replacing Qml_approaches.tiff.tif with [[File:Qml_approaches.tif]] (by [[commons:User:CommonsDelinker|CommonsDelinker]] because: Robot: Removing double file extension).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33538">[[File:Qml approaches.tif|thumb|Four different approaches to combine the disciplines of quantum computing and machine learning. The first letter refers to whether the data was generated by a classical or quantum system, while the second letter defines whether a classical or quantum information processing device is used.]]
'''Quantum machine learning''' is an emerging interdisciplinary research area between [[quantum physics]] and [[data mining]] that summarises efforts to combine methods from [[quantum information science]] and [[machine learning]].&lt;ref&gt;Maria Schuld, Ilya Sinayiskiy, and Francesco Petruccione (2014) An introduction to quantum machine learning, Contemporary Physics, {{doi|10.1080/00107514.2014.964942}} (preprint available at {{arXiv|1409.3097}})&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Wittek |first=Peter |title=Quantum Machine Learning: What Quantum Computing Means to Data Mining |publisher=Academic Press |year=2014 |url=http://www.sciencedirect.com/science/book/9780128009536 |isbn=978-0-12-800953-6}}&lt;/ref&gt;&lt;ref&gt;Jeremy Adcock, Euan Allen, Matt Day, Stefan Frick, Janna Hinchliff, Mack Johnson, Sam Morley-Short, Sam Pallister, Alasdair Price and Stasja Stanisic (2015) Advances in Quantum Machine Learning, {{arXiv|1512.02900}}&lt;/ref&gt; One can distinguish four different ways of how to merge the two parent disciplines. Quantum machine learning algorithms can use the advantages of [[Quantum computing|quantum computation]] in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a [[quantum computer]].&lt;ref&gt;see for example, Nathan Wiebe, Ashish Kapoor, and Krysta M. Svorey (2014) Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning, {{arXiv|1401.2142v2}}&lt;/ref&gt;&lt;ref&gt;Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost (2014) Quantum algorithms for supervised and unsupervised machine learning, {{arXiv|1307.0411v2}}&lt;/ref&gt;&lt;ref&gt;Seokwon Yoo, Jeongho Bang, Changhyoup Lee, and Jinhyoung Lee, A quantum speedup in machine learning: finding an N-bit Boolean function for a classification, New Journal of Physics 16 (2014) 103014, {{arXiv|1303.6055}}&lt;/ref&gt; On the other hand, one can apply classical methods of machine learning to analyse quantum systems. Furthermore, techniques from quantum physics can be adapted for tasks like deep learning. Lastly, a quantum device can be used to recognise patterns in quantum data.

==Quantum-enhanced machine learning==

Quantum-enhanced machine learning refers to [[quantum algorithm]]s that solve tasks in machine learning, thereby improving a classical machine learning method. So called &quot;quantum machine learning algorithms&quot; typically require to encode a 'classical' dataset into a quantum computer, effectively turning it into quantum information. After this, quantum information processing routines can be applied and the result of the quantum computation is read out through measuring the quantum system. For example, the outcome of the measurement of a qubit could reveal the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal [[quantum computer]] to be tested, others have been imlemented on small-scale or special purpose quantum devices &lt;reference to experiments?&gt;.

===Linear algebra simulation with quantum amplitudes ===

One line of approaches is based on the idea of 'amplitude encoding', or to associate the [[Probability amplitude|amplitudes]] of a quantum state with the inputs and outputs of computations.&lt;ref name=&quot;Patrick Rebentrost 2014&quot;&gt;Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd (2014) Quantum support vector machine for big data classification, Physical Review Letters 113 130501&lt;/ref&gt;&lt;ref name=&quot;Nathan Wiebe 2012&quot;&gt;Nathan Wiebe, Daniel Braun, and Seth Lloyd (2012) Quantum algorithm for data fitting. Physical
Review Letters, 109(5):050505.&lt;/ref&gt;&lt;ref name=&quot;Maria Schuld 2016&quot;&gt;Maria Schuld, Ilya Sinayskiy, Francesco Petruccione (2016) Prediction by linear regression on a quantum computer, Physical Review A, 94, 2, 022342&lt;/ref&gt;&lt;ref&gt;Zhikuan Zhao, Jack K Fitzsimons, and Joseph F Fitzsimons (2015) Quantum assisted gaussian process regression. {{arXiv|1512.03929}}&lt;/ref&gt; Since &lt;math&gt;n&lt;/math&gt; qubits are described by &lt;math&gt;2^n&lt;/math&gt; complex amplitudes, this information encoding can allow for an exponentially compact representation. As an intuition, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomial in the number of qubits &lt;math&gt;n&lt;/math&gt;, which is a logarithmic growth in the number of amplitudes and thereby the dimension of the input.

Many quantum machine learning algorithms in this category are based on variations of the [[quantum algorithm for linear systems of equations]],&lt;ref&gt;Aram W. Harrow, Avinatan Hassidim and Seth Lloyd (2009) Quantum Algorithm for Linear Systems of Equations, Physical Review Letters 103 150502, {{arXiv|0811.3171}}&lt;/ref&gt; which under specific conditions performs a matrix inversion with logarithmic dependence on the dimensions of the matrix. One of these conditions is that a Hamiltonian which entrywise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse&lt;ref&gt;Dominic W Berry, Andrew M Childs, and Robin Kothari (2015) Hamiltonian simulation with nearly optimal dependence on all parameters. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 792–809. IEEE&lt;/ref&gt; or low rank.&lt;ref&gt;Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost (2014) Quantum principal component analysis. Nature Physics, 10:631–633&lt;/ref&gt; Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a [[System of linear equations|linear system of equations]], for example in least-squares linear regression,&lt;ref name=&quot;Nathan Wiebe 2012&quot;/&gt;&lt;ref name=&quot;Maria Schuld 2016&quot;/&gt; the least-squares version of support vector machines,&lt;ref name=&quot;Patrick Rebentrost 2014&quot;/&gt; Gaussian processes.&lt;ref&gt;Zhikuan Zhao, Jack K Fitzsimons, and Joseph F Fitzsimons (2015) Quantum assisted Gaussian process regression. {{arXiv|1512.03929}}&lt;/ref&gt;

A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires to initialise a quantum system such that its amplitudes reflect the features of the entire dataset. Although efficient methods regarding the number of qubits are known for very specific cases,&lt;ref&gt;Andrei N Soklakov and Rüdiger Schack. Efficient state preparation for a register of quantum bits. Physical Review A, 73(1):012307, 2006.&lt;/ref&gt;&lt;ref&gt;Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access memory. Physical Review Letters, 100(16):160501, 2008.&lt;/ref&gt; this step easily hides the complexity of the task.&lt;ref&gt;Scott Aaronson. Read the fine print. Nature Physics, 11(4):291–293, 2015.&lt;/ref&gt;

===Quantum machine learning algorithms based on Grover search===

Another approach to constructing quantum machine learning algorithms uses [[amplitude amplification]] methods based on [[Grover's search]] algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructered search task, such as finding the closest datapoints to a new feature vector in [[k-nearest neighbour|k-nearest neighbors algorithm]].&lt;ref&gt;see for example, Nathan Wiebe, Ashish Kapoor, and Krysta M. Svore (2014) Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning, {{arXiv|1401.2142v2}}&lt;/ref&gt; Another application is a quadratic speedup in the training of [[perceptrons|perceptron]].&lt;ref&gt;Ashish Kapoor, Nathan Wiebe, and Krysta Svore. Quantum perceptron models. In Advances In Neural Information Processing Systems, pages 3999–4007, 2016.&lt;/ref&gt;

Closely related to amplitude amplification are [[quantum walks|Quantum walk]], offering the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm &lt;ref&gt;Giuseppe Davide Paparo and MA Martin-Delgado. Google in a quantum network. Scientific Reports, 2, 2012.&lt;/ref&gt; as well as the reinforcement learning model of projective simulation.&lt;ref&gt;Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and Hans J Briegel. Quantum speedup for active learning agents. Physical Review X, 4(3):031002, 2014.&lt;/ref&gt;

===Quantum sampling techniques===

Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.

A computationally hard problem, which is key for some relevant machine learning tasks is the estimation of averages over probabilistic models defined in terms of a [[Boltzmann distribution]].

Sampling from generic probabilistic models, such as a Boltzmann distribution, is hard. For this reason, algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.&lt;ref&gt;{{cite journal|last1=Biswas|first1=Rupak|last2=Jiang|first2=Zhang|last3=Kechezi|first3=Kostya|last4=Knysh|first4=Sergey|last5=Mandrà|first5=Salvatore|last6=O’Gorman|first6=Bryan|last7=Perdomo-Ortiz|first7=Alejando|last8=Pethukov|first8=Andre|last9=Realpe-Gómez|first9=John|last10=Rieffel|first10=Eleanor|title=A NASA perspective on quantum computing: Opportunities and challenges|year=2016|journal=Parallel Computing}}&lt;/ref&gt;

Some research groups have recently explored the use of quantum annealing hardware for the learning of [[Boltzmann machine]]s and [[deep neural networks]].&lt;ref name=Adachi2015&gt;{{cite arXiv |last1=Adachi|first1=Steven H.|last2=Henderson|first2=Maxwell P.|date=2015 |title=Application of quantum annealing to training of deep neural networks|eprint=1510.06356}}&lt;/ref&gt;&lt;ref name=Benedetti2016a&gt;{{cite arXiv |last1=Benedetti|first1=Marcello|last2=Realpe-Gómez|first2=John|last3=Biswas|first3=Rupak|last4=Perdomo-Ortiz|first4=Alejandro|date=2016 |title=Quantum-assisted learning of graphical models with arbitrary pairwise connectivity|eprint=1609.02542}}&lt;/ref&gt;&lt;ref name=Benedetti2016b&gt;{{cite journal |last1=Benedetti|first1=Marcello|last2=Realpe-Gómez|first2=John|last3=Biswas|first3=Rupak|last4=Perdomo-Ortiz|first4=Alejandro|date=2016 |title=Estimation of effective temperatures in quantum annealers for sampling applications: A case study with possible applications in deep learning|doi=10.1103/PhysRevA.94.022308|volume=94|issue=2|pages=022308}}&lt;/ref&gt;&lt;ref name=&quot;William G 1611&quot;&gt;{{cite arXiv |last1=Korenkevych|first1=Dmytro|last2=Xue|first2=Yanbo|last3=Bian|first3=Zhengbing|last4=Chudak|first4=Fabian|last5=Macready|first5=William G.|last6=Rolfe|first6=Jason|last7=Andriyash|first7=Evgeny|date=2016 |title=Benchmarking quantum hardware for training of fully visible Boltzmann machines|eprint=1611.04528}}&lt;/ref&gt; The standard approach to the learning of Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. In contrast to their use for optimization, when applying quantum annealing hardware to the learning of Boltzmann machines, the control parameters (instead of the qubits’ states) are the relevant variables of the problem. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.

The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures.&lt;ref name=Benedetti2016b /&gt;   Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks.&lt;ref name=Adachi2015 /&gt; The same device was later used to train a fully connected Boltzmann Machine to generate, reconstruct, and classify images that closely resemble (low resolution) handwritten digits, among other synthetic datasets.&lt;ref name=Benedetti2016a /&gt; In both cases, the models trained by the D-Wave 2X displayed a similar or better performance in terms of quality (i.e. in term of the values of likelihood reached).

The ultimate question that drives this endeavor is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.

Inspired by the success of Boltzmann Machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed.&lt;ref&gt;{{cite arXiv |last1=Amin|first1=Mohammad H.|last2=Andriyash|first2=Evgeny|last3=Rolfe|first3=Jason|last4=Kulchytskyy|first4=Bohdan|last5=Melko|first5=Roger |date=2016 |title=Quantum Boltzmann machines|eprint=1601.02036}}&lt;/ref&gt; Due to the non-commutative nature of quantum mechanics, the training process of the Quantum Boltzmann Machine (QBM) can become nontrivial. This problem was to some extend circumvented by introducing bounds on the quantum probabilities. This allowed the authors to train the QBM efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.&lt;ref name=Benedetti2016a /&gt;&lt;ref name=&quot;William G 1611&quot;/&gt;

On a related proposal, quantum computing was used not only to reduce the time required to train a deep restricted Boltzmann machine, but also to provide a richer and more comprehensive framework for deep learning than classical computing.&lt;ref&gt;{{cite arXiv |last1=Wiebe|first1=Nathan|last2=Kapoor|first2=Ashish|last3=Svore|first3=Krysta M. |date=2014 |title=Quantum deep learning|eprint=1412.3489}}&lt;/ref&gt; The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well known classical counterparts.

Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced [[Markov logic network]]s exploit the symmetries and the locality structure of the [[Graphical model|probabilistic graphical model]] generated by a [[first-order logic]] template.&lt;ref&gt;{{cite arXiv |last1=Wittek|first1=Peter|last2=Gogolin|first2=Christian |date=2016 |title=Quantum Enhanced Inference in Markov Logic Networks|eprint=1611.08104}}&lt;/ref&gt; This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.

==Learning about quantum systems==
The term quantum machine learning can also be used for approaches that apply classical methods of machine learning to problems of quantum information theory.  For example, when experimentalists have to deal with incomplete information on a quantum system or source, [[Bayesian statistics|Bayesian]] methods and concepts of algorithmic learning can be fruitfully applied. This includes machine learning approaches to quantum state classification,&lt;ref&gt;{{cite journal|last2=Calsamiglia|first2=J.|last3=Munoz-Tapia|first3=R.|last4=Bagan|first4=E.|year=2012|title=Quantum learning without quantum memory|url=|journal=Scientific Reports|volume=2|issue=|page=708|arxiv=1106.2742|bibcode=2012NatSR...2E.708S|doi=10.1038/srep00708|last1=Sentıs|first1=G.}}&lt;/ref&gt; Hamiltonian learning,&lt;ref&gt;{{cite journal|last2=Granade|first2=Christopher|last3=Ferrie|first3=Christopher|last4=Cory|first4=David|year=2014|title=Quantum Hamiltonian learning using imperfect quantum resources|url=|journal=Physical Review A|volume=89|issue=|page=042314|arxiv=1311.5269|bibcode=2014PhRvA..89d2314W|doi=10.1103/physreva.89.042314|last1=Wiebe|first1=Nathan}}&lt;/ref&gt; and learning an unknown [[Unitary matrix|unitary transformation]].&lt;ref&gt;Alessandro Bisio, Giulio Chiribella, Giacomo Mauro D’Ariano, Stefano Facchini, and Paolo Perinotti (2010) Optimal quantum learning of a unitary transformation, Physical Review A 81, 032324, {{arXiv|arXiv:0903.0543}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last2=Junghee Ryu|first2=Bang|last3=Yoo|first3=Seokwon|last4=Pawłowski|first4=Marcin|last5=Lee|first5=Jinhyoung|year=2014|title=A strategy for quantum algorithm design assisted by machine learning|url=|journal=New Journal of Physics|volume=16|issue=|page=073017|arxiv=1304.2169|bibcode=2014NJPh...16a3017K|doi=10.1088/1367-2630/16/1/013017|last1=Jeongho|first1=}}&lt;/ref&gt;

=== Classical machine learning algorithms to learn about quantum systems ===
A variety of classical machine learning algorithms have been proposed to tackle problems in quantum information and technologies. This becomes particularly relevant with the increasing ability to experimentally control and prepare larger quantum systems.

In this context, many machine learning techniques can be used to more efficiently address experimentally relevant problems. Indeed, the problem of turning huge and noisy data sets into meaningful information is by no means unique to the quantum laboratory. This results in many machine learning techniques being naturally adapted to tackle these problems. Notable examples are those of extracting information on a given quantum state,&lt;ref name=&quot;Sasaki&quot;&gt;{{Cite journal|last=Sasaki|first=M.|last2=Carlini|first2=A.|last3=Jozsa|first3=R.|date=2001-07-17|title=Quantum Template Matching|url=http://arxiv.org/abs/quant-ph/0102020|journal=Physical Review A|volume=64|issue=2|doi=10.1103/PhysRevA.64.022317|issn=1050-2947}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Sasaki|first=Masahide|date=2002-01-01|title=Quantum learning and universal quantum matching machine|url=https://journals.aps.org/pra/abstract/10.1103/PhysRevA.66.022303|journal=Physical Review A|volume=66|issue=2|doi=10.1103/PhysRevA.66.022303}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Sentís|first=G.|last2=Calsamiglia|first2=J.|last3=Munoz-Tapia|first3=R.|last4=Bagan|first4=E.|date=2011-06-14|title=Quantum learning without quantum memory|url=http://arxiv.org/abs/1106.2742|journal=arXiv:1106.2742 [quant-ph]}}&lt;/ref&gt; or on a given quantum process.&lt;ref name=&quot;Bisio&quot;&gt;{{Cite journal|last=Bisio|first=A.|last2=Chiribella|first2=G.|last3=D'Ariano|first3=G. M.|last4=Facchini|first4=S.|last5=Perinotti|first5=P.|date=2010-03-25|title=Optimal quantum learning of a unitary transformation|url=http://arxiv.org/abs/0903.0543|journal=Physical Review A|volume=81|issue=3|doi=10.1103/PhysRevA.81.032324|issn=1050-2947}}&lt;/ref&gt;&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|last=Granade|first=Christopher E.|last2=Ferrie|first2=Christopher|last3=Wiebe|first3=Nathan|last4=Cory|first4=D. G.|date=2012-10-03|title=Robust Online Hamiltonian Learning|url=http://arxiv.org/abs/1207.1655|journal=New Journal of Physics|volume=14|issue=10|pages=103013|doi=10.1088/1367-2630/14/10/103013|issn=1367-2630}}&lt;/ref&gt;&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|last=Wiebe|first=Nathan|last2=Granade|first2=Christopher|last3=Ferrie|first3=Christopher|last4=Cory|first4=D. G.|date=2014-05-14|title=Hamiltonian Learning and Certification Using Quantum Resources|url=http://arxiv.org/abs/1309.0876|journal=Physical Review Letters|volume=112|issue=19|doi=10.1103/PhysRevLett.112.190501|issn=0031-9007}}&lt;/ref&gt;&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|last=Wiebe|first=Nathan|last2=Granade|first2=Christopher|last3=Ferrie|first3=Christopher|last4=Cory|first4=David G.|date=2014-04-17|title=Quantum Hamiltonian Learning Using Imperfect Quantum Resources|url=http://arxiv.org/abs/1311.5269|journal=Physical Review A|volume=89|issue=4|doi=10.1103/PhysRevA.89.042314|issn=1050-2947}}&lt;/ref&gt;&lt;ref name=&quot;Banchi&quot;&gt;{{Cite journal|last=Banchi|first=Leonardo|last2=Pancotti|first2=Nicola|last3=Bose|first3=Sougato|date=2016-07-19|title=Quantum gate learning in qubit networks: Toffoli gate without time-dependent control|url=http://www.nature.com/articles/npjqi201619|journal=npj Quantum Information|language=en|volume=2|doi=10.1038/npjqi.2016.19|issn=2056-6387}}&lt;/ref&gt; A partial list of problems that have been addressed with machine learning techniques includes:
# The problem of identifying an accurate model for the dynamics of a quantum system, through the reconstruction of it [[Hamiltonian]].&lt;ref name=&quot;:0&quot; /&gt;&lt;ref name=&quot;:1&quot; /&gt;&lt;ref name=&quot;:2&quot; /&gt;
# Extracting information from unknown states.&lt;ref name=&quot;Sasaki&quot;/&gt;&lt;ref&gt;{{Cite journal|last=Sasaki|first=Masahide|date=2002-01-01|title=Quantum learning and universal quantum matching machine|url=http://journals.aps.org/pra/abstract/10.1103/PhysRevA.66.022303|journal=Physical Review A|volume=66|issue=2|doi=10.1103/PhysRevA.66.022303}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Sentís|first=Gael|last2=Guţă|first2=Mădălin|last3=Adesso|first3=Gerardo|date=2015-07-09|title=Quantum learning of coherent states|url=http://link.springer.com/article/10.1140/epjqt/s40507-015-0030-4|journal=EPJ Quantum Technology|language=en|volume=2|issue=1|pages=17|doi=10.1140/epjqt/s40507-015-0030-4|issn=2196-0763}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Sentís|first=G.|last2=Calsamiglia|first2=J.|last3=Muñoz-Tapia|first3=R.|last4=Bagan|first4=E.|date=2012-10-05|title=Quantum learning without quantum memory|url=http://www.nature.com/articles/srep00708|journal=Scientific Reports|language=en|volume=2|doi=10.1038/srep00708|issn=2045-2322|pmc=3464493|pmid=23050092}}&lt;/ref&gt;
# Learning unknown unitaries and measurements.&lt;ref name=&quot;Bisio&quot;/&gt;
# Engineering of quantum gates from qubit networks with pairwise interactions, using time dependent&lt;ref&gt;{{Cite journal|last=Zahedinejad|first=Ehsan|last2=Ghosh|first2=Joydip|last3=Sanders|first3=Barry C.|date=2016-11-16|title=Designing High-Fidelity Single-Shot Three-Qubit Gates: A Machine Learning Approach|url=http://arxiv.org/abs/1511.08862|journal=Physical Review Applied|volume=6|issue=5|doi=10.1103/PhysRevApplied.6.054005|issn=2331-7019}}&lt;/ref&gt; or independent&lt;ref name=&quot;Banchi&quot;/&gt; Hamiltonians.
# Generation of new quantum experiments by finding the combination of quantum operations that allow for the generation of a quantum state with a certain desired property.&lt;ref&gt;{{Cite journal|last=Krenn|first=Mario|date=2016-01-01|title=Automated Search for new Quantum Experiments|url=http://link.aps.org/doi/10.1103/PhysRevLett.116.090405|journal=Physical Review Letters|volume=116|issue=9|doi=10.1103/PhysRevLett.116.090405}}&lt;/ref&gt;

===Quantum Reinforcement Learning===
Inside the field of [[quantum machine learning]], a topic that has risen interest in the past few years is quantum reinforcement learning.&lt;ref&gt;Daoyi Dong, Chunlin Chen, Hanxiong Li, Tzih-Jong Tarn, Quantum reinforcement learning. {{doi|10.1109/TSMCB.2008.925743}}, IEEE Trans. Syst. Man Cybern. B Cybern. 38, 1207-1220 (2008).&lt;/ref&gt;&lt;ref&gt;Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and Hans J. Briegel, {{doi|10.1103/PhysRevX.4.031002}} Phys. Rev. X 4, 031002 (2014).&lt;/ref&gt;&lt;ref&gt;Vedran Dunjko, Jacob M. Taylor, and Hans J. Briegel, {{doi|10.1103/PhysRevLett.117.130501}} Phys. Rev. Lett. 117, 130501 (2016).&lt;/ref&gt;&lt;ref&gt;D. Crawford, A. Levit, N. Ghadermarzy, J. S. Oberoi, and P. Ronagh, Reinforcement Learning Using Quantum Boltzmann Machines. {{arXiv|1612.05695}}&lt;/ref&gt;&lt;ref&gt;Lucas Lamata, Basic protocols in quantum reinforcement learning with superconducting circuits, {{arXiv|1701.05131}}&lt;/ref&gt; The aim in this research line is to analyze the behaviour of quantum agents interacting with a certain environment, which may be either classical or quantum. Depending on the chosen interaction between the quantum agent and the environment, the former will receive a different reward that will allow it, after some iterations, to achieve a policy for succeeding in its goal. A tradeoff must be balanced between exploration and exploitation. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Preliminary implementations of these kind of protocols in superconducting circuits have been proposed.

== Quantum learning theory ==

While quantum computers can outperform classical ones in some machine learning tasks, a full characterization of statistical learning theory with quantum resources is still under development.&lt;ref&gt;{{cite arXiv |last1=Arunachalam |first1=Srinivasan|last2=de Wolf|first2=Ronald |date=2017 |title=A Survey of Quantum Learning Theory |eprint=1701.06806}}&lt;/ref&gt; The sampling complexity of broad classes of quantum and classical machine learning algorithms are polynomially equivalent,&lt;ref&gt;{{cite arXiv |last1=Arunachalam |first1=Srinivasan|last2=de Wolf|first2=Ronald |date=2016 |title=Optimal Quantum Sample Complexity of Learning Algorithms|eprint=1607.00932}}&lt;/ref&gt; but their computational complexities need not be. The ability of quantum computers to search for the most informative samples can polynomially improve the expected number of samples necessary to learn a concept.&lt;ref&gt;{{cite journal|last1=Wiebe|first1=Nathan|last2=Kapoor|first2=Ashish|last3=Svore|first3=Krysta|title=Quantum Perceptron Models|journal=Advances in Neural Information Processing Systems|volume=29|pages=3999–4007|year=2016}}&lt;/ref&gt;

Classically, learning an inductive model splits into a training and and an application phase. The model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.&lt;ref&gt;{{cite arXiv |last1=Monràs |first1=Alex|last2=Sentís|first2=Gael|last3=Wittek|first3=Peter |date=2016 |title=Inductive quantum learning: Why you are doing it almost right|eprint=1605.07541}}&lt;/ref&gt; This clear splitting allows to talk about bounds on generalization performance in the quantum case, which, in turn, makes it relevant to analyse model complexity.&lt;ref&gt;{{cite journal|last1=Atıcı|first1=Alp|last2=Servedio|first2=Rocco A.|title=Improved Bounds on Quantum Learning Algorithms|journal=Quantum Information Processing|volume=4|issue=5|pages=355–386|year=2005|doi=10.1109/CCC.2001.933881}}&lt;/ref&gt;

==Implementations and experiments==

Li et al. (2015) &lt;ref&gt;Zhaokai Li, Xiaomei Liu,Nanyang Xu, and Jiangfeng Du (2015), Experimental Realization of a Quantum Support Vector Machine,{{doi|10.1103/PhysRevLett.114.140504}}&lt;/ref&gt; did the first experimental implementation of quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state [[Nuclear Magnetic Resonance]] (NMR) quantum computer.  The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors, this mapping helps to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space the Quantum support vector machine was implemented to classify the unknown input vector. The readout avoids quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal. Neigovzen et al. (2009)&lt;ref&gt;Rodion Neigovzen, Jorge L. Neves, Rudolf Sollacher, and Steffen J. Glaser (2009), Quantum pattern recognition with liquid-state nuclear magnetic resonance {{doi|10.1103/PhysRevA.79.042321}}&lt;/ref&gt; devised and implemented a quantum pattern recognition algorithm on NMR. They implemented the quantum Hopfield network and the input data and memorized data is being mapped to Hamiltonians, this eventually helps to use the adiabatic quantum computation.

Cai et al. (2015)&lt;ref&gt;X.-D. Cai, D. Wu, Z.-E. Su, M.-C. Chen, X.-L. Wang, Li Li, N.-L. Liu, C.-Y. Lu, and J.-W. Pan. Entanglement-Based Machine Learning on a Quantum Computer. Phys. Rev. Lett. 114, 110504, 20 March 2015{{doi|10.1103/PhysRevLett.114.110504}}&lt;/ref&gt; were first to experimentally demonstrate quantum machine learning on a photonic quantum computer and showed that the distance between two vectors and their inner product can indeed be computed quantum mechanically. Brunner et al. (2013)&lt;ref&gt;Daniel Brunner, Miguel C. Soriano, Claudio R. Mirasso and Ingo Fischer. Parallel photonic information processing  at  gigabyte  per  second  data  rates  using  transient states. Nature Communication, 4:1364, January 2013.{{doi|10.1038/ncomms2368}}&lt;/ref&gt; used photonics to demonstrate simultaneous spoken digit and speaker recognition and chaotic time-series prediction at data rates beyond 1 gigabyte per second. Furthermore, Tezak &amp; Mabuchi (2015)&lt;ref&gt;Nikolas Tezak and Hideo Mabuchi. A coherent perceptron  for  all-optical  learning. EPJ Quantum Technology20152:10, 30 April 2015.{{doi|10.1140/epjqt/s40507-015-0023-3}}&lt;/ref&gt; proposed using non-linear photonics to implement an all-optical linear classifier capable of learning the classification boundary iteratively from training data through a feedback rule.

In an early experiment, Neven et al. (2009)&lt;ref&gt;{{cite web|url=http://static.googleusercontent.com/media/www.google.com/de//googleblogs/pdfs/nips_demoreport_120709_research.pdf|format=PDF|title=NIPS 2009 Demonstration: Binary Classification using Hardware Implementation of Quantum Annealing |publisher=Static.googleusercontent.com |accessdate=26 November 2014}}&lt;/ref&gt; used the adiabatic [[D-Wave Systems|D-Wave]] quantum computer to detect cars in digital images using regularized boosting with a nonconvex objective function. Benedetti et al. (2016)&lt;ref&gt;J. Benedetti, Marcello, John Realpe-Gómez, Rupak Biswas, and Alejandro Perdomo-Ortiz, Quantum-assisted learning of graphical models with arbitrary pairwise connectivity,{{arXiv|1609.02542}}&lt;/ref&gt; trained a probabilistic generative models with arbitrary pairwise connectivity on the [[D-Wave Systems|D-Wave]] quantum computer.  In their paper, they showed that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.

In recent years, also leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, [[NASA]] and the [[Universities Space Research Association]] launched the [[Quantum Artificial Intelligence Lab]] which explores the use of the adiabatic [[D-Wave Systems|D-Wave]] quantum computer.&lt;ref&gt;{{cite web |url=https://plus.google.com/+QuantumAILab |title=Google Quantum A.I. Lab Team
 |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=31 January 2017 |website= Google Plus |publisher=Google |access-date=31 January 2017 |quote=}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://ti.arc.nasa.gov/tech/dash/physics/quail/ |title= NASA Quantum Artificial Intelligence Laboratory
 |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=31 January 2017 |website= NASA |publisher=NASA |access-date=31 January 2017 |quote=}}&lt;/ref&gt; Researchers at Microsoft Research conduct theoretical research in the field of quantum machine learning but until this point no experimental implementations are known.&lt;ref&gt;{{cite web |url=https://www.microsoft.com/en-us/research/publication/quantum-nearest-neighbor-algorithms-for-machine-learning/ |title=Quantum Nearest-neighbor Algorithms for Machine Learning |last=Wiebe |first=Nathan |last2=Kapoor |first2=Ashish |last3=Svore |first3=Krysta |date=1 March 2015 |website=Microsoft Research |publisher=Microsoft Research |access-date=31 January 2017 |quote=}}&lt;/ref&gt;

Recently, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical [[memristor]].&lt;ref&gt;P. Pfeiffer, I. L. Egusquiza, M. Di Ventra, M. Sanz, and E. Solano, Quantum Memristors Scientific Reports 6, 29507 (2016) {{doi|10.1038/srep29507}}&lt;/ref&gt; This kind of device incorporates a superconducting quantum bit together with the possibility to perform a weak measurement on the system and apply a feed-forward mechanism. Recently, an implementation of a quantum memristor in superconducting circuits has been proposed.&lt;ref&gt;J. Salmilehto, F. Deppe, M. Di Ventra, M. Sanz, and E. Solano, Quantum Memristors with Superconducting Circuits,{{arXiv|1603.04487}}&lt;/ref&gt; A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional [[quantum neural network]].

==References==
{{Reflist|colwidth=30em}}




</text>
      <sha1>85bm33971vw813ptryvuhh6fuojp8jw</sha1>
    </revision>
  </page>
  <page>
    <title>Qloo</title>
    <ns>0</ns>
    <id>41732818</id>
    <revision>
      <id>814270077</id>
      <parentid>807871092</parentid>
      <timestamp>2017-12-07T20:16:15Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v478)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9330">{{Infobox dot-com company
| name     = Qloo
| logo     = [[File:Qloo-app-logo.png|160px]]
| caption          =
| type     = Private
| foundation       = 2012
| founder          = Alex Elias &lt;br /&gt; Jay Alger
| location         = [[New York City]]
| industry         = Internet &lt;br /&gt; [[Artificial Intelligence]]
| revenue          =
| operating_income =
| net_income       =
| num_employees    = &lt;!-- two? --&gt;
| url              = {{url|http://qloo.com}}
}}

'''Qloo''' (pronounced &quot;clue&quot;) is a company that uses [[artificial intelligence]] (AI). An [[application programming interface]] (API) provides cultural correlations.&lt;ref name=culturalai&gt;{{Cite web|url=https://techcrunch.com/2016/06/15/leonardo-dicaprio-barry-sternlicht-back-qloo-a-cultural-recommendation-engine/|title=Leonardo DiCaprio, Barry Sternlicht back Qloo, a cultural recommendation engine|last=Kolodny|first=Lora|website=TechCrunch|access-date=2016-06-15}}&lt;/ref&gt; It was founded by [[Alex Elias]] and received funding from [[Leonardo DiCaprio]], [[Barry Sternlicht]] and [[Pierre Lagrange]].

Qloo establishes consumer preference correlations via [[machine learning]] across multiple proprietary, customer and open-source data across cultural domains including music, film, television, dining, nightlife, fashion, books and travel. The [[recommender system]] uses AI to predict correlations for further applications.&lt;ref name=culturalai /&gt;

==History==
Qloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger.&lt;ref&gt;Jon Swartz, [https://www.usatoday.com/story/tech/2012/11/08/qloo-amazon-pandora-netflix/1684053/ “Getting a Qloo on where to find similar tastes,”] ''[[USA Today]]'', November 8, 2012.&lt;/ref&gt; Elias was formerly a [[hedge fund]] manager with APE Capital.&lt;ref name=&quot;mflamm&quot;&gt;Matthew Flamm, [http://www.crainsnewyork.com/article/20131114/TECHNOLOGY/131119933 “Cedric the Entertainer gets a Qloo,”] ''[[Crain's New York Business]]'', November 14, 2013.&lt;/ref&gt;&lt;ref&gt;Joao-Pierre S. Ruth, [http://www.xconomy.com/new-york/2013/11/14/qloo-now-beta-wants-learn-different-cultural-tastes/ “Qloo, Now Out of Beta, Wants to Learn Your Different Cultural Tastes,”] [[Xconomy]], November 14, 2013.&lt;/ref&gt;&lt;ref&gt;Jessica Naziri, [http://articles.latimes.com/2013/mar/17/business/la-fi-tn-qloo-inspiration-engine-20130317 “Start-up Sunday: Qloo, a ‘cultural discovery’ search engine,”] ''[[Los Angeles Times]]'', March 17, 2013.&lt;/ref&gt;
He graduated from the [[University of Southern California]], and then developed his idea at law school at [[New York University]].&lt;ref name=&quot;sclayton&quot;&gt;Sara Clayton, [http://dailytrojan.com/2014/01/14/alumnus-creates-standout-recommendation-app/ “Alumnus creates standout recommendation app,”] ''[[Daily Trojan]]'', January 14, 2014.&lt;/ref&gt;
Alger was formerly the CEO of the digital agency Deepend.&lt;ref name=&quot;lkolodny&quot;&gt;Lora Kolodny, [https://blogs.wsj.com/venturecapital/2013/11/19/cedric-the-entertainer-building-an-audience-is-like-building-a-startup/ “Cedric The Entertainer: Building An Audience Is Like Building A Startup,”] ''[[Wall Street Journal]]'', November 19, 2013.&lt;/ref&gt;&lt;ref name=&quot;pbond&quot;&gt;Paul Bond, [http://www.hollywoodreporter.com/news/new-digital-firm-qloo-raises-655512 “New Digital Firm Qloo Raises $3 Million From Hollywood Insiders,”] ''[[The Hollywood Reporter]]'', November 14, 2013.&lt;/ref&gt;

Qloo was tested on a private website in April 2012.
In 2012, Qloo raised $1.4 million in seed funding from investors including [[Cedric the Entertainer]], [[Danny Masterson]], and venture capital firm Kindler Capital.&lt;ref name=&quot;lkolodny&quot;/&gt;&lt;ref name=&quot;tspangler&quot;/&gt;&lt;ref name=&quot;cshu&quot;/&gt;
Qloo had a public beta release in November 2012 after its initial funding.&lt;ref name=&quot;bahernandez&quot;&gt;Brian Anthony Hernandez, [http://mashable.com/2012/11/29/qloo/ “Qloo Finds New Things For You To Watch, Read, Listen, Eat and Wear,”] [[Mashable]], November 29, 2012.&lt;/ref&gt;&lt;ref&gt;Ki Mae Heussner, [http://gigaom.com/2012/11/08/with-1-4m-qloo-finds-food-films-and-fashion-that-match-your-taste/ “With $1.4M, Qloo finds films, fashion, food and more that match your taste,”] [[GigaOm]], November 8, 2012.&lt;/ref&gt;

In 2013, the company raised an additional $1.6 million from [[Cross Creek Pictures]] founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of [[Maktoob]], an Internet services company purchased by [[Yahoo!]] for $164 million in 2009.&lt;ref name=&quot;pbond&quot;/&gt;
On November 14, 2013, a website and an [[iPhone]] app were announced.&lt;ref&gt;{{Cite news|url=https://techcrunch.com/2013/11/14/with-3m-in-funding-qloo-launches-to-let-you-discover-interesting-content-in-eight-categories/|title=With $3M In Funding, Qloo Launches To Let You Discover Interesting Content In Eight Categories|last=Shu|first=Catherine|work=TechCrunch|access-date=2017-09-27|language=en}}&lt;/ref&gt;&lt;ref name=&quot;mflamm&quot;/&gt; The company later released an [[Android (operating system)|Android]] app, and [[Tablet computer|tablet]] versions, in mid-2014.&lt;ref name=&quot;tspangler&quot;&gt;Todd Spangler, [https://variety.com/2013/digital/news/recommendation-app-qloo-raise-1-6-mil-from-investors-including-producer-tommy-thompson-1200831885/ “Recommendation App Qloo Raises $1.6 Mil from Investors Including Producer Tommy Thompson,”] ''[[Variety (magazine)|Variety]]'', November 14, 2013.&lt;/ref&gt;

In 2016, Qloo secured $4.5 million in [[venture capital]] investment.&lt;ref&gt;{{cite web|title=Recommendation service Qloo closes Series A with DiCaprio as a backer|url=http://www.bizjournals.com/newyork/news/2016/06/15/recommendation-service-qloo-closes-series-a-with.html|publisher=Biz Journals|date=June 15, 2016}}&lt;/ref&gt; The $4.5 million was split between a number of investors, including [[Barry Sternlicht]], [[Pierre Lagrange]] and [[Leonardo DiCaprio]].&lt;ref name=techcrunch&gt;{{cite web|last1=Kolodny|first1=Lora|title=Leonardo DiCaprio, Barry Sternlicht back Qloo, a cultural recommendation engine|url=https://techcrunch.com/2016/06/15/leonardo-dicaprio-barry-sternlicht-back-qloo-a-cultural-recommendation-engine/|publisher=[[TechCrunch]]|date=June 15, 2016}}&lt;/ref&gt; In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures and [[Elton John]].&lt;ref&gt;{{Cite web|url=http://oneclickitsolution.com/2017/07/12/elton-john-invests-in-qloo-a-startup-that-analyzes-your-taste/|title=Elton John invests in Qloo, a startup that analyzes your taste|website=OneClick|language=en-US|access-date=2017-10-06}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://techcrunch.com/2017/07/11/elton-john-backs-qloo/|title=Elton John invests in Qloo, a startup that analyzes your taste|last=Ha|first=Anthony|work=TechCrunch|access-date=2017-10-06|language=en}}&lt;/ref&gt;

Following the investment, the founders stated in an interview with [[Tech Crunch]] that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients.&lt;ref name=techcrunch /&gt; At the time, clients already included [[Fortune 500]] companies such as [[Twitter]], [[PepsiCo]] and [[BMW]].&lt;ref&gt;{{cite web|last1=Chakrapani|first1=Harini|title=Leonardo DiCaprio is the latest celebrity to invest in this Manhattan startup|url=http://www.crainsnewyork.com/article/20160615/TECHNOLOGY/160619929/recommendation-app-developer-qloo-raises-4-5-million-from-leonardo-dicaprio-in-a-funding-round-that-included-barry-sternlicht-of-starwood-capital-group-and-pierre-lagrange-of-glg-partners-dicaprio-joins-cedric-the-entertainer-danny-masterson-of-that-70s-show|publisher=[[Crain Communications]]|date=June 15, 2016}}&lt;/ref&gt;

==Services and features==
Qloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including:  film, music, television, dining, nightlife, fashion, books and travel.&lt;ref&gt;Alan McGlade, [https://www.forbes.com/sites/alanmcglade/2013/12/27/cracking-the-code-for-film-marketing/ “Cracking The Code For Film Marketing,”] ''[[Forbes]]'', December 27, 2013.&lt;/ref&gt; Each category contains subcategories.&lt;ref&gt;Dani Fankhauser, [http://mashable.com/2013/12/21/books-taylor-swift/ “If You Like Taylor Swift’s Music, You Might Enjoy These Books,”] Mashable, December 21, 2013.&lt;/ref&gt;

Qloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories.&lt;ref name=&quot;bahernandez&quot;/&gt;&lt;ref name=&quot;cshu&quot;&gt;Catherine Shu, [https://techcrunch.com/2013/11/14/with-3m-in-funding-qloo-launches-to-let-you-discover-interesting-content-in-eight-categories/ “With $3M In Funding, Qloo Launches To Let You Discover Interesting Content In Eight Categories,”] [[TechCrunch]], November 14, 2013.&lt;/ref&gt; Users then rate the suggestions, providing it with feedback for future suggestions.&lt;ref name=&quot;mflamm&quot;/&gt;&lt;ref name=&quot;pbond&quot;/&gt;&lt;ref name=&quot;lkolodny&quot;/&gt;&lt;ref name=&quot;mflamm&quot;/&gt;&lt;ref name=&quot;sclayton&quot;/&gt;
Qloo has partnerships with companies such as [[Expedia]] and [[iTunes]].&lt;ref name=&quot;pbond&quot;/&gt;

==References==
{{Reflist|2}}

==External links==
{{Official website|http://www.qloo.com/}}

[[Category:Companies based in New York City]]






</text>
      <sha1>8cbxi6800h8cw6zbt2snbq44b13x7wo</sha1>
    </revision>
  </page>
  <page>
    <title>Instance selection</title>
    <ns>0</ns>
    <id>53279262</id>
    <revision>
      <id>800426842</id>
      <parentid>796818326</parentid>
      <timestamp>2017-09-13T13:39:41Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>/* Instance selection algorithms */clean up spacing around punctuation, replaced: ,v → , v using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4520">{{cleanup|date=March 2017|reason=Article needs linked into other articles in WP.}}

'''Instance selection''' &lt;ref name=GARCIA_2015&gt;S. García, J. Luengo, and F. Herrera, Data preprocessing in data mining. Springer, 2015.&lt;/ref&gt; (or dataset reduction, or dataset condensation) is an important [[Data pre-processing]] step that can be applied in many [[Machine learning]] (or [[Data mining]]) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.

Algorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality.

== Instance selection algorithms ==

The literature provides several different algorithms for instance selection. They can be distinguished from each other according to several different criteria. Considering this, instance selection algorithms can be grouped in two main classes, according to what instances they select: algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes. Within the category of algorithms that select instances at the boundaries it is possible to cite DROP3,&lt;ref name=DROP_2000&gt;D. R. Wilson and T. R. Martinez, Reduction techniques for instance-based learning algorithms, Machine learning, vol. 38, no. 3, pp. 257–286, 2000.&lt;/ref&gt; ICF &lt;ref name=ICF_2002&gt;H. Brighton and C. Mellish, Advances in instance selection for instance-based learning algorithms, Data mining and knowledge discovery, vol. 6, no. 2, pp. 153–172, 2002.&lt;/ref&gt; and LSBo.&lt;ref name=LSBo_LSSm_2015&gt;E. Leyva, A. González, and R. Pérez, Three new instance selection methods based on local sets: A comparative study with several approaches from a bi-objective perspective, Pattern Recognition, vol. 48, no. 4, pp. 1523–1537, 2015.&lt;/ref&gt; On the other hand, within the category of algorithms that select internal instances it is possible to mention ENN &lt;ref name=ENN_1972&gt;D. L. Wilson, “Asymptotic properties of nearest neighbor rules using edited data,” Systems, Man and Cybernetics, IEEE Transactions on, no. 3, pp. 408–421, 1972.&lt;/ref&gt; and LSSm.&lt;ref name=LSBo_LSSm_2015 /&gt; In general, algorithm such as ENN and LSSm are used for removing harmful (noisy) instances from the dataset. They do not reduce the data as the algorithms that select border instances, but they remove instances at the boundaries that have negative impact in the data ming task. They can be used bay other instance selection algorithms, as a filtering step. For example, the ENN algorithm is used by DROP3 as the first step, and the LSSm algorithm is used by LSBo.

There is also another group os algorithms that adopt different selection criteria. For example, the algorithms LDIS &lt;ref name=LDIS_2015&gt;Carbonera, Joel Luis, and Mara Abel. A density-based approach for instance selection. IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI), 2015.&lt;/ref&gt; and CDIS &lt;ref name=CDIS_2016&gt;Carbonera, Joel Luis, and Mara Abel. A novel density-based approach for instance selection. IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI), 2016.&lt;/ref&gt; select the densest instances in a given arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and effective running time) than other algorithms, such as DROP3 and ICF.

==References==
{{reflist}}



</text>
      <sha1>qw1bfml7arqh54h2gnkl8qbclbu9xzo</sha1>
    </revision>
  </page>
  <page>
    <title>Machine learning control</title>
    <ns>0</ns>
    <id>53802271</id>
    <revision>
      <id>798863935</id>
      <parentid>787169027</parentid>
      <timestamp>2017-09-04T07:31:24Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6167">'''Machine learning control (MLC)''' is a subfield of [[machine learning]] and [[control theory]]
which solves [[optimal control]] problems with methods of [[machine learning]].
Key applications are complex nonlinear systems
for which [[linear control theory]] methods are not applicable.

== Types of problems and tasks ==
Four types of problems are commonly encountered.
* Control parameter identification: MLC translates to a parameter identification&lt;ref name=Baeck1993&gt;Thomas Bäck &amp; Hans-Paul Schwefel (Spring 1993) [http://doi.org/10.1162/evco.1993.1.1.1 &quot;An overview of evolutionary algorithms for parameter optimization&quot;], [[Evolutionary Computation (journal)|Journal of Evolutionary Computation (MIT Press)]], vol. 1, no. 1, pp. 1-23&lt;/ref&gt; if the structure of the control law is given but the parameters are unknown. One example is the [[genetic algorithm]] for optimizing coefficients of a [[PID controller]]&lt;ref name=Benard2015aiaa&gt;N. Benard, J. Pons-Prats, J. Periaux, G. Bugeda, J.-P. Bonnet &amp; E. Moreau, (2015) [https://arc.aiaa.org/doi/abs/10.2514/6.2015-2957 &quot;Multi-Input Genetic Algorithm for Experimental Optimization of the Reattachment Downstream of a Backward-Facing Step with Surface Plasma Actuator&quot;], Paper AIAA 2015-2957 at 46th AIAA Plasmadynamics and Lasers Conference, Dallas, TX, USA, pp. 1-23.&lt;/ref&gt; or discrete-time optimal control.&lt;ref&gt;Zbigniew Michalewicz, Cezary Z. Janikow &amp; Jacek B. Krawczyk (July 1992) [https://doi.org/10.1016/0898-1221(92)90094-X &quot;A modified genetic algorithm for optimal control problems&quot;], [Computers &amp; Mathematics with Applications], vol. 23, no 12, pp. 83-94.&lt;/ref&gt;
* Control design as regression problem of the first kind:  MLC approximates a general nonlinear mapping from sensor signals to actuation commands, if the sensor signals and the optimal actuation command are known for every state. One example is the computation of sensor feedback from a known [[full state feedback]]. A [[neural network]] is commonly used technique for this task.&lt;ref&gt;C. Lee, J. Kim, D. Babcock &amp; R. Goodman (1997) [https://dx.doi.org/10.1063/1.869290 &quot;Application of neural networks to turbulence control for drag reduction&quot;], [[Physics of Fluids]], vol. 6, no. 9, pp. 1740-1747&lt;/ref&gt;
* Control design as regression problem of the second kind: MLC may also identify arbitrary nonlinear control laws  which minimize the cost function of the plant. In this case, neither a model, nor the control law structure,  nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. [[Genetic programming]] is a powerful regression technique for this purpose.&lt;ref&gt;D. C. Dracopoulos &amp; S. Kent (December 1997) [http://doi.org/10.1007/BF01501508 &quot;Genetic programming for prediction and control&quot;], Neural Computing &amp; Applications (Springer), vol. 6, no. 4, pp. 214-228.&lt;/ref&gt;
* Reinforcement learning control: The control law may be continually updated over measured performance changes (rewards) using [[reinforcement learning]].&lt;ref&gt;Andrew G. Barto (December 1994) [http://doi.org/10.1016/0959-4388(94)90138-4 &quot;Reinforcement learning control&quot;], [[Current Opinion in Neurobiology]], vol. 6, no. 4, pp. 888–893&lt;/ref&gt;

MLC comprises, for instance, neural network control,
genetic algorithm based control,
genetic programming control,
reinforcement learning control,
and has methodolical overlaps with other data-driven control,
like [[artificial intelligence]] and [[robot control]].

== Applications ==
MLC has been successfully applied
to many nonlinear control problems,
exploring unknown and often unexpected actuation mechanisms.
Example applications include

* Altitude control of satellites.&lt;ref&gt;Dimitris. C. Dracopoulos &amp; Antonia. J. Jones (1994)
[http://doi.org/10.1007/BF01414807.org Neuro-genetic adaptive attitude control], Neural Computing &amp; Applications (Springer), vol. 2, no. 4, pp. 183-204.&lt;/ref&gt;
* Building thermal control.&lt;ref&gt;Jonathan A. Wright, Heather A. Loosemore &amp; Raziyeh Farmani (2002) [http://doi.org/10.1016/S0378-7788(02)00071-3 &quot;Optimization of building thermal design and control by multi-criterion genetic algorithm], [Energy and Buildings], vol. 34, no. 9, pp. 959-972.&lt;/ref&gt;
* Feedback turbulence control.&lt;ref name=Benard2015aiaa /&gt;&lt;ref&gt;Steven J. Brunton &amp; Bernd R. Noack (2015)  [http://doi.org/10.1115/1.4031175 Closed-loop turbulence control: Progress and challenges], [[Applied Mechanics Reviews]], vol. 67, no. 5, article 050801, pp. 1-48.&lt;/ref&gt;
* Remotely operated under water vehicle.&lt;ref&gt;J. Javadi-Moghaddam, &amp; A. Bagheri (2010 [http://doi.org/10.1016/j.eswa.2009.06.015 &quot;An adaptive neuro-fuzzy sliding mode based genetic algorithm control system for under water remotely operated vehicle&quot;], [https://www.journals.elsevier.com/expert-systems-with-applications/ Expert Systems with Applications], vol. 37 no. 1, pp. 647-660.&lt;/ref&gt;
* Many more engineering MLC application are summarized in the review article of PJ Fleming &amp; RC Purshouse (2002).&lt;ref&gt;Peter J. Fleming, R. C. Purshouse (2002 [http://doi.org/10.1016/S0967-0661(02)00081-3 &quot;Evolutionary algorithms in control systems engineering: a survey&quot;]
[https://nl.wikipedia.org/wiki/Control_Engineering_Practice Control Engineering Practice], vol. 10, no. 11, pp. 1223-1241&lt;/ref&gt;

As for all general nonlinear methods,
MLC comes with no guaranteed convergence,
optimality or robustness for a range of operating conditions.

== References ==
{{Reflist|30em}}

== Further reading ==
{{Refbegin|1}}
* [http://users.wmin.ac.uk/~dracopd/ Dimitris C Dracopoulos] (August 1997) [https://www.springer.com/fr/book/9783540761617 &quot;Evolutionary Learning Algorithms for Neural Adaptive Control&quot;], Springer. {{ISBN|978-3-540-76161-7}}.
* [http://laboratorios.fi.uba.ar/lfd/thomas-duriez/ Thomas Duriez], [http://faculty.washington.edu/sbrunton/home/ Steven L. Brunton] &amp; [[Bernd Noack|Bernd R. Noack]] (November 2016) [https://www.springer.com/fr/book/9783319406237 &quot;Machine Learning Control - Taming Nonlinear Dynamics and Turbulence&quot;], Springer. {{ISBN|978-3-319-40624-4}}.



</text>
      <sha1>oenyg43ioq6vopqwvvtth4z0i1bne02</sha1>
    </revision>
  </page>
  <page>
    <title>Machine learning in bioinformatics</title>
    <ns>0</ns>
    <id>53970843</id>
    <revision>
      <id>815386787</id>
      <parentid>815386676</parentid>
      <timestamp>2017-12-14T14:54:03Z</timestamp>
      <contributor>
        <username>Larry.europe</username>
        <id>11040783</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15132">{{Use mdy dates|date=September 2017}}
'''[[Machine learning]]''', a subfield of [[computer science]] involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of [[bioinformatics]]. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data&lt;ref&gt;{{cite journal
| vauthors = Chicco D
| title = Ten quick tips for machine learning in computational biology
| journal = BioData Mining
| volume = 10
| issue =  35
| pages = 1-17
| date = December 2017
| pmid = 29234465
| doi = 10.1186/s13040-017-0155-3
| pmc= 5721660}}&lt;/ref&gt;.

Prior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as [[protein structure prediction]], proves extremely difficult.&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|last=Yang|first=Yuedong|last2=Gao|first2=Jianzhao|last3=Wang|first3=Jihua|last4=Heffernan|first4=Rhys|last5=Hanson|first5=Jack|last6=Paliwal|first6=Kuldip|last7=Zhou|first7=Yaoqi|title=Sixty-five years of the long march in protein secondary structure prediction: the final stretch?|url=https://academic.oup.com/bib/article/doi/10.1093/bib/bbw129/2769436/Sixty-five-years-of-the-long-march-in-protein|journal=Briefings in Bioinformatics|doi=10.1093/bib/bbw129}}&lt;/ref&gt; Machine learning techniques such as [[deep learning]] enable the algorithm to make use of automatic [[feature learning]] which means that based on the dataset alone, the algorithm can learn how to combine multiple [[Feature (machine learning)|features]] of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems.&lt;ref name=&quot;:0&quot; /&gt; Machine learning has been applied to six main subfields of bioinformatics: [[genomics]], [[proteomics]], [[microarrays]], [[systems biology]], [[evolution]], and [[text mining]].&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|last=Larrañaga|first=Pedro|last2=Calvo|first2=Borja|last3=Santana|first3=Roberto|last4=Bielza|first4=Concha|last5=Galdiano|first5=Josu|last6=Inza|first6=Iñaki|last7=Lozano|first7=José A.|last8=Armañanzas|first8=Rubén|last9=Santafé|first9=Guzmán|title=Machine learning in bioinformatics|url=https://academic.oup.com/bib/article/doi/10.1093/bib/bbk007/264025/Machine-learning-in-bioinformatics|journal=Briefings in Bioinformatics|pages=86–112|doi=10.1093/bib/bbk007}}&lt;/ref&gt;

== Applications ==

=== Genomics ===
[[File:Growth of GenBank.png|thumb|The exponential growth of GenBank, a genomic sequence database provided by the National center for Biotechnology Information (NCBI)]]
[[Genomics]] involves the study of the [[genome]], the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially.&lt;ref&gt;{{Cite web|url=https://www.ncbi.nlm.nih.gov/genbank/statistics/|title=GenBank and WGS Statistics|website=www.ncbi.nlm.nih.gov|language=en|access-date=2017-05-06}}&lt;/ref&gt; However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace.&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|last=Mathé|first=Catherine|last2=Sagot|first2=Marie-France|last3=Schiex|first3=Thomas|last4=Rouzé|first4=Pierre|date=October 1, 2002|title=Current methods of gene prediction, their strengths and weaknesses|journal=Nucleic Acids Research|volume=30|issue=19|pages=4103–4117|issn=1362-4962|pmc=140543|pmid=12364589|doi=10.1093/nar/gkf543}}&lt;/ref&gt; Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence.&lt;ref name=&quot;:1&quot; /&gt; This is a problem in computational biology known as [[gene prediction]].

Gene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches.&lt;ref name=&quot;:1&quot; /&gt; For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are [[Homology (biology)|homologous]] to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.&lt;ref name=&quot;:1&quot; /&gt;

Machine learning is also been used for the problem of [[multiple sequence alignment]] which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.&lt;ref name=&quot;:0&quot; /&gt;
It can also be used to detect and visualize genome rearrangements.&lt;ref name=&quot;rearrang&quot;&gt;{{cite journal|last=Pratas|first=D|author2=Silva, R|author3= Pinho, A|author4= Ferreira, P|title=An alignment-free method to find and visualise rearrangements between pairs of DNA sequences.|journal=Scientific Reports (Group Nature)|date=May 18, 2015|volume=5|number=10203|pmid=25984837|doi=10.1038/srep10203|pages=10203|pmc=4434998}}&lt;/ref&gt;

=== Proteomics ===
[[File:C16orf95 protein secondary structure prediction.png|thumb|A protein's amino acid sequence annotated with the protein secondary structure. Each amino acid is labeled as either an alpha helix, a beta sheet, or a coil.]]
[[Protein]]s, strings of [[amino acid]]s, gain much of their function from [[protein folding]] in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the [[Protein primary structure|primary structure]] (i.e. the flat string of amino acids), the [[Protein secondary structure|secondary structure]] ([[Alpha helix|alpha helices]] and [[beta sheet]]s), the [[Protein tertiary structure|tertiary structure]], and the [[Protein quaternary structure|quartenary structure]].

Protein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure.&lt;ref name=&quot;:2&quot; /&gt; Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly.&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;:0&quot; /&gt; Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain.&lt;ref&gt;{{Cite journal|last=Pauling|first=L.|last2=Corey|first2=R. B.|last3=Branson|first3=H. R.|date=April 1, 1951|title=The structure of proteins; two hydrogen-bonded helical configurations of the polypeptide chain|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=37|issue=4|pages=205–211|issn=0027-8424|pmc=1063337|pmid=14816373|doi=10.1073/pnas.37.4.205}}&lt;/ref&gt; Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%.&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;:3&quot; /&gt; The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of [[artificial neural network]]s to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil).&lt;ref name=&quot;:3&quot;&gt;{{cite arxiv|last=Wang|first=Sheng|last2=Peng|first2=Jian|last3=Ma|first3=Jianzhu|last4=Xu|first4=Jinbo|date=December 1, 2015|title=Protein secondary structure prediction using deep convolutional neural fields|eprint=1512.00843}}&lt;/ref&gt; The theoretical limit for three-state protein secondary structure is 88–90%.&lt;ref name=&quot;:2&quot; /&gt;

Machine learning has also been applied to proteomics problems such as [[Side chain|protein side-chain]] prediction, [[Turn (biochemistry)|protein loop]] modeling, and [[protein contact map]] prediction.&lt;ref name=&quot;:0&quot; /&gt;

=== Microarrays ===
Microarrays, a type of [[lab-on-a-chip]], are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.&lt;ref name=&quot;:0&quot; /&gt;
[[File:DNA-microarray analysis.jpg|thumb|A DNA-microarray analysis of Burkitt's lymphoma and diffuse large B-cell lymphoma (DLBCL) is shown and identifies differences in gene expression patterns.]]
This technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed.&lt;ref name=&quot;:4&quot;&gt;{{Cite journal|last=Pirooznia|first=Mehdi|last2=Yang|first2=Jack Y.|last3=Yang|first3=Mary Qu|last4=Deng|first4=Youping|date=2008|title=A comparative study of different machine learning methods on microarray gene expression data|url=https://dx.doi.org/10.1186/1471-2164-9-S1-S13|journal=BMC Genomics|volume=9|issue=1|pages=S13|doi=10.1186/1471-2164-9-S1-S13|issn=1471-2164|pmc=2386055|pmid=18366602}}&lt;/ref&gt; One of the main problems in this field is identifying which genes are expressed based on the collected data.&lt;ref name=&quot;:0&quot; /&gt; In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are [[radial basis function network]]s, [[deep learning]], [[Naive Bayes classifier|Bayesian classification]], [[decision tree]]s, and [[random forest]].&lt;ref name=&quot;:4&quot; /&gt;

=== Systems biology ===
Systems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.&lt;ref&gt;{{Cite web|url=http://journal.frontiersin.org/researchtopic/2362/machine-learning-in-molecular-systems-biology|title=Machine Learning in Molecular Systems Biology|website=Frontiers|language=en|access-date=2017-06-09}}&lt;/ref&gt;

Machine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways.&lt;ref name=&quot;:0&quot; /&gt; [[Graphical model|Probabilistic graphical models]], a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks.&lt;ref name=&quot;:0&quot; /&gt; In addition, machine learning has been applied to systems biology problems such as identifying [[DNA binding site|transcription factor binding sites]] using a technique known as [[Markov chain|Markov chain optimization]].&lt;ref name=&quot;:0&quot; /&gt; [[Genetic algorithm]]s, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.&lt;ref name=&quot;:0&quot; /&gt;

Other systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of Multiple Sclerosis, protein function prediction, and identification of NCR-sensitivity of genes in yeast.&lt;ref&gt;{{Cite journal|last=d'Alché-Buc|first=Florence|last2=Wehenkel|first2=Louis|date=2008|title=Machine Learning in Systems Biology|url=https://dx.doi.org/10.1186/1753-6561-2-S4-S1|journal=BMC Proceedings|volume=2|issue=4|pages=S1|doi=10.1186/1753-6561-2-S4-S1|issn=1753-6561}}&lt;/ref&gt;

=== Text mining ===
The increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as [[knowledge extraction]]. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge.&lt;ref name=&quot;:0&quot; /&gt;&lt;ref name=&quot;:5&quot;&gt;{{Cite journal|last=Krallinger|first=Martin|last2=Erhardt|first2=Ramon Alonso-Allende|last3=Valencia|first3=Alfonso|date=March 15, 2005|title=Text-mining approaches in molecular biology and biomedicine|url=http://www.sciencedirect.com/science/article/pii/S1359644605033763|journal=Drug Discovery Today|volume=10|issue=6|pages=439–445|doi=10.1016/S1359-6446(05)03376-3}}&lt;/ref&gt; Machine learning can be used for this knowledge extraction task using techniques such as [[natural language processing]] to extract the useful information from human-generated reports in a database. [[Text Nailing]], an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.

This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals.&lt;ref name=&quot;:5&quot; /&gt; Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the [[Protein targeting|subcellular localization of a protein]], analysis of [[Gene expression|DNA-expression arrays]], large-scale [[List of protein interactions|protein interaction]] analysis, and molecule interaction analysis.&lt;ref name=&quot;:5&quot; /&gt;

Other application is the detection and visualization of regions that share high degree of similarity or are new according to a reference.&lt;ref name=&quot;sing&quot;&gt;{{cite journal|last=Pratas|first=D|author2=Hosseini, M|author3=Silva, R|author4= Pinho, A|author5= Ferreira, P|title=Visualization of Distinct DNA Regions of the Modern Human Relatively to a Neanderthal Genome|journal=Iberian Conference on Pattern Recognition and Image Analysis. Springer|pages=235–242|date= June 20–23, 2017|url=https://link.springer.com/chapter/10.1007/978-3-319-58838-4_26}}&lt;/ref&gt;

==References==
{{reflist}}

__FORCETOC__
__INDEX__




</text>
      <sha1>bx5wripqb0jqj7bwyfay7j91u64yf0u</sha1>
    </revision>
  </page>
  <page>
    <title>Labeled data</title>
    <ns>0</ns>
    <id>54033657</id>
    <revision>
      <id>790560851</id>
      <parentid>780313439</parentid>
      <timestamp>2017-07-14T15:11:59Z</timestamp>
      <contributor>
        <username>Kazfernandes</username>
        <id>5917229</id>
      </contributor>
      <minor/>
      <comment>Added link to &quot;Sample (statistics)&quot; article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1615">{{refimprove
| date = May 2017
}}{{Machine learning bar}}

'''Labeled data''' is a group of [[Sample (statistics)|samples]] that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative. For example, labels might be indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, whether the dot in an x-ray is a tumor, etc.

Labels can be obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., &quot;Does this photo contain a horse or a cow?&quot;), and are significantly more expensive to obtain than the raw unlabeled data.

After obtaining a labeled dataset, [[machine learning]] models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.&lt;ref&gt;Johnson, Leif. [http://stackoverflow.com/questions/19170603/what-is-the-difference-between-labeled-and-unlabeled-data/19172720#19172720 &quot;What is the difference between labeled and unlabeled data?&quot;], ''[[Stack Overflow]]'', 4 October 2013. Retrieved on 13 May 2017. {{CC-notice|cc=bysa3|url=http://stackoverflow.com/questions/19170603/what-is-the-difference-between-labeled-and-unlabeled-data/19172720#19172720|author=[http://stackoverflow.com/users/2014584/lmjohns3 lmjohns3]}}&lt;/ref&gt;

==References==
{{Reflist}}

</text>
      <sha1>j0en5dpja55rw9qqyg0xxtp0d6qs61m</sha1>
    </revision>
  </page>
  <page>
    <title>Caffe (software)</title>
    <ns>0</ns>
    <id>53631046</id>
    <revision>
      <id>813127894</id>
      <parentid>812891022</parentid>
      <timestamp>2017-12-01T23:13:07Z</timestamp>
      <contributor>
        <ip>199.201.64.129</ip>
      </contributor>
      <comment>/* Applications */ Removed a redirect to the same article.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4559">{{Infobox software
| name                   = Caffe
| logo                   =
| screenshot             =
| caption                =
| collapsible            =
| author                 = Yangqing Jia
| developer              = Berkeley Vision and Learning Center
| released               =
| latest release version = 1.0&lt;ref&gt;{{cite web|url=https://github.com/BVLC/caffe/releases/tag/1.0|title=Release 1.0}}&lt;/ref&gt;
| latest release date    = {{Start date and age|2017|04|18|df=yes}}
| latest preview version =
| latest preview date    =
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[C++]]
| operating system       = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]]&lt;ref&gt;{{cite web|url=https://github.com/Microsoft/caffe|title=Microsoft/caffe|work=GitHub}}&lt;/ref&gt;
| platform               =
| size                   =
| language               =
| status                 =
| genre                  = Library for [[deep learning]]
| license                = [[BSD License|BSD]]&lt;ref&gt;{{cite web|url=https://github.com/BVLC/caffe/blob/master/LICENSE|title=caffe/LICENSE at master|work=GitHub}}&lt;/ref&gt;
| website                = {{url|http://caffe.berkeleyvision.org/}}
}}

{{machine learning bar}}

'''CAFFE''' (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at UC Berkeley. It is [[open source]], under a [[BSD license]].&lt;ref&gt;{{cite web|url=https://github.com/BVLC/caffe/|title=BVLC/caffe|work=GitHub}}&lt;/ref&gt; It is written in [[C++]], with a [[Python (programming language)|Python]] interface.&lt;ref&gt;{{cite web|url=https://deeplearning4j.org/compare-dl4j-torch7-pylearn#caffe|title=Comparing Frameworks: Deeplearning4j, Torch, Theano, TensorFlow, Caffe, Paddle, MxNet, Keras &amp; CNTK}}&lt;/ref&gt;

== History ==
Yangqing Jia created the caffe project during his PhD at UC Berkeley.&lt;ref&gt;{{cite web|title=The Caffe Deep Learning Framework: An Interview with the Core Developers|url=http://www.embedded-vision.com/industry-analysis/technical-articles/caffe-deep-learning-framework-interview-core-developers|publisher=Embedded Vision}}&lt;/ref&gt; Now there are many contributors to the project, and it is hosted at [[GitHub]].&lt;ref&gt;{{cite web|title=Caffe: a fast open framework for deep learning.|url=https://github.com/BVLC/caffe|publisher=GitHub}}&lt;/ref&gt;

== Features ==
Caffe supports many different types of deep learning architectures geared towards [[image classification]] and [[image segmentation]]. It supports [[Convolutional neural network|CNN]], RCNN, [[LSTM]] and fully connected neural network designs.&lt;ref&gt;{{cite web|title=Caffe tutorial - vision.princeton.edu|url=https://vision.princeton.edu/courses/COS598/2015sp/slides/Caffe/caffe_tutorial.pdf|archiveurl=https://web.archive.org/web/20170405073658/https://vision.princeton.edu/courses/COS598/2015sp/slides/Caffe/caffe_tutorial.pdf|archivedate=April 5, 2017}}&lt;/ref&gt; Caffe supports GPU based accleration using CuDNN of Nvidia.&lt;ref&gt;{{cite web|title=Deep Learning for Computer Vision with Caffe and cuDNN|url=https://devblogs.nvidia.com/parallelforall/deep-learning-computer-vision-caffe-cudnn/}}&lt;/ref&gt;

== Applications ==
Caffe is being used in academic research projects, startup prototypes, and even large-scale industrial applications in vision, speech, and multimedia. [[Yahoo!]] has also integrated caffe with [[Apache Spark]] to create CaffeOnSpark, a distributed deep learning framework.&lt;ref&gt;{{cite news|title=Yahoo enters artificial intelligence race with CaffeOnSpark|url=https://jaxenter.com/yahoo-enters-artificial-intelligence-race-with-caffeonspark-124324.html}}&lt;/ref&gt;

In April 2017, Facebook announced Caffe2,&lt;ref&gt;{{cite web|url=http://caffe2.ai/blog/2017/04/18/caffe2-open-source-announcement.html|title=Caffe2 Open Source Brings Cross Platform Machine Learning Tools to Developers}}&lt;/ref&gt; which includes new features such as [[Recurrent neural network|Recurrent Neural Networks]].

==See also==
* [[Comparison of deep learning software]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://caffe.berkeleyvision.org/}} (GitHub)

{{Deep Learning Software}}

{{DEFAULTSORT:Caffe}}


[[Category:Data mining and machine learning software]]




[[Category:Information technology companies of the United States]]
</text>
      <sha1>lkux7ed8zgeu0efk3orrjkssdc80p78</sha1>
    </revision>
  </page>
  <page>
    <title>Occam learning</title>
    <ns>0</ns>
    <id>44577560</id>
    <revision>
      <id>806206188</id>
      <parentid>793355422</parentid>
      <timestamp>2017-10-20T12:44:02Z</timestamp>
      <contributor>
        <username>Onel5969</username>
        <id>10951369</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Ockham]] (link removed) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10409">{{Machine learning bar}}

In [[computational learning theory]], '''Occam learning''' is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to [[probably approximately correct learning|probably approximately correct (PAC) learning]], where the learner is evaluated on its predictive power of a test set.

Occam learnability implies PAC learning, and for a wide variety of [[concept class]]es, the converse is also true: PAC learnability implies Occam learnability.

==Introduction==
Occam Learning is named after [[Occam’s Razor#Science and the scientific method|Occam's razor,]] which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al.&lt;ref name=&quot;def&quot; /&gt; that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, ''parsimony'' (of the output hypothesis) implies ''predictive power''.

==Definition of Occam learning==
The succinctness of a concept &lt;math&gt;c&lt;/math&gt; in [[concept class]] &lt;math&gt;\mathcal{C}&lt;/math&gt; can be expressed by the length &lt;math&gt;size(c)&lt;/math&gt; of the shortest bit string that can represent &lt;math&gt;c&lt;/math&gt; in &lt;math&gt;\mathcal{C}&lt;/math&gt;. Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.

Let &lt;math&gt;\mathcal{C}&lt;/math&gt; and &lt;math&gt;\mathcal{H}&lt;/math&gt; be concept classes containing target concepts and hypotheses respectively. Then, for constants &lt;math&gt;\alpha \ge 0&lt;/math&gt; and &lt;math&gt;0 \le \beta &lt;1&lt;/math&gt;, a learning algorithm &lt;math&gt;L&lt;/math&gt; is an '''&lt;math&gt;(\alpha,\beta)&lt;/math&gt;-Occam algorithm''' for &lt;math&gt;\mathcal{C}&lt;/math&gt; using &lt;math&gt;\mathcal{H}&lt;/math&gt; if, given a set &lt;math&gt;S&lt;/math&gt; of &lt;math&gt;m&lt;/math&gt; samples labeled according to a concept &lt;math&gt;c \in \mathcal{C}&lt;/math&gt;, &lt;math&gt;L&lt;/math&gt; outputs a hypothesis &lt;math&gt;h \in \mathcal{H}&lt;/math&gt; such that
* &lt;math&gt;h&lt;/math&gt; is consistent with &lt;math&gt;c&lt;/math&gt; on &lt;math&gt;S&lt;/math&gt; (that is, &lt;math&gt; h(x)=c(x),\forall x \in S &lt;/math&gt;), and
* &lt;math&gt;size(h) \le (n \cdot size(c))^\alpha m^\beta &lt;/math&gt; &lt;ref name=&quot;kv&quot;&gt;Kearns, M. J., &amp; Vazirani, U. V. (1994). An introduction to computational learning theory, chapter 2. MIT press.&lt;/ref&gt;&lt;ref name=&quot;def&quot;&gt;Blumer, A., Ehrenfeucht, A., Haussler, D., &amp; Warmuth, M. K. (1987). ''[http://www.cse.buffalo.edu/~hungngo/classes/2008/694/papers/occam.pdf Occam's razor]''. Information processing letters, 24(6), 377-380.&lt;/ref&gt;
where &lt;math&gt;n&lt;/math&gt; is the maximum length of any sample &lt;math&gt;x \in S&lt;/math&gt;. An Occam algorithm is called ''efficient'' if it runs in time polynomial in &lt;math&gt;n&lt;/math&gt;, &lt;math&gt;m&lt;/math&gt;, and &lt;math&gt;size(c)&lt;/math&gt;. We say a concept class &lt;math&gt;\mathcal{C}&lt;/math&gt; is ''Occam learnable'' with respect to a hypothesis class &lt;math&gt;\mathcal{H}&lt;/math&gt; if there exists an efficient Occam algorithm for  &lt;math&gt;\mathcal{C}&lt;/math&gt; using &lt;math&gt;\mathcal{H}&lt;/math&gt;.

==The relation between Occam and PAC learning==
Occam learnability implies PAC learnability, as the following theorem of Blumer, et al.&lt;ref name=&quot;kv&quot; /&gt; shows:

=== Theorem (''Occam learning implies PAC learning'') ===
&lt;blockquote&gt;Let &lt;math&gt;L&lt;/math&gt; be an efficient  '''&lt;math&gt;(\alpha,\beta)&lt;/math&gt;'''-Occam algorithm for &lt;math&gt;\mathcal{C}&lt;/math&gt; using &lt;math&gt;\mathcal{H}&lt;/math&gt;. Then there exists a constant &lt;math&gt;a &gt; 0&lt;/math&gt; such that for any &lt;math&gt;0 &lt; \epsilon, \delta &lt; 1&lt;/math&gt;, for any distribution &lt;math&gt;\mathcal{D} &lt;/math&gt;, given &lt;math&gt;m \ge a \left( \frac 1 \epsilon \log \frac 1 \delta + \left(\frac {(n \cdot size(c))^\alpha)} \epsilon \right)^{\frac 1 {1-\beta}}\right)&lt;/math&gt;  samples drawn from &lt;math&gt;\mathcal{D} &lt;/math&gt; and labelled according to a concept &lt;math&gt;c \in \mathcal{C} &lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; bits each, the algorithm  &lt;math&gt;L&lt;/math&gt; will output a hypothesis  &lt;math&gt;h \in \mathcal{H} &lt;/math&gt; such that &lt;math&gt;error(h)\le \epsilon&lt;/math&gt; with probability at least &lt;math&gt;1-\delta&lt;/math&gt; .&lt;/blockquote&gt;Here, &lt;math&gt;error(h)&lt;/math&gt; is with respect to the concept &lt;math&gt;c&lt;/math&gt; and distribution &lt;math&gt;\mathcal{D} &lt;/math&gt;. This implies that the algorithm &lt;math&gt;L&lt;/math&gt; is also a PAC learner for the concept class &lt;math&gt;\mathcal{C}&lt;/math&gt; using hypothesis class &lt;math&gt;\mathcal{H}&lt;/math&gt;.  A slightly more general formulation is as follows:

=== Theorem (''Occam learning implies PAC learning, cardinality version'') ===
&lt;blockquote&gt;Let &lt;math&gt;0 &lt; \epsilon, \delta &lt; 1&lt;/math&gt;. Let &lt;math&gt;L&lt;/math&gt; be an algorithm such that, given &lt;math&gt;m&lt;/math&gt; samples drawn from a fixed but unknown distribution &lt;math&gt;\mathcal{D}&lt;/math&gt; and labeled according to a concept &lt;math&gt;c \in \mathcal{C} &lt;/math&gt; of length &lt;math&gt;n&lt;/math&gt; bits each, outputs a hypothesis &lt;math&gt;h \in \mathcal{H}_{n,m} &lt;/math&gt; that is consistent with the labeled samples. Then, there exists a constant &lt;math&gt;b&lt;/math&gt; such that if &lt;math&gt;\log |\mathcal{H}_{n,m}| \leq b \epsilon m - \log \frac{1}{\delta}&lt;/math&gt;, then &lt;math&gt;L&lt;/math&gt;  is guaranteed to output a hypothesis &lt;math&gt;h \in \mathcal{H}_{n,m} &lt;/math&gt; such that &lt;math&gt;error(h)\le \epsilon&lt;/math&gt; with probability at least &lt;math&gt;1-\delta&lt;/math&gt;.&lt;/blockquote&gt;

While the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about ''necessity.'' Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning.&lt;ref&gt;Board, R., &amp; Pitt, L. (1990, April). On the necessity of Occam algorithms. In Proceedings of the twenty-second annual ACM symposium on Theory of computing (pp. 54-63). ACM.&lt;/ref&gt; They proved that for any concept class that is ''polynomially closed under exception lists,'' PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.

A concept class &lt;math&gt;\mathcal{C}&lt;/math&gt; is polynomially closed under exception lists if there exists a polynomial-time algorithm &lt;math&gt;A&lt;/math&gt; such that, when given the representation of a concept &lt;math&gt;c \in \mathcal{C} &lt;/math&gt; and a finite list &lt;math&gt;E&lt;/math&gt; of ''exceptions'', outputs a representation of a concept &lt;math&gt;c' \in \mathcal{C}&lt;/math&gt; such that the concepts &lt;math&gt;c&lt;/math&gt; and &lt;math&gt;c'&lt;/math&gt; agree except on the set &lt;math&gt;E&lt;/math&gt;.

==Proof that Occam learning implies PAC learning==

We first prove the Cardinality version. Call a hypothesis &lt;math&gt;h\in \mathcal{H} &lt;/math&gt; ''bad'' if &lt;math&gt;error(h) \geq \epsilon&lt;/math&gt;, where again &lt;math&gt;error(h)&lt;/math&gt; is with respect to the true concept &lt;math&gt;c&lt;/math&gt; and the underlying distribution &lt;math&gt;\mathcal{D}&lt;/math&gt;. The probability that a set of samples &lt;math&gt;S&lt;/math&gt; is consistent with &lt;math&gt;h&lt;/math&gt; is at most &lt;math&gt;(1 - \epsilon)^m&lt;/math&gt;, by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in &lt;math&gt;\mathcal{H}_{n,m}&lt;/math&gt; is at most &lt;math&gt; | \mathcal{H}_{n,m} | (1 - \epsilon)^m&lt;/math&gt;, which is less than &lt;math&gt;\delta &lt;/math&gt; if &lt;math&gt;\log | \mathcal{H}_{n,m} |  \leq O(\epsilon m) - \log \frac{1}{\delta}&lt;/math&gt;. This concludes the proof of the second theorem above.

Using the second theorem, we can prove the first theorem. Since we have a &lt;math&gt;(\alpha,\beta)&lt;/math&gt;-Occam algorithm, this means that any hypothesis output by &lt;math&gt;L&lt;/math&gt; can be represented by at most &lt;math&gt; (n \cdot size(c))^\alpha m^\beta&lt;/math&gt; bits, and thus &lt;math&gt; \log |\mathcal{H}_{n,m}| \leq (n \cdot size(c))^\alpha m^\beta&lt;/math&gt;. This is less than &lt;math&gt;O(\epsilon m) - \log \frac{1}{\delta}&lt;/math&gt; if we set &lt;math&gt; m \geq a \left( \frac 1 \epsilon \log \frac 1 \delta + \left(\frac {(n \cdot size(c))^\alpha)} \epsilon \right)^{\frac 1 {1-\beta}}\right)&lt;/math&gt; for some constant &lt;math&gt;a &gt; 0&lt;/math&gt;. Thus, by the Cardinality version Theorem, &lt;math&gt;L&lt;/math&gt; will output a consistent hypothesis &lt;math&gt;h&lt;/math&gt; with probability at least &lt;math&gt;1 - \delta&lt;/math&gt;. This concludes the proof of the first theorem above.

==Improving sample complexity for common problems==
Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions,&lt;ref name=&quot;kv&quot; /&gt; conjunctions with few relevant variables,&lt;ref&gt;Haussler, D. (1988). ''[http://cs.ecs.baylor.edu/~hamerly/courses/5325_10s/papers/learning_theory/haussler1988inductive-bias.pdf Quantifying inductive bias: AI learning algorithms and Valiant's learning framework]''. Artificial intelligence, 36(2), 177-221.&lt;/ref&gt; and decision lists.&lt;ref&gt;Rivest, R. L. (1987). ''[http://people.csail.mit.edu/rivest/pubs/Riv87b.pdf Learning decision lists. Machine learning]'', 2(3), 229-246.&lt;/ref&gt;

== Extensions ==
Occam algorithms have also been shown to be successful for PAC learning in the presence of errors,&lt;ref&gt;Angluin, D., &amp; Laird, P. (1988). Learning from noisy examples. Machine Learning, 2(4), 343-370.&lt;/ref&gt;&lt;ref&gt;Kearns, M., &amp; Li, M. (1993). Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4), 807-837.&lt;/ref&gt; probabilistic concepts,&lt;ref&gt;Kearns, M. J., &amp; Schapire, R. E. (1990, October). ''[http://www.cis.upenn.edu/~mkearns/papers/pconcepts.pdf Efficient distribution-free learning of probabilistic concepts]''. In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on (pp. 382-391). IEEE.&lt;/ref&gt; function learning&lt;ref&gt;Natarajan, B. K. (1993, August). Occam's razor for functions. In Proceedings of the sixth annual conference on Computational learning theory (pp. 370-376). ACM.&lt;/ref&gt; and Markovian non-independent examples.&lt;ref&gt;Aldous, D., &amp; Vazirani, U. (1990, October). ''[http://www.eecs.berkeley.edu/~vazirani/pubs/markovian.pdf A Markovian extension of Valiant's learning model]''. In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on (pp. 392-396). IEEE.&lt;/ref&gt;

==See also==
* [[Structural Risk Minimization]]
* [[Computational learning theory]]

==References==
{{reflist|30em}}



</text>
      <sha1>k4yspmg9sfgt0ptf75etujitnqlbdxf</sha1>
    </revision>
  </page>
  <page>
    <title>Right to explanation</title>
    <ns>0</ns>
    <id>54625345</id>
    <revision>
      <id>807902850</id>
      <parentid>806128678</parentid>
      <timestamp>2017-10-30T19:08:28Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: removing misplaced special [[no-break space]] character</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10452">In the [[regulation]] of [[algorithm]]s, particularly [[artificial intelligence]] and its subfield of [[machine learning]], a '''right to explanation''' (or '''right to ''an'' explanation''') is a [[right]] to be given an [[explanation]] for an output of the algorithm. Such rights primarily refer to [[individual right]]s to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be &quot;[[Credit bureau]] X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.&quot;

Some such [[legal right]]s already exist, while the scope of a general &quot;right to explanation&quot; is a matter of ongoing debate.

== Examples ==
=== Credit score in the United States ===
{{details|Credit score in the United States}}

[[Credit score in the United States]] – more generally, credit actions – have a well-established right to explanation. Under the [[Equal Credit Opportunity Act]] (Regulation B of the [[Code of Federal Regulations]]),
Title 12, Chapter X, Part 1002, [https://www.ecfr.gov/cgi-bin/text-idx?SID=1bb14fdfd1afc7d3f2d4756d223aeadb&amp;mc=true&amp;node=se12.8.1002_19 §1002.9], creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):&lt;ref&gt;[[Consumer Financial Protection Bureau]], [https://www.consumerfinance.gov/eregulations/1002-9/2011-31714#1002-9-b-2 §1002.9(b)(2)]&lt;/ref&gt;

{{quote|(2) Statement of specific reasons. The statement of reasons for adverse action required by paragraph (a)(2)(i) of this section must be specific and indicate the principal reason(s) for the adverse action. Statements that the adverse action was based on the creditor's internal standards or policies or that the applicant, joint applicant, or similar party failed to achieve a qualifying score on the creditor's credit scoring system are insufficient.}}

The [https://www.consumerfinance.gov/eregulations/1002-Subpart-Interp/2011-31714#1002-9-b-2-Interp-1 official interpretation] of this section details what types of statements are acceptable.

Credit agencies and data analysis firms such as [[FICO]] comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric '''{{visible anchor|reason code}}''' (as identifier) and an associated explanation, identifying the main factors affecting a credit score.&lt;ref&gt;[http://www.creditscoring.com/creditscore/fico/factors/reason-codes.html US FICO credit risk score reason codes: Fundamental document from FICO listing all of the FICO credit score reasons that a score is not higher], March 31, 2010, by Greg Fisher&lt;/ref&gt; An example might be:&lt;ref&gt;[https://www.reasoncode.org/reasoncode101 Reason Codes 101]&lt;/ref&gt;

:32: Balances on bankcard or revolving accounts too high compared to credit limits

=== European Union ===

The European Union [[General Data Protection Regulation]] (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 [[Data Protection Directive]] to provide a legally disputed form of a right to an explanation, stated as such in [https://www.privacy-regulation.eu/en/r71.htm Recital 71]: &quot;[the data subject should have] the right ... to obtain an explanation of the decision reached&quot;. In full:

{{quote|
The data subject should have the right not to be subject to a decision, which may include a measure, evaluating personal aspects relating to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention.

...

In any case, such processing should be subject to suitable safeguards, which should include specific information to the data subject and the right to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision.}}

However, the extent to which the regulations themselves provide a &quot;right to explanation&quot; is heavily debated.&lt;ref&gt;{{cite arXiv|eprint =1606.08813|last1 =Goodman|first1 =Bryce|title =European Union regulations on algorithmic decision-making and a &quot;right to explanation&quot;|last2 =Flaxman|first2 =Seth|class =stat.ML|year =2016}}&lt;/ref&gt;&lt;ref&gt;[https://iapp.org/news/a/is-there-a-right-to-explanation-for-machine-learning-in-the-gdpr/ Is there a 'right to explanation' for machine learning in the GDPR?], Jun 1, 2017, Andrew Burt&lt;/ref&gt; There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process.&lt;ref name=&quot;:0&quot;&gt;{{cite journal |ssrn=2903469 |last1=Wachter |first1=Sandra |last2=Mittelstadt |first2=Brent |last3=Floridi |first3=Luciano |title=Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation |date=December 28, 2016 |journal=International Data Privacy Law}}&lt;/ref&gt; In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both &quot;solely&quot; based on automated processing, and have legal or similarly significant effect — which limits their applicability in many of the cases of algorithmic controversy that have been picked up in the media.&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|last=Edwards|first=Lilian|last2=Veale|first2=Michael|date=2017|title=Slave to the algorithm? Why a &quot;right to an explanation&quot; is probably not the remedy you are looking for|url=http://ssrn.com/abstract=2972855|journal=Duke Law and Technology Review|volume=|pages=|ssrn=2972855|via=}}&lt;/ref&gt;

A second source of such a right has been pointed to in Article 15, the &quot;right of access by the data subject&quot;. This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to &quot;meaningful information about the logic involved&quot; in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.&lt;ref name=&quot;:0&quot; /&gt;&lt;ref name=&quot;:1&quot; /&gt;

=== France ===
In [[France]] the 2016 ''Loi pour une République numérique'' (Digital Republic Act or ''loi numérique'') amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals.&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|last=Edwards|first=Lilian|last2=Veale|first2=Michael|date=2017|title=Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?|url=https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3052831|journal=Available on SSRN|volume=|pages=|ssrn=3052831|via=}}&lt;/ref&gt; It notes that where there is &quot;a decision taken on the basis of an algorithmic treatment&quot;, the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:
# the degree and the mode of contribution of the algorithmic processing to the decision- making;
# the data processed and its source;
# the treatment parameters, and where appropriate, their weighting, applied to the situation of the person concerned;
# the operations carried out by the treatment.
Scholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions &quot;solely&quot; based on automated processing, as well as provides a framework for explaining specific decisions.&lt;ref name=&quot;:2&quot; /&gt; Indeed, the GDPR automated decision-making rights in the European Union, one of the places a &quot;right to an explanation&quot; has been sought within, find their origins in French law in the late 1970s.&lt;ref&gt;{{Cite journal|last=Bygrave|first=L A|date=2001|title=Minding the Machine: Article 15 of the EC Data Protection Directive and Automated Profiling|url=http://folk.uio.no/lee/oldpage/articles/Minding_machine.pdf|journal=Computer Law &amp; Security Review|volume=17|issue=1|pages=|doi=10.1016/S0267-3649(01)00104-2|via=}}&lt;/ref&gt;

== Criticism ==

Some argue that a &quot;right to explanation&quot; is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.&lt;ref&gt;[http://www.techzone360.com/topics/techzone/articles/2017/01/25/429101-eus-right-explanation-harmful-restriction-artificial-intelligence.htm EU's Right to Explanation: A Harmful Restriction on Artificial Intelligence], Nick Wallace, Center for Data Innovation, January 25, 2017&lt;/ref&gt;

More fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a [[deep neural network]] depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of [[Explainable AI]] seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.&lt;ref name=&quot;:1&quot; /&gt;

Similarly, human decisions often cannot be easily explained: they may be based on intuition or a &quot;[[gut feeling]]&quot; that is hard to put into words. Requiring machines to meet a higher standard than humans is thus arguably unreasonable.

== See also ==
* [[Explainable AI]]

== References ==
{{reflist}}

== External links ==
* [http://www.slate.com/articles/technology/future_tense/2017/05/why_artificial_intelligences_should_have_to_explain_their_actions.html Artificial Intelligence Owes You an Explanation], by John Frank Weaver, ''Slate'', May 8, 2017





</text>
      <sha1>2m7xumv0ukh66xwiduqk42cu5dcqvdd</sha1>
    </revision>
  </page>
  <page>
    <title>Connectionist temporal classification</title>
    <ns>0</ns>
    <id>54550729</id>
    <revision>
      <id>810970341</id>
      <parentid>804947712</parentid>
      <timestamp>2017-11-18T17:50:03Z</timestamp>
      <contributor>
        <username>Gorobay</username>
        <id>15371737</id>
      </contributor>
      <minor/>
      <comment>Gorobay moved page [[Connectionist temporal classification (CTC)]] to [[Connectionist temporal classification]]: unnecessary disambiguation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1772">{{Use dmy dates|date=September 2017}}
{{one source|date=July 2017}}
'''Connectionist temporal classification''' ('''CTC''') is a type of neural network output and associated scoring function, for training [[recurrent neural network]]s (RNNs) such as [[Long short-term memory|LSTM]] networks to tackle sequence problems where the timing is variable. It can be used for tasks like recognising phonemes in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. It was introduced in.&lt;ref name=&quot;graves2006&quot;&gt;{{Cite journal |last=Graves |first=Alex |last2=Fernández |first2=Santiago |last3=Gomez |first3=Faustino |date=2006 |title=Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks |citeseerx=10.1.1.75.6306 |journal=In Proceedings of the International Conference on Machine Learning, ICML 2006 |pages=369–376}}&lt;/ref&gt;

A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. The input is a sequence of observations, and the outputs are a sequence of labels, which can include blank outputs. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task. Fortunately there is an efficient forward-backwards algorithm.

CTC scores can then be used with the back-propagation algorithm to update the neural network weights.

Alternative approaches to a CTC-fitted neural network include a [[hidden Markov model]] (HMM).

==References==
{{reflist}}


</text>
      <sha1>1k6qzlqbtw45q35yvabjfeg4j3zwhzu</sha1>
    </revision>
  </page>
  <page>
    <title>End-to-end reinforcement learning</title>
    <ns>0</ns>
    <id>52003586</id>
    <revision>
      <id>814815405</id>
      <parentid>797474354</parentid>
      <timestamp>2017-12-11T02:37:40Z</timestamp>
      <contributor>
        <username>Neo-Jay</username>
        <id>561624</id>
      </contributor>
      <comment>completed the author list of [[DeepMind]]'s paper on [[AlphaGo]] published in the journal [[Nature (journal)|Nature]] on [[28 January]] [[2016]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12805">In '''end-to-end reinforcement learning''', the end-to-end process, in other words, the entire process from sensors to motors in a robot or agent consists of only one layered or [[recurrent neural network]] without modularization, and the network is trained comprehensively by [[reinforcement learning]].&lt;ref name=&quot;Hassabis&quot;&gt;{{cite speech |last1=Demis |first1=Hassabis | date=March 11, 2016 |title= Artificial Intelligence and the Future. |url= https://www.youtube.com/watch?v=8Z2eLTSCuBk}}&lt;/ref&gt; '''End-to-end reinforcement learning''' has been propounded for a long time,&lt;ref name=&quot;Shibata1&quot;&gt;{{cite book |last=Shibata |first=Katsunari |editor-last=Mellouk |editor-first=Abdelhamid |title=Advances in Reinforcement Learning |publisher=Intech |date=January 14, 2011 |pages=99–120 |chapter=Chapter 6: Emergence of Intelligence through Reinforcement Learning with a Neural Network |url= http://www.intechopen.com/books/advances-in-reinforcement-learning |chapterurl= http://www.intechopen.com/books/advances-in-reinforcement-learning/emergence-of-intelligence-through-reinforcement-learning-with-a-neural-network |isbn=978-953-307-369-9}}&lt;/ref&gt;&lt;ref name=&quot;Shibata2&quot;&gt;{{cite arXiv |last=Shibata |first=Katsunari |title=Functions that Emerge through End-to-End Reinforcement Learning | date=March 7, 2017 |eprint=1703.02239 }}&lt;/ref&gt; and has been sparked by the successful results in learning to play ATARI TV games (2013–15)&lt;ref name=&quot;DQN1&quot;&gt;{{cite conference |first= Volodymyr et al. |last= Mnih |date=December 2013 |title= Playing Atari with Deep Reinforcement Learning |url= https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf |conference= NIPS Deep Learning Workshop 2013}}&lt;/ref&gt;&lt;ref name=&quot;DQN2&quot;&gt;{{cite journal |first= Volodymyr et al. |last= Mnih |year=2015 |title= Human-level control through deep reinforcement learning |url= http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html |journal=Nature|volume=518 |issue=7540 |pages=529–533 |doi=10.1038/nature14236}}&lt;/ref&gt;&lt;ref name=&quot;Invaders&quot;&gt;{{cite video |people= V. Mnih et al. | date=26 February 2015 |title= Performance of DQN in the Game Space Invaders |url= http://www.nature.com/nature/journal/v518/n7540/extref/nature14236-sv1.mov}}&lt;/ref&gt;&lt;ref name=&quot;Breakout&quot;&gt;{{cite video |people= V. Mnih et al. | date=26 February 2015 |title= Demonstration of Learning Progress in the Game Breakout |url= http://www.nature.com/nature/journal/v518/n7540/extref/nature14236-sv2.mov}}&lt;/ref&gt; and [[AlphaGo]] (2016)&lt;ref name=&quot;AlphaGo&quot;&gt;{{Cite journal|title = Mastering the game of Go with deep neural networks and tree search|url = https://www.nature.com/nature/journal/v529/n7587/full/nature16961.html|journal = [[Nature (journal)|Nature]]| issn= 0028-0836|pages = 484–489|volume = 529|issue = 7587|doi = 10.1038/nature16961|pmid = 26819042|first1 = David|last1 = Silver|author-link1=David Silver (programmer)|first2 = Aja|last2 = Huang|author-link2=Aja Huang|first3 = Chris J.|last3 = Maddison|first4 = Arthur|last4 = Guez|first5 = Laurent|last5 = Sifre|first6 = George van den|last6 = Driessche|first7 = Julian|last7 = Schrittwieser|first8 = Ioannis|last8 = Antonoglou|first9 = Veda|last9 = Panneershelvam|first10= Marc|last10= Lanctot|first11= Sander|last11= Dieleman|first12=Dominik|last12= Grewe|first13= John|last13= Nham|first14= Nal|last14= Kalchbrenner|first15= Ilya|last15= Sutskever|author-link15=Ilya Sutskever|first16= Timothy|last16= Lillicrap|first17= Madeleine|last17= Leach|first18= Koray|last18= Kavukcuoglu|first19= Thore|last19= Graepel|first20= Demis |last20=Hassabis|author-link20=Demis Hassabis|date= 28 January 2016|bibcode = 2016Natur.529..484S|accessdate=10 December 2017}}{{closed access}}&lt;/ref&gt; by [[Google DeepMind]]. As well as [[deep learning]], by using a [[neural network]], it enables to learn massively parallel processing that humans can hardly design by hand, and to surpass what humans design.  Unlike [[supervised learning]], [[reinforcement learning]] makes autonomous learning possible.  Therefore, it can make the interference by human design minimum, and very flexible and purposive learning on a huge degree of freedom can be realized.  That is the reason why it is expected to solve the [[frame problem]] or [[symbol grounding problem]] and to open up the way to [[artificial general intelligence]] (AGI) or strong AI.

In reinforcement learning research, it has been general that state space and action space are designed in advance and only the mapping from state space to action space is learned.&lt;ref name=&quot;RL&quot;&gt;{{cite book | last1 = Sutton | first1 = Richard S.  |last2=Barto |first2=Andrew G. | title = Reinforcement Learning: An Introduction | publisher = MIT Press | year = 1998 | isbn = 978-0262193986}}&lt;/ref&gt;  Therefore, [[reinforcement learning]] has been limited to learning only for action, and human designers have to design how to construct state space from sensor signals and to give how the motion commands are generated for each action before learning.  [[Neural networks]] have been often used in [[reinforcement learning]], but that has been for non-linear function approximation to avoid the [[curse of dimensionality]] problem that occurs when table-lookup approach is used.&lt;ref name=&quot;RL&quot; /&gt;  [[Recurrent neural networks]] have been also used sometimes, but the main purpose of the use is to avoid perceptual aliasing or POMDP ([[partially observable Markov decision process]]).&lt;ref name=&quot;Lin&quot;&gt;{{cite conference |first1= Long-Ji |last1= Lin |first2= Tom M. |last2= Mitchell |year=1993 |title= Reinforcement Learning with Hidden States | journal=From Animals to Animats |volume=2 | pages=271–280 }}&lt;/ref&gt;&lt;ref name=&quot;Onat1&quot;&gt;{{cite conference |first1= Ahmet |last1= Onat |first2= Hajime et al. |last2= Kita |year=1998 |title= Q-learning with Recurrent Neural Networks as a Controller for the Inverted Pendulum Problem | conference=The 5th International Conference on Neural Information Processing (ICONIP) |pages=837–840}}&lt;/ref&gt;&lt;ref name=&quot;Onat2&quot;&gt;{{cite conference |first1= Ahmet |last1= Onat |first2= Hajime et al. |last2= Kita |year=1998 |title= Recurrent Neural Networks for Reinforcement Learning: Architecture, Learning Algorithms and Internal Representation |url=http://ieeexplore.ieee.org/document/687168/ |conference=International Joint Conference on Neural Networks (IJCNN) |pages=2010–2015}}&lt;/ref&gt;&lt;ref name=&quot;Bakker1&quot;&gt;{{cite conference |first1= Bram |last1= Bakker |first2= Fredrik et al. |last2= Linaker |year=2002 |title= Reinforcement Learning in Partially Observable Mobile Robot Domains Using Unsupervised Event Extraction |url=ftp://ftp.idsia.ch/pub/juergen/bakkeriros2002.pdf| conference= 2002 IEEE/RSJ International Conference on. Intelligent Robots and Systems (IROS) |pages=938–943}}&lt;/ref&gt;&lt;ref name=&quot;Bakker2&quot;&gt;{{cite conference |first1= Bram |last1= Bakker |first2= Viktor et al. |last2= Zhumatiy |year=2003 |title= A Robot that Reinforcement-Learns to Identify and Memorize Important Previous Observation |url=ftp://ftp.idsia.ch/pub/juergen/bakkeriros2003.pdf| conference= 2003 IEEE/RSJ International Conference on. Intelligent Robots and Systems (IROS) |pages=430–435}}&lt;/ref&gt;

However, the end-to-end reinforcement learning extends [[reinforcement learning]] from learning only for actions to learning for entire process by extending the learned process to the entire process from sensors to motors.  Therefore, not only actions, but also various functions including recognition and memory are expected to emerge.  Especially, in higher functions, they do not connect directly with either sensors or motors, and so even deciding either their inputs or outputs is very difficult. Since that has disturbed the understanding or developing of them, the progress in it is expected by this approach.

== History ==
The origin of this approach can be seen in [[TD-Gammon]] by G. Tesauro (1992).&lt;ref name=&quot;TD-Gammon&quot;&gt;{{cite journal | url=http://www.bkgm.com/articles/tesauro/tdl.html | title=Temporal Difference Learning and TD-Gammon | date=March 1995 | last=Tesauro | first=Gerald | journal=Communications of the ACM  | volume=38 | issue=3 | doi = 10.1145/203330.203343 | pages=58–68}}&lt;/ref&gt; In a popular game named Back Gammon, the evaluation of the game situation during self-play was learned through TD(&lt;math&gt;\lambda&lt;/math&gt;) using a layered neural network.  4 inputs were used for the number of men of a given color at a given location on the board and in total there are 198 input signals. With zero knowledge built in, the network was able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, and the internal representation after learning was observed.

K. Shibata's group has persisted in this framework and done so many works since around 1997.&lt;ref name=&quot;Shibata3&quot;&gt;{{cite conference |first1= Katsunari |last1= Shibata |first2= Yoichi |last2= Okabe |year=1997 |title= Reinforcement Learning When Visual Sensory Signals are Directly Given as Inputs |url= http://shws.cc.oita-u.ac.jp/~shibata/pub/ICNN97.pdf |conference= International Conference on Neural Networks (ICNN) 1997}}&lt;/ref&gt;&lt;ref name=&quot;Shibata2&quot; /&gt; Other than [[Q-learning]], they also employed actor-critic for continuous motion tasks,&lt;ref name=&quot;Shibata4&quot;&gt;{{cite conference |first1= Katsunari |last1= Shibata |first2= Masaru |last2= Iida |year=2003 |title= Acquisition of Box Pushing by Direct-Vision-Based Reinforcement Learning |url= http://shws.cc.oita-u.ac.jp/~shibata/pub/SICE03.pdf |conference= SICE Annual Conference 2003}}&lt;/ref&gt; and used a [[recurrent neural network]] for memory-required tasks.&lt;ref name=&quot;Shibata5&quot;&gt;{{cite conference |first1= Hiroki |last1= Utsunomiya |first2= Katsunari |last2= Shibata |year=2008 |title= Contextual Behavior and Internal Representations Acquired by Reinforcement Learning with a Recurrent Neural Network in a Continuous State and Action Space Task |url= http://shws.cc.oita-u.ac.jp/~shibata/pub/ICONIP98Utsunomiya.pdf |conference= International Conference on Neural Information Processing (ICONIP) '08}}&lt;/ref&gt; They also applied this framework to some real robot tasks.&lt;ref name=&quot;Shibata4&quot; /&gt;&lt;ref name=&quot;Shibata6&quot;&gt;{{cite conference |first1= Katsunari |last1= Shibata |first2= Tomohiko |last2= Kawano |year=2008 |title= Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network |url= http://shws.cc.oita-u.ac.jp/~shibata/pub/ICONIP98.pdf |conference= International Conference on Neural Information Processing (ICONIP) '08}}&lt;/ref&gt; They have also shown that various functions emerged in this framework as in the next section.

Since around 2013, as mentioned, [[Google DeepMind]] showed very impressive learning results in learning to play TV games&lt;ref name=&quot;DQN1&quot; /&gt;&lt;ref name=&quot;DQN2&quot; /&gt; and game of Go ([[AlphaGo]]).&lt;ref name=&quot;AlphaGo&quot; /&gt; They used a deep [[convolutional neural network]] that has shown superior results in image recognition. They also used 4 frames of almost raw RGB pixels (84x84) as inputs of the network, and the network was trained based on [[reinforcement learning]] with the reward representing the sign of the change in the game score.   All the 49 games could be learned using the same network architecture and [[Q-learning]] with the minimal prior knowledge, and it outperformed competing methods in almost all the games and performed at a level that is broadly comparable with or superior to a professional human game tester in the majority of games.&lt;ref name=&quot;DQN2&quot; /&gt; It is sometimes called DQN (Deep-Q network). In [[AlphaGo]], deep neural networks are trained not only by [[reinforcement learning]], but also by [[supervised learning]]. It was also combined with [[Monte Carlo tree search]].&lt;ref name=&quot;AlphaGo&quot; /&gt;

== Function Emergence ==
It has been shown by K. Shibata's group that various functions emerge in this framework: (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)knowledge transfer, (7)memory, (8)selective attention, (9)prediction, (10)exploration and so on.&lt;ref name=&quot;Shibata2&quot; /&gt;  Communications have been also established in this framework. (1)Dynamic communication (negotiation), (2)binalization of signals, and (3)grounded communication using a real robot and camera emerged in their works&lt;ref name=&quot;Shibata7&quot;&gt;{{cite arXiv |last=Shibata |first=Katsunari |title=Communications that Emerge through Reinforcement Learning Using a (Recurrent) Neural Network | date=March 9, 2017 |eprint=1703.03543 }}&lt;/ref&gt;

== References ==
&lt;!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. --&gt;
{{reflist}}

</text>
      <sha1>7qav1f0f14atwn3cugi5r6t7qt3289k</sha1>
    </revision>
  </page>
  <page>
    <title>BigDL</title>
    <ns>0</ns>
    <id>55075082</id>
    <revision>
      <id>816097374</id>
      <parentid>816077467</parentid>
      <timestamp>2017-12-19T07:14:28Z</timestamp>
      <contributor>
        <username>Aniceday007</username>
        <id>32654393</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1468">{{Infobox software
| name                   = BigDL
| developer              = Intel
| genre                  = Library for [[deep learning]]
| license                = [[Apache 2.0]]&lt;ref&gt;{{cite web|title=BigDL LICENSE|url=https://github.com/intel-analytics/BigDL/blob/master/LICENSE|work=GitHub}}&lt;/ref&gt;
| website                = {{url|title=BigDL: Distributed Deep Learning Library for Apache Spark|https://bigdl-project.github.io/}}
}}

{{machine learning bar}}

'''BigDL'''&lt;ref&gt;{{Cite web|url=https://bigdl-project.github.io/master/|title=BigDL Project|website=bigdl-project.github.io|language=en|access-date=2017-12-19}}&lt;/ref&gt; is a distributed deep learning framework for Apache Spark, created by Jason Dai at Intel.
== History ==
It is hosted at [[GitHub]].&lt;ref&gt;{{cite web|title=BigDL: Distributed Deep Learning Library for Apache Spark|url=https://github.com/intel-analytics/BigDL|publisher=GitHub}}&lt;/ref&gt;

== Features ==

== Applications ==

==See also==
* [[Comparison of deep learning software]]

==References==
{{Reflist}}

==External links==
*

{{Deep Learning Software}}

{{DEFAULTSORT:BigDL}}


[[Category:Data mining and machine learning software]]




[[Category:Information technology companies of the United States]]
</text>
      <sha1>d0x1n258w2mgt6p12tvl4rlnaok2tzy</sha1>
    </revision>
  </page>
  <page>
    <title>Highway network</title>
    <ns>0</ns>
    <id>55375136</id>
    <revision>
      <id>810710197</id>
      <parentid>810709837</parentid>
      <timestamp>2017-11-16T23:34:28Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Add: class. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1779">{{About|a technique used in machine learning||Highway}}

In [[machine learning]], a '''highway network''' is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by [[Long short-term memory|Long Short-Term Memory]] (LSTM) [[recurrent neural networks]]. The gating mechanisms allow neural networks to have paths for information to follow across different layers (&quot;information highways&quot;).&lt;ref&gt;{{cite arxiv|last1=Srivastava|first1=Rupesh Kumar|last2=Greff|first2=Klaus|last3=Schmidhuber|first3=Jürgen|title=Highway Networks|eprint=1505.00387|date=2 May 2015|class=cs.LG}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Srivastava|first1=Rupesh K|last2=Greff|first2=Klaus|last3=Schmidhuber|first3=Juergen|title=Training Very Deep Networks|journal=Advances in Neural Information Processing Systems 28|date=2015|pages=2377–2385|url=http://papers.nips.cc/paper/5850-training-very-deep-networks|publisher=Curran Associates, Inc.}}&lt;/ref&gt;

Highway networks have been used as part of [[Semantic analysis (machine learning)|text sequence labeling]]&lt;ref&gt;{{cite arxiv|last1=Liu|first1=Liyuan|last2=Shang|first2=Jingbo|last3=Xu|first3=Frank F.|last4=Ren|first4=Xiang|last5=Gui|first5=Huan|last6=Peng|first6=Jian|last7=Han|first7=Jiawei|title=Empower Sequence Labeling with Task-Aware Neural Language Model|eprint=1709.04109|date=12 September 2017|class=cs.CL}}&lt;/ref&gt; and [[speech recognition]]&lt;ref&gt;{{cite arxiv|last1=Kurata|first1=Gakuto|last2=Ramabhadran|first2=Bhuvana|last3=Saon|first3=George|last4=Sethy|first4=Abhinav|title=Language Modeling with Highway LSTM|eprint=1709.06436|date=19 September 2017|class=cs.CL}}&lt;/ref&gt; tasks.

==References==
{{reflist}}




{{compu-ai-stub}}</text>
      <sha1>aac3li8nf0b9l8wwsllgymvsr7od5r3</sha1>
    </revision>
  </page>
  <page>
    <title>Documenting Hate</title>
    <ns>0</ns>
    <id>54994687</id>
    <revision>
      <id>811717077</id>
      <parentid>811716932</parentid>
      <timestamp>2017-11-23T13:20:08Z</timestamp>
      <contributor>
        <username>Syrenka V</username>
        <id>31616631</id>
      </contributor>
      <comment>Added info on number of participants to lead paragraph</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14050">{{Infobox project
| name = Documenting Hate
| logo = [[File:Logo_for_the_Documenting_Hate_project.jpg]]
| type = [[Journalism]], [[data science]]
| products = News and data on [[hate crime]]s and [[bias incident]]s
| country = United States of America
| founder = [[ProPublica]]
| established = {{Start date|2017|01|17|df=y}}
| website = {{URL|www.documentinghate.com}}
}}
'''Documenting Hate''' is a project of [[ProPublica]], in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of [[hate crime]]s and [[bias incident]]s. It uses an online form to facilitate reporting of incidents by the general public.{{r|Wang-2017-01-23}} Since August 2017, it has also used [[machine learning]] and [[natural language processing]] techniques to monitor and collect news stories about hate crimes and bias incidents.{{r|Glickhouse-2017-08-18}}{{r|Rogers-2017-08-18}}{{r|Wang-2017-08-18}}{{r|Hatmaker-2017-08-18}}{{r|Morris-2017-08-19}} {{asof|2017|10}}, over 100 news organizations had joined the project.{{r|Gockowski-2017-10-10}}

== History ==

=== Origin ===

Documenting Hate was created in response to ProPublica's dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the [[United States presidential election, 2016|United States presidential election of 2016]]. The project was launched on 17 January 2017,{{r|New-2017-02-17}}{{r|Ciobanu-2017-01-20}}{{r|Anzilotti-2017-02-03}}{{r|ProPublica-2017-01-17}} after the publication on 15 November 2016 of a ProPublica news story about the difficulty of obtaining hard data on hate crimes.{{r|Thompson-and-Schwenke-2016-11-15}}

=== Introduction of the Documenting Hate News Index ===

On 18 August 2017, ProPublica and [[Google]] announced the creation of the Documenting Hate News Index, which uses the Google Cloud Natural Language API for automated monitoring and collection of news stories about hate crimes and bias incidents. The API uses [[machine learning]] and [[natural language processing]] techniques. The findings of the Index are integrated with reports from members of the public. The Index is a joint project of ProPublica, Google News Lab, and the [[data visualization]] studio Pitch Interactive.{{r|Glickhouse-2017-08-18}}{{r|Rogers-2017-08-18}}{{r|Wang-2017-08-18}}{{r|Hatmaker-2017-08-18}}{{r|Morris-2017-08-19}}

== Response ==

=== Participation ===

{{asof|2017|05}}, thousands of incidents had been reported via Documenting Hate.{{r|Latino-USA-2017-05-10}} {{asof|2017|10}}, over 100 news organizations had joined the project, including the [[The Boston Globe|''Boston Globe'']], the [[The New York Times|''New York Times'']], [[Vox (website)|''Vox'']], and the [[Georgetown University]] [[The Hoya|''Hoya'']].{{r|Gockowski-2017-10-10}}

=== Conservative response ===

An education reporter for the conservative ''[[The Daily Caller|Daily Caller]]'' has criticized the project for ambiguity in the terms it uses to describe hate crimes, and for neglect of hate-crime hoaxes.{{r|Shimshock-2017-03-23}} Another ''[[The Daily Caller|Daily Caller]]'' journalist has likewise criticized the Documenting Hate News Index for underrepresentation of conservative outlets among the news sources it monitors.{{r|Cheong-2017-08-20}}

=== Relationship to government statistical monitoring ===

A policy analyst for the Center for Data Innovation (an affiliate of the [[Information Technology and Innovation Foundation]]), while supporting ProPublica's critique of the present state of hate-crime statistics, and praising ProPublica for drawing attention to the problem, has argued that a nongovernmental project like Documenting Hate cannot solve it unaided; instead, intervention at the federal level is needed.{{r|New-2017-02-17}}

== References ==
{{reflist | 30em | refs =
&lt;ref name=Anzilotti-2017-02-03&gt;
{{cite web
 |url          = https://www.fastcompany.com/3067505/this-new-reporting-project-aims-to-shine-a-light-on-the-next-year-of-hate-crimes
 |last         = Anzilotti
 |first        = Eillie
 |date         = 3 February 2017
 |title        = This new reporting project aims to shine a light on the next year of hate crimes
 |website      = [[Fast Company (magazine)|Fast Company]]
 |language     = en
 |access-date  = 2017-08-12
 |archive-url  = https://www.webcitation.org/6sfugf2Qb?url=https://www.fastcompany.com/3067505/this-new-reporting-project-aims-to-shine-a-light-on-the-next-year-of-hate-crimes
 |dead-url     = no
 |archive-date = 2017-08-13
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Cheong-2017-08-20&gt;
{{cite journal
 |url          = http://dailycaller.com/2017/08/20/google-launches-hate-crime-tracking-tool-omits-conservative-websites/
 |last         = Cheong
 |first        = Ian Miles
 |date         = 20 August 2017
 |title        = Google launches hate crime tracking tool, omits conservative websites
 |website      = [[The Daily Caller|Daily Caller]]
 |access-date  = 2017-08-22
 |archive-url  = https://www.webcitation.org/6sv71m6jT?url=http://dailycaller.com/2017/08/20/google-launches-hate-crime-tracking-tool-omits-conservative-websites/
 |dead-url     = no
 |archive-date = 2017-08-23
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Ciobanu-2017-01-20&gt;
{{cite journal
 |url          = https://www.journalism.co.uk/news/propublica-is-collaborating-with-newsrooms-to-create-a-national-database-for-hate-crimes-and-bias-incidents-in-the-us/s2/a698540/
 |last         = Ciobanu
 |first        = Mădălina
 |date         = 20 January 2017
 |title        = ProPublica is collaborating with newsrooms to create a national database for hate crimes and bias incidents in the US
 |website      = [[Journalism.co.uk]]
 |access-date  = 2017-08-11
 |archive-url  = https://www.webcitation.org/6seFKVR1k?url=https://www.journalism.co.uk/news/propublica-is-collaborating-with-newsrooms-to-create-a-national-database-for-hate-crimes-and-bias-incidents-in-the-us/s2/a698540/
 |dead-url     = no
 |archive-date = 2017-08-11
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Glickhouse-2017-08-18&gt;
{{cite web
 |url          = https://www.propublica.org/article/track-news-stories-about-hate-with-the-documenting-hate-news-index
 |last         = Glickhouse
 |first        = Rachel
 |date         = 18 August 2017
 |title        = Track news stories about hate with the Documenting Hate News Index
 |website      = [[ProPublica]]
 |access-date  = 2017-08-22
 |archive-url  = https://www.webcitation.org/6suyHdqSF?url=https://www.propublica.org/article/track-news-stories-about-hate-with-the-documenting-hate-news-index
 |dead-url     = no
 |archive-date = 2017-08-22
 |df           =
}}&lt;/ref&gt;
&lt;ref name        = Gockowski-2017-10-10&gt;
{{cite web
 | url           = https://www.campusreform.org/?ID=9937
 | last          = Gockowski
 | first         = Anthony
 | date          = 10 October 2017
 | title         = Georgetown student paper begins documenting ‘bias’ incidents
 | website       = [[Campus Reform]]
 | language      = en
 | access-date   = 2017-11-23
 | archive-url   = https://web.archive.org/web/20171123131527/https://www.campusreform.org/?ID=9937
 | dead-url      = no
 | archive-date  = 2017-11-23
}}&lt;/ref&gt;
&lt;ref name=Hatmaker-2017-08-18&gt;
{{cite web
 |url          = https://techcrunch.com/2017/08/18/google-documenting-hate-news-index-propublica/
 |last         = Hatmaker
 |first        = Taylor
 |date         = 18 August 2017
 |title        = Google and ProPublica team up to build a national hate crime database
 |website      = [[TechCrunch]]
 |access-date  = 2017-08-22
 |archive-url  = https://www.webcitation.org/6suz8IJPo?url=https://techcrunch.com/2017/08/18/google-documenting-hate-news-index-propublica/
 |dead-url     = no
 |archive-date = 2017-08-22
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Latino-USA-2017-05-10&gt;
{{cite web
 |url          = http://latinousa.org/2017/05/10/thousands-hate-crimes-reported-propublicas-database/
 |date         = 10 May 2017
 |title        = Thousands of hate crimes reported to ProPublica’s database
 |website      = [[Latino USA]]
 |access-date  = 2017-08-09
 |archive-url  = https://www.webcitation.org/6saDrvGH6?url=http://latinousa.org/2017/05/10/thousands-hate-crimes-reported-propublicas-database/
 |archive-date = 2017-08-09
 |dead-url     = no
 |df           =
}}&lt;/ref&gt;
&lt;ref name      = Morris-2017-08-19&gt;
{{cite web
 | url         = http://fortune.com/2017/08/19/google-propublica-artificial-intelligence-hate-crimes/
 | last        = Morris
 | first       = David Z.
 | date        = 19 August 2017
 | title       = Google’s new site uses artificial intelligence to track hate crimes
 | website     = [[Fortune (magazine)|Fortune]]
 | access-date = 2017-08-22
}}&lt;/ref&gt;
&lt;ref name=New-2017-02-17&gt;
{{cite web
 |url          = https://www.datainnovation.org/2017/02/civil-society-shouldnt-have-to-solve-the-problem-of-bad-hate-crime-data-on-its-own/
 |last         = New
 |first        = Joshua
 |date         = 17 February 2017
 |title        = Civil society shouldn’t have to solve the problem of bad hate crime data on its own
 |website      = Center for Data Innovation
 |access-date  = 2017-08-11
 |archive-url  = https://www.webcitation.org/6seJsAhfu?url=https://www.datainnovation.org/2017/02/civil-society-shouldnt-have-to-solve-the-problem-of-bad-hate-crime-data-on-its-own/
 |dead-url     = no
 |archive-date = 2017-08-12
 |df           =
}}&lt;/ref&gt;
&lt;ref name=ProPublica-2017-01-17&gt;
{{cite web
 |url          = https://www.propublica.org/about/propublica-and-coalition-of-news-organizations-launch-documenting-hate
 |date         = 17 January 2017
 |title        = ProPublica and coalition of news organizations launch ‘Documenting Hate’ to collect data on hate crimes and bias incidents in the most complete, sustained effort to date
 |website      = [[ProPublica]]
 |access-date  = 2017-08-11
 |archive-url  = https://www.webcitation.org/6seGlUs4r?url=https://www.propublica.org/about/propublica-and-coalition-of-news-organizations-launch-documenting-hate
 |dead-url     = no
 |archive-date = 2017-08-11
 |df           =
}}&lt;/ref&gt;
&lt;ref name = Rogers-2017-08-18&gt;
{{cite web
 | url = https://www.blog.google/topics/journalism-news/new-machine-learning-app-reporting-hate-america/
 | last = Rogers
 | first = Simon
 | date = 18 August 2017
 | title = A new machine learning app for reporting on hate in America
 | website = [[Google]]
 | access-date = 2017-08-22
}}&lt;/ref&gt;&lt;ref name=Shimshock-2017-03-23&gt;
{{cite web
 |url          = http://dailycaller.com/2017/03/23/students-professors-and-media-start-database-to-document-hate/
 |last         = Shimshock
 |first        = Rob
 |date         = 23 March 2017
 |title        = Students, professors, and media start database to document ‘hate’
 |website      = [[The Daily Caller|Daily Caller]]
 |access-date  = 2017-08-11
 |archive-url  = https://www.webcitation.org/6seI3Udpf?url=http://dailycaller.com/2017/03/23/students-professors-and-media-start-database-to-document-hate/
 |dead-url     = no
 |archive-date = 2017-08-12
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Thompson-and-Schwenke-2016-11-15&gt;
{{cite web
 |url          = https://www.propublica.org/article/hate-crimes-are-up-but-the-government-isnt-keeping-good-track-of-them
 |last1        = Thompson
 |first1       = A.C.
 |last2        = Schwencke
 |first2       = Ken
 |date         = 15 November 2016
 |title        = Hate crimes are up—but the government isn’t keeping good track of them
 |website      = [[ProPublica]]
 |access-date  = 2017-08-11
 |archive-url  = https://www.webcitation.org/6seGV7Z6M?url=https://www.propublica.org/article/hate-crimes-are-up-but-the-government-isnt-keeping-good-track-of-them
 |dead-url     = no
 |archive-date = 2017-08-11
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Wang-2017-01-23&gt;
{{cite web
 |url          = http://www.niemanlab.org/2017/01/propublica-is-leading-a-nationwide-effort-to-document-hate-crimes-with-local-and-national-partners/
 |last         = Wang
 |first        = Shan
 |date         = 23 January 2017
 |title        = ProPublica is leading a nationwide effort to document hate crimes, with local and national partners
 |website      = [[Nieman Foundation for Journalism|Nieman Lab]]
 |access-date  = 2017-08-09
 |archive-url  = https://www.webcitation.org/6saCQbpKO?url=http://www.niemanlab.org/2017/01/propublica-is-leading-a-nationwide-effort-to-document-hate-crimes-with-local-and-national-partners/
 |dead-url     = no
 |archive-date = 2017-08-09
 |df           =
}}&lt;/ref&gt;
&lt;ref name=Wang-2017-08-18&gt;
{{cite web
 |url          = http://www.niemanlab.org/2017/08/with-data-from-google-news-this-new-tool-makes-it-easier-for-reporters-to-track-hate-crimes-nationwide/
 |last         = Wang
 |first        = Shan
 |date         = 18 August 2017
 |title        = With data from Google News, this new tool makes it easier for reporters to track hate crimes nationwide
 |website      = [[Nieman Foundation for Journalism|Nieman Lab]]
 |access-date  = 2017-08-22
 |archive-url  = https://www.webcitation.org/6suxWUIBB?url=http://www.niemanlab.org/2017/08/with-data-from-google-news-this-new-tool-makes-it-easier-for-reporters-to-track-hate-crimes-nationwide/
 |dead-url     = no
 |archive-date = 2017-08-22
 |df           =
}}&lt;/ref&gt;
}}

== External links ==
* [http://projects.propublica.org/graphics/hatecrimes Documenting Hate] on ProPublica ([http://www.documentinghate.com/ www.documentinghate.com] redirects to this ProPublica page)
* [https://projects.propublica.org/hate-news-index/ Documenting Hate News Index]
* [https://newslab.withgoogle.com/ Google News Lab]
* [https://newslab.withgoogle.com/ Google Cloud Natural Language API]
* [https://projects.propublica.org/hate-news-index/ Pitch Interactive]

&lt;!--- Categories ---&gt;






[[Category:Media analysis organizations and websites]]


</text>
      <sha1>e5v20y9j5czyan78h9osl0ga38r2euo</sha1>
    </revision>
  </page>
  <page>
    <title>Random forest</title>
    <ns>0</ns>
    <id>1363880</id>
    <revision>
      <id>810361658</id>
      <parentid>809766369</parentid>
      <timestamp>2017-11-14T20:14:39Z</timestamp>
      <contributor>
        <ip>128.193.154.76</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34946">{{About|the machine learning technique|other kinds of random tree|Random tree}}
{{machine learning bar}}

'''Random forests''' or '''random decision forests'''&lt;ref name=&quot;ho1995&quot;/&gt;&lt;ref name=&quot;ho1998&quot;/&gt; are an [[ensemble learning]] method for [[statistical classification|classification]], [[regression analysis|regression]] and other tasks, that operate by constructing a multitude of [[decision tree learning|decision trees]] at training time and outputting the class that is the [[mode (statistics)|mode]] of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of [[overfitting]] to their [[Test set|training set]].{{r|elemstatlearn}}{{rp|587–588}}

The first algorithm for random decision forests was created by Tin Kam Ho&lt;ref name=&quot;ho1995&quot;&gt;{{cite conference
 |first=Tin Kam |last=Ho
 |title=Random Decision Forests
 |conference=Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, QC, 14–16 August 1995
 |year=1995 |pages=278–282
 |url=http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf
}}&lt;/ref&gt; using the [[random subspace method]],&lt;ref name=&quot;ho1998&quot;&gt;{{cite journal
 |first=Tin Kam |last=Ho
 |title=The Random Subspace Method for Constructing Decision Forests
 |journal=IEEE Transactions on Pattern Analysis and Machine Intelligence
 |year=1998 |volume=20 |issue=8 |pages=832–844
 |doi=10.1109/34.709601
 |url=http://ect.bell-labs.com/who/tkh/publications/papers/df.pdf
}}&lt;/ref&gt; which, in Ho's formulation, is a way to implement the &quot;stochastic discrimination&quot; approach to classification proposed by Eugene Kleinberg.&lt;ref name=&quot;kleinberg1996&quot;&gt;{{cite journal |first=Eugene |last=Kleinberg |title=An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition |journal=[[Annals of Statistics]] |year=1996 |volume=24 |issue=6 |pages=2319–2349 |url=http://kappa.math.buffalo.edu/aos.pdf |doi=10.1214/aos/1032181157 |mr=1425956}}&lt;/ref&gt;&lt;ref name=&quot;kleinberg2000&quot;&gt;{{cite journal|first=Eugene|last=Kleinberg|title=On the Algorithmic Implementation of Stochastic Discrimination|journal=IEEE Transactions on PAMI|year=2000|volume=22|issue=5|url=http://kappa.math.buffalo.edu/473.pdf}}&lt;/ref&gt;&lt;ref name=&quot;kleinbergurl&quot;&gt;{{cite journal|first=Eugine |last=Kleinberg|title=Stochastic Discrimination and its Implementation|url=http://kappa.math.buffalo.edu/}}&lt;/ref&gt;

An extension of the algorithm was developed by [[Leo Breiman]]&lt;ref name=&quot;breiman2001&quot;&gt;{{cite journal
 |first=Leo |last=Breiman |authorlink=Leo Breiman
 |title=Random Forests
 |journal=[[Machine Learning (journal)|Machine Learning]]
 |year=2001 |volume=45 |issue=1 |pages=5–32
 |doi=10.1023/A:1010933404324
}}&lt;/ref&gt; and Adele Cutler,&lt;ref name=&quot;rpackage&quot;/&gt; and &quot;Random Forests&quot; is their [[trademark]].&lt;ref&gt;U.S. trademark registration number 3185828, registered 2006/12/19.&lt;/ref&gt; The extension combines Breiman's &quot;[[Bootstrap aggregating|bagging]]&quot; idea and random selection of features, introduced first by Ho&lt;ref name=&quot;ho1995&quot;/&gt; and later independently by Amit and [[Donald Geman|Geman]]&lt;ref name=&quot;amitgeman1997&quot;&gt;{{cite journal
 |last=Amit |first=Yali
 |last2=Geman  |first2=Donald |authorlink2=Donald Geman
 |title=Shape quantization and recognition with randomized trees
 |journal=[[Neural Computation (journal)|Neural Computation]]
 |year=1997 |volume=9 |issue=7 |pages=1545–1588
 |doi=10.1162/neco.1997.9.7.1545
 |url=http://www.cis.jhu.edu/publications/papers_in_database/GEMAN/shape.pdf
}}&lt;/ref&gt; in order to construct a collection of decision trees with controlled variance.

== History ==
The general method of random decision forests was first proposed by Ho in 1995,&lt;ref name=&quot;ho1995&quot;/&gt; who established that forests of trees splitting with oblique hyperplanes, if randomly restricted to be sensitive to only selected [[Feature (machine learning)|feature]] dimensions, can gain accuracy as they grow without suffering from overtraining.  A subsequent work along the same lines &lt;ref name=&quot;ho1998&quot;/&gt; concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly.  Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.&lt;ref name=&quot;kleinberg1996&quot;/&gt;&lt;ref name=&quot;kleinberg2000&quot;/&gt;&lt;ref name=&quot;kleinbergurl&quot;/&gt;

The early development of Breiman's notion of random forests was influenced by the work of Amit and
Geman&lt;ref name=&quot;amitgeman1997&quot;/&gt; who introduced the idea of searching over a random subset of the
available decisions when splitting a node, in the context of growing a single
[[Decision tree|tree]].  The idea of random subspace selection from Ho&lt;ref name=&quot;ho1998&quot;/&gt; was also influential in the design of random forests.  In this method a forest of trees is grown,
and variation among the trees is introduced by projecting the training data
into a randomly chosen [[Linear subspace|subspace]] before fitting each tree or each node.  Finally, the idea of
randomized node optimization, where the decision at each node is selected by a
randomized procedure, rather than a deterministic optimization was first
introduced by Dietterich.&lt;ref&gt;{{cite journal
 |first=Thomas |last=Dietterich
 |title=An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization
 |journal=[[Machine Learning (journal)|Machine Learning]]
 |year=2000  |pages=139–157}}&lt;/ref&gt;

The introduction of random forests proper was first made in a paper
by [[Leo Breiman]].&lt;ref name=&quot;breiman2001&quot;/&gt;  This paper describes a method of building a forest of
uncorrelated trees using a [[Classification and regression tree|CART]] like procedure, combined with randomized node
optimization and [[Bootstrap aggregating|bagging]].  In addition, this paper combines several
ingredients, some previously known and some novel, which form the basis of the
modern practice of random forests, in particular:

# Using [[out-of-bag error]] as an estimate of the [[generalization error]].
# Measuring variable importance through permutation.

The report also offers the first theoretical result for random forests in the
form of a bound on the [[generalization error]] which depends on the strength of the
trees in the forest and their [[correlation]].

==Algorithm==

===Preliminaries: decision tree learning===
{{main article|Decision tree learning}}
Decision trees are a popular method for various machine learning tasks. Tree learning &quot;come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining&quot;, say [[Trevor Hastie|Hastie]] ''et al.'', &quot;because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate&quot;.&lt;ref name=&quot;elemstatlearn&quot;&gt;{{ElemStatLearn}}&lt;/ref&gt;{{rp|352}}

In particular, trees that are grown very deep tend to learn highly irregular patterns: they [[overfitting|overfit]] their training sets, i.e. have [[Bias–variance tradeoff|low bias, but very high variance]]. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.&lt;ref name=&quot;elemstatlearn&quot;/&gt;{{rp|587–588}} This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.

===Tree bagging===
{{main article|Bootstrap aggregating}}
The training algorithm for random forests applies the general technique of [[bootstrap aggregating]], or bagging, to tree learners. Given a training set {{mvar|X}} = {{mvar|x&lt;sub&gt;1&lt;/sub&gt;}}, ..., {{mvar|x&lt;sub&gt;n&lt;/sub&gt;}} with responses {{mvar|Y}} = {{mvar|y&lt;sub&gt;1&lt;/sub&gt;}}, ..., {{mvar|y&lt;sub&gt;n&lt;/sub&gt;}}, bagging repeatedly (''B'' times) selects a [[Bootstrapping (statistics)|random sample with replacement]] of the training set and fits trees to these samples:

: For {{mvar|b}} = 1, ..., {{mvar|B}}:
:# Sample, with replacement, {{mvar|n}} training examples from {{mvar|X}}, {{mvar|Y}}; call these {{mvar|X&lt;sub&gt;b&lt;/sub&gt;}}, {{mvar|Y&lt;sub&gt;b&lt;/sub&gt;}}.
:# Train a classification or regression tree {{mvar|f&lt;sub&gt;b&lt;/sub&gt;}} on {{mvar|X&lt;sub&gt;b&lt;/sub&gt;}}, {{mvar|Y&lt;sub&gt;b&lt;/sub&gt;}}.

After training, predictions for unseen samples {{mvar|x'}} can be made by averaging the predictions from all the individual regression trees on {{mvar|x'}}:

:&lt;math&gt;\hat{f} = \frac{1}{B} \sum_{b=1}^Bf_b (x')&lt;/math&gt;

or by taking the majority vote in the case of classification trees.

This bootstrapping procedure leads to better model performance because it decreases the [[Bias–variance dilemma|variance]] of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.

Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on {{mvar|x'}}:

:&lt;math&gt;\sigma = \sqrt{\frac{\sum_{b=1}^B (f_b(x')  - \hat{f})^2}{B-1} }.&lt;/math&gt;

The number of samples/trees, {{mvar|B}}, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees {{mvar|B}} can be found using [[Cross-validation_(statistics)|cross-validation]], or by observing the ''[[out-of-bag error]]'': the mean prediction error on each training sample {{mvar|xᵢ}}, using only the trees that did not have {{mvar|xᵢ}} in their bootstrap sample.&lt;ref name=&quot;islr&quot;&gt;{{cite book |author1=Gareth James |author2=Daniela Witten |author3=Trevor Hastie |author4=Robert Tibshirani |title=An Introduction to Statistical Learning |publisher=Springer |year=2013 |url=http://www-bcf.usc.edu/~gareth/ISL/ |pages=316–321}}&lt;/ref&gt;
The training and test error tend to level off after some number of trees have been fit.

===From bagging to random forests===
{{main article|Random subspace method}}
The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a [[Random subspace method|random subset of the features]]. This process is sometimes called &quot;feature bagging&quot;. The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few [[Feature (machine learning)|features]] are very strong predictors for the response variable (target output), these features will be selected in many of the {{mvar|B}} trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.&lt;ref name=&quot;ho2002&quot;&gt;
{{cite journal
 |first=Tin Kam |last=Ho
 |title=A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors
 |journal=Pattern Analysis and Applications
 |year=2002
 |pages=102–112
 |url=http://ect.bell-labs.com/who/tkh/publications/papers/compare.pdf}}&lt;/ref&gt;

Typically, for a classification problem with {{mvar|p}} features, {{sqrt|{{mvar|p}}}} (rounded down) features are used in each split.&lt;ref name=&quot;elemstatlearn&quot;/&gt;{{rp|592}}  For regression problems the inventors recommend {{mvar|p/3}} (rounded down) with a minimum node size of 5 as the default.&lt;ref name=&quot;elemstatlearn&quot;/&gt;{{rp|592}}

===ExtraTrees===
Adding one further step of randomization yields ''extremely randomized trees'', or ExtraTrees. These are trained using bagging and the random subspace method, like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Instead of computing the locally ''optimal'' feature/split combination (based on, e.g., [[information gain]] or the [[Gini impurity]]), for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample).&lt;ref&gt;{{Cite journal | doi = 10.1007/s10994-006-6226-1| title = Extremely randomized trees| journal = Machine Learning| volume = 63| pages = 3–42| year = 2006| last1 = Geurts | first1 = P. | last2 = Ernst | first2 = D. | last3 = Wehenkel | first3 = L. | url = http://orbi.ulg.ac.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf}}&lt;/ref&gt;

==Properties==

=== Variable importance ===

Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper&lt;ref name=breiman2001/&gt; and is implemented in the [[R (programming language)|R]] package randomForest.&lt;ref name=&quot;rpackage&quot;&gt;{{cite web
 |url=https://cran.r-project.org/web/packages/randomForest/randomForest.pdf
 |title=Documentation for R package randomForest
 |first1=Andy |last1=Liaw
 |date=16 October 2012
 |accessdate=15 March 2013}}
&lt;/ref&gt;

The first step in measuring the variable importance in a data set &lt;math&gt;\mathcal{D}_n =\{(X_i, Y_i)\}_{i=1}^n&lt;/math&gt; is to fit a random forest to the data.  During the fitting process the [[out-of-bag error]] for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).

To measure the importance of the &lt;math&gt;j&lt;/math&gt;-th feature after training, the values of the &lt;math&gt;j&lt;/math&gt;-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set.  The importance score for the &lt;math&gt;j&lt;/math&gt;-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.

Features which produce large values for this score are ranked as more important than features which produce small values.

This method of determining variable importance has some drawbacks.  For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as [[partial permutation]]s&lt;ref&gt;{{cite conference
|author=Deng,H.|author2=Runger, G. |author3=Tuv, E.
 |title=Bias of importance measures for multi-valued attributes and solutions
|conference=Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN)
|year=2011|pages=293–300
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |vauthors=Altmann A, Tolosi L, Sander O, Lengauer T |title=Permutation importance:a corrected feature importance measure |journal=Bioinformatics |year=2010 |doi=10.1093/bioinformatics/btq134 |url=http://bioinformatics.oxfordjournals.org/content/early/2010/04/12/bioinformatics.btq134.abstract |volume=26 |pages=1340–1347 |pmid=20385727}}&lt;/ref&gt;
and growing unbiased trees&lt;ref&gt;{{cite journal
|last=Strobl |first=Carolin |last2=Boulesteix |first2=Anne-Laure |last3=Augustin |first3=Thomas
|title=Unbiased split selection for classification trees based on the Gini index
|journal=Computational Statistics &amp; Data Analysis
|year=2007|pages=483–501
|url=https://epub.ub.uni-muenchen.de/1833/1/paper_464.pdf}}&lt;/ref&gt; can be used to solve the problem.  If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.&lt;ref&gt;{{cite journal |vauthors=Tolosi L, Lengauer T |title=Classification with correlated features: unreliability of feature ranking and solutions. |journal=Bioinformatics |year=2011 |doi=10.1093/bioinformatics/btr300 |url=http://bioinformatics.oxfordjournals.org/content/27/14/1986.abstract |volume=27 |pages=1986–1994 |pmid=21576180}}&lt;/ref&gt;

=== Relationship to nearest neighbors ===
A relationship between random forests and the [[K-nearest neighbor algorithm|{{mvar|k}}-nearest neighbor algorithm]] ({{mvar|k}}-NN) was pointed out by Lin and Jeon in 2002.&lt;ref name=&quot;linjeon02&quot;&gt;{{Cite techreport  |first1=Yi |last1=Lin |first2=Yongho |last2=Jeon |title=Random forests and adaptive nearest neighbors |series=Technical Report No. 1055 |year=2002 |institution=University of Wisconsin |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.9168}}&lt;/ref&gt; It turns out that both can be viewed as so-called ''weighted neighborhoods schemes''. These are models built from a training set &lt;math&gt;\{(x_i, y_i)\}_{i=1}^n&lt;/math&gt; that make predictions &lt;math&gt;\hat{y}&lt;/math&gt; for new points {{mvar|x'}} by looking at the &quot;neighborhood&quot; of the point, formalized by a weight function {{mvar|W}}:

:&lt;math&gt;\hat{y} = \sum_{i=1}^n W(x_i, x') \, y_i.&lt;/math&gt;

Here, &lt;math&gt;W(x_i, x')&lt;/math&gt; is the non-negative weight of the {{mvar|i}}'th training point relative to the new point {{mvar|x'}} in the same tree. For any particular {{mvar|x'}}, the weights for points &lt;math&gt;x_i&lt;/math&gt; must sum to one. Weight functions are given as follows:

* In {{mvar|k}}-NN, the weights are &lt;math&gt;W(x_i, x') = \frac{1}{k}&lt;/math&gt; if {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} is one of the {{mvar|k}} points closest to {{mvar|x'}}, and zero otherwise.
* In a tree, &lt;math&gt;W(x_i, x') = \frac{1}{k'}&lt;/math&gt; if {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} is one of the {{mvar|k'}} points in the same leaf as {{mvar|x'}}, and zero otherwise.

Since a forest averages the predictions of a set of {{mvar|m}} trees with individual weight functions &lt;math&gt;W_j&lt;/math&gt;, its predictions are

:&lt;math&gt;\hat{y} = \frac{1}{m}\sum_{j=1}^m\sum_{i=1}^n W_{j}(x_i, x') \, y_i = \sum_{i=1}^n\left(\frac{1}{m}\sum_{j=1}^m W_{j}(x_i, x')\right) \, y_i.&lt;/math&gt;

This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of {{mvar|x'}} in this interpretation are the points &lt;math&gt;x_i&lt;/math&gt; sharing the same leaf in any tree &lt;math&gt;j&lt;/math&gt;. In this way, the neighborhood of {{mvar|x'}} depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.&lt;ref name=&quot;linjeon02&quot;/&gt;

== Unsupervised learning with random forests ==
As part of their construction, random forest predictors naturally lead to a dissimilarity measure between the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.&lt;ref name=breiman2001/&gt;&lt;ref&gt;{{cite journal |authors=Shi, T., Horvath, S. |year=2006 |title=Unsupervised Learning with Random Forest Predictors |journal=Journal of Computational and Graphical Statistics |volume=15 |issue=1 |pages=118–138  |doi=10.1198/106186006X94072 |jstor=27594168}}&lt;/ref&gt;
The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the &quot;Addcl 1&quot; random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.&lt;ref&gt;{{cite journal |url=http://www.nature.com/modpathol/journal/v18/n4/full/3800322a.html |authors=Shi, T., Seligson D., Belldegrun AS., Palotie A, Horvath, S. |year=2005 |title=Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma |journal=Modern Pathology |volume=18 |issue=4 |pages=547–557 |pmid=15529185 |doi=10.1038/modpathol.3800322 }}&lt;/ref&gt;

== Variants ==
Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular [[multinomial logistic regression]] and [[naive Bayes classifier]]s.&lt;ref&gt;{{cite journal |url=https://dx.doi.org/10.1016/j.eswa.2007.01.029 |authors=Prinzie, A., Van den Poel, D. |year=2008 |title=Random Forests for multiclass classification: Random MultiNomial Logit |journal=Expert Systems with Applications |volume=34 |issue=3 |pages=1721–1732 |doi=10.1016/j.eswa.2007.01.029}}&lt;/ref&gt;&lt;ref&gt;[https://dx.doi.org/10.1007/978-3-540-74469-6_35 Prinzie, A., Van den Poel, D. (2007). Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB, Dexa 2007, Lecture Notes in Computer Science, 4653, 349–358.]
&lt;/ref&gt;

==Kernel random forest==
In machine learning, '''kernel random forests''' establish the connection between [[random forest]]s and [[kernel method]]s. By slightly modifying their definition, [[random forests]] can be rewritten as [[kernel method]]s, which are more interpretable and easier to analyze.&lt;ref name=&quot;scornet2015random&quot;&gt;{{cite arXiv
 |first=Erwan|last=Scornet
 |title=Random forests and kernel methods
   |year= 2015|arxiv=1502.03836
}}&lt;/ref&gt;

=== History ===
[[Leo Breiman]]&lt;ref name=&quot;breiman2000some&quot;&gt;{{cite journal
 |first=Leo
 |last=Breiman
 |authorlink=Leo Breiman
 |title=Some infinity theory for predictor ensembles
 |institution=Technical Report 579, Statistics Dept. UCB
 |year=2000
 |url=http://oz.berkeley.edu/~breiman/some_theory2000.pdf
}}{{dead link|date=May 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; was the first person to notice the link between [[random forest]] and [[kernel methods]]. He pointed out that [[random forests]] which are grown using [[i.i.d.]] random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon &lt;ref name=&quot;lin2006random&quot;&gt;{{cite journal
 |first=Yi |last=Lin
 |first2=Yongho|last2=Jeon
 |title=Random forests and adaptive nearest neighbors
 |journal= Journal of the American Statistical Association
 |volume= 101
 |number= 474
 |pages=578–590
 |year= 2006
 |doi=10.1198/016214505000001230
}}&lt;/ref&gt; established the connection between [[random forests]] and adaptive nearest neighbor, implying that [[random forests]] can be seen as adaptive kernel estimates. Davies and Ghahramani&lt;ref name=&quot;davies2014random&quot;&gt;{{cite arXiv
 |first=Alex |last=Davies
 |first2=Zoubin|last2=Ghahramani
 |title=The Random Forest Kernel and other kernels for big data from random partitions
   |arxiv=1402.4293
 |year= 2014
}}&lt;/ref&gt; proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet&lt;ref name=&quot;scornet2015random&quot;/&gt; first defined '''KeRF estimates''' and gave the explicit link between '''KeRF estimates''' and [[random forest]]. He also gave explicit expressions for kernels based on centered random forest&lt;ref name=&quot;breiman2004consistency&quot;&gt;{{cite journal
 |first=Leo|last=Breiman
 |first2=Zoubin|last2=Ghahramani
 |title=Consistency for a simple model of random forests
 |journal= Statistical Department, University of California at Berkeley. Technical Report
 |number=670
 |year= 2004
}}&lt;/ref&gt; and uniform random forest,&lt;ref name=&quot;arlot2014analysis&quot;&gt;{{cite arXiv
 |first=Sylvain |last=Arlot
 |first2=Robin|last2=Genuer
 |title=Analysis of purely random forests bias
   |arxiv=1407.3939
 |year= 2014
}}&lt;/ref&gt; two simplified models of [[random forest]]. He named these two '''KeRFs''' by '''Centered KeRF''' and '''Uniform KeRF''', and proved upper bounds on their rates of consistency.

=== Notations and definitions ===
==== Preliminaries: Centered forests ====
Centered forest&lt;ref name=&quot;breiman2004consistency&quot;/&gt; is a simplified model for Breiman's original [[random forest]], which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level &lt;math&gt;k&lt;/math&gt; is built, where &lt;math&gt;k \in\mathbb{N} &lt;/math&gt; is a parameter of the algorithm.

==== Uniform forest ====
Uniform forest&lt;ref name=&quot;arlot2014analysis&quot;/&gt; is another simplified model for Breiman's original [[random forest]], which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.

==== From random forest to KeRF ====
Given a training sample  &lt;math&gt;\mathcal{D}_n =\{(\mathbf{X}_i, Y_i)\}_{i=1}^n&lt;/math&gt; of &lt;math&gt;[0,1]^p\times\mathbb{R}&lt;/math&gt;-valued independent random variables distributed as the independent prototype pair &lt;math&gt;(\mathbf{X}, Y)&lt;/math&gt;, where &lt;math&gt;\operatorname{E}[Y^2]&lt;\infty&lt;/math&gt;. We aim at predicting the response &lt;math&gt;Y&lt;/math&gt;, associated with the random variable &lt;math&gt;\mathbf{X}&lt;/math&gt;, by estimating the regression function &lt;math&gt;m(\mathbf{x})=\operatorname{E}[Y \mid \mathbf{X} = \mathbf{x}]&lt;/math&gt;. A random regression forest is an ensemble of &lt;math&gt;M&lt;/math&gt; randomized regression trees. Denote &lt;math&gt;m_n(\mathbf{x},\mathbf{\Theta}_j)&lt;/math&gt; the predicted value at point &lt;math&gt;\mathbf{x}&lt;/math&gt; by the &lt;math&gt;j&lt;/math&gt;-th tree, where &lt;math&gt;\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_M &lt;/math&gt; are independent random variables, distributed as a generic random variable &lt;math&gt;\mathbf{\Theta}&lt;/math&gt;, independent of the sample &lt;math&gt;\mathcal{D}_n&lt;/math&gt;. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate &lt;math&gt;m_{M, n}(\mathbf{x},\Theta_1,\ldots,\Theta_M) = \frac{1}{M}\sum_{j=1}^M m_n(\mathbf{x},\Theta_j)&lt;/math&gt;.
For regression trees, we have &lt;math&gt;m_n = \sum_{i=1}^n\frac{Y_i\mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x},\Theta_j)}}{N_n(\mathbf{x}, \Theta_j)}&lt;/math&gt;, where &lt;math&gt;A_n(\mathbf{x},\Theta_j)&lt;/math&gt; is the cell containing &lt;math&gt;\mathbf{x}&lt;/math&gt;, designed with randomness &lt;math&gt;\Theta_j&lt;/math&gt; and dataset &lt;math&gt;\mathcal{D}_n&lt;/math&gt;, and &lt;math&gt; N_n(\mathbf{x}, \Theta_j) = \sum_{i=1}^n \mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x}, \Theta_j)}&lt;/math&gt;.

Thus random forest estimates satisfy, for all &lt;math&gt;\mathbf{x}\in[0,1]^d&lt;/math&gt;, &lt;math&gt; m_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) =\frac{1}{M}\sum_{j=1}^M \left(\sum_{i=1}^n\frac{Y_i\mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x},\Theta_j)}}{N_n(\mathbf{x}, \Theta_j)}\right)&lt;/math&gt;. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet&lt;ref name=&quot;scornet2015random&quot;/&gt; defined '''KeRF''' by

: &lt;math&gt; \tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) = \frac{1}{\sum_{j=1}^M N_n(\mathbf{x}, \Theta_j)}\sum_{j=1}^M\sum_{i=1}^n Y_i\mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x}, \Theta_j)},&lt;/math&gt;

which is equal to the mean of the &lt;math&gt;Y_i&lt;/math&gt;'s falling in the cells containing &lt;math&gt;\mathbf{x}&lt;/math&gt; in the forest. If we define the connection function of the &lt;math&gt;M&lt;/math&gt; finite forest as &lt;math&gt;K_{M,n}(\mathbf{x}, \mathbf{z}) = \frac{1}{M} \sum_{j=1}^M \mathbf{1}_{\mathbf{z} \in A_n (\mathbf{x}, \Theta_j)}&lt;/math&gt;, i.e. the proportion of cells shared between &lt;math&gt;\mathbf{x}&lt;/math&gt; and &lt;math&gt;\mathbf{z}&lt;/math&gt;, then almost surely we have &lt;math&gt;\tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) =
\frac{\sum_{i=1}^n Y_i K_{M,n}(\mathbf{x}, \mathbf{x}_i)}{\sum_{\ell=1}^n K_{M,n}(\mathbf{x}, \mathbf{x}_{\ell})}&lt;/math&gt;, which defines the '''KeRF'''.

==== Centered KeRF ====
The construction of '''Centered KeRF''' of level &lt;math&gt;k&lt;/math&gt; is the same as for '''centered forest''', except that predictions are made by &lt;math&gt;\tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) &lt;/math&gt;, the corresponding kernel function, or connection function is

: &lt;math&gt;
\begin{align}
K_k^{cc}(\mathbf{x},\mathbf{z}) = \sum_{k_1,\ldots,k_d, \sum_{j=1}^d k_j=k} &amp;
\frac{k!}{k_1!\cdots k_d!} \left(\frac 1 d \right)^k
\prod_{j=1}^d\mathbf{1}_{\lceil2^{k_j}x_j\rceil=\lceil2^{k_j}z_j\rceil}, \\
&amp; \text{ for all } \mathbf{x},\mathbf{z}\in[0,1]^d.
\end{align}
&lt;/math&gt;

==== Uniform KeRF ====
'''Uniform KeRF''' is built in the same way as '''uniform forest''', except that predictions are made by &lt;math&gt;\tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) &lt;/math&gt;, the corresponding kernel function, or connection function is
:&lt;math&gt;K_k^{uf}(\mathbf{0},\mathbf{x}) =
\sum_{k_1,\ldots,k_d, \sum_{j=1}^d k_j=k}
\frac{k!}{k_1!\ldots k_d!}\left(\frac{1}{d}\right)^k
\prod_{m=1}^d\left(1-|x_m|\sum_{j=0}^{k_m-1}\frac{(-\ln|x_m|)^j}{j!}\right) \text{ for all } \mathbf{x}\in[0,1]^d.&lt;/math&gt;

=== Properties ===
==== Relation between KeRF and random forest ====
Predictions given by '''KeRF''' and [[random forests]] are close if the number of points in each cell is controlled:

&lt;blockquote&gt;
Assume that there exist sequences &lt;math&gt; (a_n),(b_n) &lt;/math&gt; such that, almost surely,
: &lt;math&gt; a_n\leq N_n(\mathbf{x},\Theta)\leq b_n \text{ and } a_n\leq \frac 1 M \sum_{m=1}^M N_n {\mathbf{x},\Theta_m}\leq b_n.
&lt;/math&gt;
Then almost surely,
:&lt;math&gt;|m_{M,n}(\mathbf{x}) - \tilde{m}_{M,n}(\mathbf{x})| \le\frac{b_n-a_n}{a_n} \tilde{m}_{M,n}(\mathbf{x}).
&lt;/math&gt;
&lt;/blockquote&gt;

==== Relation between infinite KeRF and infinite random forest ====
When the number of trees &lt;math&gt;M&lt;/math&gt; goes to infinity, then we have infinite [[random forest]] and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:

&lt;blockquote&gt;
Assume that there exist sequences &lt;math&gt;(\varepsilon_n), (a_n),(b_n)&lt;/math&gt; such that, almost surely
* &lt;math&gt;\operatorname{E}[N_n(\mathbf{x},\Theta)] \ge 1,&lt;/math&gt;
* &lt;math&gt;\operatorname{P}[a_n\le N_n(\mathbf{x},\Theta) \le b_n\mid \mathcal{D}_n] \ge 1-\varepsilon_n/2,&lt;/math&gt;
* &lt;math&gt;\operatorname{P}[a_n\le \operatorname{E}_\Theta [N_n(\mathbf{x},\Theta)] \le b_n\mid \mathcal{D}_n] \ge 1-\varepsilon_n/2,&lt;/math&gt;
Then almost surely,
: &lt;math&gt; |m_{\infty,n}(\mathbf{x}-\tilde{m}_{\infty,n}(\mathbf{x})| \le
\frac{b_n-a_n}{a_n}\tilde{m}_{\infty,n}(\mathbf{x}) + n \varepsilon_n \left( \max_{1\le i\le n} Y_i \right).&lt;/math&gt;
&lt;/blockquote&gt;

=== Consistency results ===
Assume that &lt;math&gt;Y = m(\mathbf{X}) + \varepsilon&lt;/math&gt;, where &lt;math&gt;\varepsilon&lt;/math&gt; is a centered Gaussian noise, independent of &lt;math&gt;\mathbf{X}&lt;/math&gt;, with finite variance &lt;math&gt;\sigma^2&lt;\infty&lt;/math&gt;. Moreover, &lt;math&gt;\mathbf{X}&lt;/math&gt; is uniformly distributed on &lt;math&gt;[0,1]^d&lt;/math&gt; and &lt;math&gt;m&lt;/math&gt; is [[Lipschitz]]. Scornet&lt;ref name=&quot;scornet2015random&quot;/&gt; proved upper bounds on the rates of consistency for '''centered KeRF''' and '''uniform KeRF'''.

==== Consistency of centered KeRF ====
Providing &lt;math&gt;k\rightarrow\infty&lt;/math&gt; and &lt;math&gt;n/2^k\rightarrow\infty&lt;/math&gt;, there exists a constant &lt;math&gt;C_1&gt;0&lt;/math&gt; such that, for all &lt;math&gt;n&lt;/math&gt;,
&lt;math&gt; \mathbb{E}[\tilde{m}_n^{cc}(\mathbf{X}) - m(\mathbf{X})]^2 \le C_1 n^{-1/(3+d\log 2)}(\log n)^2&lt;/math&gt;.

==== Consistency of uniform KeRF ====
Providing &lt;math&gt;k\rightarrow\infty&lt;/math&gt; and &lt;math&gt;n/2^k\rightarrow\infty&lt;/math&gt;, there exists a constant &lt;math&gt;C&gt;0&lt;/math&gt; such that,
&lt;math&gt;\mathbb{E}[\tilde{m}_n^{uf}(\mathbf{X})-m(\mathbf{X})]^2\le Cn^{-2/(6+3d\log2)}(\log n)^2&lt;/math&gt;.

== Open source implementations ==
* [http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm The Original RF] by Breiman and Cutler written in Fortran 77.
* [http://www.alglib.net/dataanalysis/decisionforest.php ALGLIB] contains a modification of the random forest in C#, C++, Pascal, VBA.
* [http://cran.r-project.org/web/packages/party/index.html party] Implementation based on the conditional inference trees in [[R (programming language)|R]].
* [http://cran.r-project.org/web/packages/randomForest/index.html randomForest] for classification and regression in [[R (programming language)|R]].
* [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Python implementation] with examples in [[scikit-learn]].
* [[Orange (software)|Orange data mining]] suite includes random forest learner and can visualize the trained forest.
* [http://code.google.com/p/randomforest-matlab Matlab] implementation.
* [http://sqp.upf.edu SQP] software uses random forest algorithm to predict the quality of survey questions, depending on formal and linguistic characteristics of the question.
* [http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/RandomForest.html Weka RandomForest] in Java library and GUI.
* [https://github.com/imbs-hl/ranger ranger] A C++ implementation of random forest for classification, regression, probability and survival. Includes interface for [[R (programming language)|R]].

== See also ==
*[[Decision tree learning]]
*[[Gradient boosting]]
*[[Randomized algorithm]]
*[[Ensemble learning]]
*[[Boosting (machine learning)|Boosting]]
*[[Non-parametric statistics]]
*[[Kernel random forest]]

==References==
{{Reflist|30em}}

== External links ==
* [https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm Random Forests classifier description] (Leo Breiman's site)
* [https://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf Liaw, Andy &amp; Wiener, Matthew &quot;Classification and Regression by randomForest&quot; R News (2002) Vol. 2/3 p. 18] (Discussion of the use of the random forest package for [[R programming language|R]])
* {{cite conference |doi = 10.1007/978-3-540-74469-6_35 |chapter = Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB |chapter-url = https://www.researchgate.net/profile/Dirk_Van_den_Poel/publication/225175169_Random_Multiclass_Classification_Generalizing_Random_Forests_to_Random_MNL_and_Random_NB/links/02e7e5278a0a7b8e7f000000.pdf |title = Database and Expert Systems Applications |series = [[Lecture Notes in Computer Science]] |year = 2007 |last1 = Prinzie |first1 = Anita |last2 = Poel |first2 = Dirk |isbn = 978-3-540-74467-2 |volume = 4653 |pages = 349}}






</text>
      <sha1>9nspqqei0ini17zrug4z06g2of92hr8</sha1>
    </revision>
  </page>
  <page>
    <title>Algorithmic bias</title>
    <ns>0</ns>
    <id>55817338</id>
    <revision>
      <id>814704544</id>
      <parentid>814617678</parentid>
      <timestamp>2017-12-10T11:50:08Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Fix [[:Category:Pages using citations with accessdate and no URL]] when permanent identifier present; possible ref cleanup;  on; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="60565">[[File:02-Sandvig-Seeing-the-Sort-2014-WEB.png|thumb|A flow chart showing the decisions made by a recommendation engine, circa 2001.&lt;ref&gt;{{cite patent|country=us|number=7113917}}&lt;/ref&gt;]]
'''Algorithmic bias''' occurs when [[data]] used to teach a [[machine learning]] system reflects implicit values of humans involved in that data collection, selection, or use.&lt;ref name=&quot;NissenbaumEmbody&quot;&gt;{{cite journal|last1=Nissenbaum|first1=Helen|title=How computer systems embody values|journal=Computer|date=March 2001|volume=34|issue=3|pages=120–119|doi=10.1109/2.910905|url=https://www.nyu.edu/projects/nissenbaum/papers/embodyvalues.pdf|accessdate=17 November 2017}}&lt;/ref&gt; Algorithmic bias has been identified and critiqued for its impact on search engine results,&lt;ref name=&quot;IntronaNissenbaum&quot;&gt;{{cite journal|last1=Introna|first1=Lucas|last2=Nissenbaum|first2=Helen|title=Defining the Web: the politics of search engines|journal=Computer|date=2000|volume=33|issue=1|pages=54–62|doi=10.1109/2.816269|url=http://www.nyu.edu/projects/nissenbaum/papers/Defining%20the%20Web.pdf|accessdate=17 November 2017}}&lt;/ref&gt; social media platforms,&lt;ref name=&quot;Crawford&quot;&gt;{{cite journal|last1=Crawford|first1=Kate|title=Can an Algorithm be Agonistic? Ten Scenes from Life in Calculated Publics|journal=Science, Technology, &amp; Human Values|date=24 June 2015|volume=41|issue=1|pages=77–92|doi=10.1177/0162243915589635}}&lt;/ref&gt; privacy,&lt;ref name=&quot;Tufekci&quot;&gt;{{cite journal|last1=Tufekci|first1=Zeynep|title=Algorithmic Harms beyond Facebook and Google: Emergent Challenges of Computational Agency|journal=Colorado Technology Law Journal Symposium Essays|date=2015|volume=13|pages=203–216|url=http://heinonline.org/HOL/LandingPage?handle=hein.journals/jtelhtel13&amp;div=18&amp;id=&amp;page=|accessdate=17 November 2017}}&lt;/ref&gt; and racial profiling.&lt;ref name=&quot;Nakamura1&quot;&gt;{{cite book|last1=Nakamura|first1=Lisa|editor1-last=Magnet|editor1-first=Shoshana|editor2-last=Gates|editor2-first=Kelly|title=The new media of surveillance|date=2009|publisher=Routledge|location=London|isbn=9780415568128|pages=149–162|url=https://lnakamur.files.wordpress.com/2013/04/nakamura-jihad-worlds.pdf|accessdate=17 November 2017}}&lt;/ref&gt; In search results, this bias can create results reflecting racist, sexist, or other [[Bias|social biases]], despite the presumed neutrality of the data.&lt;ref name=&quot;SydellNPR&quot;&gt;{{cite web|last1=Sydell|first1=Laura|title=Can Computers Be Racist? The Human-Like Bias Of Algorithms|url=https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms|website=NPR.org|publisher=National Public Radio / All Things Considered|accessdate=17 November 2017|language=en}}&lt;/ref&gt; The study of algorithmic bias is most concerned with algorithms that reflect &quot;systematic and unfair&quot; discrimination.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}}

As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways unanticipated output and manipulation can impact the physical world.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|2}}&lt;ref name=&quot;Graham&quot; /&gt;{{rp|563}}&lt;ref name=&quot;Tewell&quot; /&gt;{{rp|294}} Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise,&lt;ref name=&quot;Introna2&quot; /&gt;{{rp|15}}&lt;ref name=&quot;ShirkyAuthority&quot; /&gt; and in some cases, reliance on algorithms can displace human responsibility for their outcomes.&lt;ref name=&quot;Introna2&quot; /&gt;{{rp|16}} Nonetheless, bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or through use in unanticipated contexts or by audiences not considered in their initial design.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|334}}

Algorithmic bias has been discovered or theorized in cases ranging from election outcomes&lt;ref name=&quot;Zittrain&quot; /&gt;{{rp|335}} to the spread of online [[hate speech]].&lt;ref name=&quot;AngwinGrassegger&quot; /&gt; Problems in understanding, researching, and discovering algorithmic bias may come from the proprietary nature of algorithms, which are typically treated as trade secrets.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|2}} Even with full transparency, understanding algorithms can be difficult because of their complexity,&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|2}}&lt;ref name=&quot;Sandvig2&quot; /&gt;{{rp|7}} and because not every permutation of an algorithm's input or output can be anticipated or reproduced.&lt;ref name=&quot;Introna1&quot; /&gt;{{rp|118}} In many cases, even within a single use-case (such as a website or app), there is no single &quot;algorithm&quot; to examine, but a vast network of interrelated programs and data inputs,&lt;ref name=&quot;Granka&quot; /&gt;{{rp|367}} even between users of the same service.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|5}}

==Definitions==
[[File:A computer program for evaluating forestry opportunities under three investment criteria (1969) (20385500690).jpg|thumb|A 1969 diagram for how a simple computer program makes decisions, illustrating a very simple algorithm. ]]
[[Algorithms]] are [[Algorithm characterizations|difficult to define]],&lt;ref name=&quot;Striphas&quot;&gt;{{cite web|url=http://culturedigitally.org/2012/02/what-is-an-algorithm/|title=What is an Algorithm? – Culture Digitally|last1=Striphas|first1=Ted|website=culturedigitally.org|accessdate=20 November 2017}}&lt;/ref&gt; but may be generally understood as sets of instructions within [[computer programs]] that determine how these programs read, collect, process, and analyze [[data]] to generate some readable form of analysis or output.&lt;ref name=&quot;Cormen&quot;&gt;{{cite book|title=Introduction to algorithms|last1=Corman, et al.|first1=Thomas H.|date=2009|publisher=MIT Press|isbn=0262033844|edition=3rd|location=Cambridge, Mass.|page=5}}&lt;/ref&gt;{{rp|13}} Newer computers can process millions of these algorithmic instructions per second, which has boosted the design and adoption of technologies such as [[machine learning]] and [[artificial intelligence]].&lt;ref name=&quot;Kitchen&quot;&gt;{{cite journal|last1=Kitchin|first1=Rob|title=Thinking critically about and researching algorithms|journal=Information, Communication &amp; Society|date=25 February 2016|volume=20|issue=1|pages=14–29|doi=10.1080/1369118X.2016.1154087|url=http://futuredata.stanford.edu/classes/cs345s/handouts/kitchin.pdf|accessdate=19 November 2017}}&lt;/ref&gt;{{rp|14–15}} By analyzing and processing data, algorithms drive search engines,&lt;ref name=&quot;GoogleAlgorithms&quot;&gt;{{cite web|author=Google|title=How Google Search Works|url=https://www.google.com/search/howsearchworks/algorithms/|publisher=Google|accessdate=19 November 2017}}&lt;/ref&gt; social media websites,&lt;ref name=&quot;Luckerson&quot;&gt;{{cite web|last1=Luckerson|first1=Victor|title=Here's How Your Facebook News Feed Actually Works|url=http://time.com/collection-post/3950525/facebook-news-feed-algorithm/|website=TIME.com|accessdate=19 November 2017}}&lt;/ref&gt; recommendation engines,&lt;ref name=&quot;Vanderbilt&quot;&gt;{{cite web|last1=Vanderbilt|first1=Tom|title=The Science Behind the Netflix Algorithms That Decide What You’ll Watch Next|url=https://www.wired.com/2013/08/qq_netflix-algorithm/|website=WIRED|accessdate=19 November 2017}}&lt;/ref&gt; online retail,&lt;ref name=&quot;AngwinMattu&quot;&gt;{{cite web|last1=Angwin|first1=Julia|last2=Mattu|first2=Surya|title=Amazon Says It Puts Customers First. But Its Pricing Algorithm Doesn’t — ProPublica|url=https://www.propublica.org/article/amazon-says-it-puts-customers-first-but-its-pricing-algorithm-doesnt|website=ProPublica|accessdate=19 November 2017|language=en-us|date=20 September 2016}}&lt;/ref&gt; online advertising,&lt;ref name=&quot;Livingstone&quot;&gt;{{cite web|last1=Livingstone|first1=Rob|title=The future of online advertising is big data and algorithms|url=http://theconversation.com/the-future-of-online-advertising-is-big-data-and-algorithms-69297|website=The Conversation|accessdate=19 November 2017|language=en}}&lt;/ref&gt; and more.&lt;ref name=&quot;Hickman&quot;&gt;{{cite web|last1=Hickman|first1=Leo|title=How algorithms rule the world|url=https://www.theguardian.com/science/2013/jul/01/how-algorithms-rule-world-nsa|website=The Guardian|accessdate=19 November 2017|date=1 July 2013}}&lt;/ref&gt;

Contemporary [[Social science|social scientists]] are concerned with algorithmic processes embedded into hardware and software applications in order to understand their political effects, and to question the underlying assumptions of their neutrality.&lt;ref name=&quot;Seaver&quot;&gt;{{cite web|url=https://static1.squarespace.com/static/55eb004ee4b0518639d59d9b/t/55ece1bfe4b030b2e8302e1e/1441587647177/seaverMiT8.pdf|title=Knowing Algorithms|last1=Seaver|first1=Nick|publisher=Media in Transition 8, Cambridge, MA, April 2013|accessdate=18 November 2017}}&lt;/ref&gt;{{rp|2}}&lt;ref name=&quot;Graham&quot;&gt;{{cite journal|last1=Graham|first1=Stephen D.N.|title=Software-sorted geographies|journal=Progress in Human Geography|date=July 2016|volume=29|issue=5|pages=562–580|doi=10.1191/0309132505ph568oa}}&lt;/ref&gt;{{rp|563}}&lt;ref name=&quot;Tewell&quot;&gt;{{cite journal|last1=Tewell|first1=Eamon|date=4 April 2016|title=Toward the Resistant Reading of Information: Google, Resistant Spectatorship, and Critical Information Literacy|url=http://muse.jhu.edu/article/613843|journal=Portal: Libraries and the Academy|volume=16|issue=2|pages=289–310|issn=1530-7131|accessdate=19 November 2017|via=}}&lt;/ref&gt;{{rp|294}}&lt;ref&gt;{{Cite web|url=https://hbr.org/2013/04/the-hidden-biases-in-big-data|title=The Hidden Biases in Big Data|last=Crawford|first=Kate|date=1 April 2013|website=Harvard Business Review|archive-url=|archive-date=|dead-url=|access-date=}}&lt;/ref&gt; The term ''algorithmic bias'' is used to describe systematic and repeatable errors that create unfair outcomes, i.e., generating one result for certain users and another result for others. For example, a [[credit score]] algorithm may deny a loan based on certain factors without being unfair if it is consistently weighing relevant financial criteria. If that algorithm allows loans to some, but denies loans to another set of nearly identical users based on arbitrary criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as ''biased''.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}} This bias may be intentional or unintentional.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}}

== Methods ==
Bias can be introduced to an algorithm in several ways. During the assemblage of a [[database]], data must be collected, digitized, adapted, and entered according to human-assisted [[cataloging]] criteria.&lt;ref name=&quot;Gillespie et al&quot;&gt;{{cite book|last1=Gillespie|first1=Tarleton|last2=Boczkowski|first2=Pablo|last3=Foot|first3=Kristin|title=Media Technologies|publisher=MIT Press|location=Cambridge|pages=1–30|url=http://www.tarletongillespie.org/essays/Gillespie%20-%20The%20Relevance%20of%20Algorithms.pdf|accessdate=22 November 2017}}&lt;/ref&gt;{{rp|3}} Next, in the design of the algorithm, programmers assign certain priorities, or [[Hierarchy|hierarchies]], in how programs assess and sort that data. This requires human decisions about how data is categorized and which data is discarded.&lt;ref name=&quot;Gillespie et al&quot; /&gt;{{rp|4}} Some algorithms collect their own data based on human-selected criteria, which can reflect the bias of human users.&lt;ref name=&quot;Gillespie et al&quot; /&gt;{{rp|8}} Others may practice reinforcing stereotypes and preferences as they process and display &quot;relevant&quot; data for human users, as in selecting information based on previous choices of a user, or group of users.&lt;ref name=&quot;Gillespie et al&quot; /&gt;{{rp|6}}

Beyond assembling the data, bias can emerge as a result of design.&lt;ref name=&quot;TowCenter&quot;&gt;{{cite web|last1=Diakopoulas|first1=Nicholas|title=Algorithmic Accountability: On the Investigation of Black Boxes {{!}}|url=https://towcenter.org/research/algorithmic-accountability-on-the-investigation-of-black-boxes-2/|website=towcenter.org|accessdate=19 November 2017|language=en}}&lt;/ref&gt; Examples may arise: In sorting processes that determine the allocation of resources or scrutiny (as in determining school placements), or classification and identification processes that may inadvertently discriminate against a category when assigning risk (as in credit scores).&lt;ref name=&quot;Lipartito&quot;&gt;{{cite journal|last1=Lipartito|first1=Kenneth|title=The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today|journal=SSRN|date=6 January 2011|doi=10.2139/ssrn.1736283|url=http://dx.doi.org/10.2139/ssrn.1736283|accessdate=19 November 2017|publisher=Social Science Research Network}}&lt;/ref&gt;{{rp|36}} In processing associations, such as recommendation engines or inferred marketing traits, algorithms may be flawed in ways that reveal personal information. Inclusion and exclusion criteria may have unanticipated outcomes for search results, such as in flight recommendation software omitting flights that don't follow the sponsoring airline's preferred flight paths.&lt;ref name=&quot;TowCenter&quot; /&gt; Algorithms may also display an ''uncertainty bias'', offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger sample populations, which may not align with data from underrepresented populations.&lt;ref name=&quot;GoodmanFlaxman2016&quot; /&gt;{{rp|4}}

==History==

===Early critiques===
The earliest computer programs reflected simple, human-derived operations, and were deemed to be functioning when they completed those operations. [[Artificial intelligence|Artificial Intelligence]] pioneer [[Joseph Weizenbaum]] wrote that such programs are therefore understood to &quot;embody law&quot;.&lt;ref name=&quot;Weizenbaum1976&quot;&gt;{{cite book|last1=Weizenbaum|first1=Joseph|title=Computer power and human reason : from judgment to calculation|date=1976|publisher=W.H. Freeman|location=San Francisco|isbn=0716704641}}&lt;/ref&gt;{{rp|40}} Weizenbaum describes early, simple computer programs changing perceptions of machines from transferring power to transferring information.&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|41}} However, he noted that machines might transfer information with unintended consequences if there are errors in details provided to the machine, and if users interpret data in intuitive ways that cannot be formally communicated to, or from, a machine.&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|65}} Weizenbaum stated that all data fed to a machine must reflect &quot;human decisionmaking processes&quot; which have been translated into rules for the computer to follow.&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|70}}{{rp|105}} To do this, Weizenbaum asserted that programmers &quot;legislate the laws for a world one first has to create in imagination,&quot;&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|109}} and as a result, computer simulations can be built on models with incomplete or incorrect human data.&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|149}} Weizenbaum compared the results of such decisions to a tourist in a country who can make &quot;correct&quot; decisions through a coin toss, but has no basis of understanding how or why the decision was made.&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|226}}

=== Contemporary critiques ===
The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their impact may be forgotten and taken as natural results of the program's output.&lt;ref name=&quot;Introna1&quot;&gt;{{cite journal|last1=Introna|first1=Lucas D.|date=2 December 2011|title=The Enframing of Code|journal=Theory, Culture &amp; Society|volume=28|issue=6|pages=113–141|doi=10.1177/0263276411418131}}&lt;/ref&gt;{{rp|115}} These biases can create new patterns of behavior, or &quot;scripts,&quot; in relationship to specific technologies as the code [[Cybernetics|interacts]] with other elements of society.&lt;ref name=&quot;Bogost&quot;&gt;{{cite web|url=https://www.theatlantic.com/technology/archive/2015/01/the-cathedral-of-computation/384300/|title=The Cathedral of Computation|last1=Bogost|first1=Ian|website=The Atlantic|accessdate=19 November 2017}}&lt;/ref&gt; Biases may also impact how society shapes itself around the data points that algorithms require.&lt;ref name=&quot;IntronaWood&quot; /&gt;{{rp|180}}

The decisions of algorithmic programs can be weighed more heavily than the decisions of the human beings they are meant to assist,&lt;ref name=&quot;Introna2&quot;&gt;{{cite journal|last1=Introna|first1=Lucas D.|date=21 December 2006|title=Maintaining the reversibility of foldings: Making the ethics (politics) of information technology visible|journal=Ethics and Information Technology|volume=9|issue=1|pages=11–25|doi=10.1007/s10676-006-9133-z}}&lt;/ref&gt;{{rp|15}} a process described by author [[Clay Shirky]] as &quot;algorithmic authority&quot;.&lt;ref name=&quot;ShirkyAuthority&quot;&gt;{{cite web|url=http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/|title=A Speculative Post on the Idea of Algorithmic Authority Clay Shirky|last1=Shirky|first1=Clay|website=www.shirky.com|accessdate=20 November 2017}}&lt;/ref&gt; Shirky uses the term to describe &quot;the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,&quot; such as search results.&lt;ref name=&quot;ShirkyAuthority&quot; /&gt; This neutrality can also be misrepresented through language frames used when results are presented to the public. For example, a list of news items selected and presented as &quot;trending&quot; or &quot;popular&quot; may be weighed based on significantly wider criteria than their popularity.&lt;ref name=&quot;Gillespie et al&quot; /&gt;{{rp|14}}

Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility in decision making away from humans.&lt;ref name=&quot;Introna2&quot; /&gt;{{rp|16}}&lt;ref name=&quot;Ziewitz1&quot;&gt;{{cite journal|last1=Ziewitz|first1=Malte|title=Governing Algorithms: Myth, Mess, and Methods|journal=Science, Technology, &amp; Human Values|date=1 January 2016|volume=41|issue=1|pages=3–16|doi=10.1177/0162243915608948|url=http://sth.sagepub.com/content/early/2015/09/30/0162243915608948.abstract|accessdate=22 November 2017|language=en|issn=0162-2439}}&lt;/ref&gt;{{rp|6}} This can have the effect of reducing alternative options, compromises, or flexibility.&lt;ref name=&quot;Introna2&quot; /&gt;{{rp|16}} Sociologist [[Scott Lash]] has critiqued algorithms as a new form of &quot;generative power&quot; in that they are a virtual means of generating actual ends.&lt;ref name=&quot;Lash&quot;&gt;{{cite journal|last1=Lash|first1=Scott|date=30 June 2016|title=Power after Hegemony|journal=Theory, Culture &amp; Society|volume=24|issue=3|pages=55–78|doi=10.1177/0263276407075956}}&lt;/ref&gt;{{rp|71}}

==Types==

===Pre-existing===
[[File:Used Punchcard (5151286161).jpg|thumb|This card was used to load software into an old mainframe computer. Each byte (the letter 'A', for example) is entered by punching holes. Though contemporary computers are more complex, they reflect this human decision-making process in collecting and processing data.&lt;ref name=&quot;Weizenbaum1976&quot; /&gt;{{rp|70}}&lt;ref name=&quot;Goffrey&quot;/&gt;{{rp|16}}]]
Pre-existing bias in an algorithm is a consequence of underlying social and institutional [[Ideology|ideologies]]. Such ideas may reflect personal biases of individual designers or programmers, or can reflect social, institutional, or cultural assumptions. In both cases, such prejudices can be explicit and conscious, or implicit and unconscious.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|334}}&lt;ref name=&quot;Tewell&quot; /&gt;{{rp|294}} Poorly selected input data will influence the outcomes created by machines.&lt;ref name=&quot;Goffrey&quot;&gt;{{cite book|last1=Goffrey|first1=Andrew|editor1-last=Fuller|editor1-first=Matthew|title=Software studies: a lexicon|date=2008|publisher=MIT Press|location=Cambridge, Mass.|isbn=9781435647879|pages=15–20|chapter=Algorithm}}&lt;/ref&gt;{{rp|17}} In a critical view, encoding pre-existing bias into software can preserve social and institutional bias, and replicate it into all possible uses of the algorithm into the future.&lt;ref name=&quot;Introna1&quot; /&gt;{{rp|116}}&lt;ref name=&quot;Ziewitz1&quot; /&gt;{{rp|8}}

An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 [[British Nationality Act]].&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|341}} The program accurately reflected the tenets of the law, which stated that &quot;a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.&quot;&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|341}}&lt;ref name=&quot;SergotEtAl&quot;&gt;{{cite journal|last1=Sergot|first1=MJ|last2=Sadri|first2=F|last3=Kowalski|first3=RA|last4=Kriwaczek|first4=F|last5=Hammond|first5=P|last6=Cory|first6=HT|title=The British Nationality Act as a Logic Program|journal=Communications of the ACM|date=May 1986|volume=29|issue=5|pages=370–386|url=https://web.stanford.edu/class/cs227/Readings/BritishNationalityAct.pdf|accessdate=18 November 2017}}&lt;/ref&gt;{{rp|375}} By attempting to appropriately articulate this logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|342}}

===Technical===
Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}} Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|336}} Flaws in [[random number generation]] can also introduce bias into results.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}}

A ''decontextualized algorithm'' uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}} The opposite may also apply, in which results are evaluated in different contexts from which they are collected. For example, data may be collected without crucial external context when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's [[Visual field|field of vision]].&lt;ref name=&quot;Graham&quot; /&gt;{{rp|574}}

Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior will correlate. For example, software that weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury, is displaying a form of technical bias.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|332}}

===Emergent===
Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|334}} New forms of knowledge, such as drug or medical breakthroughs, new laws, business models, or shifting cultural norms, may be discovered without algorithms being adjusted to consider them.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|334,336}} This may exclude groups through technology, without delineating clear outlines of authorship or personal responsibility.&lt;ref name=&quot;IntronaWood&quot; /&gt;{{rp|179}}&lt;ref name=&quot;Tewell&quot; /&gt;{{rp|294}} Similarly, problems may emerge when [[training data]], i.e., the samples &quot;fed&quot; to a machine by which it models certain conclusions, do not align with uses that algorithm encounters in the real world.&lt;ref name=&quot;Gillespie&quot;&gt;{{cite web|url=http://culturedigitally.org/2014/06/algorithm-draft-digitalkeyword/|title=Algorithm [draft] [#digitalkeywords] – Culture Digitally|last1=Gillespie|first1=Tarleton|website=culturedigitally.org|accessdate=20 November 2017}}&lt;/ref&gt;

In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|338}} The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process calls for each applicant to provide a list of preferences for placement across the US, which is then sorted and assigned when a hospital and an applicant both agree to a match. In the case of married couples where both sought residencies, the algorithm weighed a &quot;lead member's&quot; location choices first. Once it identified an optimum placement for that person, it removed distant locations from their partner's preferences, reducing their list to the preferred locations within the same city as the partner. The result was a frequent assignment of high-rated schools for the first partner and lower-preference schools to the second partner, rather than sorting for compromises in placement preference.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|338}}&lt;ref name=&quot;Roth&quot;&gt;{{cite journal|last1=Roth|first1=A. E. 1524 –1528.|title=New physicians: A natural experiment in market organization.|journal=Science|date=14 December 1990|volume=250|pages=1524–1528|url=https://stanford.edu/~alroth/science.html|accessdate=18 November 2017}}&lt;/ref&gt;

Additional emergent biases include:

====Correlations====
Unpredictable correlations can emerge when large data sets are compared to each other in practice. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By &quot;discrimination&quot; against certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or orientation data.&lt;ref name=&quot;GoodmanFlaxman2016&quot; /&gt;{{rp|6}} In other cases, correlations can be inferred for reasons beyond the algorithm's ability to understand them, as when a triage program gave lower priority to asthmatics who had pneumonia. Because asthmatics with pneumonia were at the highest risk, hospitals typically give them the best and most immediate care; the algorithm simply compared survival rates.&lt;ref name=&quot;Kuang&quot;&gt;{{cite news|last1=Kuang|first1=Cliff|title=Can A.I. Be Taught to Explain Itself?|url=https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html|accessdate=26 November 2017|work=The New York Times|date=21 November 2017}}&lt;/ref&gt;

====Unanticipated uses====
Emergent bias can occur when an algorithm is used by unanticipated audiences, such as machines that demand users can read, write, or understand numbers. Certain metaphors may not carry across different populations or skill sets.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|334}} For example, the British National Act Program was created as a [[Proof of concept|proof-of-concept]] by computer scientists and immigration lawyers to evaluate suitability for [[British nationality law|British citizenship]]. The designers therefore have expertise beyond the user, whose understanding of both software and immigration law would likely be unsophisticated. The agents administering the questions would not be aware of alternative pathways to citizenship outside of the software, and shifting case law and legal interpretations would lead the algorithm to outdated results.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|342}}

An area of concern around emergent bias is that it may be compounded as biased technology is more deeply integrated into society. For example, users with vision impairments may not be able to use an [[Automated teller machine|ATM]], but can easily go to a [[Branch (banking)|bank branch]]. If bank branches begin to close because ATMs replace them, they begin to exclude vision-impaired users from banking, an unintended consequence of a technology.&lt;ref name=&quot;IntronaWood&quot;&gt;{{cite journal|last1=Introna|first1=Lucas|last2=Wood|first2=David|title=Picturing algorithmic surveillance: the politics of facial recognition systems|journal=Surveillance &amp; Society|date=2004|volume=2|pages=177–198|url=http://nbn-resolving.de/urn:nbn:de:0168-ssoar-200675|accessdate=19 November 2017}}&lt;/ref&gt;{{rp|179}}

====Feedback loops====
Emergent bias may also create a [[feedback loop]], or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm.&lt;ref name=&quot;JouvenalPredPol&quot;&gt;{{cite web|last1=Jouvenal|first1=Justin|title=Police are using software to predict crime. Is it a ‘holy grail’ or biased against minorities?|url=https://www.washingtonpost.com/local/public-safety/police-are-using-software-to-predict-crime-is-it-a-holy-grail-or-biased-against-minorities/2016/11/17/525a6649-0472-440a-aae1-b283aa8e5de8_story.html|website=Washington Post|accessdate=25 November 2017|date=17 November 2016}}&lt;/ref&gt;&lt;ref name=&quot;Chamma&quot;&gt;{{cite web|last1=Chamma|first1=Maurice|title=Policing the Future|url=https://www.themarshallproject.org/2016/02/03/policing-the-future?ref=hp-2-111#.UyhBLnmlj|website=The Marshall Project|accessdate=25 November 2017}}&lt;/ref&gt; For example, simulations of the [[predictive policing]] software, PredPol, deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public.&lt;ref name=&quot;LumIsaac&quot;&gt;{{cite journal|last1=Lum|first1=Kristian|last2=Isaac|first2=William|title=To predict and serve?|journal=Significance|date=October 2016|volume=13|issue=5|pages=14–19|doi=10.1111/j.1740-9713.2016.00960.x|url=http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x/full|accessdate=25 November 2017}}&lt;/ref&gt; The simulation showed that public reports of crime could rise based on the sight of increased police activity, and could be interpreted by the software in modeling predictions of crime, and to encourage a further increase in police presence within the same neighborhoods.&lt;ref name=&quot;JouvenalPredPol&quot; /&gt;&lt;ref name=&quot;SmithPredPol&quot;&gt;{{cite web|last1=Smith|first1=Jack|title=Predictive policing only amplifies racial bias, study shows|url=https://mic.com/articles/156286/crime-prediction-tool-pred-pol-only-amplifies-racially-biased-policing-study-shows|website=Mic|accessdate=25 November 2017|language=en}}&lt;/ref&gt;&lt;ref name=&quot;LumIsaacFAQ&quot;&gt;{{cite web|last1=Lum|first1=Kristian|last2=Isaac|first2=William|title=FAQs on Predictive Policing and Bias|url=https://hrdag.org/2016/11/04/faqs-predpol/|website=HRDAG|accessdate=25 November 2017|language=en|date=1 October 2016}}&lt;/ref&gt; The [[Human Rights Data Analysis Group]], which conducted the study, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.&lt;ref name=&quot;Chamma&quot; /&gt;

==Examples==
[[File:Three Surveillance cameras.jpg|thumb|Facial recognition software used in conjunction with surveillance cameras was found to display bias in recognizing Asian and black faces over white faces.&lt;ref name=&quot;IntronaWood&quot; /&gt;{{rp|191}}]]
===College admissions===
An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to [[St George's, University of London|St. George's Hospital Medical School]] per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with &quot;foreign-sounding names&quot; based on historical trends in admissions.&lt;ref name=&quot;LowryMacpherson&quot;&gt;{{cite journal|last1=Lowry|first1=Stella|last2=Macpherson|first2=Gordon|title=A Blot on the Profession|journal=British Medical Journal|date=5 March 1988|volume=296|issue=6623|page=657|url=http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2545288&amp;blobtype=pdf|accessdate=17 November 2017}}&lt;/ref&gt; Other examples include the display of higher-paying jobs to male applicants on job search websites.&lt;ref name=&quot;SimoniteMIT&quot;&gt;{{cite web|last1=Simonite|first1=Tom|title=Study Suggests Google’s Ad-Targeting System May Discriminate|url=https://www.technologyreview.com/s/539021/probing-the-dark-side-of-googles-ad-targeting-system/|website=MIT Technology Review|publisher=Massachusetts Institute of Technology|accessdate=17 November 2017|language=en}}&lt;/ref&gt;

===Plagiarism detection===
The plagiarism-detection software [[Turnitin]] compares student-written texts to information found online and returns a probability that the student's work is copied. Because the software compares strings of text, it is more likely to identify non-native speakers of English than native speakers, who might be better able to adapt individual words and break up strings of plagiarized text, or obscure them through synonyms.&lt;ref name=&quot;Introna2&quot; /&gt;{{rp|21–22}}

===Facial recognition===
Surveillance camera software may be considered inherently political as it requires algorithms to identify and flag normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.&lt;ref name=&quot;Graham&quot; /&gt;{{rp|572}} A 2002 analysis of facial recognition software used to identify individuals in [[CCTV]] images found several examples of bias. Software was assessed as identifying men more frequently than women, older people more frequently than younger people, and identified Asians, African-Americans and other races more often than whites.&lt;ref name=&quot;IntronaWood&quot; /&gt;{{rp|191}}&lt;ref name=&quot;Furl2002&quot;&gt;{{cite journal|last1=Furl|first1=N|title=Face recognition algorithms and the other-race effect: computational mechanisms for a developmental contact hypothesis|journal=Cognitive Science|date=December 2002|volume=26|issue=6|pages=797–815|doi=10.1016/S0364-0213(02)00084-8}}&lt;/ref&gt;

===Social media===
In 2017 a [[Facebook]] algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents.&lt;ref name=&quot;AngwinGrassegger&quot;&gt;{{cite web|last1=Angwin|first1=Julia|last2=Grassegger|first2=Hannes|title=Facebook’s Secret Censorship Rules Protect White Men From Hate Speech But Not Black Children — ProPublica|url=https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms|website=ProPublica|accessdate=20 November 2017|language=en-us|date=28 June 2017}}&lt;/ref&gt; The algorithm, which is a blend of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing &quot;Muslims&quot; would be blocked, while posts denouncing &quot;Radical Muslims&quot; would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the &quot;children&quot; subset of blacks, rather than &quot;all blacks,&quot; whereas &quot;all white men&quot; would trigger a block, because whites and males are not considered subsets.&lt;ref name=&quot;AngwinGrassegger&quot; /&gt; Facebook was also found to allow ad purchasers to target &quot;Jew haters&quot; as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.&lt;ref name=&quot;AngwinVarnerTobin&quot;&gt;{{cite news|last1=Angwin|first1=Julia|last2=Varner|first2=Madeleine|last3=Tobin|first3=Ariana|title=Facebook Enabled Advertisers to Reach ‘Jew Haters’ — ProPublica|url=https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters|accessdate=20 November 2017|work=ProPublica|date=14 September 2017|language=en-us}}&lt;/ref&gt;

==Impact==

===Commercial influences===
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.&lt;ref name=&quot;Sandvig1&quot; /&gt;{{rp|2}}&lt;ref name=&quot;FriedmanNissenbaum&quot;&gt;{{cite journal|last1=Friedman|first1=Batya|last2=Nissenbaum|first2=Helen|title=Bias in Computer Systems|journal=ACM Transactions on Information Systems|date=July 1996|volume=14|issue=3|pages=330–347|url=http://www.nyu.edu/projects/nissenbaum/papers/biasincomputers.pdf|accessdate=18 November 2017}}&lt;/ref&gt;{{rp|331}}

In a 1998 paper describing [[Google]], the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that “advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.”&lt;ref name=&quot;BrinPage98&quot;&gt;{{cite web|last1=Brin|first1=Sergey|last2=Page|first2=Lawrence|title=The Anatomy of a Search Engine|url=http://www7.scu.edu.au/1921/com1921.htm|website=www7.scu.edu.au|accessdate=18 November 2017}}&lt;/ref&gt; This bias would be an &quot;invisible&quot; manipulation of the user.&lt;ref name=&quot;Sandvig1&quot;&gt;{{cite journal|last1=Sandvig|first1=Christian|last2=Hamilton|first2=Kevin|last3=Karahalios|first3=Karrie|last4=Langbort|first4=Cedric|title=Auditing Algorithms: Research Methods for Detecting Discrimination on Internet Platforms|journal=64th Annual Meeting of the International Communication Association|date=22 May 2014|url=http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf|accessdate=18 November 2017}}&lt;/ref&gt;{{rp|3}}

===Voting behavior===
A series of studies of undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have &quot;no means of competing&quot; if an algorithm, with or without intent, boosted page listings for a rival candidate.&lt;ref name=&quot;Epstein&quot;&gt;{{cite journal|last1=Epstein|first1=Robert|last2=Robertson|first2=Ronald E.|title=The search engine manipulation effect (SEME) and its possible impact on the outcomes of elections|journal=Proceedings of the National Academy of Sciences|date=18 August 2015|volume=112|issue=33|pages=E4512–E4521|doi=10.1073/pnas.1419828112|url=http://www.pnas.org/content/112/33/E4512.full?sid=0176389d-5a8b-4955-8a8e-43d25e0332b0|accessdate=19 November 2017}}&lt;/ref&gt;

Facebook users who saw messages related to voting were more likely to vote themselves. A randomized trial of Facebook users showing an increased effect of 340,000 votes among users, and friends of users, who saw pro-voting messages in 2010.&lt;ref name=&quot;Bond-etal&quot;&gt;{{cite journal|last1=Bond|first1=Robert M.|last2=Fariss|first2=Christopher J.|last3=Jones|first3=Jason J.|last4=Kramer|first4=Adam D. I.|last5=Marlow|first5=Cameron|last6=Settle|first6=Jaime E.|last7=Fowler|first7=James H.|title=A 61-million-person experiment in social influence and political mobilization|journal=Nature|date=13 September 2012|volume=489|issue=7415|doi=10.1038/nature11421|url=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3834737/|accessdate=19 November 2017|issn=0028-0836}}&lt;/ref&gt; The legal scholar Jonathan Zittrain has warned that this could create a &quot;digital gerrymandering&quot; effect in elections, &quot;the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users,&quot; if intentionally manipulated.&lt;ref name=&quot;Zittrain&quot;&gt;{{cite journal|last1=Zittrain|first1=Jonathan|title=Engineering an Election|journal=Harvard Law Review Forum|date=2014|volume=127|pages=335–341|url=http://cdn.harvardlawreview.org/wp-content/uploads/2014/06/vol127_Symposium_Zittrain.pdf|accessdate=19 November 2017}}&lt;/ref&gt;{{rp|335}}

===Gender discrimination===
In 2016, the professional networking site [[LinkedIn]] was discovered to recommend male variations of women's names in response to search queries for women. The site did not make similar recommendations in searches for male names. For example, &quot;Andrea&quot; would bring up a prompt asking if users meant &quot;Andrew,&quot; but queries for &quot;Andrew&quot; did not ask if users meant to find &quot;Andrea&quot;. The company said this was the result of an analysis of users' interactions with the site.&lt;ref name=&quot;Day&quot;&gt;{{cite web|last1=Day|first1=Matt|title=How LinkedIn’s search engine may reflect a gender bias|url=https://www.seattletimes.com/business/microsoft/how-linkedins-search-engine-may-reflect-a-bias/|website=The Seattle Times|accessdate=25 November 2017|date=31 August 2016}}&lt;/ref&gt;

In 2012, the department store franchise [[Target (company)|Target]] was cited for gathering data points to infer when women customers were pregnant, even if they hadn't announced it, and then sharing that information with marketing partners.&lt;ref name=&quot;CrawfordSchultz&quot;&gt;{{cite journal|last1=Crawford|first1=Kate|last2=Schultz|first2=Jason|title=Big Data and Due Process: Toward a Framework to Redress Predictive Privacy Harms|journal=Boston College Law Review|date=2014|volume=55|issue=1|pages=93–128|url=http://lawdigitalcommons.bc.edu/bclr/vol55/iss1/4/|accessdate=18 November 2017|language=en}}&lt;/ref&gt;{{rp|94}}&lt;ref name=&quot;Duhigg&quot;&gt;{{cite web|last1=Duhigg|first1=Charles|title=How Companies Learn Your Secrets|url=http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html|website=The New York Times|accessdate=18 November 2017|date=16 February 2012}}&lt;/ref&gt; Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.&lt;ref name=&quot;CrawfordSchultz&quot; /&gt;{{rp|98}}

Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, &quot;lesbian&quot;. This bias extends to the search engine surfacing popular but sexualized content in neutral searches, as in &quot;Top 25 Sexiest Women Athletes&quot; articles displayed as first-page results in searches for &quot;women athletes&quot;.&lt;ref name=&quot;Noble&quot;&gt;{{cite journal|last1=Noble|first1=Safiya|title=Missed Connections: What Search Engines Say about Women|journal=Bitch Magazine|date=2012|volume=12|issue=4|pages=37–41|url=https://safiyaunoble.files.wordpress.com/2012/03/54_search_engines.pdf|accessdate=19 November 2017}}&lt;/ref&gt;{{rp|31}} In 2017 Google announced plans to curb search results that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.&lt;ref name=&quot;Guynn2&quot;&gt;{{cite news|last1=Guynn|first1=Jessica|title=Google starts flagging offensive content in search results|url=https://www.usatoday.com/story/tech/news/2017/03/16/google-flags-offensive-content-search-results/99235548/|accessdate=19 November 2017|work=USA TODAY|agency=USA Today|date=16 March 2017|language=en}}&lt;/ref&gt;

===Racial discrimination===
Algorithms have been criticized as a method for obscuring racial prejudices in decision-making.&lt;ref name=&quot;Nakamura1&quot; /&gt;{{rp|158}} [[Lisa Nakamura]] has noted that [[Census|census machines]] were among the first to adopt the [[Punched card|punch-card]] processes that lead to contemporary computing, and that their use as categorization and sorting machines for race has been long established and socially tolerated.&lt;ref name=&quot;Nakamura1&quot; /&gt;{{rp|158}}

One example is the use of [[risk assessment]]s in [[Criminal sentencing in the United States|criminal sentencing]] and [[Parole board|parole hearings]], an algorithmically generated score intended to reflect the risk that a suspect or prisoner will repeat a crime.&lt;ref name=&quot;ProPublica&quot; /&gt; From 1920 until 1970, the nationality of a suspect's father was a consideration in such risk assessments.&lt;ref name=&quot;Harcourt&quot;&gt;{{cite journal|last1=Harcourt|first1=Bernard|title=Risk as a Proxy for Race|journal=Criminology and Public Policy, Forthcoming|date=16 September 2010|url=https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654|accessdate=18 November 2017|publisher=Social Science Research Network}}&lt;/ref&gt;{{rp|4}} Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by [[ProPublica]] found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.&lt;ref name=&quot;ProPublica&quot;&gt;{{cite web|last1=Angwin|first1=Julia|last2=Larson|first2=Jeff|last3=Mattu|first3=Surya|last4=Kirchner|first4=Lauren|title=Machine Bias — ProPublica|url=https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing|website=ProPublica|accessdate=18 November 2017|language=en-us|date=23 May 2016}}&lt;/ref&gt;

In 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as [[Ethnic stereotype|gorillas]].&lt;ref name=&quot;Guynn&quot;&gt;{{cite news|last1=Guynn|first1=Jessica|title=Google Photos labeled black people 'gorillas'|url=https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/|accessdate=18 November 2017|work=USA TODAY|agency=USA Today|publisher=USA Today|date=1 July 2015|language=en}}&lt;/ref&gt; In 2010, [[Nikon]] cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking.&lt;ref name=&quot;Rose&quot;&gt;{{cite web|last1=Rose|first1=Adam|title=Are Face-Detection Cameras Racist?|url=http://content.time.com/time/business/article/0,8599,1954643,00.html|website=Time|accessdate=18 November 2017|date=22 January 2010}}&lt;/ref&gt; Such examples are the product of bias in [[biometric data]] sets.&lt;ref name=&quot;Guynn&quot; /&gt; Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.&lt;ref name=&quot;Nakamura1&quot; /&gt;{{rp|154}}

Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of any police record of that individual's name.&lt;ref name=&quot;Sweeney&quot;&gt;{{cite journal|last1=Sweeney|first1=Latanya|title=Discrimination in Online Ad Delivery|journal=SSRI|date=28 January 2013|doi=10.2139/ssrn.2208240|url=http://dx.doi.org/10.2139/ssrn.2208240|accessdate=18 November 2017|publisher=Social Science Research Network}}&lt;/ref&gt;

===Sexual discrimination===
In 2011, users of the gay hookup app [[Grindr]] reported that the app was linked to sex-offender lookup apps in the [[Google Play|Android app store]]'s recommendation algorithm. Writer Mike Ananny criticized this association in [[The Atlantic]], arguing that such associations further stigmatized [[History of gay men in the United States|gay men]] and may discourage closeted men to maintain secrecy.&lt;ref name=&quot;Ananny&quot;&gt;{{cite web|last1=Ananny|first1=Mike|title=The Curious Connection Between Apps for Gay Men and Sex Offenders|url=https://www.theatlantic.com/technology/archive/2011/04/the-curious-connection-between-apps-for-gay-men-and-sex-offenders/237340/|website=The Atlantic|accessdate=18 November 2017}}&lt;/ref&gt; A 2009 incident with online retailer [[Amazon (company)|Amazon]] saw 57,000 books de-listed after a shift in the algorithm expanded its &quot;adult content&quot; blacklist for pornographic works to any books addressing sexuality or gay themes, for example, the critically acclaimed novel ''[[Brokeback Mountain]]''.&lt;ref name=&quot;Kafka2&quot;&gt;{{cite web|last1=Kafka|first1=Peter|title=Did Amazon Really Fail This Weekend? The Twittersphere Says “Yes,” Online Retailer Says “Glitch.”|url=http://allthingsd.com/20090412/did-amazon-really-fail-this-weekend-the-twittersphere-says-yes/|website=AllThingsD|accessdate=22 November 2017|language=en-us}}&lt;/ref&gt;&lt;ref name=&quot;Gillespie et al&quot; /&gt;{{rp|5}}&lt;ref name=&quot;Kafka&quot;&gt;{{cite web|last1=Kafka|first1=Peter|title=Amazon Apologizes for “Ham-fisted Cataloging Error”|url=http://allthingsd.com/20090413/amazon-apologizes-for-ham-fisted-cataloging-error/|website=AllThingsD|publisher=AllThingsD|accessdate=22 November 2017|language=en-us}}&lt;/ref&gt;

==Challenges==

Several challenges impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|5}}

===Lack of transparency===
Commercial algorithms are proprietary, and may be treated as [[trade secrets]].&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|2}}&lt;ref name=&quot;Sandvig2&quot;&gt;{{cite journal|last1=Sandvig|first1=Christian|last2=Hamilton|first2=Kevin|last3=Karahalios|first3=Karrie|last4=Langbort|first4=Cedric|editor1-last=Gangadharan|editor1-first=Seeta Pena|editor2-last=Eubanks|editor2-first=Virginia|editor3-last=Barocas|editor3-first=Solon|title=An Algorithm Audit|journal=DATA AND DISCRIMINATION: COLLECTED ESSAYS|date=2014|url=http://www-personal.umich.edu/~csandvig/research/An%20Algorithm%20Audit.pdf|publisher=Open Technology Institute}}&lt;/ref&gt;{{rp|7}}&lt;ref name=&quot;IntronaWood&quot; /&gt;{{rp|183}} This protects companies, such as a [[Web search engine|search engine]], in cases where a transparent algorithm for ranking results would reveal techniques for manipulating the service.&lt;ref name=&quot;Granka&quot;&gt;{{cite journal|last1=Granka|first1=Laura A.|title=The Politics of Search: A Decade Retrospective|journal=The Information Society|date=27 September 2010|volume=26|issue=5|pages=364–374|doi=10.1080/01972243.2010.511560|url=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36914.pdf|accessdate=18 November 2017}}&lt;/ref&gt;{{rp|366}} This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.&lt;ref name=&quot;Kitchin&quot;&gt;{{cite journal|last1=Kitchin|first1=Rob|title=Thinking critically about and researching algorithms|journal=Information, Communication &amp; Society|date=25 February 2016|volume=20|issue=1|pages=14–29|doi=10.1080/1369118X.2016.1154087}}&lt;/ref&gt;{{rp|20}} It can also be used to obscure possible unethical methods used in producing or processing algorithmic output.&lt;ref name=&quot;Granka&quot; /&gt;{{rp|369}} The closed nature of the code is not the only concern, however; as a certain degree of obscurity is protected by the complexity of contemporary programs, and the inability to know every permutations of a code's input or output.&lt;ref name=&quot;IntronaWood&quot; /&gt;{{rp|183}}

Social scientist [[Bruno Latour]] has identified this process as [[blackboxing]], a process in which &quot;scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.&quot;&lt;ref&gt;{{Cite book|author=Bruno Latour|title=Pandora's hope: essays on the reality of science studies|publisher=[[Harvard University Press]]|location=[[Cambridge, Massachusetts]]|year=1999}}&lt;/ref&gt; Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.&lt;ref name=&quot;KubitschkoKaun&quot;&gt;{{cite book|last1=Kubitschko|first1=Sebastian|last2=Kaun|first2=Anne|title=Innovative Methods in Media and Communication Research|date=2016|publisher=Springer|isbn=9783319407005|url=https://books.google.com/books?id=ZdzMDQAAQBAJ|accessdate=19 November 2017}}&lt;/ref&gt;{{rp|92}}

===Complexity===
Algorithmic processes are [[Complex system|complex]], often exceeding the understanding of the people who use them.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|2}}&lt;ref name=&quot;Sandvig2&quot; /&gt;{{rp|7}} Large-scale operations may not be understood even by those involved in creating them.&lt;ref name=&quot;LaFrance&quot;&gt;{{cite web|last1=LaFrance|first1=Adrienne|title=The Algorithms That Power the Web Are Only Getting More Mysterious|url=https://www.theatlantic.com/technology/archive/2015/09/not-even-the-people-who-write-algorithms-really-know-how-they-work/406099/|website=The Atlantic|accessdate=19 November 2017}}&lt;/ref&gt; The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013.&lt;ref name=&quot;McGee&quot;&gt;{{cite web|last1=McGee|first1=Matt|title=EdgeRank Is Dead: Facebook's News Feed Algorithm Now Has Close To 100K Weight Factors|url=https://marketingland.com/edgerank-is-dead-facebooks-news-feed-algorithm-now-has-close-to-100k-weight-factors-55908|website=Marketing Land|accessdate=18 November 2017|date=16 August 2013}}&lt;/ref&gt; Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions with nested sections of sprawling algorithmic processes.&lt;ref name=&quot;Introna1&quot; /&gt;{{rp|118}} Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.&lt;ref name=&quot;Kitchin&quot; /&gt;{{rp|22}}

=== Lack of data about sensitive categories ===
A significant barrier to understanding tackling bias in practice is that categories, such as demographics of individuals protected by [[anti-discrimination law]], are often not explicitly held by those collecting and processing data.&lt;ref&gt;{{Cite journal|last=Veale|first=Michael|last2=Binns|first2=Reuben|date=2017|title=Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data|url=http://journals.sagepub.com/doi/full/10.1177/2053951717743530|journal=Big Data &amp; Society|volume=4|issue=2|pages=|doi=10.1177/2053951717743530|ssrn=3060763|url-access=|via=|doi-access=free}}&lt;/ref&gt; In some cases, there is little opportunity to collect this data explicitly, such as in [[device fingerprint]]ing, [[ubiquitous computing]] and the [[Internet of things|Internet of Things]]. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the [[General Data Protection Regulation]], such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.

Algorithmic bias does not only relate to protected categories, but can also concern something less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial [[ground truth]], and 'debiasing' such a system becomes considerably more tricky.&lt;ref&gt;{{Cite journal|last=Binns|first=Reuben|last2=Veale|first2=Michael|last3=Kleek|first3=Max Van|last4=Shadbolt|first4=Nigel|date=2017-09-13|orig-year=|title=Like Trainer, Like Bot? Inheritance of Bias in Algorithmic Content Moderation|url=https://link.springer.com/chapter/10.1007/978-3-319-67256-4_32|journal=Social Informatics|series=Lecture Notes in Computer Science|language=en|publisher=Springer, Cham|volume=|pages=405–415|arxiv=1707.01477|doi=10.1007/978-3-319-67256-4_32|isbn=9783319672557|via=}}&lt;/ref&gt;

Furthermore, false and accidental [[correlations]] can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap with residential clusters of ethnic minorities.&lt;ref name=&quot;Claburn&quot;&gt;{{cite web|last1=Claburn|first1=Thomas|title=EU Data Protection Law May End The Unknowable Algorithm – InformationWeek|url=https://www.informationweek.com/government/big-data-analytics/eu-data-protection-law-may-end-the-unknowable-algorithm/d/d-id/1326294?|website=InformationWeek|accessdate=25 November 2017|language=en}}&lt;/ref&gt;

===Rapid pace of change===
Personalization of algorithms based on user interactions such as clicks, time on site, and other metrics, can confuse attempts to understand them.&lt;ref name=&quot;Granka&quot; /&gt;{{rp|367}}&lt;ref name=&quot;Sandvig2&quot; /&gt;{{rp|7}} One unidentified streaming radio service reported it had five unique music-selection algorithms it selected for its users based on behavior. This creates widely disparate experiences of the same streaming product between different users.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|5}}
Companies also run frequent [[A/B tests]] to fine-tune algorithms based on user response. For example, the search engine [[Bing (search engine)|Bing]] can run up to ten million subtle variations of its service per day, segmenting the experience of an algorithm between users, or among the same users.&lt;ref name=&quot;Seaver&quot; /&gt;{{rp|5}}

===Rapid pace of dissemination===
Computer programs and systems can quickly spread among users, embedding biased algorithms into broader society before their impact can be recognized or remedied.&lt;ref name=&quot;FriedmanNissenbaum&quot; /&gt;{{rp|331}}

== Regulation ==

=== Europe ===
On April 14, 2016, the [[Parliament of the European Union]] passed the General Data Protection Regulation (GDPR). Within the GDRP, Article 22 addresses &quot;Automated individual decision-making&quot; processes, regulating [[Machine learning|machine-learning]] and other algorithms related to the information of persons in the EU.&lt;ref name=&quot;GoodmanFlaxman2016&quot;&gt;{{cite journal|last1=Goodman|first1=Bryce|last2=Flaxman|first2=Seth|date=2016|title=EU regulations on algorithmic decision-making and a “right to explanation”|url=https://arxiv.org/abs/1606.08813|journal=arXiv:1606.08813 [stat.ML]|volume=|pages=|doi=|accessdate=25 November 2017|via=}}&lt;/ref&gt;{{rp|1}}{{rp|2}} The regulations call for a ban on, among other topics, any &quot;decision based solely on automated processing, including profiling” that may harm an individual through use of their data.&lt;ref name=&quot;MetzWIRED&quot;&gt;{{cite web|last1=Metz|first1=Cade|title=Artificial Intelligence Is Setting Up the Internet for a Huge Clash With Europe|url=https://www.wired.com/2016/07/artificial-intelligence-setting-internet-huge-clash-europe/|website=WIRED|accessdate=25 November 2017}}&lt;/ref&gt; Violators of the regulations can be fined the greater of 20 million euro or 4% of the offending corporation's global revenue.&lt;ref name=&quot;GoodmanFlaxman2016&quot; /&gt;{{rp|2}} Another EU regulation, the [[Data Protection Directive]], requires corporations to disclose the logic of automated decisions to affected parties.&lt;ref name=&quot;Claburn&quot; /&gt; The GDPR expands protections through Article 13 and 14 which calls for the right to “meaningful information about the logic involved&quot; in data collection. It remains uncertain how these regulations in cases where an algorithm behaves in ways that cannot be explained or understood by the corporations making use of them.&lt;ref name=&quot;GoodmanFlaxman2016&quot; /&gt;{{rp|6}}

=== United States ===
The United States has no overall legislation regulating controls for algorithmic bias, approaching the topic through various state and federal laws that might vary by industries, sectors, and uses.&lt;ref name=&quot;Singer&quot;&gt;{{cite news|last1=Singer|first1=Natasha|title=Consumer Data Protection Laws, an Ocean Apart|url=http://www.nytimes.com/2013/02/03/technology/consumer-data-protection-laws-an-ocean-apart.html|accessdate=26 November 2017|work=The New York Times|date=2 February 2013}}&lt;/ref&gt; Many policies are self-enforced or controlled by the [[Federal Trade Commission]].&lt;ref name=&quot;Singer&quot; /&gt; In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan,&lt;ref name=&quot;ObamaAdmin&quot;&gt;{{cite web|last1=Obama|first1=Barack|title=The Administration’s Report on the Future of Artificial Intelligence|url=https://obamawhitehouse.archives.gov/blog/2016/10/12/administrations-report-future-artificial-intelligence|website=whitehouse.gov|publisher=National Archives|accessdate=26 November 2017|language=en|date=12 October 2016}}&lt;/ref&gt; which called for a critical assessment of algorithms and for researchers to &quot;design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases&quot;.&lt;ref name=&quot;NSTC&quot;&gt;{{cite book|last1=and Technology Council|first1=National Science|title=National Artificial Intelligence Research and Development Strategic Plan|date=2016|publisher=US Government|url=https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/national_ai_rd_strategic_plan.pdf|accessdate=26 November 2017}}&lt;/ref&gt;{{rp|26}}

==References==
{{Reflist}}





</text>
      <sha1>lo4tbs00gmht1x7mgfnbrigjbl75b0b</sha1>
    </revision>
  </page>
  <page>
    <title>Automated machine learning</title>
    <ns>0</ns>
    <id>55843837</id>
    <revision>
      <id>811287497</id>
      <parentid>811257809</parentid>
      <timestamp>2017-11-20T17:57:39Z</timestamp>
      <contributor>
        <username>Dawnseeker2000</username>
        <id>1544984</id>
      </contributor>
      <comment>[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: etc  → etc. , eg, → e.g., (3) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2690">{{Machine learning bar}}
'''Automated machine learning''' or '''AutoML''' is the process of automating the end-to-end process of [[machine learning]]. The complexity of model selection and hyperparameter optimization is often beyond the non-experts.&lt;ref name=&quot;AutoML2017ICMLCFP&quot;/&gt; Automating the end-to-end creation of machine learning models offers the possibility of simplification, faster creation, and better models.

==Targets of automation==
Automated machine learning can target various stages of the machine learning process:&lt;ref name=&quot;AutoML2017ICMLCFP&quot;&gt;{{Cite web | title = AutoML 2017 @ ICML | author = ICML | work = AutoML 2017 Workshop | date = | accessdate = 2017-11-20 | url = https://sites.google.com/site/automl2017icml/call-for-papers | quote = }}&lt;/ref&gt;
* Automated [[data preparation]] and ingestion (from raw data and miscellaneous formats)
** Automated column type detection; e.g., boolean, discrete numerical, continuous numerical, or text
** Automated column intent detection; e.g., target/label, [[Stratified sampling|stratification]] field, numerical feature, categorical text feature, or free text feature
** Automated task detection; e.g., [[binary classification]], clustering, or [[learning to rank|ranking]]
* Automated [[feature engineering]]
** [[Feature selection]]
** [[Feature extraction]]
** [[Meta learning (computer science)|Meta learning]] and [[transfer learning]]
** Detection and handling of skewed data and/or missing values
* Automated [[model selection]]
* [[Hyperparameter (machine learning)#Optimization|Hyperparameter optimization]] of the learning algorithm and featurization
* Automated pipeline selection under time / memory / etc. constraints
* Automated selection of evaluation metics / validation procedures
* Automated problem checking
** Leakage detection
** Misconfiguration detection
* Automated analysis of results obtained
* User interfaces and visualizations for automated machine learning

==Examples==
Software tackling stages of AutoML:
* [https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html Google AutoML] for deep learning model architecture selection.
* [https://github.com/mlr-org/mlr mlr] is a [[R]] package that contains a large number of different hyperparameter optimization techniques for machine learning problems.
* [https://github.com/rhiever/tpot TPOT] is a Python library that automatically creates and optimizes full machine learning pipelines using genetic programming.

==See also==
* [[Hyperparameter (machine learning)#Optimization|Hyperparameter optimization]]
* [[Model selection]]
* [[Self-tuning]]

==References==
{{Reflist}}




{{compu-ai-stub}}</text>
      <sha1>oo8dum7ruhrxtgpm5ztl4zc9a39rqde</sha1>
    </revision>
  </page>
  <page>
    <title>Proaftn</title>
    <ns>0</ns>
    <id>30992863</id>
    <revision>
      <id>811429918</id>
      <parentid>811429719</parentid>
      <timestamp>2017-11-21T15:31:15Z</timestamp>
      <contributor>
        <ip>132.246.129.61</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4450">

'''Proaftn''' is a [[fuzzy classification]] method that belongs to the class of [[supervised learning]] [[algorithms]]. The [[acronym]] Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: [[Fuzzy logic|Fuzzy]] Assignment Procedure for Nominal [[Sorting]].

The method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the [[ELECTRE]] III method.&lt;ref&gt;{{cite book|last=Roy|first=B.|title= Multicriteria Methodology for Decision Aiding | publisher = Kluwer Academic|location = Dordrecht|year = 1996}}&lt;/ref&gt; To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the [[Discretization of continuous features|discretization]] technique described in,&lt;ref&gt;{{cite journal|last=Ching |first = J.Y.| title = Class-dependent discretization for inductive learning from continuous and mixed-mode data|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=7|pages=641–651|doi=10.1109/34.391407}}&lt;/ref&gt; that establishes a set of pre-classified cases called a training set.

To resolve the classification problems, Proaftn proceeds by the following stages:&lt;ref&gt;{{cite journal|title= Multicriteria assignment method PROAFTN: Methodology and medical application|journal=European Journal of Operational Research|year=2000|first=N.|last=Belacel |volume=125|issue=3|pages=175–83|doi=10.1016/s0377-2217(99)00192-7}}&lt;/ref&gt;

Stage 1. Modeling of classes: In this stage, the prototypes of the classes are conceived using the two following steps:

*Step 1. Structuring: The prototypes and their parameters (thresholds, weights, etc.) are established using the available knowledge given by the expert.
*Step 2. Validation: We use one of the two following techniques in order to validate or adjust the parameters obtained in the first step through the assignment  examples known as a training set.

Direct technique: It consists in adjusting the parameters through the training set and with the expert intervention.

Indirect technique: It consists in fitting the parameters without the expert intervention as used in [[machine learning]] approaches.&lt;ref&gt;{{cite journal|last=Doumpos|first=M.|author2=Zopounidis, C.|title=Preference disaggregation and statistical learning for multicriteria decision support: A review|journal=European Journal of Operational Research|year=2011|volume=209|issue=3|pages=203–214|doi=10.1016/j.ejor.2010.05.029}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|title=Learning multicriteria fuzzy classification method PROAFTN from data |journal=Computers &amp; Operations Research|year=2007|first=N.|last=Belacel |author2= Rava, H. B.l |author3= Punnen, A. P.|volume=34|issue=7| pages=1885–1898|doi=10.1016/j.cor.2005.07.019}}&lt;/ref&gt;

In [[multicriteria classification]] problem, the indirect technique is known as ''preference disaggregation analysis''.&lt;ref&gt;{{cite journal|last=Jacquet-Lagrèze|first=E.|author2=Siskos, J.|title=Preference disaggregation: Twenty years of MCDA experience|journal=European Journal of Operational Research|year=2001|volume=130|issue=2|pages=233–245|doi=10.1016/s0377-2217(00)00035-7}}&lt;/ref&gt; This technique requires less cognitive effort than the former technique; it uses an automatic method to determine the optimal parameters, which minimize the classification errors.

Furthermore, several [[heuristics]] and [[metaheuristics]] were used to learn the multicriteria classification method Proaftn.&lt;ref&gt;{{cite journal|title=An evolutionary framework using particle swarm optimization for classification method PROAFTN|journal= Applied Soft Computing|year=2011|first=F.|last=Al-Obeidat|volume=11|issue=8|pages=4971–4980|id= |url=|format=|doi=10.1016/j.asoc.2011.06.003|display-authors=etal}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|title=Differential Evolution for learning the classification method PROAFTN|journal= Knowledge Based System|year=2010|first=f.|last=Al-Obeidat|volume=23|issue=5|pages= 418–426 |id= |url=|format=|doi=10.1016/j.knosys.2010.02.003|display-authors=etal}}&lt;/ref&gt;

Stage 2. Assignment: After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes.

==References==
{{reflist}}

== External links ==
* [http://mcsc2.ist.utl.pt/index.html Site dedicated to the sorting problematic of MCDA]

[[Category: Operations research]]
[[Category: Machine learning]]
[[Category: Statistical classification]]</text>
      <sha1>0gew4615fqe1yb52rume3gjkm16n7sb</sha1>
    </revision>
  </page>
  <page>
    <title>Data exploration</title>
    <ns>0</ns>
    <id>43385931</id>
    <revision>
      <id>814580573</id>
      <parentid>791149245</parentid>
      <timestamp>2017-12-09T17:53:15Z</timestamp>
      <contributor>
        <ip>178.92.49.94</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4894">{{Multiple issues|{{refimprove|date=July 2017}}{{more footnotes|date=July 2017}}}}

'''Data exploration''' is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems&lt;ref name=&quot;Foster&quot;&gt;[https://www.fosteropenscience.eu/sites/default/files/pdf/2933.pdf FOSTER Open Science], Overview of Data Exploration Techniques: Stratos Idreos, Olga Papaemmonouil, Surajit Chaudhuri.&lt;/ref&gt;. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.

Data exploration is typically conducted using a combination of automated and manual activities.&lt;ref name=&quot;Foster&quot; /&gt;&lt;ref name=&quot;Stanford2011&quot;&gt;[http://vis.stanford.edu/files/2011-Wrangler-CHI.pdf Stanford.edu], 2011 Wrangler: Interactive Visual Specification of Data Transformation Scripts, Kandel, Paepcke, Hellerstein Heer.&lt;/ref&gt; Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.

This is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions.  Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.&lt;ref name=&quot;Stanford2012&quot;&gt;[http://vis.stanford.edu/files/2012-EnterpriseAnalysisInterviews-VAST.pdf Stanford.edu], IEEE Visual Analytics Science &amp; Technology (VAST), Oct 2012 Enterprise Data Analysis and Visualization: An Interview Study., Sean Kandel, Andreas Paepcke, Joseph Hellerstein, Jeffrey Heer Proc.&lt;/ref&gt;

All of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.

Once this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets&lt;ref name=&quot;Stanford2011&quot; /&gt;. This process is also known as determining [[data quality]]&lt;ref name=&quot;Stanford2012&quot; /&gt;.

At this stage, the data can be considered ready for deeper analysis or be handed off to other analysts or users who have specific needs for the data.

Data exploration can also refer to the adhoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data&lt;ref name=&quot;Foster&quot; /&gt;.  In this scenario, hypotheses may be created and then the data is explored to identify whether those hypotheses are correct.

Traditionally, this had been a key area of focus for statisticians, with [[John Tukey]] being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and [[data scientists]]; the latter being a relatively new role within enterprises and larger organizations.

== Interactive Data Exploration ==
This area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving.&lt;ref name=&quot;Stanford2012&quot; /&gt;  As it’s most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data.&lt;ref name=&quot;Stanford2011&quot; /&gt; Common patterns include regression, classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.

By employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.

==Software==
* [[Trifacta]] – a data preparation and analysis platform
* [[Paxata]] – self-service data preparation software
* [[Alteryx]] – data blending and advanced data analytics software
* IBM Infosphere Analyzer – a data profiling tool
* Microsoft [[Power BI]] -  interactive visualization and data analysis tool
* [[OpenRefine]] -  a standalone open source desktop application for data clean-up and data transformation
* [[Tableau software]] – interactive data visualization software

==See also==
{{Portal|Information technology}}
* [[Exploratory Data Analysis]]
* [[Machine Learning]]
* [[Data profiling]]
* [[Data Visualization]]
{{-}}

== References ==
{{reflist}}




</text>
      <sha1>mkto5h6frprh56asbv12ox3gt1af9e4</sha1>
    </revision>
  </page>
  <page>
    <title>Outline of machine learning</title>
    <ns>0</ns>
    <id>53587467</id>
    <revision>
      <id>815390135</id>
      <parentid>814580707</parentid>
      <timestamp>2017-12-14T15:20:55Z</timestamp>
      <contributor>
        <ip>66.203.38.252</ip>
      </contributor>
      <comment>Fix 'non-negative...' spelling</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="41908">&lt;!--... Attention:  THIS IS AN OUTLINE

        part of the set of 720+ outlines listed at
             [[Portal:Contents/Outlines]].

                 Wikipedia outlines are
              a special type of list article.
              They make up one of Wikipedia's
                content navigation systems

                See
                      for more details.
                   Further improvements
              to this outline are on the way
...--&gt;
&lt;!--{{see also|Index of machine learning articles}}
--&gt;
The following [[Outline (list)|outline]] is provided as an overview of and topical guide to machine learning:

'''[[Machine learning]]''' &amp;ndash; subfield of [[computer science]]&lt;ref name=Britannica&gt;http://www.britannica.com/EBchecked/topic/1116194/machine-learning {{tertiary}}&lt;/ref&gt; (more particularly [[soft computing]]) that evolved from the study of [[pattern recognition]] and [[computational learning theory]] in [[artificial intelligence]].&lt;ref name=Britannica /&gt; In 1959, [[Arthur Samuel]] defined machine learning as a &quot;Field of study that gives computers the ability to learn without being explicitly programmed&quot;.&lt;ref name=&quot;arthur_samuel_machine_learning_def&quot;&gt;{{cite book | title=Too Big to Ignore: The Business Case for Big Data | publisher=Wiley | author=Phil Simon | date=March 18, 2013 | pages=89 | isbn=978-1-118-63817-0 | url=https://books.google.com/books?id=Dn-Gdoh66sgC&amp;pg=PA89#v=onepage&amp;q&amp;f=false}}&lt;/ref&gt; Machine learning explores the study and construction of [[algorithm]]s that can [[learning|learn]] from and make predictions on [[data]].&lt;ref&gt;{{cite journal |title=Glossary of terms |author1=Ron Kohavi |author2=Foster Provost |journal=[[Machine Learning (journal)|Machine Learning]] |volume=30 |pages=271–274 |year=1998 |url=http://ai.stanford.edu/~ronnyk/glossary.html}}&lt;/ref&gt; Such algorithms operate by building a [[Mathematical model|model]] from an example ''[[training set]]'' of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.

{{TOC limit|limit=3}}

== What ''type'' of thing is machine learning? ==

* An [[academic discipline]]
* A branch of [[science]]
** An [[applied science]]
*** A subfield of [[computer science]]
**** A branch of [[artificial intelligence]]
**** A subfield of [[soft computing]]

== Branches of machine learning ==

=== Subfields of machine learning ===

[[Subfields of machine learning]]
* [[Computational learning theory]] &amp;ndash; studying the design and analysis of [[machine learning]] algorithms.&lt;ref name=&quot;ACL&quot;&gt;http://www.learningtheory.org/&lt;/ref&gt;
* [[Grammar induction]]
* [[Meta learning (computer science)|Meta learning]]

=== Cross-disciplinary fields involving machine learning ===

[[List of cross-disciplinary fields involving machine learning|Cross-disciplinary fields involving machine learning]]
* [[Adversarial machine learning]]
* [[Predictive analytics]]
* [[Quantum machine learning]]
* [[Robot learning]]
** [[Developmental robotics]]

== Applications of machine learning ==

[[Applications of machine learning]]
* [[Biomedical informatics]]
* [[Computer vision]]
* [[Customer relationship management]] &amp;ndash;
* [[Data mining]]
* [[Email filtering]]
* [[Inverted pendulum]] &amp;ndash; balance and equilibrium system.
* [[Natural language processing]] (NLP)
** [[Automatic summarization]]
** [[Automatic taxonomy construction]]
** [[Dialog system]]
** [[Grammar checker]]
** Language recognition
*** [[Handwriting recognition]]
*** [[Optical character recognition]]
*** [[Speech recognition]]
** [[Machine translation]]
** [[Question answering]]
** [[Speech synthesis]]
** [[Text mining]]
***[[Term frequency–inverse document frequency]] (tf–idf)
** [[Text simplification]]
* [[Pattern recognition]]
** [[Facial recognition system]]
** [[Handwriting recognition]]
** [[Image recognition]]
** [[Optical character recognition]]
** [[Speech recognition]]
* [[Recommendation system]]
**[[Collaborative filtering]]
**[[Content-based filtering]]
**[[Recommender system#Hybrid recommender systems|Hybrid recommender systems]] (Collaborative and content-based filtering)
* [[Search engine]]
**[[Search engine optimization]]

== Machine learning hardware ==

[[Machine learning hardware]]
* [[Graphics processing unit]]
* [[Tensor processing unit]]
* [[Vision processing unit]]

== Machine learning tools ==

[[Machine learning tools]] &amp;nbsp; ([[List of machine learning tools|list]])
* [[Comparison of deep learning software]]
** [[Comparison of deep learning software/Resources]]

=== Machine learning frameworks ===

[[Machine learning framework]]

==== Proprietary machine learning frameworks ====

[[List of proprietary machine learning frameworks|Proprietary machine learning frameworks]]
* [[Amazon Machine Learning]]
* [[Microsoft_Azure#Machine_Learning | Microsoft Azure Machine Learning Studio]]
* [[DistBelief]] &amp;ndash; replaced by TensorFlow
* [[Microsoft Cognitive Toolkit]]

==== Open source machine learning frameworks ====

[[List of open source machine learning frameworks|Open source machine learning frameworks]]
* [[Apache Singa]]
* [[Caffe (software)|Caffe]]
* [[H2O (software)|H2O]]
* [[MLPACK (C++ library)|MLPACK]]
* [[TensorFlow]]
* [[Torch (machine learning)|Torch]]
* [[Accord.NET|Accord.Net]]

=== Machine learning libraries ===

[[Machine learning library]] &amp;nbsp; ([[List of machine learning libraries|list]])
* [[Deeplearning4j]]
* [[Theano (software)|Theano]]
* [[Scikit-learn]]

=== Machine learning algorithms ===

[[Machine learning algorithm]]

==== Types of machine learning algorithms ====
* [[Almeida–Pineda recurrent backpropagation]]
* [[ALOPEX]]
* [[Almeida–Pineda recurrent backpropagation]]
* [[Backpropagation]]
* [[Bootstrap aggregating]]
* [[CN2 algorithm]]
* [[Constructing skill trees]]
* [[Dehaene–Changeux model]]
* [[Diffusion map]]
* [[Dominance-based rough set approach]]
* [[Dynamic time warping]]
* [[Error-driven learning]]
* [[Evolutionary multimodal optimization]]
* [[Expectation–maximization algorithm]]
* [[FastICA]]
* [[Forward–backward algorithm]]
* [[GeneRec]]
* [[Genetic Algorithm for Rule Set Production]]
* [[Growing self-organizing map]]
* [[HEXQ]]
* [[Hyper basis function network]]
* [[IDistance]]
* [[K-nearest neighbors algorithm]]
* [[Kernel methods for vector output]]
* [[Kernel principal component analysis]]
* [[Leabra]]
* [[Linde–Buzo–Gray algorithm]]
* [[Local outlier factor]]
* [[Logic learning machine]]
* [[LogitBoost]]
* [[Manifold alignment]]
* [[Minimum redundancy feature selection]]
* [[Mixture of experts]]
* [[Multiple kernel learning]]
* [[Non-negative matrix factorization]]
* [[Online machine learning]]
* [[Out-of-bag error]]
* [[Prefrontal cortex basal ganglia working memory]]
* [[PVLV]]
* [[Q-learning]]
* [[Quadratic unconstrained binary optimization]]
* [[Query-level feature]]
* [[Quickprop]]
* [[Radial basis function network]]
* [[Randomized weighted majority algorithm]]
* [[Reinforcement learning]]
* [[Repeated incremental pruning to produce error reduction (RIPPER)]]
* [[Rprop]]
* [[Rule-based machine learning]]
* [[Skill chaining]]
* [[Sparse PCA]]
* [[State–action–reward–state–action]]
* [[Stochastic gradient descent]]
* [[Structured kNN]]
* [[T-distributed stochastic neighbor embedding]]
* [[Temporal difference learning]]
* [[Wake-sleep algorithm]]
* [[Weighted majority algorithm (machine learning)]]

== Machine learning methods ==

[[Machine learning method]] &amp;nbsp; ([[List of machine learning methods|list]])
* [[Instance-based algorithm]]
** [[K-nearest neighbors algorithm]] (KNN)
** [[Learning vector quantization]] (LVQ)
** [[Self-organizing map]] (SOM)
* [[Regression analysis]]
** [[Logistic regression]]
** [[Ordinary least squares regression]] (OLSR)
** [[Linear regression]]
** [[Stepwise regression]]
** [[Multivariate adaptive regression splines]] (MARS)
* [[Regularization algorithm]]
** [[Ridge regression]]
** [[Least Absolute Shrinkage and Selection Operator]] (LASSO)
** [[Elastic net]]
** [[Least-angle regression]] (LARS)
* [[Statistical classification|Classifiers]]
** [[Probabilistic classifier]]
*** [[Naive Bayes classifier]]
** [[Binary classifier]]
** [[Linear classifier]]
** [[Hierarchical classifier]]

=== Dimensionality reduction ===

[[Dimensionality reduction]]
* [[Canonical correlation analysis]] (CCA)
* [[Factor analysis]]
* [[Feature extraction]]
* [[Feature selection]]
* [[Independent component analysis]] (ICA)
* [[Linear discriminant analysis]] (LDA)
* [[Multidimensional scaling]] (MDS)
* [[Non-negative matrix factorization]] (NMF)
* [[Partial least squares regression]] (PLSR)
* [[Principal component analysis]] (PCA)
* [[Principal component regression]] (PCR)
* [[Projection pursuit]]
* [[Sammon mapping]]
* [[t-distributed stochastic neighbor embedding]] (t-SNE)

=== Ensemble learning ===

[[Ensemble learning]]
* [[AdaBoost]]
* [[Boosting (machine learning)|Boosting]]
* [[Bootstrap aggregating]] (Bagging)
* [[Ensemble averaging (machine learning)|Ensemble averaging]] &amp;ndash; process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models &quot;average out.&quot;
* [[Gradient boosted decision tree]] (GBRT)
* [[Gradient boosting]] machine (GBM)
* [[Random Forest]]
* [[Stacked Generalization]] (blending)

=== Meta learning ===

[[Meta learning (computer science)|Meta learning]]
* [[Inductive bias]]
* [[Metadata]]

=== Reinforcement learning ===

[[Reinforcement learning]]
* [[Q-learning]]
* [[State–action–reward–state–action]] (SARSA)
* [[Temporal difference learning]] (TD)
* [[Learning Automata]]

=== Supervised learning ===

[[Supervised learning]]
* [[AODE]]
* [[Artificial neural network]]
* [[Association rule learning]] algorithms
** [[Apriori algorithm]]
** [[Eclat algorithm]]
* [[Case-based reasoning]]
* [[Gaussian process regression]]
* [[Gene expression programming]]
* [[Group method of data handling]] (GMDH)
* [[Inductive logic programming]]
* [[Instance-based learning]]
* [[Lazy learning]]
* [[Learning Automata]]
* [[Learning Vector Quantization]]
* [[Logistic Model Tree]]
* [[Minimum message length]] (decision trees, decision graphs, etc.)
** [[Nearest neighbor (pattern recognition)|Nearest Neighbor Algorithm]]
** [[Analogical modeling]]
* [[Probably approximately correct learning]] (PAC) learning
* [[Ripple down rules]], a knowledge acquisition methodology
* [[Symbolic machine learning]] algorithms
* [[Support vector machine]]s
* [[Random forest|Random Forests]]
* [[Ensembles of classifiers]]
** [[Bootstrap aggregating]] (bagging)
** [[Boosting (meta-algorithm)]]
* [[Ordinal classification]]
* [[Information Fuzzy Networks|Information fuzzy networks]] (IFN)
* [[Conditional Random Field]]
* [[ANOVA]]
* [[Quadratic classifier]]s
* [[Nearest neighbor (pattern recognition)|k-nearest neighbor]]
* [[Boosting (machine learning)|Boosting]]
** [[SPRINT (machine learning)|SPRINT]]
* [[Bayesian network]]s
** [[Naive Bayes]]
* [[Hidden Markov model]]s
**[[Hierarchical hidden Markov model]]

==== Bayesian ====

[[Bayesian statistics]]
* [[Bayesian knowledge base]]
* [[Naive Bayes]]
* [[Gaussian Naive Bayes]]
* [[Multinomial Naive Bayes]]
* [[Averaged One-Dependence Estimators]] (AODE)
* [[Bayesian Belief Network]] (BBN)
* [[Bayesian Network]] (BN)

==== Decision tree algorithms ====

[[Decision tree algorithm]]
* [[Decision tree]]
* [[Classification and regression tree]] (CART)
* [[Iterative Dichotomiser 3]] (ID3)
* [[C4.5 algorithm]]
* [[C5.0 algorithm]]
* [[Chi-squared Automatic Interaction Detection]] (CHAID)
* [[Decision stump]]
* [[Conditional decision tree]]
* [[ID3 algorithm]]
* [[Random forest]]
* [[SLIQ]]

==== Linear classifier ====

[[Linear classifier]]
* [[Fisher's linear discriminant]]
* [[Linear regression]]
* [[Logistic regression]]
* [[Multinomial logistic regression]]
* [[Naive Bayes classifier]]
* [[Perceptron]]
* [[Support vector machine]]

=== Unsupervised learning ===

[[Unsupervised learning]]
* [[Expectation-maximization algorithm]]
* [[Vector Quantization]]
* [[Generative topographic map]]
* [[Information bottleneck method]]

==== Artificial neural networks ====

[[Artificial neural network]]
* [[Feedforward neural network]]
** [[Extreme learning machine]]
* [[Logic learning machine]]
* [[Self-organizing map]]

==== Association rule learning ====

[[Association rule learning]]
* [[Apriori algorithm]]
* [[Eclat algorithm]]
* [[Association rule learning#FP-growth algorithm|FP-growth algorithm]]

==== Hierarchical clustering ====

[[Hierarchical clustering]]
* [[Single-linkage clustering]]
* [[Conceptual clustering]]

==== Cluster analysis ====

[[Cluster analysis]]
* [[BIRCH]]
* [[DBSCAN]]
* [[Expectation-maximization algorithm|Expectation-maximization (EM)]]
* [[Fuzzy clustering]]
* [[Hierarchical Clustering]]
* [[K-means algorithm]]
* [[K-means clustering]]
* [[K-medians]]
* [[Mean-shift]]
* [[OPTICS algorithm]]

==== Anomaly detection ====

[[Anomaly detection]]
* [[k-nearest neighbors classification]] (''k''-NN)
* [[Local outlier factor]]

=== Semi-supervised learning ===

[[Semi-supervised learning]]
* [[Active learning (machine learning)|Active learning]] &amp;ndash; special case of semi-supervised learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.&lt;ref name=&quot;settles&quot;&gt;{{Citation
 | title = Active Learning Literature Survey
 | url = http://pages.cs.wisc.edu/~bsettles/pub/settles.activelearning.pdf
 | author = Settles, Burr
 | journal = Computer Sciences Technical Report 1648. University of Wisconsin–Madison
 | year = 2010
 | accessdate = 2014-11-18
}}&lt;/ref&gt; &lt;ref name=&quot;rubens2016&quot;&gt;{{cite book
|last1=Rubens |first1=Neil
|last2= [https://www.linkedin.com/in/mehdielahi Elahi]|first2=Mehdi
 |last3=Sugiyama|first3=Masashi|last4=Kaplan|first4=Dain|editor1-last=Ricci
 |editor1-first=Francesco
 |editor2-last=Rokach|editor2-first=Lior
 |editor3-last=Shapira |editor3-first=Bracha
 |title=Recommender Systems Handbook
 |date=2016
 |publisher=Springer US
 |isbn=978-1-4899-7637-6
 |edition=2
 |chapter=Active Learning in Recommender Systems
 |chapter-url= https://rd.springer.com/chapter/10.1007/978-1-4899-7637-6_24
 |url = https://rd.springer.com/book/10.1007/978-1-4899-7637-6
|doi=10.1007/978-1-4899-7637-6
}}&lt;/ref&gt;
* [[Semi-supervised learning#Generative models|Generative models]]
* [[Semi-supervised learning#Low-density separation|Low-density separation]]
* [[Semi-supervised learning#Graph-based methods|Graph-based methods]]
* [[Co-training]]
* [[Transduction (machine learning)|Transduction]]

=== Deep learning ===

[[Deep learning]]
* [[Deep belief network]]s
* Deep [[Boltzmann machine]]s
* Deep [[Convolutional neural network]]s
* Deep [[Recurrent neural network]]s
* [[Hierarchical temporal memory]]
* [[Deep Boltzmann Machine]] (DBM)
* [[Stacked Auto-Encoders]]

=== Other machine learning methods and problems ===

* [[Anomaly detection]]
* [[Association rule learning|Association rules]]
* [[Bias-variance dilemma]]
* [[Statistical classification|Classification]]
** [[Multi-label classification]]
* [[Cluster analysis|Clustering]]
* [[Data Pre-processing]]
* [[Empirical risk minimization]]
* [[Feature engineering]]
* [[Feature learning]]
* [[Learning to rank]]
* [[Occam learning]]
* [[Online machine learning]]
* [[PAC learning]]
* [[Regression analysis|Regression]]
* [[Reinforcement Learning]]
* [[Semi-supervised learning]]
* [[Statistical learning]]
* [[Structured prediction]]
** [[Graphical model]]s
*** [[Bayesian network]]
*** [[Conditional random field]] (CRF)
*** [[Hidden Markov model]] (HMM)
* [[Unsupervised learning]]
* [[VC theory]]

== Machine learning research ==

[[Machine learning research]]
* [[List of artificial intelligence projects]]
* [[List of datasets for machine learning research]]

== History of machine learning ==

[[History of machine learning]]
* [[Timeline of machine learning]]

== Machine learning projects ==

[[List of machine learning projects|Machine learning projects]]
* [[DeepMind]]
* [[Google Brain]]

== Machine learning organizations ==

[[List of machine learning organizations|Machine learning organizations]]
* [[Knowledge Engineering and Machine Learning Group]]

=== Machine learning conferences and workshops ===

* Artificial Intelligence and Security (AISec) (co-located workshop with CCS)
* [[Conference on Neural Information Processing Systems]] (NIPS)
* [[ECML PKDD]]
* [[International Conference on Machine Learning]] (ICML)

== Machine learning publications ==

=== Books on machine learning ===

[[List of books about machine learning|Books about machine learning]]

=== Machine learning journals ===

* ''[[Machine Learning (journal)|Machine Learning]]''
* ''[[Journal of Machine Learning Research]]'' (JMLR)
* ''[[Neural Computation (journal)|Neural Computation]]''

== Persons influential in machine learning ==

* [[Alberto Broggi]]
* [[Andrei Knyazev (mathematician)|Andrei Knyazev]]
* [[Andrew McCallum]]
* [[Andrew Ng]]
* [[Armin B. Cremers]]
* [[Ayanna Howard]]
* [[Barney Pell]]
* [[Ben Goertzel]]
* [[Ben Taskar]]
* [[Bernhard Schölkopf]]
* [[Brian D. Ripley]]
* [[Christopher G. Atkeson]]
* [[Corinna Cortes]]
* [[Demis Hassabis]]
* [[Douglas Lenat]]
* [[Eric Xing]]
* [[Ernst Dickmanns]]
* [[Geoffrey Hinton]] &amp;ndash; co-inventor of the backpropagation and contrastive divergence training algorithms
* [[Hans-Peter Kriegel]]
* [[Hartmut Neven]]
* [[Heikki Mannila]]
* [[Jacek M. Zurada]]
* [[Jaime Carbonell]]
* [[Jerome H. Friedman]]
* [[John D. Lafferty]]
* [[John Platt (computer scientist)|John Platt]] &amp;ndash; invented SMO and Platt scaling
* [[Julie Beth Lovins]]
* [[Jürgen Schmidhuber]]
* [[Karl Steinbuch]]
* [[Katia Sycara]]
* [[Leo Breiman]] &amp;ndash; invented bagging and random forests
* [[Lise Getoor]]
* [[Luca Maria Gambardella]]
* [[Léon Bottou]]
* [[Marcus Hutter]]
* [[Mehryar Mohri]]
* [[Michael Collins (computational linguist)|Michael Collins]]
* [[Michael I. Jordan]]
* [[Michael L. Littman]]
* [[Nando de Freitas]]
* [[Ofer Dekel (researcher)|Ofer Dekel]]
* [[Oren Etzioni]]
* [[Pedro Domingos]]
* [[Peter Flach]]
* [[Pierre Baldi]]
* [[Pushmeet Kohli]]
* [[Ray Kurzweil]]
* [[Rayid Ghani]]
* [[Ross Quinlan]]
* [[Salvatore J. Stolfo]]
* [[Sebastian Thrun]]
* [[Selmer Bringsjord]]
* [[Sepp Hochreiter]]
* [[Shane Legg]]
* [[Stephen Muggleton]]
* [[Steve Omohundro]]
* [[Tom M. Mitchell]]
* [[Trevor Hastie]]
* [[Vasant Honavar]]
* [[Vladimir Vapnik]] &amp;ndash; co-inventor of the SVM and VC theory
* [[Yann LeCun]] &amp;ndash; invented convolutional neural networks
* [[Yasuo Matsuyama]]
* [[Yoshua Bengio]]
* [[Zoubin Ghahramani]]

== See also ==
{{Portal|Machine learning}}

* [[Outline of artificial intelligence]]
** [[Outline of computer vision]]
** [[Outline of natural language processing]]
* [[Outline of robotics]]
&lt;!-- Place these in the body of the outline above --&gt;
* [[Accuracy paradox]]
* [[Action model learning]]
* [[Activation function]]
* [[Activity recognition]]
* [[ADALINE]]
* [[Adaptive neuro fuzzy inference system]]
* [[Adaptive resonance theory]]
* [[Additive smoothing]]
* [[Adjusted mutual information]]
* [[Aika (software)]]
* [[AIVA]]
* [[AIXI]]
* [[AlchemyAPI]]
* [[AlexNet]]
* [[Algorithm selection]]
* [[Algorithmic inference]]
* [[Algorithmic learning theory]]
* [[AlphaGo]]
* [[AlphaGo Zero]]
* [[Alternating decision tree]]
* [[Apprenticeship learning]]
* [[Causal Markov condition]] &lt;!--this is an ML concept; subtopic of Markov blanket--&gt;
* [[Competitive learning]]
* [[Concept learning]]
* [[Decision tree learning]]
* [[Distribution learning theory]]
* [[Eager learning]]
* [[End-to-end reinforcement learning]]
* [[Error tolerance (PAC learning)]]
* [[Explanation-based learning]]
* [[Feature (machine learning)|Feature]]
* [[GloVe (machine learning)|GloVe]]
* [[Hyperparameter (machine learning)|Hyperparameter]]
* [[IBM Machine Learning Hub]]
* [[Inferential theory of learning]]
* [[Learning automata]]
* [[Learning classifier system]]
* [[Learning rule]]
* [[Learning with errors]]
* [[M-Theory (learning framework)]]
* [[Machine learning control]]
* [[Machine learning in bioinformatics]]
* [[Margin (machine learning)|Margin]]
* [[Markov chain geostatistics]] &lt;!--stub article; this topic is probably an application of ML--&gt;
* [[Markov chain Monte Carlo]] (MCMC) &lt;!--this is an ML topic; applications to Markov logic networks, among others--&gt;
* [[Markov information source]] &lt;!--topic with direct applications to ML in NLP--&gt;
* [[Markov logic network]] &lt;!--Topic with applications to ML through MCMC models (https://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf) --&gt;
* [[Markov model]]&lt;!--this is an ML topic--&gt;
* [[Markov random field]]&lt;!--this seems to be an ML topic--&gt;
* [[Markovian discrimination]] &lt;!--seems to be an ML-related with applications to NLP; the article lacks context, so it's difficult to tell--&gt;
* [[Maximum-entropy Markov model]] &lt;!--this is definitely an ML topic--&gt;
* [[Multi-armed bandit]] &lt;!--Definitely an ML concept--&gt;
* [[Multi-task learning]]
* [[Multilinear subspace learning]]
* [[Multimodal learning]]
* [[Multiple instance learning]]
* [[Multiple-instance learning]]
* [[Never-Ending Language Learning]]
* [[Offline learning]]
* [[Parity learning]]
* [[Population-based incremental learning]]
* [[Predictive learning]]
* [[Preference learning]]
* [[Proactive learning]]
* [[Proximal gradient methods for learning]]
* [[Semantic analysis (machine learning)|Semantic analysis]]
* [[Similarity learning]]
* [[Sparse dictionary learning]]
* [[Stability (learning theory)]]
* [[Statistical learning theory]]
* [[Statistical relational learning]]
* [[Tanagra (machine learning)|Tanagra]]
* [[Transfer learning]]
* [[Variable-order Markov model]] &lt;!--a Markov model with applications to ML--&gt;
* [[Version space learning]]
* [[Waffles (machine learning)|Waffles]]
* [[Weka (machine learning)|Weka]]

* [[Loss function]]
** [[Loss functions for classification]]
** [[Mean squared error]] (MSE)
** [[Mean squared prediction error]] (MSPE)
** [[Taguchi loss function]]
* [[Low-energy adaptive clustering hierarchy]]

=== Other ===

&lt;!-- Filter out those that are not subtopics of machine learning. There's a section on the talk page for links you are not sure about. --&gt;

* [[Anne O'Tate]]
* [[Ant colony optimization algorithms]]
* [[Anthony Levandowski]]
* [[Anti-unification (computer science)]]
* [[Apache Flume]]
* [[Apache Giraph]]
* [[Apache Mahout]]
* [[Apache SINGA]]
* [[Apache Spark]]
* [[Apache SystemML]]
* [[Aphelion (software)]]
* [[Arabic Speech Corpus]]
* [[Archetypal analysis]]
* [[Arthur Zimek]]
* [[Artificial ants]]
* [[Artificial bee colony algorithm]]
* [[Artificial development]]
* [[Artificial immune system]]
* [[Astrostatistics]]
* [[Averaged one-dependence estimators]]
* [[Bag-of-words model]]
* [[Balanced clustering]]
* [[Ball tree]]
* [[Base rate]]
* [[Bat algorithm]]
* [[Baum–Welch algorithm]]
* [[Bayesian hierarchical modeling]]
* [[Bayesian interpretation of kernel regularization]]
* [[Bayesian optimization]]
* [[Bayesian structural time series]]
* [[Bees algorithm]]
* [[Behavioral clustering]]
* [[Bernoulli scheme]]
* [[Bias–variance tradeoff]]
* [[Biclustering]]
* [[Binarization of consensus partition matrices]]
* [[Binary classification]]
* [[Bing Predicts]]
* [[Bio-inspired computing]]
* [[Biogeography-based optimization]]
* [[Biplot]]
* [[Bondy's theorem]]
* [[Bongard problem]]
* [[Bradley–Terry model]]
* [[BrownBoost]]
* [[Brown clustering]]
* [[Burst error]]
* [[CBCL (MIT)]]
* [[CIML community portal]]
* [[CMA-ES]]
* [[CURE data clustering algorithm]]
* [[Cache language model]]
* [[Calibration (statistics)]]
* [[Canonical correspondence analysis]]
* [[Canopy clustering algorithm]]
* [[Cascading classifiers]]
* [[Category utility]]
* [[CellCognition]]
* [[Cellular evolutionary algorithm]]
* [[Chi-square automatic interaction detection]]
* [[Chromosome (genetic algorithm)]]
* [[Classifier chains]]
* [[Cleverbot]]
* [[Clonal selection algorithm]]
* [[Cluster-weighted modeling]]
* [[Clustering high-dimensional data]]
* [[Clustering illusion]]
* [[CoBoosting]]
* [[Cobweb (clustering)]]
* [[Cognitive computer]]
* [[Cognitive robotics]]
* [[Collostructional analysis]]
* [[Common-method variance]]
* [[Complete-linkage clustering]]
* [[Computer-automated design]]
* [[Concept class]]
* [[Concept drift]]
* [[Conference on Artificial General Intelligence]]
* [[Conference on Knowledge Discovery and Data Mining]]
* [[Confirmatory factor analysis]]
* [[Confusion matrix]]
* [[Congruence coefficient]]
* [[Connect (computer system)]]
* [[Consensus clustering]]
* [[Constrained clustering]]
* [[Constrained conditional model]]
* [[Constructive cooperative coevolution]]
* [[Correlation clustering]]
* [[Correspondence analysis]]
* [[Cortica]]
* [[Coupled pattern learner]]
* [[Cross-entropy method]]
* [[Cross-validation (statistics)]]
* [[Crossover (genetic algorithm)]]
* [[Cuckoo search]]
* [[Cultural algorithm]]
* [[Cultural consensus theory]]
* [[Curse of dimensionality]]
* [[DADiSP]]
* [[DARPA LAGR Program]]
* [[Darkforest]]
* [[Dartmouth workshop]]
* [[DarwinTunes]]
* [[Data Mining Extensions]]
* [[Data exploration]]
* [[Data pre-processing]]
* [[Data stream clustering]]
* [[Dataiku]]
* [[Davies–Bouldin index]]
* [[Decision boundary]]
* [[Decision list]]
* [[Decision tree model]]
* [[Deductive classifier]]
* [[DeepArt]]
* [[DeepDream]]
* [[Deep Web Technologies]]
* [[Defining length]]
* [[Dendrogram]]
* [[Dependability state model]]
* [[Detailed balance]]
* [[Determining the number of clusters in a data set]]
* [[Detrended correspondence analysis]]
* [[Developmental robotics]]
* [[Diffbot]]
* [[Differential evolution]]
* [[Discrete phase-type distribution]]
* [[Discriminative model]]
* [[Dissociated press]]
* [[Distributed R]]
* [[Dlib]]
* [[Document classification]]
* [[Documenting Hate]]
* [[Domain adaptation]]
* [[Doubly stochastic model]]
* [[Dual-phase evolution]]
* [[Dunn index]]
* [[Dynamic Bayesian network]]
* [[Dynamic Markov compression]]
* [[Dynamic topic model]]
* [[Dynamic unobserved effects model]]
* [[EDLUT]]
* [[ELKI]]
* [[Edge recombination operator]]
* [[Effective fitness]]
* [[Elastic map]]
* [[Elastic matching]]
* [[Elbow method (clustering)]]
* [[Emergent (software)]]
* [[Encog]]
* [[Entropy rate]]
* [[Erkki Oja]]
* [[Eurisko]]
* [[European Conference on Artificial Intelligence]]
* [[Evaluation of binary classifiers]]
* [[Evolution strategy]]
* [[Evolution window]]
* [[Evolutionary Algorithm for Landmark Detection]]
* [[Evolutionary algorithm]]
* [[Evolutionary art]]
* [[Evolutionary music]]
* [[Evolutionary programming]]
* [[Evolvability (computer science)]]
* [[Evolved antenna]]
* [[Evolver (software)]]
* [[Evolving classification function]]
* [[Expectation propagation]]
* [[Exploratory factor analysis]]
* [[F1 score]]
* [[FLAME clustering]]
* [[Factor analysis of mixed data]]
* [[Factor graph]]
* [[Factor regression model]]
* [[Factored language model]]
* [[Farthest-first traversal]]
* [[Fast-and-frugal trees]]
* [[Feature Selection Toolbox]]
* [[Feature hashing]]
* [[Feature scaling]]
* [[Feature vector]]
* [[Firefly algorithm]]
* [[First-difference estimator]]
* [[First-order inductive learner]]
* [[Fish School Search]]
* [[Fisher kernel]]
* [[Fitness approximation]]
* [[Fitness function]]
* [[Fitness proportionate selection]]
* [[Fluentd]]
* [[Folding@home]]
* [[Formal concept analysis]]
* [[Forward algorithm]]
* [[Fowlkes–Mallows index]]
* [[Frederick Jelinek]]
* [[Frrole]]
* [[Functional principal component analysis]]
* [[GATTO]]
* [[GLIMMER]]
* [[Gary Bryce Fogel]]
* [[Gaussian adaptation]]
* [[Gaussian process]]
* [[Gaussian process emulator]]
* [[Gene prediction]]
* [[General Architecture for Text Engineering]]
* [[Generalization error]]
* [[Generalized canonical correlation]]
* [[Generalized filtering]]
* [[Generalized iterative scaling]]
* [[Generalized multidimensional scaling]]
* [[Generative adversarial network]]
* [[Generative model]]
* [[Genetic algorithm]]
* [[Genetic algorithm scheduling]]
* [[Genetic algorithms in economics]]
* [[Genetic fuzzy systems]]
* [[Genetic memory (computer science)]]
* [[Genetic operator]]
* [[Genetic programming]]
* [[Genetic representation]]
* [[Geographical cluster]]
* [[Gesture Description Language]]
* [[Geworkbench]]
* [[Glossary of artificial intelligence]]
* [[Glottochronology]]
* [[Golem (ILP)]]
* [[Google matrix]]
* [[Grafting (decision trees)]]
* [[Gramian matrix]]
* [[Grammatical evolution]]
* [[Granular computing]]
* [[GraphLab]]
* [[Graph kernel]]
* [[Gremlin (programming language)]]
* [[Growth function]]
* [[HUMANT (HUManoid ANT) algorithm]]
* [[Hammersley–Clifford theorem]]
* [[Harmony search]]
* [[Hebbian theory]]
* [[Hidden Markov random field]]
* [[Hidden semi-Markov model]]
* [[Hierarchical hidden Markov model]]
* [[Higher-order factor analysis]]
* [[Highway network]]
* [[Hinge loss]]
* [[Holland's schema theorem]]
* [[Hopkins statistic]]
* [[Hoshen–Kopelman algorithm]]
* [[Huber loss]]
* [[IRCF360]]
* [[Ian Goodfellow]]
* [[Ilastik]]
* [[Ilya Sutskever]]
* [[Immunocomputing]]
* [[Imperialist competitive algorithm]]
* [[Inauthentic text]]
* [[Incremental decision tree]]
* [[Induction of regular languages]]
* [[Inductive bias]]
* [[Inductive probability]]
* [[Inductive programming]]
* [[Influence diagram]]
* [[Information Harvesting]]
* [[Information fuzzy networks]]
* [[Information gain in decision trees]]
* [[Information gain ratio]]
* [[Inheritance (genetic algorithm)]]
* [[Instance selection]]
* [[Intel RealSense]]
* [[Interacting particle system]]
* [[Interactive machine translation]]
* [[International Joint Conference on Artificial Intelligence]]
* [[International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics]]
* [[International Semantic Web Conference]]
* [[Iris flower data set]]
* [[Island algorithm]]
* [[Isotropic position]]
* [[Item response theory]]
* [[Iterative Viterbi decoding]]
* [[JOONE]]
* [[Jabberwacky]]
* [[Jaccard index]]
* [[Jackknife variance estimates for random forest]]
* [[Java Grammatical Evolution]]
* [[Joseph Nechvatal]]
* [[Jubatus]]
* [[Julia (programming language)]]
* [[Junction tree algorithm]]
* [[K-SVD]]
* [[K-means++]]
* [[K-medians clustering]]
* [[K-medoids]]
* [[KNIME]]
* [[KXEN Inc.]]
* [[K q-flats]]
* [[Kaggle]]
* [[Kalman filter]]
* [[Katz's back-off model]]
* [[Keras]]
* [[Kernel adaptive filter]]
* [[Kernel density estimation]]
* [[Kernel eigenvoice]]
* [[Kernel embedding of distributions]]
* [[Kernel method]]
* [[Kernel perceptron]]
* [[Kernel random forest]]
* [[Kinect]]
* [[Klaus-Robert Müller]]
* [[Kneser–Ney smoothing]]
* [[Knowledge Vault]]
* [[Knowledge integration]]
* [[LIBSVM]]
* [[LPBoost]]
* [[Labeled data]]
* [[LanguageWare]]
* [[Language Acquisition Device (computer)]]
* [[Language identification in the limit]]
* [[Language model]]
* [[Large margin nearest neighbor]]
* [[Latent Dirichlet allocation]]
* [[Latent class model]]
* [[Latent semantic analysis]]
* [[Latent variable]]
* [[Latent variable model]]
* [[Lattice Miner]]
* [[Layered hidden Markov model]]
* [[Learnable function class]]
* [[Least squares support vector machine]]
* [[Leave-one-out error]]
* [[Leslie P. Kaelbling]]
* [[Linear genetic programming]]
* [[Linear predictor function]]
* [[Linear separability]]
* [[Lingyun Gu]]
* [[Linkurious]]
* [[Lior Ron (business executive)]]
* [[List of genetic algorithm applications]]
* [[List of metaphor-based metaheuristics]]
* [[List of text mining software]]
* [[Local case-control sampling]]
* [[Local independence]]
* [[Local tangent space alignment]]
* [[Locality-sensitive hashing]]
* [[Log-linear model]]
* [[Logistic model tree]]
* [[Low-rank approximation]]
* [[Low-rank matrix approximations]]
* [[MATLAB]]
* [[MIMIC (immunology)]]
* [[MXNet]]
* [[Mallet (software project)]]
* [[Manifold regularization]]
* [[Margin-infused relaxed algorithm]]
* [[Margin classifier]]
* [[Mark V. Shaney]]
* [[Massive Online Analysis]]
* [[Matrix regularization]]
* [[Matthews correlation coefficient]]
* [[Mean shift]]
* [[Mean squared error]]
* [[Mean squared prediction error]]
* [[Measurement invariance]]
* [[Medoid]]
* [[MeeMix]]
* [[Melomics]]
* [[Memetic algorithm]]
* [[Meta-optimization]]
* [[Mexican International Conference on Artificial Intelligence]]
* [[Michael Kearns (computer scientist)]]
* [[MinHash]]
* [[Mixture model]]
* [[Mlpy]]
* [[Models of DNA evolution]]
* [[Moral graph]]
* [[Mountain car problem]]
* [[Movidius]]
* [[Multi-armed bandit]]
* [[Multi-label classification]]
* [[Multi expression programming]]
* [[Multiclass classification]]
* [[Multidimensional analysis]]
* [[Multifactor dimensionality reduction]]
* [[Multilinear principal component analysis]]
* [[Multiple correspondence analysis]]
* [[Multiple discriminant analysis]]
* [[Multiple factor analysis]]
* [[Multiple sequence alignment]]
* [[Multiplicative weight update method]]
* [[Multispectral pattern recognition]]
* [[Mutation (genetic algorithm)]]
* [[MysteryVibe]]
* [[N-gram]]
* [[NOMINATE (scaling method)]]
* [[Native-language identification]]
* [[Natural Language Toolkit]]
* [[Natural evolution strategy]]
* [[Nearest-neighbor chain algorithm]]
* [[Nearest centroid classifier]]
* [[Nearest neighbor search]]
* [[Neighbor joining]]
* [[Nest Labs]]
* [[NetMiner]]
* [[NetOwl]]
* [[Neural Designer]]
* [[Neural Engineering Object]]
* [[Neural Lab]]
* [[Neural modeling fields]]
* [[Neural network software]]
* [[NeuroSolutions]]
* [[Neuro Laboratory]]
* [[Neuroevolution]]
* [[Neuroph]]
* [[Niki.ai]]
* [[Noisy channel model]]
* [[Noisy text analytics]]
* [[Nonlinear dimensionality reduction]]
* [[Novelty detection]]
* [[Nuisance variable]]
* [[Numenta]]
* [[One-class classification]]
* [[Onnx]]
* [[OpenNLP]]
* [[Optimal discriminant analysis]]
* [[Oracle Data Mining]]
* [[Orange (software)]]
* [[Ordination (statistics)]]
* [[Overfitting]]
* [[PROGOL]]
* [[PSIPRED]]
* [[Pachinko allocation]]
* [[PageRank]]
* [[Parallel metaheuristic]]
* [[Parity benchmark]]
* [[Part-of-speech tagging]]
* [[Particle swarm optimization]]
* [[Path dependence]]
* [[Pattern language (formal languages)]]
* [[Peltarion Synapse]]
* [[Perplexity]]
* [[Persian Speech Corpus]]
* [[Picas (app)]]
* [[Pietro Perona]]
* [[Pipeline Pilot]]
* [[Piranha (software)]]
* [[Pitman–Yor process]]
* [[Plate notation]]
* [[Polynomial kernel]]
* [[Pop music automation]]
* [[Population process]]
* [[Portable Format for Analytics]]
* [[Predictive Model Markup Language]]
* [[Predictive state representation]]
* [[Preference regression]]
* [[Premature convergence]]
* [[Principal geodesic analysis]]
* [[Prior knowledge for pattern recognition]]
* [[Prisma (app)]]
* [[Probabilistic Action Cores]]
* [[Probabilistic context-free grammar]]
* [[Probabilistic latent semantic analysis]]
* [[Probabilistic soft logic]]
* [[Probability matching]]
* [[Probit model]]
* [[Product of experts]]
* [[Programming with Big Data in R]]
* [[Proper generalized decomposition]]
* [[Pruning (decision trees)]]
* [[Pushpak Bhattacharyya]]
* [[Q methodology]]
* [[Qloo]]
* [[Quality control and genetic algorithms]]
* [[Quantum Artificial Intelligence Lab]]
* [[Queueing theory]]
* [[Quick, Draw!]]
* [[R (programming language)]]
* [[Rada Mihalcea]]
* [[Rademacher complexity]]
* [[Radial basis function kernel]]
* [[Rand index]]
* [[Random indexing]]
* [[Random projection]]
* [[Random subspace method]]
* [[Ranking SVM]]
* [[RapidMiner]]
* [[Rattle GUI]]
* [[Raymond Cattell]]
* [[Reasoning system]]
* [[Regularization perspectives on support vector machines]]
* [[Relational data mining]]
* [[Relationship square]]
* [[Relevance vector machine]]
* [[Relief (feature selection)]]
* [[Renjin]]
* [[Repertory grid]]
* [[Representer theorem]]
* [[Reward-based selection]]
* [[Richard Zemel]]
* [[Right to explanation]]
* [[RoboEarth]]
* [[Robust principal component analysis]]
* [[RuleML Symposium]]
* [[Rule induction]]
* [[Rules extraction system family]]
* [[SAS (software)]]
* [[SNNS]]
* [[SPSS Modeler]]
* [[SUBCLU]]
* [[Sample complexity]]
* [[Sample exclusion dimension]]
* [[Santa Fe Trail problem]]
* [[Savi Technology]]
* [[Schema (genetic algorithms)]]
* [[Search-based software engineering]]
* [[Selection (genetic algorithm)]]
* [[Self-Service Semantic Suite]]
* [[Semantic folding]]
* [[Semantic mapping (statistics)]]
* [[Semidefinite embedding]]
* [[Sense Networks]]
* [[Sensorium Project]]
* [[Sequence labeling]]
* [[Sequential minimal optimization]]
* [[Shattered set]]
* [[Shogun (toolbox)]]
* [[Silhouette (clustering)]]
* [[SimHash]]
* [[SimRank]]
* [[Similarity measure]]
* [[Simple matching coefficient]]
* [[Simultaneous localization and mapping]]
* [[Sinkov statistic]]
* [[Sliced inverse regression]]
* [[SmartMatch]]
* [[Snakes and Ladders]]
* [[Soft independent modelling of class analogies]]
* [[Soft output Viterbi algorithm]]
* [[Solomonoff's theory of inductive inference]]
* [[SolveIT Software]]
* [[Spectral clustering]]
* [[Spike-and-slab variable selection]]
* [[Statistical machine translation]]
* [[Statistical parsing]]
* [[Statistical semantics]]
* [[Stefano Soatto]]
* [[Stephen Wolfram]]
* [[Stochastic block model]]
* [[Stochastic cellular automaton]]
* [[Stochastic diffusion search]]
* [[Stochastic grammar]]
* [[Stochastic matrix]]
* [[Stochastic universal sampling]]
* [[Stress majorization]]
* [[String kernel]]
* [[Structural equation modeling]]
* [[Structural risk minimization]]
* [[Structured sparsity regularization]]
* [[Structured support vector machine]]
* [[Subclass reachability]]
* [[Sufficient dimension reduction]]
* [[Sukhotin's algorithm]]
* [[Sum of absolute differences]]
* [[Sum of absolute transformed differences]]
* [[Swarm intelligence]]
* [[Switching Kalman filter]]
* [[Symbolic regression]]
* [[Synchronous context-free grammar]]
* [[Syntactic pattern recognition]]
* [[TD-Gammon]]
* [[TIMIT]]
* [[Teaching dimension]]
* [[Teuvo Kohonen]]
* [[Textual case-based reasoning]]
* [[Theory of conjoint measurement]]
* [[Thomas G. Dietterich]]
* [[Thurstonian model]]
* [[Topic model]]
* [[Tournament selection]]
* [[Training, test, and validation sets]]
* [[Transiogram]]
* [[Trax Image Recognition]]
* [[Trigram tagger]]
* [[Truncation selection]]
* [[Tucker decomposition]]
* [[UIMA]]
* [[UPGMA]]
* [[Ugly duckling theorem]]
* [[Uncertain data]]
* [[Uniform convergence in probability]]
* [[Unique negative dimension]]
* [[Universal portfolio algorithm]]
* [[User behavior analytics]]
* [[VC dimension]]
* [[VGG Image Annotator]]
* [[VIGRA]]
* [[Validation set]]
* [[Vapnik–Chervonenkis theory]]
* [[Variable-order Bayesian network]]
* [[Variable kernel density estimation]]
* [[Variable rules analysis]]
* [[Variational message passing]]
* [[Varimax rotation]]
* [[Vector quantization]]
* [[Vicarious (company)]]
* [[Viterbi algorithm]]
* [[Vowpal Wabbit]]
* [[WACA clustering algorithm]]
* [[WPGMA]]
* [[Ward's method]]
* [[Weasel program]]
* [[Whitening transformation]]
* [[Winnow (algorithm)]]
* [[Win–stay, lose–switch]]
* [[Witness set]]
* [[Wolfram Language]]
* [[Wolfram Mathematica]]
* [[Writer invariant]]
* [[Xgboost]]
* [[Yooreeka]]
* [[Zeroth (software)]]

== Further reading ==

* [[Trevor Hastie]], [[Robert Tibshirani]] and [[Jerome H. Friedman]] (2001). ''[http://www-stat.stanford.edu/~tibs/ElemStatLearn/ The Elements of Statistical Learning]'', Springer. {{ISBN|0-387-95284-5}}.
* [[Pedro Domingos]] (September 2015), [[The Master Algorithm]], Basic Books, {{ISBN|978-0-465-06570-7}}
* [[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012). ''[http://www.cs.nyu.edu/~mohri/mlbook/ Foundations of Machine Learning]'', The MIT Press. {{ISBN|978-0-262-01825-8}}.
* Ian H. Witten and Eibe Frank (2011). ''Data Mining: Practical machine learning tools and techniques'' Morgan Kaufmann, 664pp., {{ISBN|978-0-12-374856-0}}.
* [[David J. C. MacKay]]. ''[http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. {{ISBN|0-521-64298-1}}
* [[Richard O. Duda]], [[Peter E. Hart]], David G. Stork (2001) ''Pattern classification'' (2nd edition), Wiley, New York, {{ISBN|0-471-05669-3}}.
* [[Christopher Bishop]] (1995). ''Neural Networks for Pattern Recognition'', Oxford University Press. {{ISBN|0-19-853864-2}}.
* [[Vladimir Vapnik]] (1998). ''Statistical Learning Theory''. Wiley-Interscience, {{ISBN|0-471-03003-1}}.
* [[Ray Solomonoff]], ''An Inductive Inference Machine'', IRE Convention Record, Section on Information Theory, Part 2, pp., 56-62, 1957.
* [[Ray Solomonoff]], &quot;[http://world.std.com/~rjs/indinf56.pdf An Inductive Inference Machine]&quot; A privately circulated report from the 1956 [[Dartmouth Conferences|Dartmouth Summer Research Conference on AI]].

== References ==
{{Reflist}}

== External links ==
{{Sister project links|Machine learning}}

* [https://mitprofessionalx.mit.edu/courses/course-v1:MITProfessionalX+DSx+2016_T1/about Data Science: Data to Insights from MIT (machine learning)]
* [http://machinelearning.org/ International Machine Learning Society]
* Popular online course by [[Andrew Ng]], at [https://www.coursera.org/course/ml Coursera]. It uses [[GNU Octave]]. The course is a free version of [[Stanford University]]'s actual course taught by Ng, whose lectures are also [https://see.stanford.edu/Course/CS229 available for free].
* [https://mloss.org/ mloss] is an academic database of open-source machine learning software.

{{Outline footer}}


[[Category:Computing-related lists]]
[[Category:Machine learning|*]]
[[Category:Artificial intelligence|Machine learning]]
[[Category:Data mining|Machine learning]]</text>
      <sha1>9o65cr3m9b12n92qjs7azh1mdyfdq9b</sha1>
    </revision>
  </page>
  <page>
    <title>Hyperparameter optimization</title>
    <ns>0</ns>
    <id>54361643</id>
    <revision>
      <id>815792037</id>
      <parentid>815791764</parentid>
      <timestamp>2017-12-17T04:52:54Z</timestamp>
      <contributor>
        <username>Acyclic</username>
        <id>29997616</id>
      </contributor>
      <comment>Rewrote introductory paragraph using a reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13168">In [[machine learning]], '''hyperparameter optimization''' or tuning is the problem of choosing a set of optimal [[Hyperparameter (machine learning)|hyperparameters]] for a learning algorithm.

The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined [[loss function]] on given independent data.&lt;ref name=abs1502.02127&gt;{{cite article |url=https://arxiv.org/abs/1502.02127 |title=Claesen, Marc, and Bart De Moor. &quot;Hyperparameter Search in Machine Learning.&quot; arXiv preprint arXiv:1502.02127 (2015).}}&lt;/ref&gt;  The objective function takes a tuple of hyperparameters and returns the associated loss.&lt;ref name=abs1502.02127/&gt; [[Cross-validation (statistics)|Cross-validation]] is often used to estimate this generalization performance.&lt;ref name=&quot;bergstra&quot; /&gt;

== Algorithms ==

=== Grid search ===
The traditional way of performing hyperparameter optimization has been ''grid search'', or a ''parameter sweep'', which is simply an [[Brute-force search|exhaustive searching]] through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by [[Cross-validation (statistics)|cross-validation]] on the training set&lt;ref&gt;Chin-Wei Hsu, Chih-Chung Chang and Chih-Jen Lin (2010). [http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf A practical guide to support vector classification]. Technical Report, [[National Taiwan University]].&lt;/ref&gt;
or evaluation on a held-out validation set.

Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.

For example, a typical soft-margin [[support vector machine|SVM]] [[statistical classification|classifier]] equipped with an [[radial basis function kernel|RBF kernel]] has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant ''C'' and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of &quot;reasonable&quot; values for each, say

:&lt;math&gt;C \in \{10, 100, 1000\}&lt;/math&gt;
:&lt;math&gt;\gamma \in \{0.1, 0.2, 0.5, 1.0\}&lt;/math&gt;

Grid search then trains an SVM with each pair (''C'', γ) in the [[Cartesian product]] of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.

Grid search suffers from the [[curse of dimensionality]], but is often [[embarrassingly parallel]] because typically the hyperparameter settings it evaluates are independent of each other.&lt;ref name=&quot;bergstra&quot;/&gt;

=== Bayesian optimization ===
{{main article|Bayesian optimization}}

Bayesian optimization is a methodology for the global optimization of noisy black-box functions.  Applied to hyperparameter optimization, Bayesian optimization consists of developing a statistical model of the function from hyperparameter values to the objective evaluated on a validation set.  Intuitively, the methodology assumes that there is some smooth but noisy function that acts as a mapping from hyperparameters to the objective.  In Bayesian optimization, one aims to gather observations in such a manner as to evaluate the machine learning model the least number of times while revealing as much information as possible about this function and, in particular, the location of the optimum.  Bayesian optimization relies on assuming a very general prior over functions which when combined with observed hyperparameter values and corresponding outputs yields a distribution over functions.  The methodology proceeds by iteratively picking hyperparameters to observe (experiments to run) in a manner that trades off exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters which are expected to have a good outcome).  In practice, Bayesian optimization has been shown&lt;ref name=&quot;hutter&quot;&gt;{{Citation
 | last = Hutter
 | first = Frank
 | last2 = Hoos
 | first2 = Holger
 | last3 = Leyton-Brown
 | first3 = Kevin
 | title = Sequential model-based optimization for general algorithm configuration
 | journal = Learning and Intelligent Optimization
 | year = 2011
 | url = http://www.cs.ubc.ca/labs/beta/Projects/SMAC/papers/11-LION5-SMAC.pdf }}&lt;/ref&gt;&lt;ref name=&quot;bergstra11&quot;&gt;{{Citation
 | last = Bergstra
 | first = James
 | last2 = Bardenet
 | first2 = Remi
 | last3 = Bengio
 | first3 = Yoshua
 | last4 = Kegl
 | first4 = Balazs
 | title = Algorithms for hyper-parameter optimization
 | journal = Advances in Neural Information Processing Systems
 | year = 2011
 | url = http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf }}&lt;/ref&gt;&lt;ref name=&quot;snoek&quot;&gt;{{Citation
 | last = Snoek
 | first = Jasper
 | last2 = Larochelle
 | first2 = Hugo
 | last3 = Adams
 | first3 = Ryan
 | title = Practical Bayesian Optimization of Machine Learning Algorithms
 | journal = Advances in Neural Information Processing Systems
 | year = 2012
 | url = http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf }}&lt;/ref&gt;&lt;ref name=&quot;thornton&quot;&gt;{{Citation
 | last = Thornton
 | first = Chris
 | last2 = Hutter
 | first2 = Frank
 | last3 = Hoos
 | first3 = Holger
 | last4 = Leyton-Brown
 | first4 = Kevin
 | title = Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms
 | journal = Knowledge discovery and data mining
 | year = 2013
 | url = http://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf}}&lt;/ref&gt; to obtain better results in fewer experiments than grid search and random search, due to the ability to reason about the quality of experiments before they are run.

=== Random search ===
{{main article|Random search}}

Since grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search. This is because oftentimes, it turns out some hyperparameters do not significantly affect the loss. Therefore, having randomly dispersed data gives more &quot;textured&quot; data than an exhaustive search over parameters that ultimately do not affect the loss.&lt;ref name=&quot;bergstra&quot;&gt;{{cite journal
| title = Random Search for Hyper-Parameter Optimization
| first1 = James
| last1 = Bergstra
| first2 = Yoshua
| last2 = Bengio
| journal = J. Machine Learning Research
| volume = 13
| year = 2012
| pages = 281–305
| url = http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf
}}&lt;/ref&gt;

=== Gradient-based optimization ===
For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks.&lt;ref&gt;{{cite journal |last1=Larsen|first1=Jan|last2= Hansen |first2=Lars Kai|last3=Svarer|first3=Claus|last4=Ohlsson|first4=M|title=Design and regularization of neural networks: the optimal use of a validation set|journal=Proceedings of the 1996 IEEE Signal Processing Society Workshop|date=1996}}&lt;/ref&gt; Since then, these methods have been extended to other models such as [[support vector machine]]s&lt;ref&gt;{{cite journal |author1=Olivier Chapelle |author2=Vladimir Vapnik |author3=Olivier Bousquet |author4=Sayan Mukherjee |title=Choosing multiple parameters for support vector machines |journal=Machine Learning |year=2002 |volume=46 |pages=131–159 |url=http://www.chapelle.cc/olivier/pub/mlj02.pdf | doi = 10.1023/a:1012450327387 }}&lt;/ref&gt; or logistic regression.&lt;ref&gt;{{cite journal |author1 =Chuong B|author2= Chuan-Sheng Foo|author3=Andrew Y Ng|journal = Advances in Neural Information Processing Systems 20|title = Efficient multiple hyperparameter learning for log-linear models|year =2008}}&lt;/ref&gt;

A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using  [[automatic differentiation]].&lt;ref&gt;{{cite journal|last1=Domke|first1=Justin|title=Generic Methods for Optimization-Based Modeling|journal=AISTATS|date=2012|volume=22|url=http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf}}&lt;/ref&gt;&lt;ref name=abs1502.03492&gt;{{cite arXiv |last1=Maclaurin|first1=Douglas|last2=Duvenaud|first2=David|last3=Adams|first3=Ryan P.|eprint=1502.03492|title=Gradient-based Hyperparameter Optimization through Reversible Learning|class=stat.ML|date=2015}}&lt;/ref&gt;

=== Others ===
[[Evolutionary algorithm|Evolutionary]]&lt;ref name=abs1711.09846&gt;[https://arxiv.org/abs/1711.09846 Population Based Training of Neural Networks (2017)]&lt;/ref&gt;, [[Radial basis function|RBF]]&lt;ref name=abs1705.08520&gt;[https://arxiv.org/abs/1705.08520 An effective algorithm for hyperparameter optimization of neural networks (2017)]&lt;/ref&gt; and [[spectral method|spectral]]&lt;ref name=abs1706.00764&gt;[https://arxiv.org/abs/1706.00764 Hyperparameter Optimization: A Spectral Approach (2017)]&lt;/ref&gt; approaches have been used.

== Software ==

===Grid search===
* [[LIBSVM]] comes with scripts for performing grid search.
* [[scikit-learn]] includes [http://scikit-learn.sourceforge.net/modules/grid_search.html grid] search.

===Bayesian===
* [https://github.com/HIPS/Spearmint spearmint] Spearmint is a package to perform Bayesian optimization of machine learning algorithms.
* [https://rmcantin.bitbucket.io/html/ Bayesopt],&lt;ref name=&quot;martinezcantin&quot;&gt;{{cite journal
| title = BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits
| first1 = Ruben
| last1 = Martinez-Cantin
| journal = J. Machine Learning Research
| volume = 15
| year = 2014
| pages = 3915−3919
| url = http://jmlr.org/papers/volume15/martinezcantin14a/martinezcantin14a.pdf
}}&lt;/ref&gt; an efficient implementation of Bayesian optimization in C/C++ with support for Python, Matlab and Octave.
* [https://github.com/yelp/MOE MOE] MOE is a Python/C++/CUDA library implementing Bayesian Global Optimization using Gaussian Processes.
* [http://www.cs.ubc.ca/labs/beta/Projects/autoweka/ Auto-WEKA] is a Bayesian hyperparameter optimization layer on top of [[Weka (machine learning)|WEKA]].

===Random search===
* [[scikit-learn]] includes [http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html random] search.

===Gradient based===
* [https://github.com/HIPS/hypergrad hypergrad] is a Python package for differentiation with respect to hyperparameters.&lt;ref name=abs1502.03492/&gt;

===Other===
* [https://github.com/hyperopt/hyperopt hyperopt] and [https://github.com/hyperopt/hyperopt-sklearn hyperopt-sklearn] are Python packages for [[kernel density estimation|Tree of Parzen Estimators]] based distributed hyperparameter optimization.
* [https://github.com/CMA-ES/pycma pycma] is a Python implementation of [[CMA-ES|Covariance Matrix Adaptation Evolution Strategy]].
* [http://sumo.intec.ugent.be SUMO-Toolbox]&lt;ref name=&quot;gorissen&quot;&gt;{{cite journal
| title = A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design
| first1 = Dirk
| last1 = Gorissen
| first2 = Karel
| last2 = Crombecq
| first3 = Ivo
| last3 = Couckuyt
| first4 = Piet
| last4 = Demeester
| first5 = Tom
| last5 = Dhaene
| journal = J. Machine Learning Research
| volume = 11
| year = 2010
| pages = 2051–2055
| url = http://www.jmlr.org/papers/volume11/gorissen10a/gorissen10a.pdf
}}&lt;/ref&gt; is a [[MATLAB]] toolbox for [[surrogate model]]ing supporting a wide collection of hyperparameter optimization algorithm for many model types.
* [https://github.com/coin-or/rbfopt rbfopt] is a Python package that uses a [[radial basis function]] model&lt;ref name=abs1705.08520/&gt;
* [https://github.com/callowbird/Harmonica Harmonica] is a Python package for spectral hyperparameter optimization.&lt;ref name=abs1706.00764/&gt;

===Multiple===
* [https://github.com/mlr-org/mlr mlr] is a [[R]] package that contains a large number of different hyperparameter optimization techniques for machine learning problems.
* [https://github.com/rhiever/tpot TPOT] is a Python library that automatically creates and optimizes full machine learning pipelines using genetic programming.

== See also ==
* [[Automated machine learning]] (AutoML)
* [[Bias-variance dilemma]]
* [[Dimensionality reduction]]
* [[Feature selection]]
* [[Meta-optimization]]
* [[Model selection]]
* [[Self-tuning]]

== References ==
{{Reflist|30em}}



</text>
      <sha1>a4i5x77ps1awlz4bje9zlggzhwy80fx</sha1>
    </revision>
  </page>
  <page>
    <title>Time series</title>
    <ns>0</ns>
    <id>406624</id>
    <revision>
      <id>815594176</id>
      <parentid>815594115</parentid>
      <timestamp>2017-12-15T20:23:14Z</timestamp>
      <contributor>
        <username>Edeveien</username>
        <id>15046653</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="36848">[[File:Random-data-plus-trend-r2.png|thumb|250px|Time series: random data plus trend, with best-fit line and different applied filters]]
A '''time series''' is a series of [[data point]]s indexed (or listed or graphed) in time order.  Most commonly, a time series is a [[sequence]] taken at successive equally spaced points in time. Thus it is a sequence of [[discrete-time]] data. Examples of time series are heights of ocean [[tides]], counts of [[sunspots]], and the daily closing value of the [[Dow Jones Industrial Average]].

Time series are very frequently plotted via [[line chart]]s. Time series are used in [[statistics]], [[signal processing]], [[pattern recognition]], [[econometrics]], [[mathematical finance]], [[weather forecasting]], [[earthquake prediction]], [[electroencephalography]], [[control engineering]], [[astronomy]], [[communications engineering]], and largely in any domain of applied [[Applied science|science]] and [[engineering]] which involves [[Time|temporal]] measurements.

'''Time series ''analysis''''' comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. '''Time series ''forecasting''''' is the use of a [[model (abstract)|model]] to predict future values based on previously observed values. While [[regression analysis]] is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called &quot;time series analysis&quot;, which focuses on comparing values of a single time series or multiple dependent time series at different points in time.&lt;ref&gt;{{cite web|last=Imdadullah|title=Time Series Analysis|url=http://itfeature.com/time-series-analysis-and-forecasting/time-series-analysis-forecasting|work=Basic Statistics and Data Analysis|publisher=itfeature.com|accessdate=2 January 2014}}&lt;/ref&gt; [[Interrupted time series]] analysis is the analysis of interventions on a single time series

Time series data have a natural temporal ordering.  This makes time series analysis distinct from [[cross-sectional study|cross-sectional studies]], in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from [[spatial data analysis]] where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A [[stochastic]] model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see [[time reversibility]].)

Time series analysis can be applied to [[real number|real-valued]], continuous data, [[:wikt:discrete|discrete]] [[Data type#Numeric types|numeric]] data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the [[English language]]&lt;ref&gt;{{cite book |last=Lin |first=Jessica |last2=Keogh |first2=Eamonn |last3=Lonardi |first3=Stefano |last4=Chiu |first4=Bill |chapter=A symbolic representation of time series, with implications for streaming algorithms |title=Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery |year=2003 |location=New York |publisher=ACM Press |doi=10.1145/882082.882086 }}&lt;/ref&gt;).

==Methods for analysis==

Methods for time series analysis may be divided into two classes: [[frequency-domain]] methods and [[time-domain]] methods. The former include [[frequency spectrum#Spectrum analysis|spectral analysis]] and [[wavelet analysis]]; the latter include [[auto-correlation]] and [[cross-correlation]] analysis. In the time domain, correlation and analysis can be made in a filter-like manner using [[scaled correlation]], thereby mitigating the need to operate in the frequency domain.

Additionally, time series analysis techniques may be divided into [[Parametric estimation|parametric]] and [[Non-parametric statistics|non-parametric]] methods. The [[Parametric estimation|parametric approaches]] assume that the underlying [[stationary process|stationary stochastic process]] has a certain structure which can be described using a small number of parameters (for example, using an [[autoregressive]] or [[moving average]] model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, [[Non-parametric statistics|non-parametric approaches]] explicitly estimate the [[covariance]] or the [[spectrum]] of the process without assuming that the process has any particular structure.

Methods of time series analysis may also be divided into [[Linear regression|linear]] and [[Nonlinear regression|non-linear]], and [[Univariate analysis|univariate]] and [[Multivariate analysis|multivariate]].

==Panel data==

A time series is one type of [[panel data]]. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a [[cross-sectional data]]set).  A data set may exhibit characteristics of both panel data and time series data.  One way to tell is to ask what makes one data record unique from the other records.  If the answer is the time data field, then this is a time series data set candidate.  If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate.  If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.

==Analysis==

There are several types of motivation and data analysis available for time series which are appropriate for different purposes and etc.

===Motivation===

In the context of [[statistics]], [[econometrics]], [[quantitative finance]], [[seismology]], [[meteorology]], and [[geophysics]] the primary goal of time series analysis is [[forecasting]]. In the context of [[signal processing]], [[control engineering]] and [[communication engineering]] it is used for signal detection and [[estimation]], while in the context of [[data mining]], [[pattern recognition]] and [[machine learning]]  time series analysis can be used for [[cluster analysis|clustering]], [[Statistical classification|classification]], query by content, [[anomaly detection]] as well as [[forecasting]]{{citation needed|date=October 2017}}.

===Exploratory analysis===
[[File:Tuberculosis incidence US 1953-2009.png|thumb|Tuberculosis incidence US 1953-2009]]
{{further|Exploratory analysis}}
The clearest way to examine a regular time series manually is with a [[line chart]] such as the one shown for tuberculosis in the United States, made with a spreadsheet program. The number of cases was standardized to a rate per 100,000 and the percent change per year in this rate was calculated. The nearly steadily dropping line shows that the TB incidence was decreasing in most years, but the percent change in this rate varied by as much as +/- 10%, with 'surges' in 1975 and around the early 1990s. The use of both vertical axes allows the comparison of two time series in one graphic.

Other techniques include:

* [[Autocorrelation]] analysis to examine [[serial dependence]]
* [[frequency spectrum#Spectrum analysis|Spectral analysis]] to examine cyclic behavior which need not be related to [[seasonality]]. For example, sun spot activity varies over 11 year cycles.&lt;ref&gt;{{cite book |last=Bloomfield |first=P. |year=1976 |title=Fourier analysis of time series: An introduction |location=New York |publisher=Wiley |isbn=0471082562 }}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Shumway |first=R. H. |year=1988 |title=Applied statistical time series analysis |location=Englewood Cliffs, NJ |publisher=Prentice Hall |isbn=0130415006 }}&lt;/ref&gt; Other common examples include celestial phenomena, weather patterns, neural activity, commodity prices, and economic activity.
* Separation into components representing trend, seasonality, slow and fast variation, and cyclical irregularity: see [[trend estimation]] and [[decomposition of time series]]

===Curve fitting===
{{main article|Curve fitting}}

Curve fitting&lt;ref&gt;Sandra Lach Arlinghaus, PHB Practical Handbook of Curve Fitting. CRC Press, 1994.&lt;/ref&gt;&lt;ref&gt;William M. Kolb. Curve Fitting for Programmable Calculators. Syntec, Incorporated, 1984.&lt;/ref&gt; is the process of constructing a [[curve]], or [[function (mathematics)|mathematical function]], that has the best fit to a series of [[data]] points,&lt;ref&gt;S.S. Halli, K.V. Rao. 1992. Advanced Techniques of Population Analysis. {{isbn|0306439972}} Page 165 (''cf''. ... functions are fulfilled if we have a good to moderate fit for the observed data.)&lt;/ref&gt; possibly subject to constraints.&lt;ref&gt;[https://books.google.com/books?id=SI-VqAT4_hYC ''[[The Signal and the Noise]]]: Why So Many Predictions Fail-but Some Don't.'' By Nate Silver&lt;/ref&gt;&lt;ref&gt;[https://books.google.com/books?id=hhdVr9F-JfAC Data Preparation for Data Mining]: Text. By Dorian Pyle.&lt;/ref&gt; Curve fitting can involve either [[interpolation]],&lt;ref&gt;Numerical Methods in Engineering with MATLAB®. By [[Jaan Kiusalaas]]. Page 24.&lt;/ref&gt;&lt;ref&gt;Numerical Methods in Engineering with Python 3. By Jaan Kiusalaas. Page 21.&lt;/ref&gt; where an exact fit to the data is required, or [[smoothing]],&lt;ref&gt;Numerical Methods of Curve Fitting. By P. G. Guest, Philip George Guest. Page 349.&lt;/ref&gt;&lt;ref&gt;See also: [[Mollifier]]&lt;/ref&gt; in which a &quot;smooth&quot; function is constructed that approximately fits the data.  A related topic is [[regression analysis]],&lt;ref&gt;Fitting Models to Biological Data Using Linear and Nonlinear Regression. By Harvey Motulsky, Arthur Christopoulos.&lt;/ref&gt;&lt;ref&gt;Regression Analysis By Rudolf J. Freund, William J. Wilson, Ping Sa. Page 269.&lt;/ref&gt; which focuses more on questions of [[statistical inference]] such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization,&lt;ref&gt;Visual Informatics. Edited by Halimah Badioze Zaman, Peter Robinson, Maria Petrou, Patrick Olivier, Heiko Schröder. Page 689.&lt;/ref&gt;&lt;ref&gt;Numerical Methods for Nonlinear Engineering Models. By John R. Hauser. Page 227.&lt;/ref&gt; to infer values of a function where no data are available,&lt;ref&gt;Methods of Experimental Physics: Spectroscopy, Volume 13, Part 1. By Claire Marton. Page 150.&lt;/ref&gt; and to summarize the relationships among two or more variables.&lt;ref&gt;Encyclopedia of Research Design, Volume 1. Edited by Neil J. Salkind. Page 266.&lt;/ref&gt; [[Extrapolation]] refers to the use of a fitted curve beyond the [[range (mathematics)|range]] of the observed data,&lt;ref&gt;Community Analysis and Planning Techniques. By Richard E. Klosterman. Page 1.&lt;/ref&gt; and is subject to a [[Uncertainty|degree of uncertainty]]&lt;ref&gt;An Introduction to Risk and Uncertainty in the Evaluation of Environmental Investments. DIANE Publishing. [https://books.google.com/books?id=rJ23LWaZAqsC&amp;pg=PA69 Pg 69]&lt;/ref&gt; since it may reflect the method used to construct the curve as much as it reflects the observed data.

The construction of economic time series involves the estimation of some components for some dates by [[interpolation]] between values (&quot;benchmarks&quot;) for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information (&quot;reading between the lines&quot;).&lt;ref&gt;Hamming, Richard. Numerical methods for scientists and engineers. Courier Corporation, 2012.&lt;/ref&gt; Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates.&lt;ref&gt;Friedman, Milton. &quot;The interpolation of time series by related series.&quot; Journal of the American Statistical Association 57.300 (1962): 729-757.&lt;/ref&gt; Alternatively [[polynomial interpolation]] or [[spline interpolation]] is used where piecewise [[polynomial]] functions are fit into time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called [[Polynomial regression|regression]]).The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set.  Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.

[[Extrapolation]] is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to [[interpolation]], which produces estimates between known observations, but extrapolation is subject to greater [[uncertainty]] and a higher risk of producing meaningless results.

===Function approximation===
{{main article|Function approximation}}
In general, a function approximation problem asks us to select a [[function (mathematics)|function]] among a well-defined class that closely matches (&quot;approximates&quot;) a target function in a task-specific way.
One can distinguish two major classes of function approximation problems: First, for known target functions [[approximation theory]]  is the branch of [[numerical analysis]] that investigates how certain known functions (for example, [[special function]]s) can be approximated by a specific class of functions (for example, [[polynomial]]s or [[rational function]]s) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).

Second, the target function, call it ''g'', may be unknown; instead of an explicit formula, only a set of points (a time series) of the form (''x'', ''g''(''x'')) is provided.  Depending on the structure of the [[domain of a function|domain]] and [[codomain]] of ''g'', several techniques for approximating ''g'' may be applicable.  For example, if ''g'' is an operation on the [[real number]]s, techniques of [[interpolation]], [[extrapolation]], [[regression analysis]], and [[curve fitting]] can be used.  If the [[codomain]] (range or target set) of ''g'' is a finite set, one is dealing with a [[statistical classification|classification]] problem instead. A related problem of ''online'' time series approximation&lt;ref&gt;Gandhi, Sorabh, Luca Foschini, and Subhash Suri. &quot;Space-efficient online approximation of time series data: Streams, amnesia, and out-of-order.&quot; Data Engineering (ICDE), 2010 IEEE 26th International Conference on. IEEE, 2010.&lt;/ref&gt; is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.

To some extent the different problems ([[regression analysis|regression]], [[Statistical classification|classification]], [[fitness approximation]]) have received a unified treatment in [[statistical learning theory]], where they are viewed as [[supervised learning]] problems.

===Prediction and forecasting===
In [[statistics]], [[prediction]] is a part of [[statistical inference]]. One particular approach to such inference is known as [[predictive inference]], but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as [[forecasting]].
* Fully formed statistical models for [[stochastic simulation]] purposes, so as to generate alternative versions of the time series, representing what might happen over non-specific time-periods in the future
* Simple or fully formed statistical models to describe the likely outcome of the time series in the immediate future, given knowledge of the most recent outcomes (forecasting).
* Forecasting on time series is usually done using automated statistical software packages and programming languages, such as [[R (programming language)|R]], [[S (programming language)|S]], [[SAS (software)|SAS]], [[SPSS]], [[Minitab]], [[Pandas (software)|pandas (Python)]] and many others.

===Classification===
{{main article|Statistical classification}}
Assigning time series pattern to a specific category, for example identify a word based on series of hand movements in [[sign language]]

===Signal estimation===
{{see also|Signal processing|Estimation theory}}
This approach is based on [[harmonic analysis]] and filtering of signals in the [[frequency domain]] using the [[Fourier transform]], and [[spectral density estimation]], the development of which was significantly accelerated during [[World War II]] by mathematician [[Norbert Wiener]], electrical engineers [[Rudolf E. Kálmán]], [[Dennis Gabor]] and others for filtering signals from noise and predicting signal values at a certain point in time. See [[Kalman filter]], [[Estimation theory]], and [[Digital signal processing]]

===Segmentation===
{{main article|Time-series segmentation}}
Splitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using [[Change detection|change-point detection]], or by modeling the time-series as a more sophisticated system, such as a [[Markov jump linear system]].

==Models==

Models for time series data can have many forms and represent different [[stochastic processes]]. When modeling variations in the level of a process, three broad classes of practical importance are the ''[[autoregressive]]'' (AR) models, the ''integrated'' (I) models, and the ''[[moving average model|moving average]]'' (MA) models. These three classes depend linearly on previous data points.&lt;ref name=&quot;linear time series&quot;&gt;{{cite book |authorlink=Neil Gershenfeld |last=Gershenfeld |first=N. |year=1999 |title=The Nature of Mathematical Modeling |location=New York |publisher=Cambridge University Press |pages=205–208 |isbn=0521570956 }}&lt;/ref&gt; Combinations of these ideas produce [[autoregressive moving average]] (ARMA) and [[autoregressive integrated moving average]] (ARIMA) models. The [[autoregressive fractionally integrated moving average]] (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial &quot;V&quot; for &quot;vector&quot;, as in VAR for [[vector autoregression]]. An additional set of extensions of these models is available for use where the observed time-series is driven by some &quot;forcing&quot; time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final &quot;X&quot; for &quot;exogenous&quot;.

Non-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a [[chaos theory|chaotic]] time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in [[nonlinear autoregressive exogenous model]]s. Further references on nonlinear time series analysis: (Kantz and Schreiber),&lt;ref&gt;{{cite book|last1=Kantz|first1=Holger|last2=Thomas|first2=Schreiber|title=Nonlinear Time Series Analysis|date=2004|publisher=Cambridge University Press|location=London|isbn=978-0521529020}}&lt;/ref&gt; and (Abarbanel)&lt;ref&gt;{{cite book|last1=Abarbanel|first1=Henry|title=Analysis of Observed Chaotic Data|date=Nov 25, 1997|publisher=Springer|location=New York|isbn=978-0387983721}}&lt;/ref&gt;

Among other types of non-linear time series models, there are models to represent the changes of variance over time ([[heteroskedasticity]]). These models represent [[autoregressive conditional heteroskedasticity]] (ARCH) and the collection comprises a wide variety of representation ([[GARCH]], TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a [[doubly stochastic model]].

In recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also [[Markov switching multifractal]] (MSMF) techniques for modeling volatility evolution.

A [[Hidden Markov model]] (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest [[dynamic Bayesian network]]. HMM models are widely used in [[speech recognition]], for translating a time series of spoken words into text.

===Notation===
A number of different notations are in use for time-series analysis. A common notation specifying a time series ''X'' that is indexed by the [[natural number]]s is written
:''X'' = {''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, ...}.

Another common notation is
:''Y'' = {''Y&lt;sub&gt;t&lt;/sub&gt;'': ''t'' ∈ ''T''},
where ''T'' is the [[index set]].

===Conditions===
There are two sets of conditions under which much of the theory is built:
* [[Stationary process]]
* [[Ergodic process]]

However, ideas of stationarity must be expanded to consider two important ideas: [[strict stationarity]] and [[Stationary process#Weaker forms of stationarity|second-order stationarity]]. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.

In addition, time-series analysis can be applied where the series are [[Cyclostationary process|seasonally stationary]] or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in [[time-frequency analysis]] which makes use of a [[time–frequency representation]] of a time-series or signal.&lt;ref&gt;Boashash, B. (ed.), (2003) ''Time-Frequency Signal Analysis and Processing: A Comprehensive Reference'', Elsevier Science, Oxford, 2003 {{isbn|0-08-044335-4}}&lt;/ref&gt;

===Tools===
{{Prose|section|date=February 2012}}
Tools for investigating time-series data include:

* Consideration of the [[autocorrelation|autocorrelation function]] and the [[Spectral density|spectral density function]] (also [[cross-correlation function]]s and cross-spectral density functions)
* [[Scaled correlation|Scaled]] cross- and auto-correlation functions to remove contributions of slow components&lt;ref name=&quot;Nikolicetal&quot;&gt;{{cite journal |last=Nikolić |first=D. |last2=Muresan |first2=R. C. |last3=Feng |first3=W. |last4=Singer |first4=W. |year=2012 |title=Scaled correlation analysis: a better way to compute a cross-correlogram |journal=European Journal of Neuroscience |volume=35 |issue=5 |pages=742–762 |doi=10.1111/j.1460-9568.2011.07987.x }}&lt;/ref&gt;
* Performing a [[Fourier transform]] to investigate the series in the [[frequency domain]]
* Use of a [[digital filter|filter]] to remove unwanted [[noise (physics)|noise]]
* [[Principal component analysis]] (or [[empirical orthogonal function]] analysis)
* [[Singular spectrum analysis]]
* &quot;Structural&quot; models:
** General [[State Space Model]]s
** Unobserved Components Models
* [[Machine Learning]]
** [[Artificial neural network]]s
** [[Support Vector Machine]]
** [[Fuzzy Logic]]
** [[Gaussian Processes]]
** [[Hidden Markov model]]
* [[Queueing Theory]] Analysis
* [[Control chart]]
** [[Shewhart individuals control chart]]
** [[CUSUM]] chart
** [[EWMA chart]]
* [[Detrended fluctuation analysis]]
* [[Dynamic time warping]]&lt;ref name=&quot;Sakoe 1978&quot;&gt;{{cite book |last=Sakoe |first=Hiroaki |last2=Chiba |first2=Seibi |year=1978 |chapter=Dynamic programming algorithm optimization for spoken word recognition | title=IEEE Transactions on Acoustics, Speech and Signal Processing |doi=10.1109/TASSP.1978.1163055 }}&lt;/ref&gt;
* [[Cross-correlation]]&lt;ref&gt;{{cite book |last=Goutte |first=Cyril |last2=Toft |first2=Peter |last3=Rostrup |first3=Egill |last4=Nielsen |first4=Finn Å. |last5=Hansen |first5=Lars Kai |year=1999 |chapter=On Clustering fMRI Time Series | title=NeuroImage |doi=10.1006/nimg.1998.0391 }}&lt;/ref&gt;
* [[Dynamic Bayesian network]]
* [[Time-frequency representation|Time-frequency analysis techniques:]]
** [[Fast Fourier Transform]]
** [[Continuous wavelet transform]]
** [[Short-time Fourier transform]]
** [[Chirplet transform]]
** [[Fractional Fourier transform]]
* [[Chaos theory|Chaotic analysis]]
** [[Correlation dimension]]
** [[Recurrence plot]]s
** [[Recurrence quantification analysis]]
** [[Lyapunov exponent]]s
** [[Entropy encoding]]

===Measures===
Time series metrics or [[Features (pattern recognition)|features]] that can be used for time series [[Classification (machine learning)|classification]] or [[Regression analysis|regression]] analysis:&lt;ref&gt;{{cite journal |last=Mormann |first=Florian |last2=Andrzejak |first2=Ralph G. |last3=Elger |first3=Christian E. |last4=Lehnertz |first4=Klaus |title=Seizure prediction: the long and winding road |journal=[[Brain (journal)|Brain]] |year=2007 |volume=130 |issue=2 |pages=314–333 |doi=10.1093/brain/awl241 |pmid=17008335}}&lt;/ref&gt;

* '''Univariate linear measures'''
** [[Moment (mathematics)]]
** [[Spectral band power]]
** [[Spectral edge frequency]]
** Accumulated [[Energy (signal processing)]]
** Characteristics of the [[autocorrelation]] function
** [[Hjorth parameters]]
** [[Fast Fourier transform|FFT]] parameters
** [[Autoregressive model]] parameters
** [[Mann–Kendall test]]
* '''Univariate non-linear measures'''
** Measures based on the [[correlation]] sum
** [[Correlation dimension]]
** [[Correlation integral]]
** [[Correlation density]]
** [[Correlation entropy]]
** [[Approximate entropy]]&lt;ref&gt;{{cite web |last=Land |first=Bruce |last2=Elias |first2=Damian |title=Measuring the ‘Complexity’ of a time series |url=http://www.nbb.cornell.edu/neurobio/land/PROJECTS/Complexity/ }}&lt;/ref&gt;
** [[Sample entropy]]
** {{iw2|Fourier entropy||uk|Ентропія Фур'є}}
** Wavelet entropy
** [[Rényi entropy]]
** Higher-order methods
** [[Marginal predictability]]
** [[Dynamical similarity]] index
** [[State space]] dissimilarity measures
** [[Lyapunov exponent]]
** Permutation methods
** [[Local flow]]
* '''Other univariate measures'''
** [[Algorithmic information theory|Algorithmic complexity]]
** [[Kolmogorov complexity]] estimates
** [[Hidden Markov Model]] states
** [[Rough path#Signature|Rough path signature]]&lt;ref&gt;[1] Chevyrev, I., Kormilitzin, A. (2016) &quot;A Primer on the Signature Method in Machine Learning, arXiv:1603.03788v1&quot;&lt;/ref&gt;
** Surrogate time series and surrogate correction
** Loss of recurrence (degree of non-stationarity)
* '''Bivariate linear measures'''
** Maximum linear [[cross-correlation]]
** Linear [[Coherence (signal processing)]]
* '''Bivariate non-linear measures'''
** Non-linear interdependence
** Dynamical Entrainment (physics)
** Measures for [[Phase synchronization]]
** Measures for [[Phase locking]]
* '''Similarity measures''':&lt;ref&gt;{{cite journal |last=Ropella |first=G. E. P. |last2=Nag |first2=D. A. |last3=Hunt |first3=C. A. |title=Similarity measures for automated comparison of in silico and in vitro experimental results |journal=Engineering in Medicine and Biology Society |year=2003 |volume=3 |issue= |pages=2933–2936 |doi=10.1109/IEMBS.2003.1280532 }}&lt;/ref&gt;
** [[Cross-correlation]]
** [[Dynamic Time Warping]]&lt;ref name=&quot;Sakoe 1978&quot;/&gt;
** [[Hidden Markov Models]]
** [[Edit distance]]
** [[Total correlation]]
** [[Newey–West estimator]]
** [[Prais–Winsten estimation|Prais–Winsten transformation]]
** Data as Vectors in a Metrizable Space
*** [[Minkowski distance]]
*** [[Mahalanobis distance]]
** Data as Time Series with Envelopes
*** Global [[Standard Deviation]]
*** Local [[Standard Deviation]]
*** Windowed [[Standard Deviation]]
** Data Interpreted as Stochastic Series
*** [[Pearson product-moment correlation coefficient]]
*** [[Spearman's rank correlation coefficient]]
** Data Interpreted as a [[Probability Distribution]] Function
*** [[Kolmogorov–Smirnov test]]
*** [[Cramér–von Mises criterion]]

==Visualization==
Time series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)&lt;ref&gt;{{cite web|last1=Tominski|first1=Christian|last2= Aigner|first2=Wolfgang|title=The TimeViz Browser:A Visual Survey of Visualization Techniques for Time-Oriented Data|url=http://survey.timeviz.net/|accessdate=1 June 2014}}&lt;/ref&gt;

===Overlapping charts===
* Braided Graphs
* Line Charts
* Slope Graphs
* [[GapChart]]

===Separated charts===
* [[Horizon Graphs]]
* Reduced Line Charts (small multiples)
* Silhouette Graph
* Circular Silhouette Graph

== Software ==
Working with Time Series data is a relatively common use for statistical analysis software. As a result of this, there are many offerings both commercial and open source. Some examples include:
* CRAN supplementary statistics package for [[R (programming language)|R]]&lt;ref&gt;{{Cite journal|last=Hyndman|first=Rob J|date=2016-01-22|title=CRAN Task View: Time Series Analysis|url=https://cran.r-project.org/web/views/TimeSeries.html}}&lt;/ref&gt;
* Analysis and Forecasting with [[Weka (machine learning)|Weka]]&lt;ref&gt;{{Cite web|url=http://wiki.pentaho.com/display/DATAMINING/Time+Series+Analysis+and+Forecasting+with+Weka|title=Time Series Analysis and Forecasting with Weka - Pentaho Data Mining - Pentaho Wiki|website=wiki.pentaho.com|access-date=2016-07-07}}&lt;/ref&gt;
* Predictive modeling with GMDH Shell&lt;ref&gt;{{Cite web|url=https://www.gmdhshell.com/time-series-analysis-software|title=Time Series Analysis &amp; Forecasting Software 2016 [Free Download]|language=en-US|access-date=2016-07-07}}&lt;/ref&gt;
* Functions and Modeling in the [[Wolfram Language]]&lt;ref&gt;{{Cite web|url=http://reference.wolfram.com/language/guide/TimeSeries.html|title=Time Series—Wolfram Language Documentation|website=reference.wolfram.com|access-date=2016-07-07}}&lt;/ref&gt;
* Time Series Objects in [[MATLAB]]&lt;ref&gt;{{Cite web|url=http://www.mathworks.com/help/matlab/data_analysis/time-series-objects.html|title=Time Series Objects - MATLAB &amp; Simulink|website=www.mathworks.com|access-date=2016-07-07}}&lt;/ref&gt;
* SAS/ETS in [[SAS (software)|SAS]] software&lt;ref&gt;{{Cite web|url=http://www.sas.com/en_us/software/analytics/ets.html|title=Econometrics and Time Series Analysis, SAS/ETS Software|access-date=2016-07-07}}&lt;/ref&gt;
* Expert Modeler in [[IBM SPSS Statistics]] and IBM [[SPSS Modeler]]
* Automatic Time series Forecasting with LDT&lt;ref&gt;{{Cite web|url=https://sourceforge.net/projects/letdatalk/|title=LDT|website=SourceForge|access-date=2016-09-04}}&lt;/ref&gt;
* [[EViews]] is a statistical package for Windows, used mainly for time-series oriented econometric analysis.
* bayesloop: Probabilistic programming framework that facilitates objective model selection for time-varying parameter models&lt;ref&gt;{{Cite web|url=http://bayesloop.com|title=bayesloop: Probabilistic programming framework that facilitates objective model selection for time-varying parameter models|access-date=2016-12-06}}&lt;/ref&gt;
* Slycat Web-based ensemble analysis and visualization platform, created at Sandia National Laboratoriesl&lt;ref&gt;{{Cite web|url=https://github.com/sandialabs/slycat|title=Time Slycat Web-based ensemble analysis and visualization platform|language=en-US|access-date=2017-10-03}}&lt;/ref&gt;

==See also==
{{Columns-list|2|
* [[Anomaly time series]]
* [[Chirp]]
* [[Decomposition of time series]]
* [[Detrended fluctuation analysis]]
* [[Digital signal processing]]
* [[Distributed lag]]
* [[Estimation theory]]
* [[Forecasting]]
* [[Hurst exponent]]
* [[Monte Carlo method]]
* [[Panel analysis]]
* [[Random walk]]
* [[Scaled correlation]]
* [[Seasonal adjustment]]
* [[Sequence analysis]]
* [[Signal processing]]
* [[Trend estimation]]
* [[Unevenly spaced time series]]
* [[Time series database]]
}}

==References==
{{Reflist|2}}

==Further reading==
* {{Citation
 | authorlink = George E. P. Box
 | last1 = Box | first1 = George
 | last2 = Jenkins   | first2 = Gwilym
 | title = Time Series Analysis: forecasting and control, rev. ed.
 | publisher = Holden-Day
 | location = Oakland, California
 | year = 1976
}}
* Cowpertwait P.S.P., Metcalfe A.V. (2009), ''Introductory Time Series with R'', [[Springer Science+Business Media|Springer]].
* [[James Durbin|Durbin J.]], Koopman S.J. (2001), ''Time Series Analysis by State Space Methods'', [[Oxford University Press]].
* {{Citation
 | last = Gershenfeld | first =  Neil
 | year = 2000
 | title = The Nature of Mathematical Modeling
 | ISBN = 978-0-521-57095-4
 | publisher = [[Cambridge University Press]]
 | oclc = 174825352
}}
* {{Citation
 | authorlink = James D. Hamilton
 | last = Hamilton | first =  James
 | year = 1994
 | title = Time Series Analysis
 | ISBN = 0-691-04289-6
 | publisher = [[Princeton University Press]]
}}
* [[Maurice Priestley|Priestley, M. B.]] (1981), ''Spectral Analysis and Time Series'', [[Academic Press]]. {{isbn|978-0-12-564901-8}}
* {{Citation | last = Shasha | first = D. | title = High Performance Discovery in Time Series | publisher = [[Springer Science+Business Media|Springer]] | year = 2004 | ISBN = 0-387-00857-8 }}
* Shumway R. H., Stoffer (2011), ''Time Series Analysis and its Applications'', Springer.
* Weigend A. S., Gershenfeld N. A. (Eds.) (1994), ''Time Series Prediction: Forecasting the Future and Understanding the Past''. Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis (Santa Fe, May 1992), [[Addison-Wesley]].
* [[Norbert Wiener|Wiener, N.]] (1949), ''Extrapolation, Interpolation, and Smoothing of Stationary Time Series'', [[MIT Press]].
* Woodward, W. A., Gray, H. L. &amp; Elliott, A. C. (2012), ''Applied Time Series Analysis'', [[CRC Press]].

==External links==
* [https://www.encyclopediaofmath.org/index.php/Time_series Time series] at Encyclopaedia of Mathematics.
* [http://statistik.mathematik.uni-wuerzburg.de/timeseries/ A First Course on Time Series Analysis] — An open source book on time series analysis with [[SAS (software)|SAS]].
* [http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm Introduction to Time series Analysis (Engineering Statistics Handbook)] — A practical guide to Time series analysis.
* [http://www.jstatsoft.org/v33/i05/paper MATLAB Toolkit for Computation of Multiple Measures on Time Series Data Bases].
* [https://www.nbtwiki.net/doku.php?id=tutorial:power_spectra_wavelet_analysis_and_coherence A Matlab tutorial on power spectra, wavelet analysis, and coherence] on website with many other tutorials.
* [http://survey.timeviz.net/: TimeViz survey]
* [http://www.gaussianprocess.org/gpml/ Gaussian Processes for Machine Learning: Book webpage]
* [https://cran.r-project.org/web/views/TimeSeries.html CRAN Time Series Task View - Time Series in R]
* [http://earthpy.org/pandas-basics.html TimeSeries Analysis with Pandas]
{{Statistics}}
{{Portal bar|Statistics}}

{{Authority control}}

{{DEFAULTSORT:Time Series}}
[[Category:Time series| ]]

[[Category:Mathematical and quantitative methods (economics)]]
</text>
      <sha1>7352qi1jape0b1pjfju92qmgny8f0gz</sha1>
    </revision>
  </page>
</mediawiki>
